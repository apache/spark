/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *    http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

syntax = 'proto3';

package spark.connect;

import "spark/connect/commands.proto";
import "spark/connect/relations.proto";

option java_multiple_files = true;
option java_package = "org.apache.spark.connect.proto";

// A [[Plan]] is the structure that carries the runtime information for the execution from the
// client to the server. A [[Plan]] can either be of the type [[Relation]] which is a reference
// to the underlying logical plan or it can be of the [[Command]] type that is used to execute
// commands on the server.
message Plan {
  oneof op_type {
    Relation root = 1;
    Command command = 2;
  }
}

// A request to be executed by the service.
message Request {
  // The client_id is set by the client to be able to collate streaming responses from
  // different queries.
  string client_id = 1;
  // User context
  UserContext user_context = 2;
  // The logical plan to be executed / analyzed.
  Plan plan = 3;

  // User Context is used to refer to one particular user session that is executing
  // queries in the backend.
  message UserContext {
    string user_id = 1;
    string user_name = 2;
  }
}

// The response of a query, can be one or more for each request. Responses belonging to the
// same input query, carry the same `client_id`.
message Response {
  string client_id = 1;

  // Result type
  oneof result_type {
    ArrowBatch batch = 2;
    JSONBatch json_batch = 3;
  }

  // Metrics for the query execution. Typically, this field is only present in the last
  // batch of results and then represent the overall state of the query execution.
  Metrics metrics = 4;

  // Batch results of metrics.
  message ArrowBatch {
    int64 row_count = 1;
    int64 uncompressed_bytes = 2;
    int64 compressed_bytes = 3;
    bytes data = 4;
    bytes schema = 5;
  }

  // Message type when the result is returned as JSON. This is essentially a bulk wrapper
  // for the JSON result of a Spark DataFrame. All rows are returned in the JSON record format
  // of `{col -> row}`.
  message JSONBatch {
    int64 row_count = 1;
    bytes data = 2;
  }

  message Metrics {

    repeated MetricObject metrics = 1;

    message MetricObject {
      string name = 1;
      int64 plan_id = 2;
      int64 parent = 3;
      map<string, MetricValue> execution_metrics = 4;
    }

    message MetricValue {
      string name = 1;
      int64 value = 2;
      string metric_type = 3;
    }
  }
}

// Response to performing analysis of the query. Contains relevant metadata to be able to
// reason about the performance.
message AnalyzeResponse {
  string client_id = 1;
  repeated string column_names = 2;
  repeated string column_types = 3;

  // The extended explain string as produced by Spark.
  string explain_string = 4;
}

// Main interface for the SparkConnect service.
service SparkConnectService {

  // Executes a request that contains the query and returns a stream of [[Response]].
  rpc ExecutePlan(Request) returns (stream Response) {}

  // Analyzes a query and returns a [[AnalyzeResponse]] containing metadata about the query.
  rpc AnalyzePlan(Request) returns (AnalyzeResponse) {}
}

