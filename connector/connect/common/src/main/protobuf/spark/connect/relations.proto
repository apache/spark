/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *    http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

syntax = 'proto3';

package spark.connect;

import "spark/connect/expressions.proto";
import "spark/connect/types.proto";
import "spark/connect/catalog.proto";

option java_multiple_files = true;
option java_package = "org.apache.spark.connect.proto";

// The main [[Relation]] type. Fundamentally, a relation is a typed container
// that has exactly one explicit relation type set.
//
// When adding new relation types, they have to be registered here.
message Relation {
  RelationCommon common = 1;
  oneof rel_type {
    Read read = 2;
    Project project = 3;
    Filter filter = 4;
    Join join = 5;
    SetOperation set_op = 6;
    Sort sort = 7;
    Limit limit = 8;
    Aggregate aggregate = 9;
    SQL sql = 10;
    LocalRelation local_relation = 11;
    Sample sample = 12;
    Offset offset = 13;
    Deduplicate deduplicate = 14;
    Range range = 15;
    SubqueryAlias subquery_alias = 16;
    Repartition repartition = 17;
    RenameColumnsBySameLengthNames rename_columns_by_same_length_names = 18;
    RenameColumnsByNameToNameMap rename_columns_by_name_to_name_map = 19;
    ShowString show_string = 20;
    Drop drop = 21;
    Tail tail = 22;
    WithColumns with_columns = 23;
    Hint hint = 24;
    Unpivot unpivot = 25;
    ToSchema to_schema = 26;
    RepartitionByExpression repartition_by_expression = 27;

    // NA functions
    NAFill fill_na = 90;
    NADrop drop_na = 91;
    NAReplace replace = 92;

    // stat functions
    StatSummary summary = 100;
    StatCrosstab crosstab = 101;
    StatDescribe describe = 102;
    StatCov cov = 103;
    StatCorr corr = 104;

    // Catalog API (experimental / unstable)
    Catalog catalog = 200;

    Unknown unknown = 999;
  }
}

// Used for testing purposes only.
message Unknown {}

// Common metadata of all relations.
message RelationCommon {
  // (Required) Shared relation metadata.
  string source_info = 1;
}

// Relation that uses a SQL query to generate the output.
message SQL {
  // (Required) The SQL query.
  string query = 1;
}

// Relation that reads from a file / table or other data source. Does not have additional
// inputs.
message Read {
  oneof read_type {
    NamedTable named_table = 1;
    DataSource data_source = 2;
  }

  message NamedTable {
    // (Required) Unparsed identifier for the table.
    string unparsed_identifier = 1;
  }

  message DataSource {
    // (Required) Supported formats include: parquet, orc, text, json, parquet, csv, avro.
    string format = 1;

    // (Optional) If not set, Spark will infer the schema.
    optional string schema = 2;

    // Options for the data source. The context of this map varies based on the
    // data source format. This options could be empty for valid data source format.
    // The map key is case insensitive.
    map<string, string> options = 3;
  }
}

// Projection of a bag of expressions for a given input relation.
//
// The input relation must be specified.
// The projected expression can be an arbitrary expression.
message Project {
  // (Optional) Input relation is optional for Project.
  //
  // For example, `SELECT ABS(-1)` is valid plan without an input plan.
  Relation input = 1;

  // (Required) A Project requires at least one expression.
  repeated Expression expressions = 3;
}

// Relation that applies a boolean expression `condition` on each row of `input` to produce
// the output result.
message Filter {
  // (Required) Input relation for a Filter.
  Relation input = 1;

  // (Required) A Filter must have a condition expression.
  Expression condition = 2;
}

// Relation of type [[Join]].
//
// `left` and `right` must be present.
message Join {
  // (Required) Left input relation for a Join.
  Relation left = 1;

  // (Required) Right input relation for a Join.
  Relation right = 2;

  // (Optional) The join condition. Could be unset when `using_columns` is utilized.
  //
  // This field does not co-exist with using_columns.
  Expression join_condition = 3;

  // (Required) The join type.
  JoinType join_type = 4;

  // Optional. using_columns provides a list of columns that should present on both sides of
  // the join inputs that this Join will join on. For example A JOIN B USING col_name is
  // equivalent to A JOIN B on A.col_name = B.col_name.
  //
  // This field does not co-exist with join_condition.
  repeated string using_columns = 5;

  enum JoinType {
    JOIN_TYPE_UNSPECIFIED = 0;
    JOIN_TYPE_INNER = 1;
    JOIN_TYPE_FULL_OUTER = 2;
    JOIN_TYPE_LEFT_OUTER = 3;
    JOIN_TYPE_RIGHT_OUTER = 4;
    JOIN_TYPE_LEFT_ANTI = 5;
    JOIN_TYPE_LEFT_SEMI = 6;
    JOIN_TYPE_CROSS = 7;
  }
}

// Relation of type [[SetOperation]]
message SetOperation {
  // (Required) Left input relation for a Set operation.
  Relation left_input = 1;

  // (Required) Right input relation for a Set operation.
  Relation right_input = 2;

  // (Required) The Set operation type.
  SetOpType set_op_type = 3;

  // (Optional) If to remove duplicate rows.
  //
  // True to preserve all results.
  // False to remove duplicate rows.
  optional bool is_all = 4;

  // (Optional) If to perform the Set operation based on name resolution.
  //
  // Only UNION supports this option.
  optional bool by_name = 5;

  enum SetOpType {
    SET_OP_TYPE_UNSPECIFIED = 0;
    SET_OP_TYPE_INTERSECT = 1;
    SET_OP_TYPE_UNION = 2;
    SET_OP_TYPE_EXCEPT = 3;
  }
}

// Relation of type [[Limit]] that is used to `limit` rows from the input relation.
message Limit {
  // (Required) Input relation for a Limit.
  Relation input = 1;

  // (Required) the limit.
  int32 limit = 2;
}

// Relation of type [[Offset]] that is used to read rows staring from the `offset` on
// the input relation.
message Offset {
  // (Required) Input relation for an Offset.
  Relation input = 1;

  // (Required) the limit.
  int32 offset = 2;
}

// Relation of type [[Tail]] that is used to fetch `limit` rows from the last of the input relation.
message Tail {
  // (Required) Input relation for an Tail.
  Relation input = 1;

  // (Required) the limit.
  int32 limit = 2;
}

// Relation of type [[Aggregate]].
message Aggregate {
  // (Required) Input relation for a RelationalGroupedDataset.
  Relation input = 1;

  // (Required) How the RelationalGroupedDataset was built.
  GroupType group_type = 2;

  // (Required) Expressions for grouping keys
  repeated Expression grouping_expressions = 3;

  // (Required) List of values that will be translated to columns in the output DataFrame.
  repeated Expression aggregate_expressions = 4;

  // (Optional) Pivots a column of the current `DataFrame` and performs the specified aggregation.
  Pivot pivot = 5;

  enum GroupType {
    GROUP_TYPE_UNSPECIFIED = 0;
    GROUP_TYPE_GROUPBY = 1;
    GROUP_TYPE_ROLLUP = 2;
    GROUP_TYPE_CUBE = 3;
    GROUP_TYPE_PIVOT = 4;
  }

  message Pivot {
    // (Required) The column to pivot
    Expression col = 1;

    // (Optional) List of values that will be translated to columns in the output DataFrame.
    //
    // Note that if it is empty, the server side will immediately trigger a job to collect
    // the distinct values of the column.
    repeated Expression.Literal values = 2;
  }
}

// Relation of type [[Sort]].
message Sort {
  // (Required) Input relation for a Sort.
  Relation input = 1;

  // (Required) The ordering expressions
  repeated Expression.SortOrder order = 2;

  // (Optional) if this is a global sort.
  optional bool is_global = 3;
}


// Drop specified columns.
message Drop {
  // (Required) The input relation.
  Relation input = 1;

  // (Required) columns to drop.
  //
  // Should contain at least 1 item.
  repeated Expression cols = 2;
}


// Relation of type [[Deduplicate]] which have duplicate rows removed, could consider either only
// the subset of columns or all the columns.
message Deduplicate {
  // (Required) Input relation for a Deduplicate.
  Relation input = 1;

  // (Optional) Deduplicate based on a list of column names.
  //
  // This field does not co-use with `all_columns_as_keys`.
  repeated string column_names = 2;

  // (Optional) Deduplicate based on all the columns of the input relation.
  //
  // This field does not co-use with `column_names`.
  optional bool all_columns_as_keys = 3;
}

// A relation that does not need to be qualified by name.
message LocalRelation {
  // Local collection data serialized into Arrow IPC streaming format which contains
  // the schema of the data.
  bytes data = 1;

  // (Optional) The user provided schema.
  //
  // The Sever side will update the column names and data types according to this schema.
  oneof schema {

    DataType datatype = 2;

    // Server will use Catalyst parser to parse this string to DataType.
    string datatype_str = 3;
  }
}

// Relation of type [[Sample]] that samples a fraction of the dataset.
message Sample {
  // (Required) Input relation for a Sample.
  Relation input = 1;

  // (Required) lower bound.
  double lower_bound = 2;

  // (Required) upper bound.
  double upper_bound = 3;

  // (Optional) Whether to sample with replacement.
  optional bool with_replacement = 4;

  // (Optional) The random seed.
  optional int64 seed = 5;

  // (Optional) Explicitly sort the underlying plan to make the ordering deterministic.
  // This flag is only used to randomly splits DataFrame with the provided weights.
  optional bool force_stable_sort = 6;
}

// Relation of type [[Range]] that generates a sequence of integers.
message Range {
  // (Optional) Default value = 0
  optional int64 start = 1;

  // (Required)
  int64 end = 2;

  // (Required)
  int64 step = 3;

  // Optional. Default value is assigned by 1) SQL conf "spark.sql.leafNodeDefaultParallelism" if
  // it is set, or 2) spark default parallelism.
  optional int32 num_partitions = 4;
}

// Relation alias.
message SubqueryAlias {
  // (Required) The input relation of SubqueryAlias.
  Relation input = 1;

  // (Required) The alias.
  string alias = 2;

  // (Optional) Qualifier of the alias.
  repeated string qualifier = 3;
}

// Relation repartition.
message Repartition {
  // (Required) The input relation of Repartition.
  Relation input = 1;

  // (Required) Must be positive.
  int32 num_partitions = 2;

  // (Optional) Default value is false.
  optional bool shuffle = 3;
}

// Compose the string representing rows for output.
// It will invoke 'Dataset.showString' to compute the results.
message ShowString {
  // (Required) The input relation.
  Relation input = 1;

  // (Required) Number of rows to show.
  int32 num_rows = 2;

  // (Required) If set to more than 0, truncates strings to
  // `truncate` characters and all cells will be aligned right.
  int32 truncate = 3;

  // (Required) If set to true, prints output rows vertically (one line per column value).
  bool vertical = 4;
}

// Computes specified statistics for numeric and string columns.
// It will invoke 'Dataset.summary' (same as 'StatFunctions.summary')
// to compute the results.
message StatSummary {
  // (Required) The input relation.
  Relation input = 1;

  // (Optional) Statistics from to be computed.
  //
  // Available statistics are:
  //  count
  //  mean
  //  stddev
  //  min
  //  max
  //  arbitrary approximate percentiles specified as a percentage (e.g. 75%)
  //  count_distinct
  //  approx_count_distinct
  //
  // If no statistics are given, this function computes 'count', 'mean', 'stddev', 'min',
  // 'approximate quartiles' (percentiles at 25%, 50%, and 75%), and 'max'.
  repeated string statistics = 2;
}

// Computes basic statistics for numeric and string columns, including count, mean, stddev, min,
// and max. If no columns are given, this function computes statistics for all numerical or
// string columns.
message StatDescribe {
  // (Required) The input relation.
  Relation input = 1;

  // (Optional) Columns to compute statistics on.
  repeated string cols = 2;
}

// Computes a pair-wise frequency table of the given columns. Also known as a contingency table.
// It will invoke 'Dataset.stat.crosstab' (same as 'StatFunctions.crossTabulate')
// to compute the results.
message StatCrosstab {
  // (Required) The input relation.
  Relation input = 1;

  // (Required) The name of the first column.
  //
  // Distinct items will make the first item of each row.
  string col1 = 2;

  // (Required) The name of the second column.
  //
  // Distinct items will make the column names of the DataFrame.
  string col2 = 3;
}

// Calculate the sample covariance of two numerical columns of a DataFrame.
// It will invoke 'Dataset.stat.cov' (same as 'StatFunctions.calculateCov') to compute the results.
message StatCov {
  // (Required) The input relation.
  Relation input = 1;

  // (Required) The name of the first column.
  string col1 = 2;

  // (Required) The name of the second column.
  string col2 = 3;
}

// Calculates the correlation of two columns of a DataFrame. Currently only supports the Pearson
// Correlation Coefficient. It will invoke 'Dataset.stat.corr' (same as
// 'StatFunctions.pearsonCorrelation') to compute the results.
message StatCorr {
  // (Required) The input relation.
  Relation input = 1;

  // (Required) The name of the first column.
  string col1 = 2;

  // (Required) The name of the second column.
  string col2 = 3;

  // (Optional) Default value is 'pearson'.
  //
  // Currently only supports the Pearson Correlation Coefficient.
  optional string method = 4;
}

// Replaces null values.
// It will invoke 'Dataset.na.fill' (same as 'DataFrameNaFunctions.fill') to compute the results.
// Following 3 parameter combinations are supported:
//  1, 'values' only contains 1 item, 'cols' is empty:
//    replaces null values in all type-compatible columns.
//  2, 'values' only contains 1 item, 'cols' is not empty:
//    replaces null values in specified columns.
//  3, 'values' contains more than 1 items, then 'cols' is required to have the same length:
//    replaces each specified column with corresponding value.
message NAFill {
  // (Required) The input relation.
  Relation input = 1;

  // (Optional) Optional list of column names to consider.
  repeated string cols = 2;

  // (Required) Values to replace null values with.
  //
  // Should contain at least 1 item.
  // Only 4 data types are supported now: bool, long, double, string
  repeated Expression.Literal values = 3;
}


// Drop rows containing null values.
// It will invoke 'Dataset.na.drop' (same as 'DataFrameNaFunctions.drop') to compute the results.
message NADrop {
  // (Required) The input relation.
  Relation input = 1;

  // (Optional) Optional list of column names to consider.
  //
  // When it is empty, all the columns in the input relation will be considered.
  repeated string cols = 2;

  // (Optional) The minimum number of non-null and non-NaN values required to keep.
  //
  // When not set, it is equivalent to the number of considered columns, which means
  // a row will be kept only if all columns are non-null.
  //
  // 'how' options ('all', 'any') can be easily converted to this field:
  //   - 'all' -> set 'min_non_nulls' 1;
  //   - 'any' -> keep 'min_non_nulls' unset;
  optional int32 min_non_nulls = 3;
}


// Replaces old values with the corresponding values.
// It will invoke 'Dataset.na.replace' (same as 'DataFrameNaFunctions.replace')
// to compute the results.
message NAReplace {
  // (Required) The input relation.
  Relation input = 1;

  // (Optional) List of column names to consider.
  //
  // When it is empty, all the type-compatible columns in the input relation will be considered.
  repeated string cols = 2;

  // (Optional) The value replacement mapping.
  repeated Replacement replacements = 3;

  message Replacement {
    // (Required) The old value.
    //
    // Only 4 data types are supported now: null, bool, double, string.
    Expression.Literal old_value = 1;

    // (Required) The new value.
    //
    // Should be of the same data type with the old value.
    Expression.Literal new_value = 2;
  }
}


// Rename columns on the input relation by the same length of names.
message RenameColumnsBySameLengthNames {
  // (Required) The input relation of RenameColumnsBySameLengthNames.
  Relation input = 1;

  // (Required)
  //
  // The number of columns of the input relation must be equal to the length
  // of this field. If this is not true, an exception will be returned.
  repeated string column_names = 2;
}


// Rename columns on the input relation by a map with name to name mapping.
message RenameColumnsByNameToNameMap {
  // (Required) The input relation.
  Relation input = 1;


  // (Required)
  //
  // Renaming column names of input relation from A to B where A is the map key
  // and B is the map value. This is a no-op if schema doesn't contain any A. It
  // does not require that all input relation column names to present as keys.
  // duplicated B are not allowed.
  map<string, string> rename_columns_map = 2;
}

// Adding columns or replacing the existing columns that have the same names.
message WithColumns {
  // (Required) The input relation.
  Relation input = 1;

  // (Required)
  //
  // Given a column name, apply the corresponding expression on the column. If column
  // name exists in the input relation, then replace the column. If the column name
  // does not exist in the input relation, then adds it as a new column.
  //
  // Only one name part is expected from each Expression.Alias.
  //
  // An exception is thrown when duplicated names are present in the mapping.
  repeated Expression.Alias name_expr_list = 2;
}

// Specify a hint over a relation. Hint should have a name and optional parameters.
message Hint {
  // (Required) The input relation.
  Relation input = 1;

  // (Required) Hint name.
  //
  // Supported Join hints include BROADCAST, MERGE, SHUFFLE_HASH, SHUFFLE_REPLICATE_NL.
  //
  // Supported partitioning hints include COALESCE, REPARTITION, REPARTITION_BY_RANGE.
  string name = 2;

  // (Optional) Hint parameters.
  repeated Expression.Literal parameters = 3;
}

// Unpivot a DataFrame from wide format to long format, optionally leaving identifier columns set.
message Unpivot {
  // (Required) The input relation.
  Relation input = 1;

  // (Required) Id columns.
  repeated Expression ids = 2;

  // (Optional) Value columns to unpivot.
  repeated Expression values = 3;

  // (Required) Name of the variable column.
  string variable_column_name = 4;

  // (Required) Name of the value column.
  string value_column_name = 5;
}

message ToSchema {
  // (Required) The input relation.
  Relation input = 1;

  // (Required) The user provided schema.
  //
  // The Sever side will update the dataframe with this schema.
  DataType schema = 2;
}

  message RepartitionByExpression {
  // (Required) The input relation.
  Relation input = 1;

  // (Required) The partitioning expressions.
  repeated Expression partition_exprs = 2;

  // (Optional) number of partitions, must be positive.
  optional int32 num_partitions = 3;
}
