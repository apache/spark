/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *    http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

package org.apache.spark.sql.catalyst.analysis.resolver

import java.util.HashSet

import org.apache.spark.SparkException
import org.apache.spark.sql.catalyst.analysis.{AnsiTypeCoercion, TypeCoercion, TypeCoercionBase}
import org.apache.spark.sql.catalyst.expressions.{Alias, Attribute, Cast, ExprId}
import org.apache.spark.sql.catalyst.plans.logical._
import org.apache.spark.sql.catalyst.types.DataTypeUtils
import org.apache.spark.sql.errors.DataTypeErrors.cannotMergeIncompatibleDataTypesError
import org.apache.spark.sql.errors.QueryCompilationErrors
import org.apache.spark.sql.types.{DataType, MetadataBuilder}

/**
 * The [[SetOperationLikeResolver]] performs [[Union]], [[Intersect]] or [[Except]] operator
 * resolution. These operators have 2+ children. Resolution involves checking and normalizing child
 * output attributes (data types and nullability).
 */
class SetOperationLikeResolver(resolver: Resolver, expressionResolver: ExpressionResolver)
    extends TreeNodeResolver[LogicalPlan, LogicalPlan] {
  private val expressionIdAssigner = expressionResolver.getExpressionIdAssigner
  private val autoGeneratedAliasProvider = expressionResolver.getAutoGeneratedAliasProvider
  private val scopes = resolver.getNameScopes
  private val cteRegistry = resolver.getCteRegistry

  /**
   * Resolve the [[Union]], [[Intersect]] or [[Except]] operators:
   *  - Resolve each child in the context of a) New [[NameScope]] b) New [[ExpressionIdAssigner]]
   *    mapping c) CTE scope. Collect child outputs to coerce them later.
   *  - Create a new mapping in [[ExpressionIdAssigner]] for the current operator. We only need the
   *    left child mapping, because that's the only child whose expression IDs get propagated
   *    upwards for [[Union]], [[Intersect]] or [[Except]]. This is an optimization.
   *  - For [[Union]] operators with two children, check if this is part of a recursive CTE. If so,
   *    replace the [[Union]] with a [[UnionLoop]] operator. The [[UnionLoop]] output expression IDs
   *    are created through [[ExpressionIdAssigner]] to ensure they are properly tracked in
   *    `globalExpressionIds` and define a stable output schema across all iterations of the
   *    recursive CTE. To assert that only the topmost [[Union]] is transformed into UnionLoop, the
   *    depth of the first [[Union]] node in each recursive CTE scope is tracked.
   *  - Compute widened data types for child output attributes using
   *    [[getTypeCoercion.findWiderTypeForTwo]] or throw "INCOMPATIBLE_COLUMN_TYPE" if coercion
   *    fails.
   *  - Perform individual output deduplication to handle the distinct union case described in
   *    [[performIndividualOutputExpressionIdDeduplication]] scaladoc.
   *  - Deduplicate expression IDs across all the operator child outputs. See more in
   *    [[deduplicateOutputExpressionIds]] scala doc.
   *  - Validate that child outputs have same length or throw "NUM_COLUMNS_MISMATCH" otherwise.
   *  - Add [[Project]] with [[Cast]] on children needing attribute data type widening.
   *  - Assert that coerced outputs don't have conflicting expression IDs.
   *  - Merge transformed outputs using a separate logic for each operator type. For [[UnionLoop]],
   *    the merged output uses the pre-assigned expression IDs from the [[UnionLoop]] operator.
   *  - Store merged output in current [[NameScope]].
   *  - Validate that the operator doesn't have unsupported data types in the output
   *  - Create a new mapping in [[ExpressionIdAssigner]] using the coerced and validated outputs.
   *  - Return the resolved operator with new children optionally wrapped in [[WithCTE]]. See
   *    [[CteScope]] scaladoc for more info.
   */
  override def resolve(unresolvedOperator: LogicalPlan): LogicalPlan = {
    cteRegistry.trySetExpectedUnionDepth()

    val (resolvedChildren, childScopes) = resolveChildren(unresolvedOperator)

    expressionIdAssigner.createMappingFromChildMappings(
      newOutputIds = childScopes.head.getOutputIds
    )

    val childOutputs = childScopes.map(_.output)

    val (withUnionLoopEmplaced, isRecursiveCTE) =
      tryPlaceUnionLoop(resolvedChildren, childOutputs, unresolvedOperator)

    val (coercedChildren, coercedChildOutputs) =
      if (needToCoerceChildOutputs(childOutputs, unresolvedOperator)) {
        coerceChildOutputs(
          resolvedChildren,
          childOutputs,
          validateAndDeduceTypes(unresolvedOperator, childOutputs, isRecursiveCTE)
        )
      } else {
        (resolvedChildren, childOutputs)
      }

    val (newChildren, newChildOutputs) =
      performIndividualOutputExpressionIdDeduplication(
        coercedChildren,
        coercedChildOutputs,
        withUnionLoopEmplaced
      )

    val (deduplicatedChildren, deduplicatedChildOutputs) = deduplicateOutputExpressionIds(
      unresolvedOperator = unresolvedOperator,
      children = newChildren,
      childOutputs = newChildOutputs
    )

    ExpressionIdAssigner.assertOutputsHaveNoConflictingExpressionIds(deduplicatedChildOutputs)

    val output = mergeChildOutputs(withUnionLoopEmplaced, deduplicatedChildOutputs)
    scopes.overwriteCurrent(output = Some(output), hiddenOutput = Some(output))

    OperatorWithUncomparableTypeValidator.validate(withUnionLoopEmplaced, output)

    val resolvedOperator = withUnionLoopEmplaced.withNewChildren(deduplicatedChildren)

    cteRegistry.currentScope.tryPutWithCTE(
      unresolvedOperator = withUnionLoopEmplaced,
      resolvedOperator = resolvedOperator
    )
  }

  /**
   * Resolve `unresolvedSetOperationLike`'s children in the context of new [[NameScope]],
   * [[ExpressionIdAssigner]] mapping and [[CteScope]].
   *
   * [[ExpressionIdAssigner]] child mapping is collected just or the left child, because that's
   * the only child whose expression IDs get propagated upwards through [[Union]], [[Intersect]] or
   * [[Except]]. This is an optimization to avoid fast-growing expression ID mappings.
   *
   * For [[Union]] operators, the first child's resolved output is registered as the anchor output
   * schema for potential recursive CTEs. This establishes the schema that the recursion branch
   * must conform to during type coercion, and the output format of any self reference.
   */
  private def resolveChildren(
      unresolvedOperator: LogicalPlan): (Seq[LogicalPlan], Seq[NameScope]) = {
    unresolvedOperator.children.zipWithIndex.map {
      case (unresolvedChild, childIndex) =>
        expressionIdAssigner.pushMapping()
        scopes.pushScope()
        cteRegistry.pushScopeForMultiChildOperator(
          unresolvedOperator = unresolvedOperator,
          unresolvedChild = unresolvedChild
        )

        try {
          val resolvedChild = resolver.resolve(unresolvedChild)
          if (childIndex == 0) {
            tryRegisterAnchorOutput(unresolvedOperator)
          }
          (resolvedChild, scopes.current)
        } finally {
          cteRegistry.popScope()
          scopes.popScope()
          expressionIdAssigner.popMapping(collectChildMapping = childIndex == 0)
        }
    }.unzip
  }

  /**
   * For [[Union]] operators, registers the current scope's output as the anchor output schema
   * for recursive CTEs. This establishes the schema that the recursion branch must conform to.
   */
  private def tryRegisterAnchorOutput(unresolvedOperator: LogicalPlan): Unit = {
    unresolvedOperator match {
      case Union(_, _, _) =>
        cteRegistry.tryRegisterAnchorOutput(scopes.current.output)
      case _ =>
    }
  }

  /**
   * Replaces operator node with [[UnionLoop]] if the operator that is being resolved is [[Union]]
   * with two children and inside a recursive CTE scope.
   */
  private def tryPlaceUnionLoop(
      children: Seq[LogicalPlan],
      childOutputs: Seq[Seq[Attribute]],
      unresolvedOperator: LogicalPlan
  ): (LogicalPlan, Boolean) = {
    unresolvedOperator match {
      case union: Union if union.children.size == 2 =>
        val unionLoop = cteRegistry.tryPlaceUnionLoop(
          children(0),
          childOutputs(0),
          children(1),
          expressionIdAssigner
        )
        (unionLoop.getOrElse(unresolvedOperator), unionLoop.nonEmpty)
      case _ => (unresolvedOperator, false)
    }
  }

  /**
   * Deduplicate expression IDs at the scope of each individual child output. This is necessary to
   * handle the following case:
   *
   * {{{
   * -- The correct answer is (1, 1), (1, 2). Without deduplication it would be (1, 1), because
   * -- aggregation would be done only based on the first column.
   * SELECT
   *   a, a
   * FROM
   *   VALUES (1, 1), (1, 2) AS t1 (a, b)
   * UNION
   * SELECT
   *  a, b
   * FROM
   *   VALUES (1, 1), (1, 2) AS t2 (a, b)
   * }}}
   *
   * Putting [[Alias]] introduces a new expression ID for the attribute duplicates in the output. We
   * also add `__is_duplicate` metadata so that [[AttributeSeq.getCandidatesForResolution]] doesn't
   * produce conflicting candidates when resolving names in the upper [[Project]] - this is
   * technically still the same attribute.
   *
   * Probably there's a better way to do that, but we want to stay compatible with the fixed-point
   * [[Analyzer]].
   *
   * See SPARK-37865 for more details.
   */
  private def performIndividualOutputExpressionIdDeduplication(
      children: Seq[LogicalPlan],
      childOutputs: Seq[Seq[Attribute]],
      unresolvedOperator: LogicalPlan
  ): (Seq[LogicalPlan], Seq[Seq[Attribute]]) = {
    unresolvedOperator match {
      case _: Union => doPerformIndividualOutputExpressionIdDeduplication(children, childOutputs)
      case _ => (children, childOutputs)
    }
  }

  private def doPerformIndividualOutputExpressionIdDeduplication(
      children: Seq[LogicalPlan],
      childOutputs: Seq[Seq[Attribute]]
  ): (Seq[LogicalPlan], Seq[Seq[Attribute]]) = {
    children
      .zip(childOutputs)
      .map {
        case (child, childOutput) =>
          var outputChanged = false

          val expressionIds = new HashSet[ExprId]
          val newOutput = childOutput.map { attribute =>
            if (expressionIds.contains(attribute.exprId)) {
              outputChanged = true

              val newMetadata = new MetadataBuilder()
                .withMetadata(attribute.metadata)
                .putNull("__is_duplicate")
                .build()
              autoGeneratedAliasProvider.newAlias(
                child = attribute,
                name = Some(attribute.name),
                explicitMetadata = Some(newMetadata)
              )
            } else {
              expressionIds.add(attribute.exprId)

              attribute
            }
          }

          if (outputChanged) {
            (Project(projectList = newOutput, child = child), newOutput.map(_.toAttribute))
          } else {
            (child, childOutput)
          }
      }
      .unzip
  }

  /**
   * Deduplicate expression IDs across all the operator's children outputs.
   * We do that by adding a [[Project]] node on top of a [[Union]]s child with deduplicated IDs. We
   * also tag it with the [[ResolverTag.PROJECT_FOR_EXPRESSION_ID_DEDUPLICATION]] tag so it can be
   * properly handled later. For example:
   *
   * {{{ SELECT col1 FROM values(1) WHERE 100 IN (SELECT col1 UNION SELECT col1); }}}
   *
   * In this case, both children of the [[Union]] will have output `col1` with the same expression
   * IDs, and thus we have to add a [[Project]] on top of the latter one. The analyzed plan looks
   * like:
   *
   * {{{
   * Project [col1#1]
   * +- Filter 100 IN (list#2 [col1#1 && col1#1])
   *    :  +- Distinct
   *    :     +- Union false, false
   *    :        :- Project [outer(col1#1)]
   *    :        :  +- OneRowRelation
   *    :        +- Project [col1#1 AS col1#3]
   *    :           +- Project [outer(col1#1)]
   *    :              +- OneRowRelation
   *    +- LocalRelation [col1#1]
   * }}}
   *
   * We place an [[Alias]] on every element of the project list if there is any duplicate in the
   * list. This is done only for [[Union]] children in order to to stay compatible with the
   * fixed-point analyzer. The node is later disregarded in the [[NormalizePlan]] and eventually
   * removed by the optimizer.
   */
  private def deduplicateOutputExpressionIds(
      unresolvedOperator: LogicalPlan,
      children: Seq[LogicalPlan],
      childOutputs: Seq[Seq[Attribute]]
  ): (Seq[LogicalPlan], Seq[Seq[Attribute]]) = {
    unresolvedOperator match {
      case _: Union => doDeduplicateOutputExpressionIds(children, childOutputs)
      case _ => (children, childOutputs)
    }
  }

  private def doDeduplicateOutputExpressionIds(
      children: Seq[LogicalPlan],
      childOutputs: Seq[Seq[Attribute]]
  ): (Seq[LogicalPlan], Seq[Seq[Attribute]]) = {
    val expressionIds = new HashSet[ExprId]

    children
      .zip(childOutputs)
      .map {
        case (child, childOutput) =>
          var outputChanged = false

          childOutput.foreach { attribute =>
            if (expressionIds.contains(attribute.exprId)) {
              outputChanged = true
            } else {
              expressionIds.add(attribute.exprId)

              attribute
            }
          }

          if (outputChanged) {
            val newOutput = childOutput.map { attribute =>
              autoGeneratedAliasProvider.newAlias(
                child = attribute,
                name = Some(attribute.name)
              )
            }

            val project = Project(projectList = newOutput, child = child)
            project.setTagValue(
              ResolverTag.PROJECT_FOR_EXPRESSION_ID_DEDUPLICATION,
              ()
            )
            (project, newOutput.map(_.toAttribute))
          } else {
            (child, childOutput)
          }
      }
      .unzip
  }

  /**
   * Check if we need to coerce child output attributes to wider types. We need to do this if:
   * - Output length differs between children. We will throw an appropriate error later during type
   *   coercion with more diagnostics.
   * - Output data types differ between children. We don't care about nullability for type coercion,
   *   it will be correctly assigned later by [[SetOperationLikeResolver.mergeChildOutputs]].
   */
  private def needToCoerceChildOutputs(
      childOutputs: Seq[Seq[Attribute]],
      unresolvedOperator: LogicalPlan): Boolean = {
    val firstChildOutput = childOutputs.head
    childOutputs.tail.exists { childOutput =>
      childOutput.length != firstChildOutput.length ||
      childOutput.zip(firstChildOutput).exists {
        case (lhsAttribute, rhsAttribute) =>
          !areDataTypesCompatibleInTheContextOfOperator(
            unresolvedOperator,
            lhsAttribute.dataType,
            rhsAttribute.dataType
          )
      }
    }
  }

  /**
   * This method returns whether types are compatible in the context of the specified operator.
   *
   * In fixed-point we only use [[DataType.equalsStructurally]] for [[Union]] type coercion. For
   * [[Except]] and [[Intersect]] we use [[DataTypeUtils.sameType]]. This method ensures we perform
   * the check for whether coercion is needed in the compatible way to the fixed-point.
   */
  private def areDataTypesCompatibleInTheContextOfOperator(
      unresolvedPlan: LogicalPlan,
      lhs: DataType,
      rhs: DataType): Boolean = {
    unresolvedPlan match {
      case _: UnionBase => DataType.equalsStructurally(lhs, rhs, ignoreNullability = true)
      case _: Except | _: Intersect => DataTypeUtils.sameType(lhs, rhs)
      case other =>
        throw SparkException.internalError(
          s"Set operation resolver should not be used for ${other.nodeName}"
        )
    }
  }

  /**
   * Returns a sequence of data types representing the widened data types for each column:
   *  - Validates that the number of columns in each child of the set operator is equal.
   *  - Validates that the data types of columns can be widened to a common type.
   *  - Deduces the widened data types for each column.
   *  - In the case of recursive CTEs (UnionLoop), we follow the logic of the fixed point analyzer
   *  which only widens the type of the recursion, but never the anchor.
   */
  private def validateAndDeduceTypes(
      unresolvedOperator: LogicalPlan,
      childOutputs: Seq[Seq[Attribute]],
      isRecursiveCTE: Boolean): Seq[DataType] = {
    val childDataTypes = childOutputs.map(attributes => attributes.map(attr => attr.dataType))

    val expectedNumColumns = childDataTypes.head.length

    childDataTypes.tail.zipWithIndex.foldLeft(childDataTypes.head) {
      case (widenedTypes, (childColumnTypes, childIndex)) =>
        if (childColumnTypes.length != expectedNumColumns) {
          throwNumColumnsMismatch(
            expectedNumColumns = expectedNumColumns,
            childColumnTypes = childColumnTypes,
            columnIndex = childIndex,
            unresolvedOperator = unresolvedOperator
          )
        }

        widenedTypes.zip(childColumnTypes).zipWithIndex.map {
          case ((widenedColumnType, columnTypeForCurrentRow), columnIndex) =>
            val widerType = getTypeCoercion
              .findWiderTypeForTwo(widenedColumnType, columnTypeForCurrentRow)
              .getOrElse {
                throwIncompatibleColumnTypeError(
                  unresolvedOperator = unresolvedOperator,
                  columnIndex = columnIndex,
                  childIndex = childIndex,
                  widenedColumnType = widenedColumnType,
                  columnTypeForCurrentRow = columnTypeForCurrentRow
                )
              }
            if (isRecursiveCTE) {
              validateRecursiveCteTypeCompatibility(
                widerType,
                widenedColumnType,
                columnTypeForCurrentRow
              )
            } else {
              widerType
            }
        }
    }
  }

  /**
   * Validates type compatibility for recursive CTEs. For recursive CTEs, we follow the logic
   * of the fixed point analyzer which only widens the type of the recursion, but never the anchor.
   * This means the wider type must equal the anchor type (widenedColumnType), otherwise we throw
   * an error.
   */
  private def validateRecursiveCteTypeCompatibility(
      widerType: DataType,
      widenedColumnType: DataType,
      columnTypeForCurrentRow: DataType): DataType = {
    if (widerType == widenedColumnType) {
      widerType
    } else {
      throw cannotMergeIncompatibleDataTypesError(
        widenedColumnType,
        columnTypeForCurrentRow
      )
    }
  }

  /**
   * Coerce `childOutputs` to the previously calculated `widenedTypes`. If the data types for
   * child output has changed, we have to add a [[Project]] operator with a [[Cast]] to the new
   * type.
   */
  private def coerceChildOutputs(
      children: Seq[LogicalPlan],
      childOutputs: Seq[Seq[Attribute]],
      widenedTypes: Seq[DataType]): (Seq[LogicalPlan], Seq[Seq[Attribute]]) = {
    val sessionLocalTimeZone = conf.sessionLocalTimeZone

    children
      .zip(childOutputs)
      .map {
        case (child, output) =>
          var outputChanged = false
          val newExpressions = output.zip(widenedTypes).map {
            case (attribute, widenedType) =>
              /**
               * Probably more correct way to compare data types here would be to call
               * [[DataType.equalsStructurally]] but fixed-point [[Analyzer]] rule
               * [[WidenSetOperationTypes]] uses `==`, so we do the same to stay compatible.
               */
              if (attribute.dataType == widenedType) {
                attribute
              } else {
                outputChanged = true
                autoGeneratedAliasProvider.newAlias(
                  child = Cast(attribute, widenedType, Some(sessionLocalTimeZone)),
                  name = Some(attribute.name)
                )
              }
          }

          if (outputChanged) {
            (Project(newExpressions, child), newExpressions.map(_.toAttribute))
          } else {
            (child, output)
          }
      }
      .unzip
  }

  /**
   * Helper method to call appropriate object method [[mergeChildOutputs]] for each operator.
   */
  private def mergeChildOutputs(
      unresolvedPlan: LogicalPlan,
      childOutputs: Seq[Seq[Attribute]]): Seq[Attribute] = {
    unresolvedPlan match {
      case _: Union => Union.mergeChildOutputs(childOutputs)
      case unionLoop: UnionLoop =>
        Union.mergeChildOutputs(childOutputs).zip(unionLoop.outputAttrIds).map {
          case (x, id) =>
            x.withExprId(id)
        }
      case _: Except => Except.mergeChildOutputs(childOutputs)
      case _: Intersect => Intersect.mergeChildOutputs(childOutputs)
      case other =>
        throw SparkException.internalError(
          s"Set operation resolver should not be used for ${other.nodeName}"
        )
    }
  }

  private def getTypeCoercion: TypeCoercionBase = {
    // BEGIN-EDGE
    if (conf.enforceAnsiTypeCoercion) {
      /* // END-EDGE
      if (conf.ansiEnabled) {
       */ // EDGE
      AnsiTypeCoercion
    } else {
      TypeCoercion
    }
  }

  private def throwNumColumnsMismatch(
      expectedNumColumns: Int,
      childColumnTypes: Seq[DataType],
      columnIndex: Int,
      unresolvedOperator: LogicalPlan): Unit = {
    throw QueryCompilationErrors.numColumnsMismatch(
      operator = unresolvedOperator.nodeName.toUpperCase(),
      firstNumColumns = expectedNumColumns,
      invalidOrdinalNum = columnIndex + 1,
      invalidNumColumns = childColumnTypes.length,
      origin = unresolvedOperator.origin
    )
  }

  private def throwIncompatibleColumnTypeError(
      unresolvedOperator: LogicalPlan,
      columnIndex: Int,
      childIndex: Int,
      widenedColumnType: DataType,
      columnTypeForCurrentRow: DataType): Nothing = {
    throw QueryCompilationErrors.incompatibleColumnTypeError(
      operator = unresolvedOperator.nodeName.toUpperCase(),
      columnOrdinalNumber = columnIndex,
      tableOrdinalNumber = childIndex + 1,
      dataType1 = columnTypeForCurrentRow,
      dataType2 = widenedColumnType,
      hint = "",
      origin = unresolvedOperator.origin
    )
  }
}
