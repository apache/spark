/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *    http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

package org.apache.spark.sql.execution.streaming.state

import java.util.{TimerTask, Timer}

import scala.collection.mutable
import scala.collection.mutable.HashMap
import scala.util.Random
import scala.util.control.NonFatal

import org.apache.hadoop.conf.Configuration
import org.apache.hadoop.fs.{FileStatus, Path}

import org.apache.spark.sql.catalyst.expressions.JoinedRow
import org.apache.spark.sql.execution.streaming.state.StateStore._
import org.apache.spark.util.{Utils, CompletionIterator}
import org.apache.spark.{SparkConf, Logging}
import org.apache.spark.serializer.{DeserializationStream, SerializationStream, KryoSerializer}
import org.apache.spark.sql.catalyst.InternalRow


/**
 * An implementation of [[StateStoreProvider]] and [[StateStore]] in which all the data is backed
  * by files in a HDFS-compatible file system. All updates to the store has to be done in sets
 * transactionally, and each set of updates increments the store's version. These versions can
 * be used to re-execute the updates (by retries in RDD operations) on the correct version of
 * the store, and regenerate the store version.
 *
 * Usage:
 * To update the data in the state store, the following order of operations are needed.
 *
 * - val store = StateStore.get(operatorId, partitionId, version) // to get the right store
 * - store.update(...)
 * - store.remove(...)
 * - store.commit()    // commits all the updates to made with version number
 * - store.iterator()  // key-value data after last commit as an iterator
 * - store.updates()   // updates made in the last as an iterator
 *
 * Fault-tolerance model:
 * - Every set of updates is written to a delta file before committing.
 * - The state store is responsible for managing, collapsing and cleaning up of delta files.
 * - Multiple attempts to commit the same version of updates must have the same updates.
 * - Background management of files ensures that last versions of the store is always recoverable
 * to ensure re-executed RDD operations re-apply updates on the correct past version of the
 * store.
 */
class HDFSBackedStateStoreProvider(
    val id: StateStoreId,
    val directory: String,
    numBatchesToRetain: Int = 2,
    maxDeltaChainForSnapshots: Int = 10
  ) extends StateStoreProvider with Logging {
  type MapType = mutable.HashMap[InternalRow, InternalRow]

  import StateStore._


  class HDFSBackedStateStore( val version: Long, mapToUpdate: MapType)
    extends StateStore {

    trait STATE
    case object UPDATING extends STATE
    case object COMMITTED extends STATE
    case object CANCELLED extends STATE

    private val newVersion = version + 1
    private val tempDeltaFile = new Path(baseDir, s"temp-${Random.nextLong}")
    private val tempDeltaFileStream =
      serializer.newInstance().serializeStream(fs.create(tempDeltaFile, true))

    @volatile private var state: STATE = UPDATING
    @volatile private var finalDeltaFile: Path = null

    override def id: StateStoreId = HDFSBackedStateStoreProvider.this.id

    /** Update the value of a key using the value generated by the update function */
    override def update(key: InternalRow, updateFunc: Option[InternalRow] => InternalRow): Unit = {
      verify(state == UPDATING, "Cannot update after already committed or cancelled")
      val value = updateFunc(mapToUpdate.get(key))
      mapToUpdate.put(key, value)
      tempDeltaFileStream.writeObject(ValueUpdated(key, value))
    }

    /** Remove keys that match the following condition */
    override def remove(condition: InternalRow => Boolean): Unit = {
      verify(state == UPDATING, "Cannot remove after already committed or cancelled")
      val keyIter = mapToUpdate.keysIterator
      while (keyIter.hasNext) {
        val key = keyIter.next
        if (condition(key)) {
          mapToUpdate.remove(key)
          tempDeltaFileStream.writeObject(KeyRemoved(key))
        }
      }
    }

    /** Commit all the updates that have been made to the store. */
    override def commit(): Long = {
      verify(state == UPDATING, "Cannot commit again after already committed or cancelled")

      try {
        tempDeltaFileStream.close()
        finalDeltaFile = commitUpdates(newVersion, mapToUpdate, tempDeltaFile)
        state = COMMITTED
        newVersion
      } catch {
        case NonFatal(e) =>
          throw new IllegalStateException(
            s"Error committing version $newVersion into ${HDFSBackedStateStoreProvider.this}", e)
      }
    }

    /** Cancel all the updates made on this store. This store will not be usable any more. */
    override def cancel(): Unit = {
      state = CANCELLED
      if (tempDeltaFileStream != null) {
        tempDeltaFileStream.close()
      }
      if (tempDeltaFile != null && fs.exists(tempDeltaFile)) {
        fs.delete(tempDeltaFile, true)
      }
    }

    /**
     * Get an iterator of all the store data. This can be called only after committing the
     * updates.
     */
    override def iterator(): Iterator[InternalRow] = {
      verify(state == COMMITTED, "Cannot get iterator of store data before comitting")
      HDFSBackedStateStoreProvider.this.iterator(version)
    }

    /**
     * Get an iterator of all the updates made to the store in the current version.
     * This can be called only after committing the updates.
     */
    override def updates(): Iterator[StoreUpdate] = {
      verify(state == COMMITTED, "Cannot get iterator of updates before committing")
      readDeltaFile(finalDeltaFile)
    }

    /**
     * Whether all updates have been committed
     */
    override def hasCommitted: Boolean = {
      state == COMMITTED
    }
  }

  /** Get the state store for making updates to create a new `version` of the store. */
  override def getStore(version: Long): StateStore = synchronized {
    require(version >= 0, "Version cannot be less than 0")
    val newMap = new MapType()
    if (version > 0) {
      newMap ++= loadMap(version)
    }
    new HDFSBackedStateStore(version, newMap)
  }

  override def manage(): Unit = {
    try {
      doSnapshot()
      cleanup()
    } catch {
      case NonFatal(e) =>
        logWarning(s"Error performing snapshot and cleaning up $this")
    }
  }

  override def toString(): String = {
    s"StateStore[id = (op=${id.operatorId},part=${id.partitionId}), dir = $baseDir]"
  }

  /* Internal classes and methods */

  private val loadedMaps = new mutable.HashMap[Long, MapType]
  private val baseDir = new Path(directory, s"${id.operatorId}/${id.partitionId.toString}")
  private val fs = baseDir.getFileSystem(new Configuration())
  private val serializer = new KryoSerializer(new SparkConf)

  initialize()

  private case class StoreFile(version: Long, path: Path, isSnapshot: Boolean)

  /** Commit a set of updates to the store with the given new version */
  private def commitUpdates(newVersion: Long, map: MapType, tempDeltaFile: Path): Path = {
    synchronized {
      val finalDeltaFile = deltaFile(newVersion)
      fs.rename(tempDeltaFile, finalDeltaFile)
      loadedMaps.put(newVersion, map)
      finalDeltaFile
    }
  }

  /**
   * Get iterator of all the data of the latest version of the store.
   * Note that this will look up the files to determined the latest known version.
   */
  private[state] def latestIterator(): Iterator[InternalRow] = synchronized {
    val versionsInFiles = fetchFiles().map(_.version).toSet
    val versionsLoaded = loadedMaps.keySet
    val allKnownVersions = versionsInFiles ++ versionsLoaded
    if (allKnownVersions.nonEmpty) {
      loadMap(allKnownVersions.max)
        .iterator
        .map { case (key, value) => new JoinedRow(key, value) }
    } else Iterator.empty
  }

  /** Get iterator of a specific version of the store */
  private[state] def iterator(version: Long): Iterator[InternalRow] = synchronized {
    loadMap(version)
      .iterator
      .map { case (key, value) => new JoinedRow(key, value) }
  }

  /** Initialize the store provider */
  private def initialize(): Unit = {
    if (!fs.exists(baseDir)) {
      fs.mkdirs(baseDir)
    } else {
      if (!fs.isDirectory(baseDir)) {
        throw new IllegalStateException(
          s"Cannot use $directory for storing state data as" +
            s"$baseDir already exists and is not a directory")
      }
    }
  }

  /** Load the required version of the map data from the backing files */
  private def loadMap(version: Long): MapType = {
    if (version < 0) return new MapType
    synchronized {
      loadedMaps.get(version)
    }.getOrElse {
      val mapFromFile = readSnapshotFile(version).getOrElse {
        val prevMap = loadMap(version - 1)
        val deltaUpdates = readDeltaFile(version)
        val newMap = new MapType()
        newMap ++= prevMap
        newMap.sizeHint(prevMap.size)
        while (deltaUpdates.hasNext) {
          deltaUpdates.next match {
            case ValueUpdated(key, value) => newMap.put(key, value)
            case KeyRemoved(key) => newMap.remove(key)
          }
        }
        newMap
      }
      loadedMaps.put(version, mapFromFile)
      mapFromFile
    }
  }

  private def readDeltaFile(version: Long): Iterator[StoreUpdate] = {
    readDeltaFile(deltaFile(version))
  }

  private def readDeltaFile(fileToRead: Path): Iterator[StoreUpdate] = {
    if (!fs.exists(fileToRead)) {
      throw new IllegalStateException(
        s"Cannot read delta file $fileToRead of $this: $fileToRead does not exist")
    }
    val deser = serializer.newInstance()
    var deserStream: DeserializationStream = null
    deserStream = deser.deserializeStream(fs.open(fileToRead))
    val iter = deserStream.asIterator.asInstanceOf[Iterator[StoreUpdate]]
    CompletionIterator[StoreUpdate, Iterator[StoreUpdate]](
    iter, {
      deserStream.close()
    })
  }

  private def writeSnapshotFile(version: Long, map: MapType): Unit = {
    val fileToWrite = snapshotFile(version)
    val ser = serializer.newInstance()
    var outputStream: SerializationStream = null
    Utils.tryWithSafeFinally {
      outputStream = ser.serializeStream(fs.create(fileToWrite, false))
      outputStream.writeAll(map.iterator)
    } {
      if (outputStream != null) outputStream.close()
    }
  }

  private def readSnapshotFile(version: Long): Option[MapType] = {
    val fileToRead = snapshotFile(version)
    if (!fs.exists(fileToRead)) return None

    val deser = serializer.newInstance()
    val map = new MapType()
    var deserStream: DeserializationStream = null

    try {
      deserStream = deser.deserializeStream(fs.open(fileToRead))
      val iter = deserStream.asIterator.asInstanceOf[Iterator[(InternalRow, InternalRow)]]
      while (iter.hasNext) {
        map += iter.next()
      }
      Some(map)
    } finally {
      if (deserStream != null) deserStream.close()
    }
  }


  /** Perform a snapshot of the store to allow delta files to be consolidated */
  private def doSnapshot(): Unit = {
    try {
      val files = fetchFiles()
      if (files.nonEmpty) {
        val lastVersion = files.last.version
        val deltaFilesForLastVersion =
          filesForVersion(files, lastVersion).filter(_.isSnapshot == false)
        synchronized {
          loadedMaps.get(lastVersion)
        } match {
          case Some(map) =>
            if (deltaFilesForLastVersion.size > maxDeltaChainForSnapshots) {
              writeSnapshotFile(lastVersion, map)
            }
          case None =>
          // The last map is not loaded, probably some other instance is incharge
        }

      }
    } catch {
      case NonFatal(e) =>
        logWarning(s"Error doing snapshots for $this")
    }
  }

  /**
    * Clean up old snapshots and delta files that are not needed any more. It ensures that last
    * few versions of the store can be recovered from the files, so re-executed RDD operations
    * can re-apply updates on the past versions of the store.
    */
  private[state] def cleanup(): Unit = {
    try {
      val files = fetchFiles()
      if (files.nonEmpty) {
        val earliestVersionToRetain = files.last.version - numBatchesToRetain
        if (earliestVersionToRetain >= 0) {
          val earliestFileToRetain = filesForVersion(files, earliestVersionToRetain).head
          synchronized {
            loadedMaps.keys.filter(_ < earliestVersionToRetain).foreach(loadedMaps.remove)
          }
          files.filter(_.version < earliestFileToRetain.version).foreach { f =>
            fs.delete(f.path, true)
          }
        }
      }
    } catch {
      case NonFatal(e) =>
        logWarning(s"Error cleaning up files for $this")
    }
  }

  private def filesForVersion(allFiles: Seq[StoreFile], version: Long): Seq[StoreFile] = {
    require(version >= 0)
    require(allFiles.exists(_.version == version))

    val latestSnapshotFileBeforeVersion = allFiles
      .filter(_.isSnapshot == true)
      .takeWhile(_.version <= version)
      .lastOption

    val deltaBatchFiles = latestSnapshotFileBeforeVersion match {
      case Some(snapshotFile) =>
        val deltaBatchIds = (snapshotFile.version + 1) to version

        val deltaFiles = allFiles.filter { file =>
          file.version > snapshotFile.version && file.version <= version
        }
        verify(
          deltaFiles.size == version - snapshotFile.version,
          s"Unexpected list of delta files for version $version: ${deltaFiles.mkString(",")}"
        )
        deltaFiles

      case None =>
        allFiles.takeWhile(_.version <= version)
    }
    latestSnapshotFileBeforeVersion.toSeq ++ deltaBatchFiles
  }

  private def fetchFiles(): Seq[StoreFile] = {
    val files: Seq[FileStatus] = try {
      fs.listStatus(baseDir)
    } catch {
      case _: java.io.FileNotFoundException =>
        Seq.empty
    }
    val versionToFiles = new mutable.HashMap[Long, StoreFile]
    files.foreach { status =>
      val path = status.getPath
      val nameParts = path.getName.split("\\.")
      if (nameParts.size == 2) {
        val version = nameParts(0).toLong
        nameParts(1).toLowerCase match {
          case "delta" =>
            // ignore the file otherwise, snapshot file already exists for that batch id
            if (!versionToFiles.contains(version)) {
              versionToFiles.put(version, StoreFile(version, path, isSnapshot = false))
            }
          case "snapshot" =>
            versionToFiles.put(version, StoreFile(version, path, isSnapshot = true))
          case _ =>
            logWarning(s"Could not identify file $path")
        }
      }
    }
    versionToFiles.values.toSeq.sortBy(_.version)
  }

  private def deltaFile(version: Long): Path = {
    new Path(baseDir, s"$version.delta")
  }

  private def snapshotFile(version: Long): Path = {
    new Path(baseDir, s"$version.snapshot")
  }

  private def verify(condition: => Boolean, msg: String): Unit = {
    if (!condition) {
      throw new IllegalStateException(msg)
    }
  }
}
