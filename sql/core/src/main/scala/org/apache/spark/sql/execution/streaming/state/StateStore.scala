/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *    http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

package org.apache.spark.sql.execution.streaming.state

import java.util.{Timer, TimerTask}

import scala.collection.mutable
import scala.reflect.ClassTag
import scala.util.control.NonFatal

import org.apache.spark.{Logging, SparkEnv}
import org.apache.spark.sql.catalyst.InternalRow
import org.apache.spark.util.RpcUtils

/** Unique identifier for a [[StateStore]] */
case class StateStoreId(operatorId: Long, partitionId: Int)

/**
 * Base trait for a versioned key-value store used for streaming aggregations
 */
trait StateStore {

  import StateStore._

  /** Unique identifier of the store */
  def id: StateStoreId

  /** Version of the data in this store before committing updates. */
  def version: Long

  /**
   * Update the value of a key using the value generated by the update function.
   * This can be called only after prepareForUpdates() has been called in the same thread.
   */
  def update(key: InternalRow, updateFunc: Option[InternalRow] => InternalRow): Unit

  /**
   * Remove keys that match the following condition.
   * This can be called only after prepareForUpdates() has been called in the current thread.
   */
  def remove(condition: InternalRow => Boolean): Unit

  /**
   * Commit all the updates that have been made to the store.
   * This can be called only after prepareForUpdates() has been called in the current thread.
   */
  def commit(): Long

  /** Cancel all the updates that have been made to the store. */
  def cancel(): Unit

  /**
   * Iterator of store data after a set of updates have been committed.
   * This can be called only after commitUpdates() has been called in the current thread.
   */
  def iterator(): Iterator[InternalRow]

  /**
   * Iterator of the updates that have been committed.
   * This can be called only after commitUpdates() has been called in the current thread.
   */
  def updates(): Iterator[StoreUpdate]

  /**
   * Whether all updates have been committed
   */
  def hasCommitted: Boolean
}


trait StateStoreProvider {

  /** Get the store with the existing version. */
  def getStore(version: Long): StateStore

  /** Optional method for providers to allow for background management */
  def manage(): Unit = { }
}

sealed trait StoreUpdate
case class ValueAdded(key: InternalRow, value: InternalRow) extends StoreUpdate
case class ValueUpdated(key: InternalRow, value: InternalRow) extends StoreUpdate
case class KeyRemoved(key: InternalRow) extends StoreUpdate


/**
 * Companion object to [[StateStore]] that provides helper methods to create and retrive stores
 * by their unique ids.
 */
private[state] object StateStore extends Logging {

  val MANAGEMENT_TASK_INTERVAL_SECS = 60

  private val loadedStores = new mutable.HashMap[StateStoreId, StateStoreProvider]()
  private val managementTimer = new Timer("StateStore Timer", true)
  @volatile private var managementTask: TimerTask = null

  /** Get or create a store associated with the id. */
  def get(storeId: StateStoreId, directory: String, version: Long): StateStore = {
    val storeProvider = loadedStores.synchronized {
      startIfNeeded()
      loadedStores.getOrElseUpdate(storeId, new HDFSBackedStateStoreProvider(storeId, directory))
    }
    reportActiveInstance(storeId)
    storeProvider.getStore(version)
  }

  def clearAll(): Unit = loadedStores.synchronized {
    loadedStores.clear()
    if (managementTask != null) {
      managementTask.cancel()
      managementTask = null
    }
  }

  private def remove(storeId: StateStoreId): Unit = {
    loadedStores.remove(storeId)
  }

  private def reportActiveInstance(storeId: StateStoreId): Unit = {
    val host = SparkEnv.get.blockManager.blockManagerId.host
    val executorId = SparkEnv.get.blockManager.blockManagerId.executorId
    askCoordinator[Boolean](ReportActiveInstance(storeId, host, executorId))
  }

  private def verifyIfInstanceActive(storeId: StateStoreId): Boolean = {
    val executorId = SparkEnv.get.blockManager.blockManagerId.executorId
    askCoordinator[Boolean](VerifyIfInstanceActive(storeId, executorId)).getOrElse(false)
  }

  private def askCoordinator[T: ClassTag](message: StateStoreCoordinatorMessage): Option[T] = {
    try {
      val env = SparkEnv.get
      if (env != null) {
        val coordinatorRef = RpcUtils.makeDriverRef("StateStoreCoordinator", env.conf, env.rpcEnv)
        Some(coordinatorRef.askWithRetry[T](message))
      } else {
        None
      }
    } catch {
      case NonFatal(e) =>
        clearAll()
        None
    }
  }

  private def startIfNeeded(): Unit = loadedStores.synchronized {
    if (managementTask == null) {
      managementTask = new TimerTask {
        override def run(): Unit = {
          loadedStores.synchronized { loadedStores.values.toSeq }.foreach { store =>
            try {
              store.manage()
            } catch {
              case NonFatal(e) =>
                logWarning(s"Error performing snapshot and cleaning up $store")
            }
          }
        }
      }
      managementTimer.schedule(
        managementTask,
        MANAGEMENT_TASK_INTERVAL_SECS * 1000,
        MANAGEMENT_TASK_INTERVAL_SECS * 1000)
    }
  }
}

