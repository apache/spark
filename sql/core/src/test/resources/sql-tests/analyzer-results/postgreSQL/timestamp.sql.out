-- Automatically generated by SQLQueryTestSuite
-- !query
CREATE TABLE TIMESTAMP_TBL (d1 timestamp) USING parquet
-- !query analysis
CreateDataSourceTableCommand `spark_catalog`.`default`.`TIMESTAMP_TBL`, false


-- !query
INSERT INTO TIMESTAMP_TBL VALUES (timestamp'now')
-- !query analysis
InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/timestamp_tbl, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/timestamp_tbl], Append, `spark_catalog`.`default`.`timestamp_tbl`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/timestamp_tbl), [d1]
+- Project [cast(col1#x as timestamp) AS d1#x]
   +- LocalRelation [col1#x]


-- !query
INSERT INTO TIMESTAMP_TBL VALUES (timestamp'now')
-- !query analysis
InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/timestamp_tbl, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/timestamp_tbl], Append, `spark_catalog`.`default`.`timestamp_tbl`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/timestamp_tbl), [d1]
+- Project [cast(col1#x as timestamp) AS d1#x]
   +- LocalRelation [col1#x]


-- !query
INSERT INTO TIMESTAMP_TBL VALUES (timestamp'today')
-- !query analysis
InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/timestamp_tbl, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/timestamp_tbl], Append, `spark_catalog`.`default`.`timestamp_tbl`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/timestamp_tbl), [d1]
+- Project [cast(col1#x as timestamp) AS d1#x]
   +- LocalRelation [col1#x]


-- !query
INSERT INTO TIMESTAMP_TBL VALUES (timestamp'yesterday')
-- !query analysis
InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/timestamp_tbl, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/timestamp_tbl], Append, `spark_catalog`.`default`.`timestamp_tbl`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/timestamp_tbl), [d1]
+- Project [cast(col1#x as timestamp) AS d1#x]
   +- LocalRelation [col1#x]


-- !query
INSERT INTO TIMESTAMP_TBL VALUES (timestamp'tomorrow')
-- !query analysis
InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/timestamp_tbl, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/timestamp_tbl], Append, `spark_catalog`.`default`.`timestamp_tbl`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/timestamp_tbl), [d1]
+- Project [cast(col1#x as timestamp) AS d1#x]
   +- LocalRelation [col1#x]


-- !query
INSERT INTO TIMESTAMP_TBL VALUES (timestamp'tomorrow EST')
-- !query analysis
InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/timestamp_tbl, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/timestamp_tbl], Append, `spark_catalog`.`default`.`timestamp_tbl`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/timestamp_tbl), [d1]
+- Project [cast(col1#x as timestamp) AS d1#x]
   +- LocalRelation [col1#x]


-- !query
INSERT INTO TIMESTAMP_TBL VALUES (timestamp'tomorrow Zulu')
-- !query analysis
InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/timestamp_tbl, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/timestamp_tbl], Append, `spark_catalog`.`default`.`timestamp_tbl`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/timestamp_tbl), [d1]
+- Project [cast(col1#x as timestamp) AS d1#x]
   +- LocalRelation [col1#x]


-- !query
SELECT count(*) AS One FROM TIMESTAMP_TBL WHERE d1 = timestamp 'today'
-- !query analysis
[Analyzer test output redacted due to nondeterminism]


-- !query
SELECT count(*) AS Three FROM TIMESTAMP_TBL WHERE d1 = timestamp 'tomorrow'
-- !query analysis
[Analyzer test output redacted due to nondeterminism]


-- !query
SELECT count(*) AS One FROM TIMESTAMP_TBL WHERE d1 = timestamp 'yesterday'
-- !query analysis
[Analyzer test output redacted due to nondeterminism]


-- !query
TRUNCATE TABLE TIMESTAMP_TBL
-- !query analysis
TruncateTableCommand `spark_catalog`.`default`.`timestamp_tbl`


-- !query
INSERT INTO TIMESTAMP_TBL VALUES (timestamp'epoch')
-- !query analysis
InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/timestamp_tbl, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/timestamp_tbl], Append, `spark_catalog`.`default`.`timestamp_tbl`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/timestamp_tbl), [d1]
+- Project [cast(col1#x as timestamp) AS d1#x]
   +- LocalRelation [col1#x]


-- !query
INSERT INTO TIMESTAMP_TBL VALUES (timestamp('1997-01-02'))
-- !query analysis
InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/timestamp_tbl, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/timestamp_tbl], Append, `spark_catalog`.`default`.`timestamp_tbl`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/timestamp_tbl), [d1]
+- Project [cast(col1#x as timestamp) AS d1#x]
   +- LocalRelation [col1#x]


-- !query
INSERT INTO TIMESTAMP_TBL VALUES (timestamp('1997-01-02 03:04:05'))
-- !query analysis
InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/timestamp_tbl, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/timestamp_tbl], Append, `spark_catalog`.`default`.`timestamp_tbl`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/timestamp_tbl), [d1]
+- Project [cast(col1#x as timestamp) AS d1#x]
   +- LocalRelation [col1#x]


-- !query
INSERT INTO TIMESTAMP_TBL VALUES (timestamp('1997-02-10 17:32:01-08'))
-- !query analysis
InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/timestamp_tbl, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/timestamp_tbl], Append, `spark_catalog`.`default`.`timestamp_tbl`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/timestamp_tbl), [d1]
+- Project [cast(col1#x as timestamp) AS d1#x]
   +- LocalRelation [col1#x]


-- !query
INSERT INTO TIMESTAMP_TBL VALUES (timestamp('2001-09-22T18:19:20'))
-- !query analysis
InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/timestamp_tbl, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/timestamp_tbl], Append, `spark_catalog`.`default`.`timestamp_tbl`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/timestamp_tbl), [d1]
+- Project [cast(col1#x as timestamp) AS d1#x]
   +- LocalRelation [col1#x]


-- !query
SELECT '' AS `64`, d1 FROM TIMESTAMP_TBL
-- !query analysis
Project [ AS 64#x, d1#x]
+- SubqueryAlias spark_catalog.default.timestamp_tbl
   +- Relation spark_catalog.default.timestamp_tbl[d1#x] parquet


-- !query
SELECT '' AS `48`, d1 FROM TIMESTAMP_TBL
   WHERE d1 > timestamp '1997-01-02'
-- !query analysis
[Analyzer test output redacted due to nondeterminism]


-- !query
SELECT '' AS `15`, d1 FROM TIMESTAMP_TBL
   WHERE d1 < timestamp '1997-01-02'
-- !query analysis
[Analyzer test output redacted due to nondeterminism]


-- !query
SELECT '' AS one, d1 FROM TIMESTAMP_TBL
   WHERE d1 = timestamp '1997-01-02'
-- !query analysis
[Analyzer test output redacted due to nondeterminism]


-- !query
SELECT '' AS `63`, d1 FROM TIMESTAMP_TBL
   WHERE d1 != timestamp '1997-01-02'
-- !query analysis
[Analyzer test output redacted due to nondeterminism]


-- !query
SELECT '' AS `16`, d1 FROM TIMESTAMP_TBL
   WHERE d1 <= timestamp '1997-01-02'
-- !query analysis
[Analyzer test output redacted due to nondeterminism]


-- !query
SELECT '' AS `49`, d1 FROM TIMESTAMP_TBL
   WHERE d1 >= timestamp '1997-01-02'
-- !query analysis
[Analyzer test output redacted due to nondeterminism]


-- !query
SELECT '' AS `54`, d1 - timestamp '1997-01-02' AS diff
   FROM TIMESTAMP_TBL WHERE d1 BETWEEN '1902-01-01' AND '2038-01-01'
-- !query analysis
[Analyzer test output redacted due to nondeterminism]


-- !query
SELECT '' AS date_trunc_week, date_trunc( 'week', timestamp '2004-02-29 15:44:17.71393' ) AS week_trunc
-- !query analysis
[Analyzer test output redacted due to nondeterminism]


-- !query
SELECT '' AS `54`, d1 - timestamp '1997-01-02' AS diff
  FROM TIMESTAMP_TBL
  WHERE d1 BETWEEN timestamp '1902-01-01'
   AND timestamp '2038-01-01'
-- !query analysis
[Analyzer test output redacted due to nondeterminism]


-- !query
SELECT '' AS `54`, d1 as `timestamp`,
    date_part( 'year', d1) AS `year`, date_part( 'month', d1) AS `month`,
    date_part( 'day', d1) AS `day`, date_part( 'hour', d1) AS `hour`,
    date_part( 'minute', d1) AS `minute`, date_part( 'second', d1) AS `second`
    FROM TIMESTAMP_TBL WHERE d1 BETWEEN '1902-01-01' AND '2038-01-01'
-- !query analysis
Project [ AS 54#x, d1#x AS timestamp#x, date_part(year, d1#x) AS year#x, date_part(month, d1#x) AS month#x, date_part(day, d1#x) AS day#x, date_part(hour, d1#x) AS hour#x, date_part(minute, d1#x) AS minute#x, date_part(second, d1#x) AS second#x]
+- Filter ((d1#x >= cast(1902-01-01 as timestamp)) AND (d1#x <= cast(2038-01-01 as timestamp)))
   +- SubqueryAlias spark_catalog.default.timestamp_tbl
      +- Relation spark_catalog.default.timestamp_tbl[d1#x] parquet


-- !query
SELECT make_timestamp(2014,12,28,6,30,45.887)
-- !query analysis
Project [make_timestamp(2014, 12, 28, 6, 30, cast(45.887 as decimal(16,6)), None, Some(America/Los_Angeles), true, TimestampType) AS make_timestamp(2014, 12, 28, 6, 30, 45.887)#x]
+- OneRowRelation


-- !query
DROP TABLE TIMESTAMP_TBL
-- !query analysis
DropTable false, false
+- ResolvedIdentifier V2SessionCatalog(spark_catalog), default.TIMESTAMP_TBL
