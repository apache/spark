-- Automatically generated by SQLQueryTestSuite
-- !query
WITH q1(x,y) AS (SELECT 1,2)
SELECT * FROM q1, q1 AS q2
-- !query analysis
WithCTE
:- CTERelationDef xxxx, false
:  +- SubqueryAlias q1
:     +- Project [1#x AS x#x, 2#x AS y#x]
:        +- Project [1 AS 1#x, 2 AS 2#x]
:           +- OneRowRelation
+- Project [x#x, y#x, x#x, y#x]
   +- Join Inner
      :- SubqueryAlias q1
      :  +- CTERelationRef xxxx, true, [x#x, y#x], false
      +- SubqueryAlias q2
         +- SubqueryAlias q1
            +- CTERelationRef xxxx, true, [x#x, y#x], false


-- !query
SELECT count(*) FROM (
  WITH q1(x) AS (SELECT rand() FROM (SELECT EXPLODE(SEQUENCE(1, 5))))
    SELECT * FROM q1
  UNION
    SELECT * FROM q1
) ss
-- !query analysis
[Analyzer test output redacted due to nondeterminism]


-- !query
CREATE TABLE department (
	id INTEGER,  -- department ID
	parent_department INTEGER, -- upper department ID
	name string -- department name
) USING parquet
-- !query analysis
CreateDataSourceTableCommand `spark_catalog`.`default`.`department`, false


-- !query
INSERT INTO department VALUES (0, NULL, 'ROOT')
-- !query analysis
InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/department, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/department], Append, `spark_catalog`.`default`.`department`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/department), [id, parent_department, name]
+- Project [cast(col1#x as int) AS id#x, cast(col2#x as int) AS parent_department#x, cast(col3#x as string) AS name#x]
   +- LocalRelation [col1#x, col2#x, col3#x]


-- !query
INSERT INTO department VALUES (1, 0, 'A')
-- !query analysis
InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/department, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/department], Append, `spark_catalog`.`default`.`department`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/department), [id, parent_department, name]
+- Project [cast(col1#x as int) AS id#x, cast(col2#x as int) AS parent_department#x, cast(col3#x as string) AS name#x]
   +- LocalRelation [col1#x, col2#x, col3#x]


-- !query
INSERT INTO department VALUES (2, 1, 'B')
-- !query analysis
InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/department, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/department], Append, `spark_catalog`.`default`.`department`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/department), [id, parent_department, name]
+- Project [cast(col1#x as int) AS id#x, cast(col2#x as int) AS parent_department#x, cast(col3#x as string) AS name#x]
   +- LocalRelation [col1#x, col2#x, col3#x]


-- !query
INSERT INTO department VALUES (3, 2, 'C')
-- !query analysis
InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/department, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/department], Append, `spark_catalog`.`default`.`department`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/department), [id, parent_department, name]
+- Project [cast(col1#x as int) AS id#x, cast(col2#x as int) AS parent_department#x, cast(col3#x as string) AS name#x]
   +- LocalRelation [col1#x, col2#x, col3#x]


-- !query
INSERT INTO department VALUES (4, 2, 'D')
-- !query analysis
InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/department, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/department], Append, `spark_catalog`.`default`.`department`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/department), [id, parent_department, name]
+- Project [cast(col1#x as int) AS id#x, cast(col2#x as int) AS parent_department#x, cast(col3#x as string) AS name#x]
   +- LocalRelation [col1#x, col2#x, col3#x]


-- !query
INSERT INTO department VALUES (5, 0, 'E')
-- !query analysis
InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/department, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/department], Append, `spark_catalog`.`default`.`department`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/department), [id, parent_department, name]
+- Project [cast(col1#x as int) AS id#x, cast(col2#x as int) AS parent_department#x, cast(col3#x as string) AS name#x]
   +- LocalRelation [col1#x, col2#x, col3#x]


-- !query
INSERT INTO department VALUES (6, 4, 'F')
-- !query analysis
InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/department, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/department], Append, `spark_catalog`.`default`.`department`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/department), [id, parent_department, name]
+- Project [cast(col1#x as int) AS id#x, cast(col2#x as int) AS parent_department#x, cast(col3#x as string) AS name#x]
   +- LocalRelation [col1#x, col2#x, col3#x]


-- !query
INSERT INTO department VALUES (7, 5, 'G')
-- !query analysis
InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/department, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/department], Append, `spark_catalog`.`default`.`department`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/department), [id, parent_department, name]
+- Project [cast(col1#x as int) AS id#x, cast(col2#x as int) AS parent_department#x, cast(col3#x as string) AS name#x]
   +- LocalRelation [col1#x, col2#x, col3#x]


-- !query
CREATE TABLE tree(
    id INTEGER,
    parent_id INTEGER
) USING parquet
-- !query analysis
CreateDataSourceTableCommand `spark_catalog`.`default`.`tree`, false


-- !query
INSERT INTO tree
VALUES (1, NULL), (2, 1), (3,1), (4,2), (5,2), (6,2), (7,3), (8,3),
       (9,4), (10,4), (11,7), (12,7), (13,7), (14, 9), (15,11), (16,11)
-- !query analysis
InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/tree, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/tree], Append, `spark_catalog`.`default`.`tree`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/tree), [id, parent_id]
+- Project [cast(col1#x as int) AS id#x, cast(col2#x as int) AS parent_id#x]
   +- LocalRelation [col1#x, col2#x]


-- !query
create table graph( f int, t int, label string ) USING parquet
-- !query analysis
CreateDataSourceTableCommand `spark_catalog`.`default`.`graph`, false


-- !query
insert into graph values
	(1, 2, 'arc 1 -> 2'),
	(1, 3, 'arc 1 -> 3'),
	(2, 3, 'arc 2 -> 3'),
	(1, 4, 'arc 1 -> 4'),
	(4, 5, 'arc 4 -> 5'),
	(5, 1, 'arc 5 -> 1')
-- !query analysis
InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/graph, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/graph], Append, `spark_catalog`.`default`.`graph`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/graph), [f, t, label]
+- Project [cast(col1#x as int) AS f#x, cast(col2#x as int) AS t#x, cast(col3#x as string) AS label#x]
   +- LocalRelation [col1#x, col2#x, col3#x]


-- !query
CREATE TABLE y (a INTEGER) USING parquet
-- !query analysis
CreateDataSourceTableCommand `spark_catalog`.`default`.`y`, false


-- !query
INSERT INTO y SELECT EXPLODE(SEQUENCE(1, 10))
-- !query analysis
InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/y, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/y], Append, `spark_catalog`.`default`.`y`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/y), [a]
+- Project [cast(col#x as int) AS a#x]
   +- Project [col#x]
      +- Generate explode(sequence(1, 10, None, Some(America/Los_Angeles))), false, [col#x]
         +- OneRowRelation


-- !query
DROP TABLE y
-- !query analysis
DropTable false, false
+- ResolvedIdentifier V2SessionCatalog(spark_catalog), default.y


-- !query
CREATE TABLE y (a INTEGER) USING parquet
-- !query analysis
CreateDataSourceTableCommand `spark_catalog`.`default`.`y`, false


-- !query
INSERT INTO y SELECT EXPLODE(SEQUENCE(1, 10))
-- !query analysis
InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/y, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/y], Append, `spark_catalog`.`default`.`y`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/y), [a]
+- Project [cast(col#x as int) AS a#x]
   +- Project [col#x]
      +- Generate explode(sequence(1, 10, None, Some(America/Los_Angeles))), false, [col#x]
         +- OneRowRelation


-- !query
with cte(foo) as ( select 42 ) select * from ((select foo from cte)) q
-- !query analysis
WithCTE
:- CTERelationDef xxxx, false
:  +- SubqueryAlias cte
:     +- Project [42#x AS foo#x]
:        +- Project [42 AS 42#x]
:           +- OneRowRelation
+- Project [foo#x]
   +- SubqueryAlias q
      +- Project [foo#x]
         +- SubqueryAlias cte
            +- CTERelationRef xxxx, true, [foo#x], false


-- !query
WITH outermost(x) AS (
  SELECT 1
  UNION (WITH innermost as (SELECT 2)
         SELECT * FROM innermost
         UNION SELECT 3)
)
SELECT * FROM outermost ORDER BY 1
-- !query analysis
WithCTE
:- CTERelationDef xxxx, false
:  +- SubqueryAlias innermost
:     +- Project [2 AS 2#x]
:        +- OneRowRelation
:- CTERelationDef xxxx, false
:  +- SubqueryAlias outermost
:     +- Project [1#x AS x#x]
:        +- Distinct
:           +- Union false, false
:              :- Project [1 AS 1#x]
:              :  +- OneRowRelation
:              +- Distinct
:                 +- Union false, false
:                    :- Project [2#x]
:                    :  +- SubqueryAlias innermost
:                    :     +- CTERelationRef xxxx, true, [2#x], false
:                    +- Project [3 AS 3#x]
:                       +- OneRowRelation
+- Sort [x#x ASC NULLS FIRST], true
   +- Project [x#x]
      +- SubqueryAlias outermost
         +- CTERelationRef xxxx, true, [x#x], false


-- !query
WITH outermost(x) AS (
  SELECT 1
  UNION (WITH innermost as (SELECT 2)
         SELECT * FROM outermost  -- fail
         UNION SELECT * FROM innermost)
)
SELECT * FROM outermost ORDER BY 1
-- !query analysis
org.apache.spark.sql.catalyst.ExtendedAnalysisException
{
  "errorClass" : "TABLE_OR_VIEW_NOT_FOUND",
  "sqlState" : "42P01",
  "messageParameters" : {
    "relationName" : "`outermost`"
  },
  "queryContext" : [ {
    "objectType" : "",
    "objectName" : "",
    "startIndex" : 96,
    "stopIndex" : 104,
    "fragment" : "outermost"
  } ]
}


-- !query
CREATE TABLE withz USING parquet AS SELECT i AS k, CAST(i AS string) || ' v' AS v FROM (SELECT EXPLODE(SEQUENCE(1, 16, 3)) i)
-- !query analysis
CreateDataSourceTableAsSelectCommand `spark_catalog`.`default`.`withz`, ErrorIfExists, [k, v]
   +- Project [i#x AS k#x, concat(cast(i#x as string),  v) AS v#x]
      +- SubqueryAlias __auto_generated_subquery_name
         +- Project [i#x]
            +- Generate explode(sequence(1, 16, Some(3), Some(America/Los_Angeles))), false, [i#x]
               +- OneRowRelation


-- !query
SELECT * FROM withz ORDER BY k
-- !query analysis
Sort [k#x ASC NULLS FIRST], true
+- Project [k#x, v#x]
   +- SubqueryAlias spark_catalog.default.withz
      +- Relation spark_catalog.default.withz[k#x,v#x] parquet


-- !query
DROP TABLE withz
-- !query analysis
DropTable false, false
+- ResolvedIdentifier V2SessionCatalog(spark_catalog), default.withz


-- !query
TRUNCATE TABLE y
-- !query analysis
TruncateTableCommand `spark_catalog`.`default`.`y`


-- !query
INSERT INTO y SELECT EXPLODE(SEQUENCE(1, 3))
-- !query analysis
InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/y, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/y], Append, `spark_catalog`.`default`.`y`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/y), [a]
+- Project [cast(col#x as int) AS a#x]
   +- Project [col#x]
      +- Generate explode(sequence(1, 3, None, Some(America/Los_Angeles))), false, [col#x]
         +- OneRowRelation


-- !query
CREATE TABLE yy (a INTEGER) USING parquet
-- !query analysis
CreateDataSourceTableCommand `spark_catalog`.`default`.`yy`, false


-- !query
SELECT * FROM y
-- !query analysis
Project [a#x]
+- SubqueryAlias spark_catalog.default.y
   +- Relation spark_catalog.default.y[a#x] parquet


-- !query
SELECT * FROM yy
-- !query analysis
Project [a#x]
+- SubqueryAlias spark_catalog.default.yy
   +- Relation spark_catalog.default.yy[a#x] parquet


-- !query
SELECT * FROM y
-- !query analysis
Project [a#x]
+- SubqueryAlias spark_catalog.default.y
   +- Relation spark_catalog.default.y[a#x] parquet


-- !query
SELECT * FROM yy
-- !query analysis
Project [a#x]
+- SubqueryAlias spark_catalog.default.yy
   +- Relation spark_catalog.default.yy[a#x] parquet


-- !query
CREATE TABLE parent ( id int, val string ) USING parquet
-- !query analysis
CreateDataSourceTableCommand `spark_catalog`.`default`.`parent`, false


-- !query
INSERT INTO parent VALUES ( 1, 'p1' )
-- !query analysis
InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/parent, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/parent], Append, `spark_catalog`.`default`.`parent`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/parent), [id, val]
+- Project [cast(col1#x as int) AS id#x, cast(col2#x as string) AS val#x]
   +- LocalRelation [col1#x, col2#x]


-- !query
SELECT * FROM parent
-- !query analysis
Project [id#x, val#x]
+- SubqueryAlias spark_catalog.default.parent
   +- Relation spark_catalog.default.parent[id#x,val#x] parquet


-- !query
SELECT * FROM parent
-- !query analysis
Project [id#x, val#x]
+- SubqueryAlias spark_catalog.default.parent
   +- Relation spark_catalog.default.parent[id#x,val#x] parquet


-- !query
create table foo (with baz)
-- !query analysis
org.apache.spark.sql.catalyst.parser.ParseException
{
  "errorClass" : "UNSUPPORTED_DATATYPE",
  "sqlState" : "0A000",
  "messageParameters" : {
    "typeName" : "\"BAZ\""
  },
  "queryContext" : [ {
    "objectType" : "",
    "objectName" : "",
    "startIndex" : 24,
    "stopIndex" : 26,
    "fragment" : "baz"
  } ]
}


-- !query
create table foo (with ordinality)
-- !query analysis
org.apache.spark.sql.catalyst.parser.ParseException
{
  "errorClass" : "UNSUPPORTED_DATATYPE",
  "sqlState" : "0A000",
  "messageParameters" : {
    "typeName" : "\"ORDINALITY\""
  },
  "queryContext" : [ {
    "objectType" : "",
    "objectName" : "",
    "startIndex" : 24,
    "stopIndex" : 33,
    "fragment" : "ordinality"
  } ]
}


-- !query
with ordinality as (select 1 as x) select * from ordinality
-- !query analysis
WithCTE
:- CTERelationDef xxxx, false
:  +- SubqueryAlias ordinality
:     +- Project [1 AS x#x]
:        +- OneRowRelation
+- Project [x#x]
   +- SubqueryAlias ordinality
      +- CTERelationRef xxxx, true, [x#x], false


-- !query
WITH test AS (SELECT 42) INSERT INTO test VALUES (1)
-- !query analysis
org.apache.spark.sql.catalyst.ExtendedAnalysisException
{
  "errorClass" : "TABLE_OR_VIEW_NOT_FOUND",
  "sqlState" : "42P01",
  "messageParameters" : {
    "relationName" : "`test`"
  },
  "queryContext" : [ {
    "objectType" : "",
    "objectName" : "",
    "startIndex" : 38,
    "stopIndex" : 41,
    "fragment" : "test"
  } ]
}


-- !query
create table test (i int) USING parquet
-- !query analysis
CreateDataSourceTableCommand `spark_catalog`.`default`.`test`, false


-- !query
with test as (select 42) insert into test select * from test
-- !query analysis
InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/test, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/test], Append, `spark_catalog`.`default`.`test`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/test), [i]
+- Project [cast(42#x as int) AS i#x]
   +- Project [42#x]
      +- SubqueryAlias test
         +- Project [42 AS 42#x]
            +- OneRowRelation


-- !query
select * from test
-- !query analysis
Project [i#x]
+- SubqueryAlias spark_catalog.default.test
   +- Relation spark_catalog.default.test[i#x] parquet


-- !query
drop table test
-- !query analysis
DropTable false, false
+- ResolvedIdentifier V2SessionCatalog(spark_catalog), default.test


-- !query
DROP TABLE department
-- !query analysis
DropTable false, false
+- ResolvedIdentifier V2SessionCatalog(spark_catalog), default.department


-- !query
DROP TABLE tree
-- !query analysis
DropTable false, false
+- ResolvedIdentifier V2SessionCatalog(spark_catalog), default.tree


-- !query
DROP TABLE graph
-- !query analysis
DropTable false, false
+- ResolvedIdentifier V2SessionCatalog(spark_catalog), default.graph


-- !query
DROP TABLE y
-- !query analysis
DropTable false, false
+- ResolvedIdentifier V2SessionCatalog(spark_catalog), default.y


-- !query
DROP TABLE yy
-- !query analysis
DropTable false, false
+- ResolvedIdentifier V2SessionCatalog(spark_catalog), default.yy


-- !query
DROP TABLE parent
-- !query analysis
DropTable false, false
+- ResolvedIdentifier V2SessionCatalog(spark_catalog), default.parent
