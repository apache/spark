-- Automatically generated by SQLQueryTestSuite
-- !query
CREATE TABLE test_missing_target (a int, b int, c string, d string) using parquet
-- !query analysis
CreateDataSourceTableCommand `spark_catalog`.`default`.`test_missing_target`, false


-- !query
INSERT INTO test_missing_target VALUES (0, 1, 'XXXX', 'A')
-- !query analysis
InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/test_missing_target, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/test_missing_target], Append, `spark_catalog`.`default`.`test_missing_target`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/test_missing_target), [a, b, c, d]
+- Project [cast(col1#x as int) AS a#x, cast(col2#x as int) AS b#x, cast(col3#x as string) AS c#x, cast(col4#x as string) AS d#x]
   +- LocalRelation [col1#x, col2#x, col3#x, col4#x]


-- !query
INSERT INTO test_missing_target VALUES (1, 2, 'ABAB', 'b')
-- !query analysis
InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/test_missing_target, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/test_missing_target], Append, `spark_catalog`.`default`.`test_missing_target`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/test_missing_target), [a, b, c, d]
+- Project [cast(col1#x as int) AS a#x, cast(col2#x as int) AS b#x, cast(col3#x as string) AS c#x, cast(col4#x as string) AS d#x]
   +- LocalRelation [col1#x, col2#x, col3#x, col4#x]


-- !query
INSERT INTO test_missing_target VALUES (2, 2, 'ABAB', 'c')
-- !query analysis
InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/test_missing_target, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/test_missing_target], Append, `spark_catalog`.`default`.`test_missing_target`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/test_missing_target), [a, b, c, d]
+- Project [cast(col1#x as int) AS a#x, cast(col2#x as int) AS b#x, cast(col3#x as string) AS c#x, cast(col4#x as string) AS d#x]
   +- LocalRelation [col1#x, col2#x, col3#x, col4#x]


-- !query
INSERT INTO test_missing_target VALUES (3, 3, 'BBBB', 'D')
-- !query analysis
InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/test_missing_target, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/test_missing_target], Append, `spark_catalog`.`default`.`test_missing_target`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/test_missing_target), [a, b, c, d]
+- Project [cast(col1#x as int) AS a#x, cast(col2#x as int) AS b#x, cast(col3#x as string) AS c#x, cast(col4#x as string) AS d#x]
   +- LocalRelation [col1#x, col2#x, col3#x, col4#x]


-- !query
INSERT INTO test_missing_target VALUES (4, 3, 'BBBB', 'e')
-- !query analysis
InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/test_missing_target, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/test_missing_target], Append, `spark_catalog`.`default`.`test_missing_target`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/test_missing_target), [a, b, c, d]
+- Project [cast(col1#x as int) AS a#x, cast(col2#x as int) AS b#x, cast(col3#x as string) AS c#x, cast(col4#x as string) AS d#x]
   +- LocalRelation [col1#x, col2#x, col3#x, col4#x]


-- !query
INSERT INTO test_missing_target VALUES (5, 3, 'bbbb', 'F')
-- !query analysis
InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/test_missing_target, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/test_missing_target], Append, `spark_catalog`.`default`.`test_missing_target`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/test_missing_target), [a, b, c, d]
+- Project [cast(col1#x as int) AS a#x, cast(col2#x as int) AS b#x, cast(col3#x as string) AS c#x, cast(col4#x as string) AS d#x]
   +- LocalRelation [col1#x, col2#x, col3#x, col4#x]


-- !query
INSERT INTO test_missing_target VALUES (6, 4, 'cccc', 'g')
-- !query analysis
InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/test_missing_target, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/test_missing_target], Append, `spark_catalog`.`default`.`test_missing_target`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/test_missing_target), [a, b, c, d]
+- Project [cast(col1#x as int) AS a#x, cast(col2#x as int) AS b#x, cast(col3#x as string) AS c#x, cast(col4#x as string) AS d#x]
   +- LocalRelation [col1#x, col2#x, col3#x, col4#x]


-- !query
INSERT INTO test_missing_target VALUES (7, 4, 'cccc', 'h')
-- !query analysis
InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/test_missing_target, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/test_missing_target], Append, `spark_catalog`.`default`.`test_missing_target`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/test_missing_target), [a, b, c, d]
+- Project [cast(col1#x as int) AS a#x, cast(col2#x as int) AS b#x, cast(col3#x as string) AS c#x, cast(col4#x as string) AS d#x]
   +- LocalRelation [col1#x, col2#x, col3#x, col4#x]


-- !query
INSERT INTO test_missing_target VALUES (8, 4, 'CCCC', 'I')
-- !query analysis
InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/test_missing_target, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/test_missing_target], Append, `spark_catalog`.`default`.`test_missing_target`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/test_missing_target), [a, b, c, d]
+- Project [cast(col1#x as int) AS a#x, cast(col2#x as int) AS b#x, cast(col3#x as string) AS c#x, cast(col4#x as string) AS d#x]
   +- LocalRelation [col1#x, col2#x, col3#x, col4#x]


-- !query
INSERT INTO test_missing_target VALUES (9, 4, 'CCCC', 'j')
-- !query analysis
InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/test_missing_target, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/test_missing_target], Append, `spark_catalog`.`default`.`test_missing_target`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/test_missing_target), [a, b, c, d]
+- Project [cast(col1#x as int) AS a#x, cast(col2#x as int) AS b#x, cast(col3#x as string) AS c#x, cast(col4#x as string) AS d#x]
   +- LocalRelation [col1#x, col2#x, col3#x, col4#x]


-- !query
SELECT c, count(*) FROM test_missing_target GROUP BY test_missing_target.c ORDER BY c
-- !query analysis
Sort [c#x ASC NULLS FIRST], true
+- Aggregate [c#x], [c#x, count(1) AS count(1)#xL]
   +- SubqueryAlias spark_catalog.default.test_missing_target
      +- Relation spark_catalog.default.test_missing_target[a#x,b#x,c#x,d#x] parquet


-- !query
SELECT count(*) FROM test_missing_target GROUP BY test_missing_target.c ORDER BY c
-- !query analysis
Project [count(1)#xL]
+- Sort [c#x ASC NULLS FIRST], true
   +- Aggregate [c#x], [count(1) AS count(1)#xL, c#x]
      +- SubqueryAlias spark_catalog.default.test_missing_target
         +- Relation spark_catalog.default.test_missing_target[a#x,b#x,c#x,d#x] parquet


-- !query
SELECT count(*) FROM test_missing_target GROUP BY a ORDER BY b
-- !query analysis
org.apache.spark.sql.catalyst.ExtendedAnalysisException
{
  "errorClass" : "UNRESOLVED_COLUMN.WITH_SUGGESTION",
  "sqlState" : "42703",
  "messageParameters" : {
    "objectName" : "`b`",
    "proposal" : "`count(1)`"
  },
  "queryContext" : [ {
    "objectType" : "",
    "objectName" : "",
    "startIndex" : 62,
    "stopIndex" : 62,
    "fragment" : "b"
  } ]
}


-- !query
SELECT count(*) FROM test_missing_target GROUP BY b ORDER BY b
-- !query analysis
Project [count(1)#xL]
+- Sort [b#x ASC NULLS FIRST], true
   +- Aggregate [b#x], [count(1) AS count(1)#xL, b#x]
      +- SubqueryAlias spark_catalog.default.test_missing_target
         +- Relation spark_catalog.default.test_missing_target[a#x,b#x,c#x,d#x] parquet


-- !query
SELECT test_missing_target.b, count(*)
  FROM test_missing_target GROUP BY b ORDER BY b
-- !query analysis
Sort [b#x ASC NULLS FIRST], true
+- Aggregate [b#x], [b#x, count(1) AS count(1)#xL]
   +- SubqueryAlias spark_catalog.default.test_missing_target
      +- Relation spark_catalog.default.test_missing_target[a#x,b#x,c#x,d#x] parquet


-- !query
SELECT c FROM test_missing_target ORDER BY a
-- !query analysis
Project [c#x]
+- Sort [a#x ASC NULLS FIRST], true
   +- Project [c#x, a#x]
      +- SubqueryAlias spark_catalog.default.test_missing_target
         +- Relation spark_catalog.default.test_missing_target[a#x,b#x,c#x,d#x] parquet


-- !query
SELECT count(*) FROM test_missing_target GROUP BY b ORDER BY b desc
-- !query analysis
Project [count(1)#xL]
+- Sort [b#x DESC NULLS LAST], true
   +- Aggregate [b#x], [count(1) AS count(1)#xL, b#x]
      +- SubqueryAlias spark_catalog.default.test_missing_target
         +- Relation spark_catalog.default.test_missing_target[a#x,b#x,c#x,d#x] parquet


-- !query
SELECT count(*) FROM test_missing_target ORDER BY 1 desc
-- !query analysis
Sort [count(1)#xL DESC NULLS LAST], true
+- Aggregate [count(1) AS count(1)#xL]
   +- SubqueryAlias spark_catalog.default.test_missing_target
      +- Relation spark_catalog.default.test_missing_target[a#x,b#x,c#x,d#x] parquet


-- !query
SELECT c, count(*) FROM test_missing_target GROUP BY 1 ORDER BY 1
-- !query analysis
Sort [c#x ASC NULLS FIRST], true
+- Aggregate [c#x], [c#x, count(1) AS count(1)#xL]
   +- SubqueryAlias spark_catalog.default.test_missing_target
      +- Relation spark_catalog.default.test_missing_target[a#x,b#x,c#x,d#x] parquet


-- !query
SELECT c, count(*) FROM test_missing_target GROUP BY 3
-- !query analysis
org.apache.spark.sql.AnalysisException
{
  "errorClass" : "GROUP_BY_POS_OUT_OF_RANGE",
  "sqlState" : "42805",
  "messageParameters" : {
    "index" : "3",
    "size" : "2"
  },
  "queryContext" : [ {
    "objectType" : "",
    "objectName" : "",
    "startIndex" : 54,
    "stopIndex" : 54,
    "fragment" : "3"
  } ]
}


-- !query
SELECT count(*) FROM test_missing_target x, test_missing_target y
	WHERE x.a = y.a
	GROUP BY b ORDER BY b
-- !query analysis
org.apache.spark.sql.AnalysisException
{
  "errorClass" : "AMBIGUOUS_REFERENCE",
  "sqlState" : "42704",
  "messageParameters" : {
    "name" : "`b`",
    "referenceNames" : "[`x`.`b`, `y`.`b`]"
  },
  "queryContext" : [ {
    "objectType" : "",
    "objectName" : "",
    "startIndex" : 94,
    "stopIndex" : 94,
    "fragment" : "b"
  } ]
}


-- !query
SELECT a, a FROM test_missing_target
	ORDER BY a
-- !query analysis
Sort [a#x ASC NULLS FIRST], true
+- Project [a#x, a#x]
   +- SubqueryAlias spark_catalog.default.test_missing_target
      +- Relation spark_catalog.default.test_missing_target[a#x,b#x,c#x,d#x] parquet


-- !query
SELECT a/2, a/2 FROM test_missing_target
	ORDER BY a/2
-- !query analysis
Project [(a / 2)#x, (a / 2)#x]
+- Sort [(cast(a#x as double) / cast(2 as double)) ASC NULLS FIRST], true
   +- Project [(cast(a#x as double) / cast(2 as double)) AS (a / 2)#x, (cast(a#x as double) / cast(2 as double)) AS (a / 2)#x, a#x]
      +- SubqueryAlias spark_catalog.default.test_missing_target
         +- Relation spark_catalog.default.test_missing_target[a#x,b#x,c#x,d#x] parquet


-- !query
SELECT a/2, a/2 FROM test_missing_target
	GROUP BY a/2 ORDER BY a/2
-- !query analysis
Sort [(a / 2)#x ASC NULLS FIRST], true
+- Aggregate [(cast(a#x as double) / cast(2 as double))], [(cast(a#x as double) / cast(2 as double)) AS (a / 2)#x, (cast(a#x as double) / cast(2 as double)) AS (a / 2)#x]
   +- SubqueryAlias spark_catalog.default.test_missing_target
      +- Relation spark_catalog.default.test_missing_target[a#x,b#x,c#x,d#x] parquet


-- !query
SELECT x.b, count(*) FROM test_missing_target x, test_missing_target y
	WHERE x.a = y.a
	GROUP BY x.b ORDER BY x.b
-- !query analysis
Sort [b#x ASC NULLS FIRST], true
+- Aggregate [b#x], [b#x, count(1) AS count(1)#xL]
   +- Filter (a#x = a#x)
      +- Join Inner
         :- SubqueryAlias x
         :  +- SubqueryAlias spark_catalog.default.test_missing_target
         :     +- Relation spark_catalog.default.test_missing_target[a#x,b#x,c#x,d#x] parquet
         +- SubqueryAlias y
            +- SubqueryAlias spark_catalog.default.test_missing_target
               +- Relation spark_catalog.default.test_missing_target[a#x,b#x,c#x,d#x] parquet


-- !query
SELECT count(*) FROM test_missing_target x, test_missing_target y
	WHERE x.a = y.a
	GROUP BY x.b ORDER BY x.b
-- !query analysis
Project [count(1)#xL]
+- Sort [b#x ASC NULLS FIRST], true
   +- Aggregate [b#x], [count(1) AS count(1)#xL, b#x]
      +- Filter (a#x = a#x)
         +- Join Inner
            :- SubqueryAlias x
            :  +- SubqueryAlias spark_catalog.default.test_missing_target
            :     +- Relation spark_catalog.default.test_missing_target[a#x,b#x,c#x,d#x] parquet
            +- SubqueryAlias y
               +- SubqueryAlias spark_catalog.default.test_missing_target
                  +- Relation spark_catalog.default.test_missing_target[a#x,b#x,c#x,d#x] parquet


-- !query
SELECT a%2, count(b) FROM test_missing_target
GROUP BY test_missing_target.a%2
ORDER BY test_missing_target.a%2
-- !query analysis
Sort [(a % 2)#x ASC NULLS FIRST], true
+- Aggregate [(a#x % 2)], [(a#x % 2) AS (a % 2)#x, count(b#x) AS count(b)#xL]
   +- SubqueryAlias spark_catalog.default.test_missing_target
      +- Relation spark_catalog.default.test_missing_target[a#x,b#x,c#x,d#x] parquet


-- !query
SELECT count(c) FROM test_missing_target
GROUP BY lower(test_missing_target.c)
ORDER BY lower(test_missing_target.c)
-- !query analysis
Project [count(c)#xL]
+- Sort [lower(c)#x ASC NULLS FIRST], true
   +- Aggregate [lower(c#x)], [count(c#x) AS count(c)#xL, lower(c#x) AS lower(c)#x]
      +- SubqueryAlias spark_catalog.default.test_missing_target
         +- Relation spark_catalog.default.test_missing_target[a#x,b#x,c#x,d#x] parquet


-- !query
SELECT count(a) FROM test_missing_target GROUP BY a ORDER BY b
-- !query analysis
org.apache.spark.sql.catalyst.ExtendedAnalysisException
{
  "errorClass" : "UNRESOLVED_COLUMN.WITH_SUGGESTION",
  "sqlState" : "42703",
  "messageParameters" : {
    "objectName" : "`b`",
    "proposal" : "`count(a)`"
  },
  "queryContext" : [ {
    "objectType" : "",
    "objectName" : "",
    "startIndex" : 62,
    "stopIndex" : 62,
    "fragment" : "b"
  } ]
}


-- !query
SELECT count(b) FROM test_missing_target GROUP BY b/2 ORDER BY b/2
-- !query analysis
Project [count(b)#xL]
+- Sort [(b / 2)#x ASC NULLS FIRST], true
   +- Aggregate [(cast(b#x as double) / cast(2 as double))], [count(b#x) AS count(b)#xL, (cast(b#x as double) / cast(2 as double)) AS (b / 2)#x]
      +- SubqueryAlias spark_catalog.default.test_missing_target
         +- Relation spark_catalog.default.test_missing_target[a#x,b#x,c#x,d#x] parquet


-- !query
SELECT lower(test_missing_target.c), count(c)
  FROM test_missing_target GROUP BY lower(c) ORDER BY lower(c)
-- !query analysis
Sort [lower(c)#x ASC NULLS FIRST], true
+- Aggregate [lower(c#x)], [lower(c#x) AS lower(c)#x, count(c#x) AS count(c)#xL]
   +- SubqueryAlias spark_catalog.default.test_missing_target
      +- Relation spark_catalog.default.test_missing_target[a#x,b#x,c#x,d#x] parquet


-- !query
SELECT a FROM test_missing_target ORDER BY upper(d)
-- !query analysis
Project [a#x]
+- Sort [upper(d#x) ASC NULLS FIRST], true
   +- Project [a#x, d#x]
      +- SubqueryAlias spark_catalog.default.test_missing_target
         +- Relation spark_catalog.default.test_missing_target[a#x,b#x,c#x,d#x] parquet


-- !query
SELECT count(b) FROM test_missing_target
	GROUP BY (b + 1) / 2 ORDER BY (b + 1) / 2 desc
-- !query analysis
Project [count(b)#xL]
+- Sort [((b + 1) / 2)#x DESC NULLS LAST], true
   +- Aggregate [(cast((b#x + 1) as double) / cast(2 as double))], [count(b#x) AS count(b)#xL, (cast((b#x + 1) as double) / cast(2 as double)) AS ((b + 1) / 2)#x]
      +- SubqueryAlias spark_catalog.default.test_missing_target
         +- Relation spark_catalog.default.test_missing_target[a#x,b#x,c#x,d#x] parquet


-- !query
SELECT count(x.a) FROM test_missing_target x, test_missing_target y
	WHERE x.a = y.a
	GROUP BY b/2 ORDER BY b/2
-- !query analysis
org.apache.spark.sql.AnalysisException
{
  "errorClass" : "AMBIGUOUS_REFERENCE",
  "sqlState" : "42704",
  "messageParameters" : {
    "name" : "`b`",
    "referenceNames" : "[`x`.`b`, `y`.`b`]"
  },
  "queryContext" : [ {
    "objectType" : "",
    "objectName" : "",
    "startIndex" : 96,
    "stopIndex" : 96,
    "fragment" : "b"
  } ]
}


-- !query
SELECT x.b/2, count(x.b) FROM test_missing_target x, test_missing_target y
	WHERE x.a = y.a
	GROUP BY x.b/2 ORDER BY x.b/2
-- !query analysis
Sort [(b / 2)#x ASC NULLS FIRST], true
+- Aggregate [(cast(b#x as double) / cast(2 as double))], [(cast(b#x as double) / cast(2 as double)) AS (b / 2)#x, count(b#x) AS count(b)#xL]
   +- Filter (a#x = a#x)
      +- Join Inner
         :- SubqueryAlias x
         :  +- SubqueryAlias spark_catalog.default.test_missing_target
         :     +- Relation spark_catalog.default.test_missing_target[a#x,b#x,c#x,d#x] parquet
         +- SubqueryAlias y
            +- SubqueryAlias spark_catalog.default.test_missing_target
               +- Relation spark_catalog.default.test_missing_target[a#x,b#x,c#x,d#x] parquet


-- !query
SELECT count(b) FROM test_missing_target x, test_missing_target y
	WHERE x.a = y.a
	GROUP BY x.b/2
-- !query analysis
org.apache.spark.sql.AnalysisException
{
  "errorClass" : "AMBIGUOUS_REFERENCE",
  "sqlState" : "42704",
  "messageParameters" : {
    "name" : "`b`",
    "referenceNames" : "[`x`.`b`, `y`.`b`]"
  },
  "queryContext" : [ {
    "objectType" : "",
    "objectName" : "",
    "startIndex" : 14,
    "stopIndex" : 14,
    "fragment" : "b"
  } ]
}


-- !query
DROP TABLE test_missing_target
-- !query analysis
DropTable false, false
+- ResolvedIdentifier V2SessionCatalog(spark_catalog), default.test_missing_target
