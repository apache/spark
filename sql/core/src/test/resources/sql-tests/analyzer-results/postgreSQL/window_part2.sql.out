-- Automatically generated by SQLQueryTestSuite
-- !query
CREATE TABLE empsalary (
    depname string,
    empno integer,
    salary int,
    enroll_date date
) USING parquet
-- !query analysis
CreateDataSourceTableCommand `spark_catalog`.`default`.`empsalary`, false


-- !query
INSERT INTO empsalary VALUES
  ('develop', 10, 5200, date '2007-08-01'),
  ('sales', 1, 5000, date '2006-10-01'),
  ('personnel', 5, 3500, date '2007-12-10'),
  ('sales', 4, 4800, date '2007-08-08'),
  ('personnel', 2, 3900, date '2006-12-23'),
  ('develop', 7, 4200, date '2008-01-01'),
  ('develop', 9, 4500, date '2008-01-01'),
  ('sales', 3, 4800, date '2007-08-01'),
  ('develop', 8, 6000, date '2006-10-01'),
  ('develop', 11, 5200, date '2007-08-15')
-- !query analysis
InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/empsalary, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/empsalary], Append, `spark_catalog`.`default`.`empsalary`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/empsalary), [depname, empno, salary, enroll_date]
+- Project [cast(col1#x as string) AS depname#x, cast(col2#x as int) AS empno#x, cast(col3#x as int) AS salary#x, cast(col4#x as date) AS enroll_date#x]
   +- LocalRelation [col1#x, col2#x, col3#x, col4#x]


-- !query
SELECT sum(unique1) over (order by four range between 2 preceding and 1 preceding),
unique1, four
FROM tenk1 WHERE unique1 < 10
-- !query analysis
Project [sum(unique1) OVER (ORDER BY four ASC NULLS FIRST RANGE BETWEEN 2 PRECEDING AND 1 PRECEDING)#xL, unique1#x, four#x]
+- Project [unique1#x, four#x, sum(unique1) OVER (ORDER BY four ASC NULLS FIRST RANGE BETWEEN 2 PRECEDING AND 1 PRECEDING)#xL, sum(unique1) OVER (ORDER BY four ASC NULLS FIRST RANGE BETWEEN 2 PRECEDING AND 1 PRECEDING)#xL]
   +- Window [sum(unique1#x) windowspecdefinition(four#x ASC NULLS FIRST, specifiedwindowframe(RangeFrame, -2, -1)) AS sum(unique1) OVER (ORDER BY four ASC NULLS FIRST RANGE BETWEEN 2 PRECEDING AND 1 PRECEDING)#xL], [four#x ASC NULLS FIRST]
      +- Project [unique1#x, four#x]
         +- Filter (unique1#x < 10)
            +- SubqueryAlias spark_catalog.default.tenk1
               +- Relation spark_catalog.default.tenk1[unique1#x,unique2#x,two#x,four#x,ten#x,twenty#x,hundred#x,thousand#x,twothousand#x,fivethous#x,tenthous#x,odd#x,even#x,stringu1#x,stringu2#x,string4#x] parquet


-- !query
SELECT sum(unique1) over (order by four desc range between 2 preceding and 1 preceding),
unique1, four
FROM tenk1 WHERE unique1 < 10
-- !query analysis
Project [sum(unique1) OVER (ORDER BY four DESC NULLS LAST RANGE BETWEEN 2 PRECEDING AND 1 PRECEDING)#xL, unique1#x, four#x]
+- Project [unique1#x, four#x, sum(unique1) OVER (ORDER BY four DESC NULLS LAST RANGE BETWEEN 2 PRECEDING AND 1 PRECEDING)#xL, sum(unique1) OVER (ORDER BY four DESC NULLS LAST RANGE BETWEEN 2 PRECEDING AND 1 PRECEDING)#xL]
   +- Window [sum(unique1#x) windowspecdefinition(four#x DESC NULLS LAST, specifiedwindowframe(RangeFrame, -2, -1)) AS sum(unique1) OVER (ORDER BY four DESC NULLS LAST RANGE BETWEEN 2 PRECEDING AND 1 PRECEDING)#xL], [four#x DESC NULLS LAST]
      +- Project [unique1#x, four#x]
         +- Filter (unique1#x < 10)
            +- SubqueryAlias spark_catalog.default.tenk1
               +- Relation spark_catalog.default.tenk1[unique1#x,unique2#x,two#x,four#x,ten#x,twenty#x,hundred#x,thousand#x,twothousand#x,fivethous#x,tenthous#x,odd#x,even#x,stringu1#x,stringu2#x,string4#x] parquet


-- !query
SELECT sum(unique1) over (partition by four order by unique1 range between 5 preceding and 6 following),
unique1, four
FROM tenk1 WHERE unique1 < 10
-- !query analysis
Project [sum(unique1) OVER (PARTITION BY four ORDER BY unique1 ASC NULLS FIRST RANGE BETWEEN 5 PRECEDING AND 6 FOLLOWING)#xL, unique1#x, four#x]
+- Project [unique1#x, four#x, sum(unique1) OVER (PARTITION BY four ORDER BY unique1 ASC NULLS FIRST RANGE BETWEEN 5 PRECEDING AND 6 FOLLOWING)#xL, sum(unique1) OVER (PARTITION BY four ORDER BY unique1 ASC NULLS FIRST RANGE BETWEEN 5 PRECEDING AND 6 FOLLOWING)#xL]
   +- Window [sum(unique1#x) windowspecdefinition(four#x, unique1#x ASC NULLS FIRST, specifiedwindowframe(RangeFrame, -5, 6)) AS sum(unique1) OVER (PARTITION BY four ORDER BY unique1 ASC NULLS FIRST RANGE BETWEEN 5 PRECEDING AND 6 FOLLOWING)#xL], [four#x], [unique1#x ASC NULLS FIRST]
      +- Project [unique1#x, four#x]
         +- Filter (unique1#x < 10)
            +- SubqueryAlias spark_catalog.default.tenk1
               +- Relation spark_catalog.default.tenk1[unique1#x,unique2#x,two#x,four#x,ten#x,twenty#x,hundred#x,thousand#x,twothousand#x,fivethous#x,tenthous#x,odd#x,even#x,stringu1#x,stringu2#x,string4#x] parquet


-- !query
select ss.id, ss.y,
       first(ss.y) over w,
       last(ss.y) over w
from
  (select x.id, x.id as y from range(1,6) as x
   union all select null, 42
   union all select null, 43) ss
window w as
  (order by ss.id asc nulls first range between 2 preceding and 2 following)
-- !query analysis
Project [id#xL, y#xL, first(y) OVER (ORDER BY id ASC NULLS FIRST RANGE BETWEEN (- 2) FOLLOWING AND 2 FOLLOWING)#xL, last(y) OVER (ORDER BY id ASC NULLS FIRST RANGE BETWEEN (- 2) FOLLOWING AND 2 FOLLOWING)#xL]
+- Project [id#xL, y#xL, first(y) OVER (ORDER BY id ASC NULLS FIRST RANGE BETWEEN (- 2) FOLLOWING AND 2 FOLLOWING)#xL, last(y) OVER (ORDER BY id ASC NULLS FIRST RANGE BETWEEN (- 2) FOLLOWING AND 2 FOLLOWING)#xL, first(y) OVER (ORDER BY id ASC NULLS FIRST RANGE BETWEEN (- 2) FOLLOWING AND 2 FOLLOWING)#xL, last(y) OVER (ORDER BY id ASC NULLS FIRST RANGE BETWEEN (- 2) FOLLOWING AND 2 FOLLOWING)#xL]
   +- Window [first(y#xL, false) windowspecdefinition(id#xL ASC NULLS FIRST, specifiedwindowframe(RangeFrame, cast(-2 as bigint), cast(2 as bigint))) AS first(y) OVER (ORDER BY id ASC NULLS FIRST RANGE BETWEEN (- 2) FOLLOWING AND 2 FOLLOWING)#xL, last(y#xL, false) windowspecdefinition(id#xL ASC NULLS FIRST, specifiedwindowframe(RangeFrame, cast(-2 as bigint), cast(2 as bigint))) AS last(y) OVER (ORDER BY id ASC NULLS FIRST RANGE BETWEEN (- 2) FOLLOWING AND 2 FOLLOWING)#xL], [id#xL ASC NULLS FIRST]
      +- Project [id#xL, y#xL]
         +- SubqueryAlias ss
            +- Union false, false
               :- Union false, false
               :  :- Project [id#xL, id#xL AS y#xL]
               :  :  +- SubqueryAlias x
               :  :     +- Range (1, 6, step=1)
               :  +- Project [cast(NULL#x as bigint) AS NULL#xL, cast(42#x as bigint) AS 42#xL]
               :     +- Project [null AS NULL#x, 42 AS 42#x]
               :        +- OneRowRelation
               +- Project [cast(NULL#x as bigint) AS NULL#xL, cast(43#x as bigint) AS 43#xL]
                  +- Project [null AS NULL#x, 43 AS 43#x]
                     +- OneRowRelation


-- !query
select ss.id, ss.y,
       first(ss.y) over w,
       last(ss.y) over w
from
  (select x.id, x.id as y from range(1,6) as x
   union all select null, 42
   union all select null, 43) ss
window w as
  (order by ss.id asc nulls last range between 2 preceding and 2 following)
-- !query analysis
Project [id#xL, y#xL, first(y) OVER (ORDER BY id ASC NULLS LAST RANGE BETWEEN (- 2) FOLLOWING AND 2 FOLLOWING)#xL, last(y) OVER (ORDER BY id ASC NULLS LAST RANGE BETWEEN (- 2) FOLLOWING AND 2 FOLLOWING)#xL]
+- Project [id#xL, y#xL, first(y) OVER (ORDER BY id ASC NULLS LAST RANGE BETWEEN (- 2) FOLLOWING AND 2 FOLLOWING)#xL, last(y) OVER (ORDER BY id ASC NULLS LAST RANGE BETWEEN (- 2) FOLLOWING AND 2 FOLLOWING)#xL, first(y) OVER (ORDER BY id ASC NULLS LAST RANGE BETWEEN (- 2) FOLLOWING AND 2 FOLLOWING)#xL, last(y) OVER (ORDER BY id ASC NULLS LAST RANGE BETWEEN (- 2) FOLLOWING AND 2 FOLLOWING)#xL]
   +- Window [first(y#xL, false) windowspecdefinition(id#xL ASC NULLS LAST, specifiedwindowframe(RangeFrame, cast(-2 as bigint), cast(2 as bigint))) AS first(y) OVER (ORDER BY id ASC NULLS LAST RANGE BETWEEN (- 2) FOLLOWING AND 2 FOLLOWING)#xL, last(y#xL, false) windowspecdefinition(id#xL ASC NULLS LAST, specifiedwindowframe(RangeFrame, cast(-2 as bigint), cast(2 as bigint))) AS last(y) OVER (ORDER BY id ASC NULLS LAST RANGE BETWEEN (- 2) FOLLOWING AND 2 FOLLOWING)#xL], [id#xL ASC NULLS LAST]
      +- Project [id#xL, y#xL]
         +- SubqueryAlias ss
            +- Union false, false
               :- Union false, false
               :  :- Project [id#xL, id#xL AS y#xL]
               :  :  +- SubqueryAlias x
               :  :     +- Range (1, 6, step=1)
               :  +- Project [cast(NULL#x as bigint) AS NULL#xL, cast(42#x as bigint) AS 42#xL]
               :     +- Project [null AS NULL#x, 42 AS 42#x]
               :        +- OneRowRelation
               +- Project [cast(NULL#x as bigint) AS NULL#xL, cast(43#x as bigint) AS 43#xL]
                  +- Project [null AS NULL#x, 43 AS 43#x]
                     +- OneRowRelation


-- !query
select ss.id, ss.y,
       first(ss.y) over w,
       last(ss.y) over w
from
  (select x.id, x.id as y from range(1,6) as x
   union all select null, 42
   union all select null, 43) ss
window w as
  (order by ss.id desc nulls first range between 2 preceding and 2 following)
-- !query analysis
Project [id#xL, y#xL, first(y) OVER (ORDER BY id DESC NULLS FIRST RANGE BETWEEN (- 2) FOLLOWING AND 2 FOLLOWING)#xL, last(y) OVER (ORDER BY id DESC NULLS FIRST RANGE BETWEEN (- 2) FOLLOWING AND 2 FOLLOWING)#xL]
+- Project [id#xL, y#xL, first(y) OVER (ORDER BY id DESC NULLS FIRST RANGE BETWEEN (- 2) FOLLOWING AND 2 FOLLOWING)#xL, last(y) OVER (ORDER BY id DESC NULLS FIRST RANGE BETWEEN (- 2) FOLLOWING AND 2 FOLLOWING)#xL, first(y) OVER (ORDER BY id DESC NULLS FIRST RANGE BETWEEN (- 2) FOLLOWING AND 2 FOLLOWING)#xL, last(y) OVER (ORDER BY id DESC NULLS FIRST RANGE BETWEEN (- 2) FOLLOWING AND 2 FOLLOWING)#xL]
   +- Window [first(y#xL, false) windowspecdefinition(id#xL DESC NULLS FIRST, specifiedwindowframe(RangeFrame, cast(-2 as bigint), cast(2 as bigint))) AS first(y) OVER (ORDER BY id DESC NULLS FIRST RANGE BETWEEN (- 2) FOLLOWING AND 2 FOLLOWING)#xL, last(y#xL, false) windowspecdefinition(id#xL DESC NULLS FIRST, specifiedwindowframe(RangeFrame, cast(-2 as bigint), cast(2 as bigint))) AS last(y) OVER (ORDER BY id DESC NULLS FIRST RANGE BETWEEN (- 2) FOLLOWING AND 2 FOLLOWING)#xL], [id#xL DESC NULLS FIRST]
      +- Project [id#xL, y#xL]
         +- SubqueryAlias ss
            +- Union false, false
               :- Union false, false
               :  :- Project [id#xL, id#xL AS y#xL]
               :  :  +- SubqueryAlias x
               :  :     +- Range (1, 6, step=1)
               :  +- Project [cast(NULL#x as bigint) AS NULL#xL, cast(42#x as bigint) AS 42#xL]
               :     +- Project [null AS NULL#x, 42 AS 42#x]
               :        +- OneRowRelation
               +- Project [cast(NULL#x as bigint) AS NULL#xL, cast(43#x as bigint) AS 43#xL]
                  +- Project [null AS NULL#x, 43 AS 43#x]
                     +- OneRowRelation


-- !query
select ss.id, ss.y,
       first(ss.y) over w,
       last(ss.y) over w
from
  (select x.id, x.id as y from range(1,6) as x
   union all select null, 42
   union all select null, 43) ss
window w as
  (order by ss.id desc nulls last range between 2 preceding and 2 following)
-- !query analysis
Project [id#xL, y#xL, first(y) OVER (ORDER BY id DESC NULLS LAST RANGE BETWEEN (- 2) FOLLOWING AND 2 FOLLOWING)#xL, last(y) OVER (ORDER BY id DESC NULLS LAST RANGE BETWEEN (- 2) FOLLOWING AND 2 FOLLOWING)#xL]
+- Project [id#xL, y#xL, first(y) OVER (ORDER BY id DESC NULLS LAST RANGE BETWEEN (- 2) FOLLOWING AND 2 FOLLOWING)#xL, last(y) OVER (ORDER BY id DESC NULLS LAST RANGE BETWEEN (- 2) FOLLOWING AND 2 FOLLOWING)#xL, first(y) OVER (ORDER BY id DESC NULLS LAST RANGE BETWEEN (- 2) FOLLOWING AND 2 FOLLOWING)#xL, last(y) OVER (ORDER BY id DESC NULLS LAST RANGE BETWEEN (- 2) FOLLOWING AND 2 FOLLOWING)#xL]
   +- Window [first(y#xL, false) windowspecdefinition(id#xL DESC NULLS LAST, specifiedwindowframe(RangeFrame, cast(-2 as bigint), cast(2 as bigint))) AS first(y) OVER (ORDER BY id DESC NULLS LAST RANGE BETWEEN (- 2) FOLLOWING AND 2 FOLLOWING)#xL, last(y#xL, false) windowspecdefinition(id#xL DESC NULLS LAST, specifiedwindowframe(RangeFrame, cast(-2 as bigint), cast(2 as bigint))) AS last(y) OVER (ORDER BY id DESC NULLS LAST RANGE BETWEEN (- 2) FOLLOWING AND 2 FOLLOWING)#xL], [id#xL DESC NULLS LAST]
      +- Project [id#xL, y#xL]
         +- SubqueryAlias ss
            +- Union false, false
               :- Union false, false
               :  :- Project [id#xL, id#xL AS y#xL]
               :  :  +- SubqueryAlias x
               :  :     +- Range (1, 6, step=1)
               :  +- Project [cast(NULL#x as bigint) AS NULL#xL, cast(42#x as bigint) AS 42#xL]
               :     +- Project [null AS NULL#x, 42 AS 42#x]
               :        +- OneRowRelation
               +- Project [cast(NULL#x as bigint) AS NULL#xL, cast(43#x as bigint) AS 43#xL]
                  +- Project [null AS NULL#x, 43 AS 43#x]
                     +- OneRowRelation


-- !query
select x.id, last(x.id) over (order by x.id range between current row and 2147450884 following)
from range(32764, 32767) x
-- !query analysis
Project [id#xL, last(id) OVER (ORDER BY id ASC NULLS FIRST RANGE BETWEEN CURRENT ROW AND 2147450884 FOLLOWING)#xL]
+- Project [id#xL, last(id) OVER (ORDER BY id ASC NULLS FIRST RANGE BETWEEN CURRENT ROW AND 2147450884 FOLLOWING)#xL, last(id) OVER (ORDER BY id ASC NULLS FIRST RANGE BETWEEN CURRENT ROW AND 2147450884 FOLLOWING)#xL]
   +- Window [last(id#xL, false) windowspecdefinition(id#xL ASC NULLS FIRST, specifiedwindowframe(RangeFrame, currentrow$(), cast(2147450884 as bigint))) AS last(id) OVER (ORDER BY id ASC NULLS FIRST RANGE BETWEEN CURRENT ROW AND 2147450884 FOLLOWING)#xL], [id#xL ASC NULLS FIRST]
      +- Project [id#xL]
         +- SubqueryAlias x
            +- Range (32764, 32767, step=1)


-- !query
select x.id, last(x.id) over (order by x.id desc range between current row and 2147450885 following)
from range(-32766, -32765) x
-- !query analysis
Project [id#xL, last(id) OVER (ORDER BY id DESC NULLS LAST RANGE BETWEEN CURRENT ROW AND 2147450885 FOLLOWING)#xL]
+- Project [id#xL, last(id) OVER (ORDER BY id DESC NULLS LAST RANGE BETWEEN CURRENT ROW AND 2147450885 FOLLOWING)#xL, last(id) OVER (ORDER BY id DESC NULLS LAST RANGE BETWEEN CURRENT ROW AND 2147450885 FOLLOWING)#xL]
   +- Window [last(id#xL, false) windowspecdefinition(id#xL DESC NULLS LAST, specifiedwindowframe(RangeFrame, currentrow$(), cast(2147450885 as bigint))) AS last(id) OVER (ORDER BY id DESC NULLS LAST RANGE BETWEEN CURRENT ROW AND 2147450885 FOLLOWING)#xL], [id#xL DESC NULLS LAST]
      +- Project [id#xL]
         +- SubqueryAlias x
            +- Range (-32766, -32765, step=1)


-- !query
select x.id, last(x.id) over (order by x.id range between current row and 4 following)
from range(2147483644, 2147483647) x
-- !query analysis
Project [id#xL, last(id) OVER (ORDER BY id ASC NULLS FIRST RANGE BETWEEN CURRENT ROW AND 4 FOLLOWING)#xL]
+- Project [id#xL, last(id) OVER (ORDER BY id ASC NULLS FIRST RANGE BETWEEN CURRENT ROW AND 4 FOLLOWING)#xL, last(id) OVER (ORDER BY id ASC NULLS FIRST RANGE BETWEEN CURRENT ROW AND 4 FOLLOWING)#xL]
   +- Window [last(id#xL, false) windowspecdefinition(id#xL ASC NULLS FIRST, specifiedwindowframe(RangeFrame, currentrow$(), cast(4 as bigint))) AS last(id) OVER (ORDER BY id ASC NULLS FIRST RANGE BETWEEN CURRENT ROW AND 4 FOLLOWING)#xL], [id#xL ASC NULLS FIRST]
      +- Project [id#xL]
         +- SubqueryAlias x
            +- Range (2147483644, 2147483647, step=1)


-- !query
select x.id, last(x.id) over (order by x.id desc range between current row and 5 following)
from range(-2147483646, -2147483645) x
-- !query analysis
Project [id#xL, last(id) OVER (ORDER BY id DESC NULLS LAST RANGE BETWEEN CURRENT ROW AND 5 FOLLOWING)#xL]
+- Project [id#xL, last(id) OVER (ORDER BY id DESC NULLS LAST RANGE BETWEEN CURRENT ROW AND 5 FOLLOWING)#xL, last(id) OVER (ORDER BY id DESC NULLS LAST RANGE BETWEEN CURRENT ROW AND 5 FOLLOWING)#xL]
   +- Window [last(id#xL, false) windowspecdefinition(id#xL DESC NULLS LAST, specifiedwindowframe(RangeFrame, currentrow$(), cast(5 as bigint))) AS last(id) OVER (ORDER BY id DESC NULLS LAST RANGE BETWEEN CURRENT ROW AND 5 FOLLOWING)#xL], [id#xL DESC NULLS LAST]
      +- Project [id#xL]
         +- SubqueryAlias x
            +- Range (-2147483646, -2147483645, step=1)


-- !query
select x.id, last(x.id) over (order by x.id range between current row and 4 following)
from range(9223372036854775804, 9223372036854775807) x
-- !query analysis
Project [id#xL, last(id) OVER (ORDER BY id ASC NULLS FIRST RANGE BETWEEN CURRENT ROW AND 4 FOLLOWING)#xL]
+- Project [id#xL, last(id) OVER (ORDER BY id ASC NULLS FIRST RANGE BETWEEN CURRENT ROW AND 4 FOLLOWING)#xL, last(id) OVER (ORDER BY id ASC NULLS FIRST RANGE BETWEEN CURRENT ROW AND 4 FOLLOWING)#xL]
   +- Window [last(id#xL, false) windowspecdefinition(id#xL ASC NULLS FIRST, specifiedwindowframe(RangeFrame, currentrow$(), cast(4 as bigint))) AS last(id) OVER (ORDER BY id ASC NULLS FIRST RANGE BETWEEN CURRENT ROW AND 4 FOLLOWING)#xL], [id#xL ASC NULLS FIRST]
      +- Project [id#xL]
         +- SubqueryAlias x
            +- Range (9223372036854775804, 9223372036854775807, step=1)


-- !query
select x.id, last(x.id) over (order by x.id desc range between current row and 5 following)
from range(-9223372036854775806, -9223372036854775805) x
-- !query analysis
Project [id#xL, last(id) OVER (ORDER BY id DESC NULLS LAST RANGE BETWEEN CURRENT ROW AND 5 FOLLOWING)#xL]
+- Project [id#xL, last(id) OVER (ORDER BY id DESC NULLS LAST RANGE BETWEEN CURRENT ROW AND 5 FOLLOWING)#xL, last(id) OVER (ORDER BY id DESC NULLS LAST RANGE BETWEEN CURRENT ROW AND 5 FOLLOWING)#xL]
   +- Window [last(id#xL, false) windowspecdefinition(id#xL DESC NULLS LAST, specifiedwindowframe(RangeFrame, currentrow$(), cast(5 as bigint))) AS last(id) OVER (ORDER BY id DESC NULLS LAST RANGE BETWEEN CURRENT ROW AND 5 FOLLOWING)#xL], [id#xL DESC NULLS LAST]
      +- Project [id#xL]
         +- SubqueryAlias x
            +- Range (-9223372036854775806, -9223372036854775805, step=1)


-- !query
create table numerics (
    id int,
    f_float4 float,
    f_float8 float,
    f_numeric int
) using parquet
-- !query analysis
CreateDataSourceTableCommand `spark_catalog`.`default`.`numerics`, false


-- !query
insert into numerics values
(1, -3, -3, -3),
(2, -1, -1, -1),
(3, 0, 0, 0),
(4, 1.1, 1.1, 1.1),
(5, 1.12, 1.12, 1.12),
(6, 2, 2, 2),
(7, 100, 100, 100)
-- !query analysis
InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/numerics, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/numerics], Append, `spark_catalog`.`default`.`numerics`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/numerics), [id, f_float4, f_float8, f_numeric]
+- Project [cast(col1#x as int) AS id#x, cast(col2#x as float) AS f_float4#x, cast(col3#x as float) AS f_float8#x, cast(col4#x as int) AS f_numeric#x]
   +- LocalRelation [col1#x, col2#x, col3#x, col4#x]


-- !query
select id, f_float4, first(id) over w, last(id) over w
from numerics
window w as (order by f_float4 range between
             1 preceding and 1 following)
-- !query analysis
Project [id#x, f_float4#x, first(id) OVER (ORDER BY f_float4 ASC NULLS FIRST RANGE BETWEEN (- 1) FOLLOWING AND 1 FOLLOWING)#x, last(id) OVER (ORDER BY f_float4 ASC NULLS FIRST RANGE BETWEEN (- 1) FOLLOWING AND 1 FOLLOWING)#x]
+- Project [id#x, f_float4#x, first(id) OVER (ORDER BY f_float4 ASC NULLS FIRST RANGE BETWEEN (- 1) FOLLOWING AND 1 FOLLOWING)#x, last(id) OVER (ORDER BY f_float4 ASC NULLS FIRST RANGE BETWEEN (- 1) FOLLOWING AND 1 FOLLOWING)#x, first(id) OVER (ORDER BY f_float4 ASC NULLS FIRST RANGE BETWEEN (- 1) FOLLOWING AND 1 FOLLOWING)#x, last(id) OVER (ORDER BY f_float4 ASC NULLS FIRST RANGE BETWEEN (- 1) FOLLOWING AND 1 FOLLOWING)#x]
   +- Window [first(id#x, false) windowspecdefinition(f_float4#x ASC NULLS FIRST, specifiedwindowframe(RangeFrame, cast(-1 as float), cast(1 as float))) AS first(id) OVER (ORDER BY f_float4 ASC NULLS FIRST RANGE BETWEEN (- 1) FOLLOWING AND 1 FOLLOWING)#x, last(id#x, false) windowspecdefinition(f_float4#x ASC NULLS FIRST, specifiedwindowframe(RangeFrame, cast(-1 as float), cast(1 as float))) AS last(id) OVER (ORDER BY f_float4 ASC NULLS FIRST RANGE BETWEEN (- 1) FOLLOWING AND 1 FOLLOWING)#x], [f_float4#x ASC NULLS FIRST]
      +- Project [id#x, f_float4#x]
         +- SubqueryAlias spark_catalog.default.numerics
            +- Relation spark_catalog.default.numerics[id#x,f_float4#x,f_float8#x,f_numeric#x] parquet


-- !query
select id, f_float4, first(id) over w, last(id) over w
from numerics
window w as (order by f_float4 range between
             1 preceding and 1.1 following)
-- !query analysis
Project [id#x, f_float4#x, first(id) OVER (ORDER BY f_float4 ASC NULLS FIRST RANGE BETWEEN (- 1) FOLLOWING AND 1.1 FOLLOWING)#x, last(id) OVER (ORDER BY f_float4 ASC NULLS FIRST RANGE BETWEEN (- 1) FOLLOWING AND 1.1 FOLLOWING)#x]
+- Project [id#x, f_float4#x, first(id) OVER (ORDER BY f_float4 ASC NULLS FIRST RANGE BETWEEN (- 1) FOLLOWING AND 1.1 FOLLOWING)#x, last(id) OVER (ORDER BY f_float4 ASC NULLS FIRST RANGE BETWEEN (- 1) FOLLOWING AND 1.1 FOLLOWING)#x, first(id) OVER (ORDER BY f_float4 ASC NULLS FIRST RANGE BETWEEN (- 1) FOLLOWING AND 1.1 FOLLOWING)#x, last(id) OVER (ORDER BY f_float4 ASC NULLS FIRST RANGE BETWEEN (- 1) FOLLOWING AND 1.1 FOLLOWING)#x]
   +- Window [first(id#x, false) windowspecdefinition(f_float4#x ASC NULLS FIRST, specifiedwindowframe(RangeFrame, cast(-1 as float), cast(1.1 as float))) AS first(id) OVER (ORDER BY f_float4 ASC NULLS FIRST RANGE BETWEEN (- 1) FOLLOWING AND 1.1 FOLLOWING)#x, last(id#x, false) windowspecdefinition(f_float4#x ASC NULLS FIRST, specifiedwindowframe(RangeFrame, cast(-1 as float), cast(1.1 as float))) AS last(id) OVER (ORDER BY f_float4 ASC NULLS FIRST RANGE BETWEEN (- 1) FOLLOWING AND 1.1 FOLLOWING)#x], [f_float4#x ASC NULLS FIRST]
      +- Project [id#x, f_float4#x]
         +- SubqueryAlias spark_catalog.default.numerics
            +- Relation spark_catalog.default.numerics[id#x,f_float4#x,f_float8#x,f_numeric#x] parquet


-- !query
select id, f_float4, first(id) over w, last(id) over w
from numerics
window w as (order by f_float4 range between
             'inf' preceding and 'inf' following)
-- !query analysis
Project [id#x, f_float4#x, first(id) OVER (ORDER BY f_float4 ASC NULLS FIRST RANGE BETWEEN (- inf) FOLLOWING AND inf FOLLOWING)#x, last(id) OVER (ORDER BY f_float4 ASC NULLS FIRST RANGE BETWEEN (- inf) FOLLOWING AND inf FOLLOWING)#x]
+- Project [id#x, f_float4#x, first(id) OVER (ORDER BY f_float4 ASC NULLS FIRST RANGE BETWEEN (- inf) FOLLOWING AND inf FOLLOWING)#x, last(id) OVER (ORDER BY f_float4 ASC NULLS FIRST RANGE BETWEEN (- inf) FOLLOWING AND inf FOLLOWING)#x, first(id) OVER (ORDER BY f_float4 ASC NULLS FIRST RANGE BETWEEN (- inf) FOLLOWING AND inf FOLLOWING)#x, last(id) OVER (ORDER BY f_float4 ASC NULLS FIRST RANGE BETWEEN (- inf) FOLLOWING AND inf FOLLOWING)#x]
   +- Window [first(id#x, false) windowspecdefinition(f_float4#x ASC NULLS FIRST, specifiedwindowframe(RangeFrame, cast(-cast(inf as double) as float), cast(inf as float))) AS first(id) OVER (ORDER BY f_float4 ASC NULLS FIRST RANGE BETWEEN (- inf) FOLLOWING AND inf FOLLOWING)#x, last(id#x, false) windowspecdefinition(f_float4#x ASC NULLS FIRST, specifiedwindowframe(RangeFrame, cast(-cast(inf as double) as float), cast(inf as float))) AS last(id) OVER (ORDER BY f_float4 ASC NULLS FIRST RANGE BETWEEN (- inf) FOLLOWING AND inf FOLLOWING)#x], [f_float4#x ASC NULLS FIRST]
      +- Project [id#x, f_float4#x]
         +- SubqueryAlias spark_catalog.default.numerics
            +- Relation spark_catalog.default.numerics[id#x,f_float4#x,f_float8#x,f_numeric#x] parquet


-- !query
select id, f_float4, first(id) over w, last(id) over w
from numerics
window w as (order by f_float4 range between
             1.1 preceding and 'NaN' following)
-- !query analysis
Project [id#x, f_float4#x, first(id) OVER (ORDER BY f_float4 ASC NULLS FIRST RANGE BETWEEN (- 1.1) FOLLOWING AND NaN FOLLOWING)#x, last(id) OVER (ORDER BY f_float4 ASC NULLS FIRST RANGE BETWEEN (- 1.1) FOLLOWING AND NaN FOLLOWING)#x]
+- Project [id#x, f_float4#x, first(id) OVER (ORDER BY f_float4 ASC NULLS FIRST RANGE BETWEEN (- 1.1) FOLLOWING AND NaN FOLLOWING)#x, last(id) OVER (ORDER BY f_float4 ASC NULLS FIRST RANGE BETWEEN (- 1.1) FOLLOWING AND NaN FOLLOWING)#x, first(id) OVER (ORDER BY f_float4 ASC NULLS FIRST RANGE BETWEEN (- 1.1) FOLLOWING AND NaN FOLLOWING)#x, last(id) OVER (ORDER BY f_float4 ASC NULLS FIRST RANGE BETWEEN (- 1.1) FOLLOWING AND NaN FOLLOWING)#x]
   +- Window [first(id#x, false) windowspecdefinition(f_float4#x ASC NULLS FIRST, specifiedwindowframe(RangeFrame, cast(-1.1 as float), cast(NaN as float))) AS first(id) OVER (ORDER BY f_float4 ASC NULLS FIRST RANGE BETWEEN (- 1.1) FOLLOWING AND NaN FOLLOWING)#x, last(id#x, false) windowspecdefinition(f_float4#x ASC NULLS FIRST, specifiedwindowframe(RangeFrame, cast(-1.1 as float), cast(NaN as float))) AS last(id) OVER (ORDER BY f_float4 ASC NULLS FIRST RANGE BETWEEN (- 1.1) FOLLOWING AND NaN FOLLOWING)#x], [f_float4#x ASC NULLS FIRST]
      +- Project [id#x, f_float4#x]
         +- SubqueryAlias spark_catalog.default.numerics
            +- Relation spark_catalog.default.numerics[id#x,f_float4#x,f_float8#x,f_numeric#x] parquet


-- !query
select id, f_float8, first(id) over w, last(id) over w
from numerics
window w as (order by f_float8 range between
             1 preceding and 1 following)
-- !query analysis
Project [id#x, f_float8#x, first(id) OVER (ORDER BY f_float8 ASC NULLS FIRST RANGE BETWEEN (- 1) FOLLOWING AND 1 FOLLOWING)#x, last(id) OVER (ORDER BY f_float8 ASC NULLS FIRST RANGE BETWEEN (- 1) FOLLOWING AND 1 FOLLOWING)#x]
+- Project [id#x, f_float8#x, first(id) OVER (ORDER BY f_float8 ASC NULLS FIRST RANGE BETWEEN (- 1) FOLLOWING AND 1 FOLLOWING)#x, last(id) OVER (ORDER BY f_float8 ASC NULLS FIRST RANGE BETWEEN (- 1) FOLLOWING AND 1 FOLLOWING)#x, first(id) OVER (ORDER BY f_float8 ASC NULLS FIRST RANGE BETWEEN (- 1) FOLLOWING AND 1 FOLLOWING)#x, last(id) OVER (ORDER BY f_float8 ASC NULLS FIRST RANGE BETWEEN (- 1) FOLLOWING AND 1 FOLLOWING)#x]
   +- Window [first(id#x, false) windowspecdefinition(f_float8#x ASC NULLS FIRST, specifiedwindowframe(RangeFrame, cast(-1 as float), cast(1 as float))) AS first(id) OVER (ORDER BY f_float8 ASC NULLS FIRST RANGE BETWEEN (- 1) FOLLOWING AND 1 FOLLOWING)#x, last(id#x, false) windowspecdefinition(f_float8#x ASC NULLS FIRST, specifiedwindowframe(RangeFrame, cast(-1 as float), cast(1 as float))) AS last(id) OVER (ORDER BY f_float8 ASC NULLS FIRST RANGE BETWEEN (- 1) FOLLOWING AND 1 FOLLOWING)#x], [f_float8#x ASC NULLS FIRST]
      +- Project [id#x, f_float8#x]
         +- SubqueryAlias spark_catalog.default.numerics
            +- Relation spark_catalog.default.numerics[id#x,f_float4#x,f_float8#x,f_numeric#x] parquet


-- !query
select id, f_float8, first(id) over w, last(id) over w
from numerics
window w as (order by f_float8 range between
             1 preceding and 1.1 following)
-- !query analysis
Project [id#x, f_float8#x, first(id) OVER (ORDER BY f_float8 ASC NULLS FIRST RANGE BETWEEN (- 1) FOLLOWING AND 1.1 FOLLOWING)#x, last(id) OVER (ORDER BY f_float8 ASC NULLS FIRST RANGE BETWEEN (- 1) FOLLOWING AND 1.1 FOLLOWING)#x]
+- Project [id#x, f_float8#x, first(id) OVER (ORDER BY f_float8 ASC NULLS FIRST RANGE BETWEEN (- 1) FOLLOWING AND 1.1 FOLLOWING)#x, last(id) OVER (ORDER BY f_float8 ASC NULLS FIRST RANGE BETWEEN (- 1) FOLLOWING AND 1.1 FOLLOWING)#x, first(id) OVER (ORDER BY f_float8 ASC NULLS FIRST RANGE BETWEEN (- 1) FOLLOWING AND 1.1 FOLLOWING)#x, last(id) OVER (ORDER BY f_float8 ASC NULLS FIRST RANGE BETWEEN (- 1) FOLLOWING AND 1.1 FOLLOWING)#x]
   +- Window [first(id#x, false) windowspecdefinition(f_float8#x ASC NULLS FIRST, specifiedwindowframe(RangeFrame, cast(-1 as float), cast(1.1 as float))) AS first(id) OVER (ORDER BY f_float8 ASC NULLS FIRST RANGE BETWEEN (- 1) FOLLOWING AND 1.1 FOLLOWING)#x, last(id#x, false) windowspecdefinition(f_float8#x ASC NULLS FIRST, specifiedwindowframe(RangeFrame, cast(-1 as float), cast(1.1 as float))) AS last(id) OVER (ORDER BY f_float8 ASC NULLS FIRST RANGE BETWEEN (- 1) FOLLOWING AND 1.1 FOLLOWING)#x], [f_float8#x ASC NULLS FIRST]
      +- Project [id#x, f_float8#x]
         +- SubqueryAlias spark_catalog.default.numerics
            +- Relation spark_catalog.default.numerics[id#x,f_float4#x,f_float8#x,f_numeric#x] parquet


-- !query
select id, f_float8, first(id) over w, last(id) over w
from numerics
window w as (order by f_float8 range between
             'inf' preceding and 'inf' following)
-- !query analysis
Project [id#x, f_float8#x, first(id) OVER (ORDER BY f_float8 ASC NULLS FIRST RANGE BETWEEN (- inf) FOLLOWING AND inf FOLLOWING)#x, last(id) OVER (ORDER BY f_float8 ASC NULLS FIRST RANGE BETWEEN (- inf) FOLLOWING AND inf FOLLOWING)#x]
+- Project [id#x, f_float8#x, first(id) OVER (ORDER BY f_float8 ASC NULLS FIRST RANGE BETWEEN (- inf) FOLLOWING AND inf FOLLOWING)#x, last(id) OVER (ORDER BY f_float8 ASC NULLS FIRST RANGE BETWEEN (- inf) FOLLOWING AND inf FOLLOWING)#x, first(id) OVER (ORDER BY f_float8 ASC NULLS FIRST RANGE BETWEEN (- inf) FOLLOWING AND inf FOLLOWING)#x, last(id) OVER (ORDER BY f_float8 ASC NULLS FIRST RANGE BETWEEN (- inf) FOLLOWING AND inf FOLLOWING)#x]
   +- Window [first(id#x, false) windowspecdefinition(f_float8#x ASC NULLS FIRST, specifiedwindowframe(RangeFrame, cast(-cast(inf as double) as float), cast(inf as float))) AS first(id) OVER (ORDER BY f_float8 ASC NULLS FIRST RANGE BETWEEN (- inf) FOLLOWING AND inf FOLLOWING)#x, last(id#x, false) windowspecdefinition(f_float8#x ASC NULLS FIRST, specifiedwindowframe(RangeFrame, cast(-cast(inf as double) as float), cast(inf as float))) AS last(id) OVER (ORDER BY f_float8 ASC NULLS FIRST RANGE BETWEEN (- inf) FOLLOWING AND inf FOLLOWING)#x], [f_float8#x ASC NULLS FIRST]
      +- Project [id#x, f_float8#x]
         +- SubqueryAlias spark_catalog.default.numerics
            +- Relation spark_catalog.default.numerics[id#x,f_float4#x,f_float8#x,f_numeric#x] parquet


-- !query
select id, f_float8, first(id) over w, last(id) over w
from numerics
window w as (order by f_float8 range between
             1.1 preceding and 'NaN' following)
-- !query analysis
Project [id#x, f_float8#x, first(id) OVER (ORDER BY f_float8 ASC NULLS FIRST RANGE BETWEEN (- 1.1) FOLLOWING AND NaN FOLLOWING)#x, last(id) OVER (ORDER BY f_float8 ASC NULLS FIRST RANGE BETWEEN (- 1.1) FOLLOWING AND NaN FOLLOWING)#x]
+- Project [id#x, f_float8#x, first(id) OVER (ORDER BY f_float8 ASC NULLS FIRST RANGE BETWEEN (- 1.1) FOLLOWING AND NaN FOLLOWING)#x, last(id) OVER (ORDER BY f_float8 ASC NULLS FIRST RANGE BETWEEN (- 1.1) FOLLOWING AND NaN FOLLOWING)#x, first(id) OVER (ORDER BY f_float8 ASC NULLS FIRST RANGE BETWEEN (- 1.1) FOLLOWING AND NaN FOLLOWING)#x, last(id) OVER (ORDER BY f_float8 ASC NULLS FIRST RANGE BETWEEN (- 1.1) FOLLOWING AND NaN FOLLOWING)#x]
   +- Window [first(id#x, false) windowspecdefinition(f_float8#x ASC NULLS FIRST, specifiedwindowframe(RangeFrame, cast(-1.1 as float), cast(NaN as float))) AS first(id) OVER (ORDER BY f_float8 ASC NULLS FIRST RANGE BETWEEN (- 1.1) FOLLOWING AND NaN FOLLOWING)#x, last(id#x, false) windowspecdefinition(f_float8#x ASC NULLS FIRST, specifiedwindowframe(RangeFrame, cast(-1.1 as float), cast(NaN as float))) AS last(id) OVER (ORDER BY f_float8 ASC NULLS FIRST RANGE BETWEEN (- 1.1) FOLLOWING AND NaN FOLLOWING)#x], [f_float8#x ASC NULLS FIRST]
      +- Project [id#x, f_float8#x]
         +- SubqueryAlias spark_catalog.default.numerics
            +- Relation spark_catalog.default.numerics[id#x,f_float4#x,f_float8#x,f_numeric#x] parquet


-- !query
select id, f_numeric, first(id) over w, last(id) over w
from numerics
window w as (order by f_numeric range between
             1 preceding and 1 following)
-- !query analysis
Project [id#x, f_numeric#x, first(id) OVER (ORDER BY f_numeric ASC NULLS FIRST RANGE BETWEEN 1 PRECEDING AND 1 FOLLOWING)#x, last(id) OVER (ORDER BY f_numeric ASC NULLS FIRST RANGE BETWEEN 1 PRECEDING AND 1 FOLLOWING)#x]
+- Project [id#x, f_numeric#x, first(id) OVER (ORDER BY f_numeric ASC NULLS FIRST RANGE BETWEEN 1 PRECEDING AND 1 FOLLOWING)#x, last(id) OVER (ORDER BY f_numeric ASC NULLS FIRST RANGE BETWEEN 1 PRECEDING AND 1 FOLLOWING)#x, first(id) OVER (ORDER BY f_numeric ASC NULLS FIRST RANGE BETWEEN 1 PRECEDING AND 1 FOLLOWING)#x, last(id) OVER (ORDER BY f_numeric ASC NULLS FIRST RANGE BETWEEN 1 PRECEDING AND 1 FOLLOWING)#x]
   +- Window [first(id#x, false) windowspecdefinition(f_numeric#x ASC NULLS FIRST, specifiedwindowframe(RangeFrame, -1, 1)) AS first(id) OVER (ORDER BY f_numeric ASC NULLS FIRST RANGE BETWEEN 1 PRECEDING AND 1 FOLLOWING)#x, last(id#x, false) windowspecdefinition(f_numeric#x ASC NULLS FIRST, specifiedwindowframe(RangeFrame, -1, 1)) AS last(id) OVER (ORDER BY f_numeric ASC NULLS FIRST RANGE BETWEEN 1 PRECEDING AND 1 FOLLOWING)#x], [f_numeric#x ASC NULLS FIRST]
      +- Project [id#x, f_numeric#x]
         +- SubqueryAlias spark_catalog.default.numerics
            +- Relation spark_catalog.default.numerics[id#x,f_float4#x,f_float8#x,f_numeric#x] parquet


-- !query
select id, f_numeric, first(id) over w, last(id) over w
from numerics
window w as (order by f_numeric range between
             1 preceding and 1.1 following)
-- !query analysis
Project [id#x, f_numeric#x, first(id) OVER (ORDER BY f_numeric ASC NULLS FIRST RANGE BETWEEN 1 PRECEDING AND 1.1 FOLLOWING)#x, last(id) OVER (ORDER BY f_numeric ASC NULLS FIRST RANGE BETWEEN 1 PRECEDING AND 1.1 FOLLOWING)#x]
+- Project [id#x, f_numeric#x, first(id) OVER (ORDER BY f_numeric ASC NULLS FIRST RANGE BETWEEN 1 PRECEDING AND 1.1 FOLLOWING)#x, last(id) OVER (ORDER BY f_numeric ASC NULLS FIRST RANGE BETWEEN 1 PRECEDING AND 1.1 FOLLOWING)#x, first(id) OVER (ORDER BY f_numeric ASC NULLS FIRST RANGE BETWEEN 1 PRECEDING AND 1.1 FOLLOWING)#x, last(id) OVER (ORDER BY f_numeric ASC NULLS FIRST RANGE BETWEEN 1 PRECEDING AND 1.1 FOLLOWING)#x]
   +- Window [first(id#x, false) windowspecdefinition(f_numeric#x ASC NULLS FIRST, specifiedwindowframe(RangeFrame, -1, cast(1.1 as int))) AS first(id) OVER (ORDER BY f_numeric ASC NULLS FIRST RANGE BETWEEN 1 PRECEDING AND 1.1 FOLLOWING)#x, last(id#x, false) windowspecdefinition(f_numeric#x ASC NULLS FIRST, specifiedwindowframe(RangeFrame, -1, cast(1.1 as int))) AS last(id) OVER (ORDER BY f_numeric ASC NULLS FIRST RANGE BETWEEN 1 PRECEDING AND 1.1 FOLLOWING)#x], [f_numeric#x ASC NULLS FIRST]
      +- Project [id#x, f_numeric#x]
         +- SubqueryAlias spark_catalog.default.numerics
            +- Relation spark_catalog.default.numerics[id#x,f_float4#x,f_float8#x,f_numeric#x] parquet


-- !query
select id, f_numeric, first(id) over w, last(id) over w
from numerics
window w as (order by f_numeric range between
             1 preceding and 1.1 following)
-- !query analysis
Project [id#x, f_numeric#x, first(id) OVER (ORDER BY f_numeric ASC NULLS FIRST RANGE BETWEEN 1 PRECEDING AND 1.1 FOLLOWING)#x, last(id) OVER (ORDER BY f_numeric ASC NULLS FIRST RANGE BETWEEN 1 PRECEDING AND 1.1 FOLLOWING)#x]
+- Project [id#x, f_numeric#x, first(id) OVER (ORDER BY f_numeric ASC NULLS FIRST RANGE BETWEEN 1 PRECEDING AND 1.1 FOLLOWING)#x, last(id) OVER (ORDER BY f_numeric ASC NULLS FIRST RANGE BETWEEN 1 PRECEDING AND 1.1 FOLLOWING)#x, first(id) OVER (ORDER BY f_numeric ASC NULLS FIRST RANGE BETWEEN 1 PRECEDING AND 1.1 FOLLOWING)#x, last(id) OVER (ORDER BY f_numeric ASC NULLS FIRST RANGE BETWEEN 1 PRECEDING AND 1.1 FOLLOWING)#x]
   +- Window [first(id#x, false) windowspecdefinition(f_numeric#x ASC NULLS FIRST, specifiedwindowframe(RangeFrame, -1, cast(1.1 as int))) AS first(id) OVER (ORDER BY f_numeric ASC NULLS FIRST RANGE BETWEEN 1 PRECEDING AND 1.1 FOLLOWING)#x, last(id#x, false) windowspecdefinition(f_numeric#x ASC NULLS FIRST, specifiedwindowframe(RangeFrame, -1, cast(1.1 as int))) AS last(id) OVER (ORDER BY f_numeric ASC NULLS FIRST RANGE BETWEEN 1 PRECEDING AND 1.1 FOLLOWING)#x], [f_numeric#x ASC NULLS FIRST]
      +- Project [id#x, f_numeric#x]
         +- SubqueryAlias spark_catalog.default.numerics
            +- Relation spark_catalog.default.numerics[id#x,f_float4#x,f_float8#x,f_numeric#x] parquet


-- !query
select id, f_numeric, first(id) over w, last(id) over w
from numerics
window w as (order by f_numeric range between
             1.1 preceding and 'NaN' following)
-- !query analysis
org.apache.spark.SparkNumberFormatException
{
  "errorClass" : "CAST_INVALID_INPUT",
  "sqlState" : "22018",
  "messageParameters" : {
    "ansiConfig" : "\"spark.sql.ansi.enabled\"",
    "expression" : "'NaN'",
    "sourceType" : "\"STRING\"",
    "targetType" : "\"INT\""
  },
  "queryContext" : [ {
    "objectType" : "",
    "objectName" : "",
    "startIndex" : 83,
    "stopIndex" : 163,
    "fragment" : "(order by f_numeric range between\n             1.1 preceding and 'NaN' following)"
  } ]
}


-- !query
drop table empsalary
-- !query analysis
DropTable false, false
+- ResolvedIdentifier V2SessionCatalog(spark_catalog), default.empsalary


-- !query
drop table numerics
-- !query analysis
DropTable false, false
+- ResolvedIdentifier V2SessionCatalog(spark_catalog), default.numerics
