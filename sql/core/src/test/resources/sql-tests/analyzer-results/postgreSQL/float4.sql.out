-- Automatically generated by SQLQueryTestSuite
-- !query
CREATE TABLE FLOAT4_TBL (f1  float) USING parquet
-- !query analysis
CreateDataSourceTableCommand `spark_catalog`.`default`.`FLOAT4_TBL`, false


-- !query
INSERT INTO FLOAT4_TBL VALUES (float('    0.0'))
-- !query analysis
InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/float4_tbl, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/float4_tbl], Append, `spark_catalog`.`default`.`float4_tbl`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/float4_tbl), [f1]
+- Project [cast(col1#x as float) AS f1#x]
   +- LocalRelation [col1#x]


-- !query
INSERT INTO FLOAT4_TBL VALUES (float('1004.30   '))
-- !query analysis
InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/float4_tbl, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/float4_tbl], Append, `spark_catalog`.`default`.`float4_tbl`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/float4_tbl), [f1]
+- Project [cast(col1#x as float) AS f1#x]
   +- LocalRelation [col1#x]


-- !query
INSERT INTO FLOAT4_TBL VALUES (float('     -34.84    '))
-- !query analysis
InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/float4_tbl, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/float4_tbl], Append, `spark_catalog`.`default`.`float4_tbl`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/float4_tbl), [f1]
+- Project [cast(col1#x as float) AS f1#x]
   +- LocalRelation [col1#x]


-- !query
INSERT INTO FLOAT4_TBL VALUES (float('1.2345678901234e+20'))
-- !query analysis
InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/float4_tbl, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/float4_tbl], Append, `spark_catalog`.`default`.`float4_tbl`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/float4_tbl), [f1]
+- Project [cast(col1#x as float) AS f1#x]
   +- LocalRelation [col1#x]


-- !query
INSERT INTO FLOAT4_TBL VALUES (float('1.2345678901234e-20'))
-- !query analysis
InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/float4_tbl, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/float4_tbl], Append, `spark_catalog`.`default`.`float4_tbl`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/float4_tbl), [f1]
+- Project [cast(col1#x as float) AS f1#x]
   +- LocalRelation [col1#x]


-- !query
SELECT float('NaN')
-- !query analysis
Project [cast(NaN as float) AS NaN#x]
+- OneRowRelation


-- !query
SELECT float('nan')
-- !query analysis
Project [cast(nan as float) AS nan#x]
+- OneRowRelation


-- !query
SELECT float('   NAN  ')
-- !query analysis
Project [cast(   NAN   as float) AS    NAN  #x]
+- OneRowRelation


-- !query
SELECT float('infinity')
-- !query analysis
Project [cast(infinity as float) AS infinity#x]
+- OneRowRelation


-- !query
SELECT float('          -INFINiTY   ')
-- !query analysis
Project [cast(          -INFINiTY    as float) AS           -INFINiTY   #x]
+- OneRowRelation


-- !query
SELECT float('N A N')
-- !query analysis
Project [cast(N A N as float) AS N A N#x]
+- OneRowRelation


-- !query
SELECT float('NaN x')
-- !query analysis
Project [cast(NaN x as float) AS NaN x#x]
+- OneRowRelation


-- !query
SELECT float(' INFINITY    x')
-- !query analysis
Project [cast( INFINITY    x as float) AS  INFINITY    x#x]
+- OneRowRelation


-- !query
SELECT float('Infinity') + 100.0
-- !query analysis
Project [(cast(cast(Infinity as float) as double) + cast(100.0 as double)) AS (Infinity + 100.0)#x]
+- OneRowRelation


-- !query
SELECT float('Infinity') / float('Infinity')
-- !query analysis
Project [(cast(cast(Infinity as float) as double) / cast(cast(Infinity as float) as double)) AS (Infinity / Infinity)#x]
+- OneRowRelation


-- !query
SELECT float('nan') / float('nan')
-- !query analysis
Project [(cast(cast(nan as float) as double) / cast(cast(nan as float) as double)) AS (nan / nan)#x]
+- OneRowRelation


-- !query
SELECT float(decimal('nan'))
-- !query analysis
Project [cast(cast(nan as decimal(10,0)) as float) AS nan#x]
+- OneRowRelation


-- !query
SELECT '' AS five, * FROM FLOAT4_TBL
-- !query analysis
Project [ AS five#x, f1#x]
+- SubqueryAlias spark_catalog.default.float4_tbl
   +- Relation spark_catalog.default.float4_tbl[f1#x] parquet


-- !query
SELECT '' AS four, f.* FROM FLOAT4_TBL f WHERE f.f1 <> '1004.3'
-- !query analysis
Project [ AS four#x, f1#x]
+- Filter NOT (cast(f1#x as double) = cast(1004.3 as double))
   +- SubqueryAlias f
      +- SubqueryAlias spark_catalog.default.float4_tbl
         +- Relation spark_catalog.default.float4_tbl[f1#x] parquet


-- !query
SELECT '' AS one, f.* FROM FLOAT4_TBL f WHERE f.f1 = '1004.3'
-- !query analysis
Project [ AS one#x, f1#x]
+- Filter (cast(f1#x as double) = cast(1004.3 as double))
   +- SubqueryAlias f
      +- SubqueryAlias spark_catalog.default.float4_tbl
         +- Relation spark_catalog.default.float4_tbl[f1#x] parquet


-- !query
SELECT '' AS three, f.* FROM FLOAT4_TBL f WHERE '1004.3' > f.f1
-- !query analysis
Project [ AS three#x, f1#x]
+- Filter (cast(1004.3 as double) > cast(f1#x as double))
   +- SubqueryAlias f
      +- SubqueryAlias spark_catalog.default.float4_tbl
         +- Relation spark_catalog.default.float4_tbl[f1#x] parquet


-- !query
SELECT '' AS three, f.* FROM FLOAT4_TBL f WHERE  f.f1 < '1004.3'
-- !query analysis
Project [ AS three#x, f1#x]
+- Filter (cast(f1#x as double) < cast(1004.3 as double))
   +- SubqueryAlias f
      +- SubqueryAlias spark_catalog.default.float4_tbl
         +- Relation spark_catalog.default.float4_tbl[f1#x] parquet


-- !query
SELECT '' AS four, f.* FROM FLOAT4_TBL f WHERE '1004.3' >= f.f1
-- !query analysis
Project [ AS four#x, f1#x]
+- Filter (cast(1004.3 as double) >= cast(f1#x as double))
   +- SubqueryAlias f
      +- SubqueryAlias spark_catalog.default.float4_tbl
         +- Relation spark_catalog.default.float4_tbl[f1#x] parquet


-- !query
SELECT '' AS four, f.* FROM FLOAT4_TBL f WHERE  f.f1 <= '1004.3'
-- !query analysis
Project [ AS four#x, f1#x]
+- Filter (cast(f1#x as double) <= cast(1004.3 as double))
   +- SubqueryAlias f
      +- SubqueryAlias spark_catalog.default.float4_tbl
         +- Relation spark_catalog.default.float4_tbl[f1#x] parquet


-- !query
SELECT '' AS three, f.f1, f.f1 * '-10' AS x FROM FLOAT4_TBL f
   WHERE f.f1 > '0.0'
-- !query analysis
Project [ AS three#x, f1#x, (cast(f1#x as double) * cast(-10 as double)) AS x#x]
+- Filter (cast(f1#x as double) > cast(0.0 as double))
   +- SubqueryAlias f
      +- SubqueryAlias spark_catalog.default.float4_tbl
         +- Relation spark_catalog.default.float4_tbl[f1#x] parquet


-- !query
SELECT '' AS three, f.f1, f.f1 + '-10' AS x FROM FLOAT4_TBL f
   WHERE f.f1 > '0.0'
-- !query analysis
Project [ AS three#x, f1#x, (cast(f1#x as double) + cast(-10 as double)) AS x#x]
+- Filter (cast(f1#x as double) > cast(0.0 as double))
   +- SubqueryAlias f
      +- SubqueryAlias spark_catalog.default.float4_tbl
         +- Relation spark_catalog.default.float4_tbl[f1#x] parquet


-- !query
SELECT '' AS three, f.f1, f.f1 / '-10' AS x FROM FLOAT4_TBL f
   WHERE f.f1 > '0.0'
-- !query analysis
Project [ AS three#x, f1#x, (cast(f1#x as double) / cast(-10 as double)) AS x#x]
+- Filter (cast(f1#x as double) > cast(0.0 as double))
   +- SubqueryAlias f
      +- SubqueryAlias spark_catalog.default.float4_tbl
         +- Relation spark_catalog.default.float4_tbl[f1#x] parquet


-- !query
SELECT '' AS three, f.f1, f.f1 - '-10' AS x FROM FLOAT4_TBL f
   WHERE f.f1 > '0.0'
-- !query analysis
Project [ AS three#x, f1#x, (cast(f1#x as double) - cast(-10 as double)) AS x#x]
+- Filter (cast(f1#x as double) > cast(0.0 as double))
   +- SubqueryAlias f
      +- SubqueryAlias spark_catalog.default.float4_tbl
         +- Relation spark_catalog.default.float4_tbl[f1#x] parquet


-- !query
SELECT '' AS five, * FROM FLOAT4_TBL
-- !query analysis
Project [ AS five#x, f1#x]
+- SubqueryAlias spark_catalog.default.float4_tbl
   +- Relation spark_catalog.default.float4_tbl[f1#x] parquet


-- !query
SELECT smallint(float('32767.4'))
-- !query analysis
Project [cast(cast(32767.4 as float) as smallint) AS 32767.4#x]
+- OneRowRelation


-- !query
SELECT smallint(float('32767.6'))
-- !query analysis
Project [cast(cast(32767.6 as float) as smallint) AS 32767.6#x]
+- OneRowRelation


-- !query
SELECT smallint(float('-32768.4'))
-- !query analysis
Project [cast(cast(-32768.4 as float) as smallint) AS -32768.4#x]
+- OneRowRelation


-- !query
SELECT smallint(float('-32768.6'))
-- !query analysis
Project [cast(cast(-32768.6 as float) as smallint) AS -32768.6#x]
+- OneRowRelation


-- !query
SELECT int(float('2147483520'))
-- !query analysis
Project [cast(cast(2147483520 as float) as int) AS 2147483520#x]
+- OneRowRelation


-- !query
SELECT int(float('2147483647'))
-- !query analysis
Project [cast(cast(2147483647 as float) as int) AS 2147483647#x]
+- OneRowRelation


-- !query
SELECT int(float('-2147483648.5'))
-- !query analysis
Project [cast(cast(-2147483648.5 as float) as int) AS -2147483648.5#x]
+- OneRowRelation


-- !query
SELECT int(float('-2147483900'))
-- !query analysis
Project [cast(cast(-2147483900 as float) as int) AS -2147483900#x]
+- OneRowRelation


-- !query
SELECT bigint(float('9223369837831520256'))
-- !query analysis
Project [cast(cast(9223369837831520256 as float) as bigint) AS 9223369837831520256#xL]
+- OneRowRelation


-- !query
SELECT bigint(float('9223372036854775807'))
-- !query analysis
Project [cast(cast(9223372036854775807 as float) as bigint) AS 9223372036854775807#xL]
+- OneRowRelation


-- !query
SELECT bigint(float('-9223372036854775808.5'))
-- !query analysis
Project [cast(cast(-9223372036854775808.5 as float) as bigint) AS -9223372036854775808.5#xL]
+- OneRowRelation


-- !query
SELECT bigint(float('-9223380000000000000'))
-- !query analysis
Project [cast(cast(-9223380000000000000 as float) as bigint) AS -9223380000000000000#xL]
+- OneRowRelation


-- !query
DROP TABLE FLOAT4_TBL
-- !query analysis
DropTable false, false
+- ResolvedIdentifier V2SessionCatalog(spark_catalog), default.FLOAT4_TBL
