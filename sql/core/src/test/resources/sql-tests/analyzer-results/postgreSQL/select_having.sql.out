-- Automatically generated by SQLQueryTestSuite
-- !query
CREATE TABLE test_having (a int, b int, c string, d string) USING parquet
-- !query analysis
CreateDataSourceTableCommand `spark_catalog`.`default`.`test_having`, false


-- !query
INSERT INTO test_having VALUES (0, 1, 'XXXX', 'A')
-- !query analysis
InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/test_having, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/test_having], Append, `spark_catalog`.`default`.`test_having`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/test_having), [a, b, c, d]
+- Project [cast(col1#x as int) AS a#x, cast(col2#x as int) AS b#x, cast(col3#x as string) AS c#x, cast(col4#x as string) AS d#x]
   +- LocalRelation [col1#x, col2#x, col3#x, col4#x]


-- !query
INSERT INTO test_having VALUES (1, 2, 'AAAA', 'b')
-- !query analysis
InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/test_having, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/test_having], Append, `spark_catalog`.`default`.`test_having`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/test_having), [a, b, c, d]
+- Project [cast(col1#x as int) AS a#x, cast(col2#x as int) AS b#x, cast(col3#x as string) AS c#x, cast(col4#x as string) AS d#x]
   +- LocalRelation [col1#x, col2#x, col3#x, col4#x]


-- !query
INSERT INTO test_having VALUES (2, 2, 'AAAA', 'c')
-- !query analysis
InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/test_having, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/test_having], Append, `spark_catalog`.`default`.`test_having`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/test_having), [a, b, c, d]
+- Project [cast(col1#x as int) AS a#x, cast(col2#x as int) AS b#x, cast(col3#x as string) AS c#x, cast(col4#x as string) AS d#x]
   +- LocalRelation [col1#x, col2#x, col3#x, col4#x]


-- !query
INSERT INTO test_having VALUES (3, 3, 'BBBB', 'D')
-- !query analysis
InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/test_having, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/test_having], Append, `spark_catalog`.`default`.`test_having`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/test_having), [a, b, c, d]
+- Project [cast(col1#x as int) AS a#x, cast(col2#x as int) AS b#x, cast(col3#x as string) AS c#x, cast(col4#x as string) AS d#x]
   +- LocalRelation [col1#x, col2#x, col3#x, col4#x]


-- !query
INSERT INTO test_having VALUES (4, 3, 'BBBB', 'e')
-- !query analysis
InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/test_having, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/test_having], Append, `spark_catalog`.`default`.`test_having`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/test_having), [a, b, c, d]
+- Project [cast(col1#x as int) AS a#x, cast(col2#x as int) AS b#x, cast(col3#x as string) AS c#x, cast(col4#x as string) AS d#x]
   +- LocalRelation [col1#x, col2#x, col3#x, col4#x]


-- !query
INSERT INTO test_having VALUES (5, 3, 'bbbb', 'F')
-- !query analysis
InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/test_having, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/test_having], Append, `spark_catalog`.`default`.`test_having`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/test_having), [a, b, c, d]
+- Project [cast(col1#x as int) AS a#x, cast(col2#x as int) AS b#x, cast(col3#x as string) AS c#x, cast(col4#x as string) AS d#x]
   +- LocalRelation [col1#x, col2#x, col3#x, col4#x]


-- !query
INSERT INTO test_having VALUES (6, 4, 'cccc', 'g')
-- !query analysis
InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/test_having, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/test_having], Append, `spark_catalog`.`default`.`test_having`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/test_having), [a, b, c, d]
+- Project [cast(col1#x as int) AS a#x, cast(col2#x as int) AS b#x, cast(col3#x as string) AS c#x, cast(col4#x as string) AS d#x]
   +- LocalRelation [col1#x, col2#x, col3#x, col4#x]


-- !query
INSERT INTO test_having VALUES (7, 4, 'cccc', 'h')
-- !query analysis
InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/test_having, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/test_having], Append, `spark_catalog`.`default`.`test_having`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/test_having), [a, b, c, d]
+- Project [cast(col1#x as int) AS a#x, cast(col2#x as int) AS b#x, cast(col3#x as string) AS c#x, cast(col4#x as string) AS d#x]
   +- LocalRelation [col1#x, col2#x, col3#x, col4#x]


-- !query
INSERT INTO test_having VALUES (8, 4, 'CCCC', 'I')
-- !query analysis
InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/test_having, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/test_having], Append, `spark_catalog`.`default`.`test_having`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/test_having), [a, b, c, d]
+- Project [cast(col1#x as int) AS a#x, cast(col2#x as int) AS b#x, cast(col3#x as string) AS c#x, cast(col4#x as string) AS d#x]
   +- LocalRelation [col1#x, col2#x, col3#x, col4#x]


-- !query
INSERT INTO test_having VALUES (9, 4, 'CCCC', 'j')
-- !query analysis
InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/test_having, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/test_having], Append, `spark_catalog`.`default`.`test_having`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/test_having), [a, b, c, d]
+- Project [cast(col1#x as int) AS a#x, cast(col2#x as int) AS b#x, cast(col3#x as string) AS c#x, cast(col4#x as string) AS d#x]
   +- LocalRelation [col1#x, col2#x, col3#x, col4#x]


-- !query
SELECT b, c FROM test_having
	GROUP BY b, c HAVING count(*) = 1 ORDER BY b, c
-- !query analysis
Sort [b#x ASC NULLS FIRST, c#x ASC NULLS FIRST], true
+- Project [b#x, c#x]
   +- Filter (count(1)#xL = cast(1 as bigint))
      +- Aggregate [b#x, c#x], [b#x, c#x, count(1) AS count(1)#xL]
         +- SubqueryAlias spark_catalog.default.test_having
            +- Relation spark_catalog.default.test_having[a#x,b#x,c#x,d#x] parquet


-- !query
SELECT b, c FROM test_having
	GROUP BY b, c HAVING b = 3 ORDER BY b, c
-- !query analysis
Sort [b#x ASC NULLS FIRST, c#x ASC NULLS FIRST], true
+- Filter (b#x = 3)
   +- Aggregate [b#x, c#x], [b#x, c#x]
      +- SubqueryAlias spark_catalog.default.test_having
         +- Relation spark_catalog.default.test_having[a#x,b#x,c#x,d#x] parquet


-- !query
SELECT c, max(a) FROM test_having
	GROUP BY c HAVING count(*) > 2 OR min(a) = max(a)
	ORDER BY c
-- !query analysis
Sort [c#x ASC NULLS FIRST], true
+- Project [c#x, max(a)#x]
   +- Filter ((count(1)#xL > cast(2 as bigint)) OR (min(a)#x = max(a)#x))
      +- Aggregate [c#x], [c#x, max(a#x) AS max(a)#x, count(1) AS count(1)#xL, min(a#x) AS min(a)#x]
         +- SubqueryAlias spark_catalog.default.test_having
            +- Relation spark_catalog.default.test_having[a#x,b#x,c#x,d#x] parquet


-- !query
SELECT min(a), max(a) FROM test_having HAVING min(a) = max(a)
-- !query analysis
Filter (min(a)#x = max(a)#x)
+- Aggregate [min(a#x) AS min(a)#x, max(a#x) AS max(a)#x]
   +- SubqueryAlias spark_catalog.default.test_having
      +- Relation spark_catalog.default.test_having[a#x,b#x,c#x,d#x] parquet


-- !query
SELECT min(a), max(a) FROM test_having HAVING min(a) < max(a)
-- !query analysis
Filter (min(a)#x < max(a)#x)
+- Aggregate [min(a#x) AS min(a)#x, max(a#x) AS max(a)#x]
   +- SubqueryAlias spark_catalog.default.test_having
      +- Relation spark_catalog.default.test_having[a#x,b#x,c#x,d#x] parquet


-- !query
SELECT a FROM test_having HAVING min(a) < max(a)
-- !query analysis
org.apache.spark.sql.catalyst.ExtendedAnalysisException
{
  "errorClass" : "MISSING_GROUP_BY",
  "sqlState" : "42803",
  "queryContext" : [ {
    "objectType" : "",
    "objectName" : "",
    "startIndex" : 1,
    "stopIndex" : 48,
    "fragment" : "SELECT a FROM test_having HAVING min(a) < max(a)"
  } ]
}


-- !query
SELECT 1 AS one FROM test_having HAVING a > 1
-- !query analysis
org.apache.spark.sql.catalyst.ExtendedAnalysisException
{
  "errorClass" : "UNRESOLVED_COLUMN.WITH_SUGGESTION",
  "sqlState" : "42703",
  "messageParameters" : {
    "objectName" : "`a`",
    "proposal" : "`one`"
  },
  "queryContext" : [ {
    "objectType" : "",
    "objectName" : "",
    "startIndex" : 41,
    "stopIndex" : 41,
    "fragment" : "a"
  } ]
}


-- !query
SELECT 1 AS one FROM test_having HAVING 1 > 2
-- !query analysis
Filter (one#x > 2)
+- Aggregate [1 AS one#x]
   +- SubqueryAlias spark_catalog.default.test_having
      +- Relation spark_catalog.default.test_having[a#x,b#x,c#x,d#x] parquet


-- !query
SELECT 1 AS one FROM test_having HAVING 1 < 2
-- !query analysis
Filter (one#x < 2)
+- Aggregate [1 AS one#x]
   +- SubqueryAlias spark_catalog.default.test_having
      +- Relation spark_catalog.default.test_having[a#x,b#x,c#x,d#x] parquet


-- !query
SELECT 1 AS one FROM test_having WHERE 1/a = 1 HAVING 1 < 2
-- !query analysis
Filter (one#x < 2)
+- Aggregate [1 AS one#x]
   +- Filter ((cast(1 as double) / cast(a#x as double)) = cast(1 as double))
      +- SubqueryAlias spark_catalog.default.test_having
         +- Relation spark_catalog.default.test_having[a#x,b#x,c#x,d#x] parquet


-- !query
DROP TABLE test_having
-- !query analysis
DropTable false, false
+- ResolvedIdentifier V2SessionCatalog(spark_catalog), default.test_having
