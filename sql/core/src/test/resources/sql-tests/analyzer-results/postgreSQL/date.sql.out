-- Automatically generated by SQLQueryTestSuite
-- !query
CREATE TABLE DATE_TBL (f1 date) USING parquet
-- !query analysis
CreateDataSourceTableCommand `spark_catalog`.`default`.`DATE_TBL`, false


-- !query
INSERT INTO DATE_TBL VALUES (date('1957-04-09'))
-- !query analysis
InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/date_tbl, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/date_tbl], Append, `spark_catalog`.`default`.`date_tbl`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/date_tbl), [f1]
+- Project [cast(col1#x as date) AS f1#x]
   +- LocalRelation [col1#x]


-- !query
INSERT INTO DATE_TBL VALUES (date('1957-06-13'))
-- !query analysis
InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/date_tbl, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/date_tbl], Append, `spark_catalog`.`default`.`date_tbl`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/date_tbl), [f1]
+- Project [cast(col1#x as date) AS f1#x]
   +- LocalRelation [col1#x]


-- !query
INSERT INTO DATE_TBL VALUES (date('1996-02-28'))
-- !query analysis
InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/date_tbl, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/date_tbl], Append, `spark_catalog`.`default`.`date_tbl`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/date_tbl), [f1]
+- Project [cast(col1#x as date) AS f1#x]
   +- LocalRelation [col1#x]


-- !query
INSERT INTO DATE_TBL VALUES (date('1996-02-29'))
-- !query analysis
InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/date_tbl, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/date_tbl], Append, `spark_catalog`.`default`.`date_tbl`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/date_tbl), [f1]
+- Project [cast(col1#x as date) AS f1#x]
   +- LocalRelation [col1#x]


-- !query
INSERT INTO DATE_TBL VALUES (date('1996-03-01'))
-- !query analysis
InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/date_tbl, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/date_tbl], Append, `spark_catalog`.`default`.`date_tbl`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/date_tbl), [f1]
+- Project [cast(col1#x as date) AS f1#x]
   +- LocalRelation [col1#x]


-- !query
INSERT INTO DATE_TBL VALUES (date('1996-03-02'))
-- !query analysis
InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/date_tbl, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/date_tbl], Append, `spark_catalog`.`default`.`date_tbl`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/date_tbl), [f1]
+- Project [cast(col1#x as date) AS f1#x]
   +- LocalRelation [col1#x]


-- !query
INSERT INTO DATE_TBL VALUES (date('1997-02-28'))
-- !query analysis
InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/date_tbl, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/date_tbl], Append, `spark_catalog`.`default`.`date_tbl`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/date_tbl), [f1]
+- Project [cast(col1#x as date) AS f1#x]
   +- LocalRelation [col1#x]


-- !query
INSERT INTO DATE_TBL VALUES (date('1997-03-01'))
-- !query analysis
InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/date_tbl, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/date_tbl], Append, `spark_catalog`.`default`.`date_tbl`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/date_tbl), [f1]
+- Project [cast(col1#x as date) AS f1#x]
   +- LocalRelation [col1#x]


-- !query
INSERT INTO DATE_TBL VALUES (date('1997-03-02'))
-- !query analysis
InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/date_tbl, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/date_tbl], Append, `spark_catalog`.`default`.`date_tbl`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/date_tbl), [f1]
+- Project [cast(col1#x as date) AS f1#x]
   +- LocalRelation [col1#x]


-- !query
INSERT INTO DATE_TBL VALUES (date('2000-04-01'))
-- !query analysis
InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/date_tbl, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/date_tbl], Append, `spark_catalog`.`default`.`date_tbl`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/date_tbl), [f1]
+- Project [cast(col1#x as date) AS f1#x]
   +- LocalRelation [col1#x]


-- !query
INSERT INTO DATE_TBL VALUES (date('2000-04-02'))
-- !query analysis
InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/date_tbl, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/date_tbl], Append, `spark_catalog`.`default`.`date_tbl`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/date_tbl), [f1]
+- Project [cast(col1#x as date) AS f1#x]
   +- LocalRelation [col1#x]


-- !query
INSERT INTO DATE_TBL VALUES (date('2000-04-03'))
-- !query analysis
InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/date_tbl, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/date_tbl], Append, `spark_catalog`.`default`.`date_tbl`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/date_tbl), [f1]
+- Project [cast(col1#x as date) AS f1#x]
   +- LocalRelation [col1#x]


-- !query
INSERT INTO DATE_TBL VALUES (date('2038-04-08'))
-- !query analysis
InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/date_tbl, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/date_tbl], Append, `spark_catalog`.`default`.`date_tbl`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/date_tbl), [f1]
+- Project [cast(col1#x as date) AS f1#x]
   +- LocalRelation [col1#x]


-- !query
INSERT INTO DATE_TBL VALUES (date('2039-04-09'))
-- !query analysis
InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/date_tbl, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/date_tbl], Append, `spark_catalog`.`default`.`date_tbl`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/date_tbl), [f1]
+- Project [cast(col1#x as date) AS f1#x]
   +- LocalRelation [col1#x]


-- !query
INSERT INTO DATE_TBL VALUES (date('2040-04-10'))
-- !query analysis
InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/date_tbl, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/date_tbl], Append, `spark_catalog`.`default`.`date_tbl`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/date_tbl), [f1]
+- Project [cast(col1#x as date) AS f1#x]
   +- LocalRelation [col1#x]


-- !query
SELECT f1 AS `Fifteen` FROM DATE_TBL
-- !query analysis
Project [f1#x AS Fifteen#x]
+- SubqueryAlias spark_catalog.default.date_tbl
   +- Relation spark_catalog.default.date_tbl[f1#x] parquet


-- !query
SELECT f1 AS `Nine` FROM DATE_TBL WHERE f1 < '2000-01-01'
-- !query analysis
Project [f1#x AS Nine#x]
+- Filter (f1#x < cast(2000-01-01 as date))
   +- SubqueryAlias spark_catalog.default.date_tbl
      +- Relation spark_catalog.default.date_tbl[f1#x] parquet


-- !query
SELECT f1 AS `Three` FROM DATE_TBL
  WHERE f1 BETWEEN '2000-01-01' AND '2001-01-01'
-- !query analysis
Project [f1#x AS Three#x]
+- Filter between(f1#x, 2000-01-01, 2001-01-01)
   +- SubqueryAlias spark_catalog.default.date_tbl
      +- Relation spark_catalog.default.date_tbl[f1#x] parquet


-- !query
SELECT date '1999-01-08'
-- !query analysis
[Analyzer test output redacted due to nondeterminism]


-- !query
SELECT date '1999-01-18'
-- !query analysis
[Analyzer test output redacted due to nondeterminism]


-- !query
SELECT date '1999 Jan 08'
-- !query analysis
org.apache.spark.sql.catalyst.parser.ParseException
{
  "errorClass" : "INVALID_TYPED_LITERAL",
  "sqlState" : "42604",
  "messageParameters" : {
    "value" : "'1999 Jan 08'",
    "valueType" : "\"DATE\""
  },
  "queryContext" : [ {
    "objectType" : "",
    "objectName" : "",
    "startIndex" : 8,
    "stopIndex" : 25,
    "fragment" : "date '1999 Jan 08'"
  } ]
}


-- !query
SELECT date '1999 08 Jan'
-- !query analysis
org.apache.spark.sql.catalyst.parser.ParseException
{
  "errorClass" : "INVALID_TYPED_LITERAL",
  "sqlState" : "42604",
  "messageParameters" : {
    "value" : "'1999 08 Jan'",
    "valueType" : "\"DATE\""
  },
  "queryContext" : [ {
    "objectType" : "",
    "objectName" : "",
    "startIndex" : 8,
    "stopIndex" : 25,
    "fragment" : "date '1999 08 Jan'"
  } ]
}


-- !query
SELECT date '1999-01-08'
-- !query analysis
[Analyzer test output redacted due to nondeterminism]


-- !query
SELECT date '1999-08-01'
-- !query analysis
[Analyzer test output redacted due to nondeterminism]


-- !query
SELECT date '1999 01 08'
-- !query analysis
org.apache.spark.sql.catalyst.parser.ParseException
{
  "errorClass" : "INVALID_TYPED_LITERAL",
  "sqlState" : "42604",
  "messageParameters" : {
    "value" : "'1999 01 08'",
    "valueType" : "\"DATE\""
  },
  "queryContext" : [ {
    "objectType" : "",
    "objectName" : "",
    "startIndex" : 8,
    "stopIndex" : 24,
    "fragment" : "date '1999 01 08'"
  } ]
}


-- !query
SELECT date '1999 08 01'
-- !query analysis
org.apache.spark.sql.catalyst.parser.ParseException
{
  "errorClass" : "INVALID_TYPED_LITERAL",
  "sqlState" : "42604",
  "messageParameters" : {
    "value" : "'1999 08 01'",
    "valueType" : "\"DATE\""
  },
  "queryContext" : [ {
    "objectType" : "",
    "objectName" : "",
    "startIndex" : 8,
    "stopIndex" : 24,
    "fragment" : "date '1999 08 01'"
  } ]
}


-- !query
SELECT date '1999-01-08'
-- !query analysis
[Analyzer test output redacted due to nondeterminism]


-- !query
SELECT date '1999 Jan 08'
-- !query analysis
org.apache.spark.sql.catalyst.parser.ParseException
{
  "errorClass" : "INVALID_TYPED_LITERAL",
  "sqlState" : "42604",
  "messageParameters" : {
    "value" : "'1999 Jan 08'",
    "valueType" : "\"DATE\""
  },
  "queryContext" : [ {
    "objectType" : "",
    "objectName" : "",
    "startIndex" : 8,
    "stopIndex" : 25,
    "fragment" : "date '1999 Jan 08'"
  } ]
}


-- !query
SELECT date '1999 08 Jan'
-- !query analysis
org.apache.spark.sql.catalyst.parser.ParseException
{
  "errorClass" : "INVALID_TYPED_LITERAL",
  "sqlState" : "42604",
  "messageParameters" : {
    "value" : "'1999 08 Jan'",
    "valueType" : "\"DATE\""
  },
  "queryContext" : [ {
    "objectType" : "",
    "objectName" : "",
    "startIndex" : 8,
    "stopIndex" : 25,
    "fragment" : "date '1999 08 Jan'"
  } ]
}


-- !query
SELECT date '1999-01-08'
-- !query analysis
[Analyzer test output redacted due to nondeterminism]


-- !query
SELECT date '1999-08-01'
-- !query analysis
[Analyzer test output redacted due to nondeterminism]


-- !query
SELECT date '1999 01 08'
-- !query analysis
org.apache.spark.sql.catalyst.parser.ParseException
{
  "errorClass" : "INVALID_TYPED_LITERAL",
  "sqlState" : "42604",
  "messageParameters" : {
    "value" : "'1999 01 08'",
    "valueType" : "\"DATE\""
  },
  "queryContext" : [ {
    "objectType" : "",
    "objectName" : "",
    "startIndex" : 8,
    "stopIndex" : 24,
    "fragment" : "date '1999 01 08'"
  } ]
}


-- !query
SELECT date '1999 08 01'
-- !query analysis
org.apache.spark.sql.catalyst.parser.ParseException
{
  "errorClass" : "INVALID_TYPED_LITERAL",
  "sqlState" : "42604",
  "messageParameters" : {
    "value" : "'1999 08 01'",
    "valueType" : "\"DATE\""
  },
  "queryContext" : [ {
    "objectType" : "",
    "objectName" : "",
    "startIndex" : 8,
    "stopIndex" : 24,
    "fragment" : "date '1999 08 01'"
  } ]
}


-- !query
SELECT date '1999-01-08'
-- !query analysis
[Analyzer test output redacted due to nondeterminism]


-- !query
SELECT date '1999-01-18'
-- !query analysis
[Analyzer test output redacted due to nondeterminism]


-- !query
SELECT date '1999 Jan 08'
-- !query analysis
org.apache.spark.sql.catalyst.parser.ParseException
{
  "errorClass" : "INVALID_TYPED_LITERAL",
  "sqlState" : "42604",
  "messageParameters" : {
    "value" : "'1999 Jan 08'",
    "valueType" : "\"DATE\""
  },
  "queryContext" : [ {
    "objectType" : "",
    "objectName" : "",
    "startIndex" : 8,
    "stopIndex" : 25,
    "fragment" : "date '1999 Jan 08'"
  } ]
}


-- !query
SELECT date '1999 08 Jan'
-- !query analysis
org.apache.spark.sql.catalyst.parser.ParseException
{
  "errorClass" : "INVALID_TYPED_LITERAL",
  "sqlState" : "42604",
  "messageParameters" : {
    "value" : "'1999 08 Jan'",
    "valueType" : "\"DATE\""
  },
  "queryContext" : [ {
    "objectType" : "",
    "objectName" : "",
    "startIndex" : 8,
    "stopIndex" : 25,
    "fragment" : "date '1999 08 Jan'"
  } ]
}


-- !query
SELECT date '1999-01-08'
-- !query analysis
[Analyzer test output redacted due to nondeterminism]


-- !query
SELECT date '1999-08-01'
-- !query analysis
[Analyzer test output redacted due to nondeterminism]


-- !query
SELECT date '1999 01 08'
-- !query analysis
org.apache.spark.sql.catalyst.parser.ParseException
{
  "errorClass" : "INVALID_TYPED_LITERAL",
  "sqlState" : "42604",
  "messageParameters" : {
    "value" : "'1999 01 08'",
    "valueType" : "\"DATE\""
  },
  "queryContext" : [ {
    "objectType" : "",
    "objectName" : "",
    "startIndex" : 8,
    "stopIndex" : 24,
    "fragment" : "date '1999 01 08'"
  } ]
}


-- !query
SELECT date '1999 08 01'
-- !query analysis
org.apache.spark.sql.catalyst.parser.ParseException
{
  "errorClass" : "INVALID_TYPED_LITERAL",
  "sqlState" : "42604",
  "messageParameters" : {
    "value" : "'1999 08 01'",
    "valueType" : "\"DATE\""
  },
  "queryContext" : [ {
    "objectType" : "",
    "objectName" : "",
    "startIndex" : 8,
    "stopIndex" : 24,
    "fragment" : "date '1999 08 01'"
  } ]
}


-- !query
SELECT date '4714-11-24 BC'
-- !query analysis
[Analyzer test output redacted due to nondeterminism]


-- !query
SELECT date '4714-11-23 BC'
-- !query analysis
[Analyzer test output redacted due to nondeterminism]


-- !query
SELECT date '5874897-12-31'
-- !query analysis
[Analyzer test output redacted due to nondeterminism]


-- !query
SELECT date '5874898-01-01'
-- !query analysis
[Analyzer test output redacted due to nondeterminism]


-- !query
SELECT f1 - date '2000-01-01' AS `Days From 2K` FROM DATE_TBL
-- !query analysis
[Analyzer test output redacted due to nondeterminism]


-- !query
SELECT f1 - date 'epoch' AS `Days From Epoch` FROM DATE_TBL
-- !query analysis
[Analyzer test output redacted due to nondeterminism]


-- !query
SELECT date 'yesterday' - date 'today' AS `One day`
-- !query analysis
[Analyzer test output redacted due to nondeterminism]


-- !query
SELECT date 'today' - date 'tomorrow' AS `One day`
-- !query analysis
[Analyzer test output redacted due to nondeterminism]


-- !query
SELECT date 'yesterday' - date 'tomorrow' AS `Two days`
-- !query analysis
[Analyzer test output redacted due to nondeterminism]


-- !query
SELECT date 'tomorrow' - date 'today' AS `One day`
-- !query analysis
[Analyzer test output redacted due to nondeterminism]


-- !query
SELECT date 'today' - date 'yesterday' AS `One day`
-- !query analysis
[Analyzer test output redacted due to nondeterminism]


-- !query
SELECT date 'tomorrow' - date 'yesterday' AS `Two days`
-- !query analysis
[Analyzer test output redacted due to nondeterminism]


-- !query
select make_date(2013, 7, 15)
-- !query analysis
Project [make_date(2013, 7, 15, true) AS make_date(2013, 7, 15)#x]
+- OneRowRelation


-- !query
select make_date(-44, 3, 15)
-- !query analysis
Project [make_date(-44, 3, 15, true) AS make_date(-44, 3, 15)#x]
+- OneRowRelation


-- !query
select make_date(2013, 2, 30)
-- !query analysis
Project [make_date(2013, 2, 30, true) AS make_date(2013, 2, 30)#x]
+- OneRowRelation


-- !query
select make_date(2013, 13, 1)
-- !query analysis
Project [make_date(2013, 13, 1, true) AS make_date(2013, 13, 1)#x]
+- OneRowRelation


-- !query
select make_date(2013, 11, -1)
-- !query analysis
Project [make_date(2013, 11, -1, true) AS make_date(2013, 11, -1)#x]
+- OneRowRelation


-- !query
DROP TABLE DATE_TBL
-- !query analysis
DropTable false, false
+- ResolvedIdentifier V2SessionCatalog(spark_catalog), default.DATE_TBL
