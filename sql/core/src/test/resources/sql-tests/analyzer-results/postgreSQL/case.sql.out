-- Automatically generated by SQLQueryTestSuite
-- !query
CREATE TABLE CASE_TBL (
  i integer,
  f double
) USING parquet
-- !query analysis
CreateDataSourceTableCommand `spark_catalog`.`default`.`CASE_TBL`, false


-- !query
CREATE TABLE CASE2_TBL (
  i integer,
  j integer
) USING parquet
-- !query analysis
CreateDataSourceTableCommand `spark_catalog`.`default`.`CASE2_TBL`, false


-- !query
INSERT INTO CASE_TBL VALUES (1, 10.1)
-- !query analysis
InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/case_tbl, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/case_tbl], Append, `spark_catalog`.`default`.`case_tbl`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/case_tbl), [i, f]
+- Project [cast(col1#x as int) AS i#x, cast(col2#x as double) AS f#x]
   +- LocalRelation [col1#x, col2#x]


-- !query
INSERT INTO CASE_TBL VALUES (2, 20.2)
-- !query analysis
InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/case_tbl, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/case_tbl], Append, `spark_catalog`.`default`.`case_tbl`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/case_tbl), [i, f]
+- Project [cast(col1#x as int) AS i#x, cast(col2#x as double) AS f#x]
   +- LocalRelation [col1#x, col2#x]


-- !query
INSERT INTO CASE_TBL VALUES (3, -30.3)
-- !query analysis
InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/case_tbl, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/case_tbl], Append, `spark_catalog`.`default`.`case_tbl`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/case_tbl), [i, f]
+- Project [cast(col1#x as int) AS i#x, cast(col2#x as double) AS f#x]
   +- LocalRelation [col1#x, col2#x]


-- !query
INSERT INTO CASE_TBL VALUES (4, NULL)
-- !query analysis
InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/case_tbl, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/case_tbl], Append, `spark_catalog`.`default`.`case_tbl`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/case_tbl), [i, f]
+- Project [cast(col1#x as int) AS i#x, cast(col2#x as double) AS f#x]
   +- LocalRelation [col1#x, col2#x]


-- !query
INSERT INTO CASE2_TBL VALUES (1, -1)
-- !query analysis
InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/case2_tbl, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/case2_tbl], Append, `spark_catalog`.`default`.`case2_tbl`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/case2_tbl), [i, j]
+- Project [cast(col1#x as int) AS i#x, cast(col2#x as int) AS j#x]
   +- LocalRelation [col1#x, col2#x]


-- !query
INSERT INTO CASE2_TBL VALUES (2, -2)
-- !query analysis
InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/case2_tbl, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/case2_tbl], Append, `spark_catalog`.`default`.`case2_tbl`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/case2_tbl), [i, j]
+- Project [cast(col1#x as int) AS i#x, cast(col2#x as int) AS j#x]
   +- LocalRelation [col1#x, col2#x]


-- !query
INSERT INTO CASE2_TBL VALUES (3, -3)
-- !query analysis
InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/case2_tbl, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/case2_tbl], Append, `spark_catalog`.`default`.`case2_tbl`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/case2_tbl), [i, j]
+- Project [cast(col1#x as int) AS i#x, cast(col2#x as int) AS j#x]
   +- LocalRelation [col1#x, col2#x]


-- !query
INSERT INTO CASE2_TBL VALUES (2, -4)
-- !query analysis
InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/case2_tbl, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/case2_tbl], Append, `spark_catalog`.`default`.`case2_tbl`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/case2_tbl), [i, j]
+- Project [cast(col1#x as int) AS i#x, cast(col2#x as int) AS j#x]
   +- LocalRelation [col1#x, col2#x]


-- !query
INSERT INTO CASE2_TBL VALUES (1, NULL)
-- !query analysis
InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/case2_tbl, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/case2_tbl], Append, `spark_catalog`.`default`.`case2_tbl`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/case2_tbl), [i, j]
+- Project [cast(col1#x as int) AS i#x, cast(col2#x as int) AS j#x]
   +- LocalRelation [col1#x, col2#x]


-- !query
INSERT INTO CASE2_TBL VALUES (NULL, -6)
-- !query analysis
InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/case2_tbl, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/case2_tbl], Append, `spark_catalog`.`default`.`case2_tbl`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/case2_tbl), [i, j]
+- Project [cast(col1#x as int) AS i#x, cast(col2#x as int) AS j#x]
   +- LocalRelation [col1#x, col2#x]


-- !query
SELECT '3' AS `One`,
  CASE
    WHEN 1 < 2 THEN 3
  END AS `Simple WHEN`
-- !query analysis
Project [3 AS One#x, CASE WHEN (1 < 2) THEN 3 END AS Simple WHEN#x]
+- OneRowRelation


-- !query
SELECT '<NULL>' AS `One`,
  CASE
    WHEN 1 > 2 THEN 3
  END AS `Simple default`
-- !query analysis
Project [<NULL> AS One#x, CASE WHEN (1 > 2) THEN 3 END AS Simple default#x]
+- OneRowRelation


-- !query
SELECT '3' AS `One`,
  CASE
    WHEN 1 < 2 THEN 3
    ELSE 4
  END AS `Simple ELSE`
-- !query analysis
Project [3 AS One#x, CASE WHEN (1 < 2) THEN 3 ELSE 4 END AS Simple ELSE#x]
+- OneRowRelation


-- !query
SELECT '4' AS `One`,
  CASE
    WHEN 1 > 2 THEN 3
    ELSE 4
  END AS `ELSE default`
-- !query analysis
Project [4 AS One#x, CASE WHEN (1 > 2) THEN 3 ELSE 4 END AS ELSE default#x]
+- OneRowRelation


-- !query
SELECT '6' AS `One`,
  CASE
    WHEN 1 > 2 THEN 3
    WHEN 4 < 5 THEN 6
    ELSE 7
  END AS `Two WHEN with default`
-- !query analysis
Project [6 AS One#x, CASE WHEN (1 > 2) THEN 3 WHEN (4 < 5) THEN 6 ELSE 7 END AS Two WHEN with default#x]
+- OneRowRelation


-- !query
SELECT '7' AS `None`,
  CASE WHEN rand() < 0 THEN 1
  END AS `NULL on no matches`
-- !query analysis
[Analyzer test output redacted due to nondeterminism]


-- !query
SELECT CASE WHEN 1=0 THEN 1/0 WHEN 1=1 THEN 1 ELSE 2/0 END
-- !query analysis
Project [CASE WHEN (1 = 0) THEN (cast(1 as double) / cast(0 as double)) WHEN (1 = 1) THEN cast(1 as double) ELSE (cast(2 as double) / cast(0 as double)) END AS CASE WHEN (1 = 0) THEN (1 / 0) WHEN (1 = 1) THEN 1 ELSE (2 / 0) END#x]
+- OneRowRelation


-- !query
SELECT CASE 1 WHEN 0 THEN 1/0 WHEN 1 THEN 1 ELSE 2/0 END
-- !query analysis
Project [CASE WHEN (1 = 0) THEN (cast(1 as double) / cast(0 as double)) WHEN (1 = 1) THEN cast(1 as double) ELSE (cast(2 as double) / cast(0 as double)) END AS CASE WHEN (1 = 0) THEN (1 / 0) WHEN (1 = 1) THEN 1 ELSE (2 / 0) END#x]
+- OneRowRelation


-- !query
SELECT CASE WHEN i > 100 THEN 1/0 ELSE 0 END FROM case_tbl
-- !query analysis
Project [CASE WHEN (i#x > 100) THEN (cast(1 as double) / cast(0 as double)) ELSE cast(0 as double) END AS CASE WHEN (i > 100) THEN (1 / 0) ELSE 0 END#x]
+- SubqueryAlias spark_catalog.default.case_tbl
   +- Relation spark_catalog.default.case_tbl[i#x,f#x] parquet


-- !query
SELECT CASE 'a' WHEN 'a' THEN 1 ELSE 2 END
-- !query analysis
Project [CASE WHEN (a = a) THEN 1 ELSE 2 END AS CASE WHEN (a = a) THEN 1 ELSE 2 END#x]
+- OneRowRelation


-- !query
SELECT '' AS `Five`,
  CASE
    WHEN i >= 3 THEN i
  END AS `>= 3 or Null`
  FROM CASE_TBL
-- !query analysis
Project [ AS Five#x, CASE WHEN (i#x >= 3) THEN i#x END AS >= 3 or Null#x]
+- SubqueryAlias spark_catalog.default.case_tbl
   +- Relation spark_catalog.default.case_tbl[i#x,f#x] parquet


-- !query
SELECT '' AS `Five`,
  CASE WHEN i >= 3 THEN (i + i)
       ELSE i
  END AS `Simplest Math`
  FROM CASE_TBL
-- !query analysis
Project [ AS Five#x, CASE WHEN (i#x >= 3) THEN (i#x + i#x) ELSE i#x END AS Simplest Math#x]
+- SubqueryAlias spark_catalog.default.case_tbl
   +- Relation spark_catalog.default.case_tbl[i#x,f#x] parquet


-- !query
SELECT '' AS `Five`, i AS `Value`,
  CASE WHEN (i < 0) THEN 'small'
       WHEN (i = 0) THEN 'zero'
       WHEN (i = 1) THEN 'one'
       WHEN (i = 2) THEN 'two'
       ELSE 'big'
  END AS `Category`
  FROM CASE_TBL
-- !query analysis
Project [ AS Five#x, i#x AS Value#x, CASE WHEN (i#x < 0) THEN small WHEN (i#x = 0) THEN zero WHEN (i#x = 1) THEN one WHEN (i#x = 2) THEN two ELSE big END AS Category#x]
+- SubqueryAlias spark_catalog.default.case_tbl
   +- Relation spark_catalog.default.case_tbl[i#x,f#x] parquet


-- !query
SELECT '' AS `Five`,
  CASE WHEN ((i < 0) or (i < 0)) THEN 'small'
       WHEN ((i = 0) or (i = 0)) THEN 'zero'
       WHEN ((i = 1) or (i = 1)) THEN 'one'
       WHEN ((i = 2) or (i = 2)) THEN 'two'
       ELSE 'big'
  END AS `Category`
  FROM CASE_TBL
-- !query analysis
Project [ AS Five#x, CASE WHEN ((i#x < 0) OR (i#x < 0)) THEN small WHEN ((i#x = 0) OR (i#x = 0)) THEN zero WHEN ((i#x = 1) OR (i#x = 1)) THEN one WHEN ((i#x = 2) OR (i#x = 2)) THEN two ELSE big END AS Category#x]
+- SubqueryAlias spark_catalog.default.case_tbl
   +- Relation spark_catalog.default.case_tbl[i#x,f#x] parquet


-- !query
SELECT * FROM CASE_TBL WHERE COALESCE(f,i) = 4
-- !query analysis
Project [i#x, f#x]
+- Filter (coalesce(f#x, cast(i#x as double)) = cast(4 as double))
   +- SubqueryAlias spark_catalog.default.case_tbl
      +- Relation spark_catalog.default.case_tbl[i#x,f#x] parquet


-- !query
SELECT * FROM CASE_TBL WHERE NULLIF(f,i) = 2
-- !query analysis
Project [i#x, f#x]
+- Filter (nullif(f#x, i#x) = cast(2 as double))
   +- SubqueryAlias spark_catalog.default.case_tbl
      +- Relation spark_catalog.default.case_tbl[i#x,f#x] parquet


-- !query
SELECT COALESCE(a.f, b.i, b.j)
  FROM CASE_TBL a, CASE2_TBL b
-- !query analysis
Project [coalesce(f#x, cast(i#x as double), cast(j#x as double)) AS coalesce(f, i, j)#x]
+- Join Inner
   :- SubqueryAlias a
   :  +- SubqueryAlias spark_catalog.default.case_tbl
   :     +- Relation spark_catalog.default.case_tbl[i#x,f#x] parquet
   +- SubqueryAlias b
      +- SubqueryAlias spark_catalog.default.case2_tbl
         +- Relation spark_catalog.default.case2_tbl[i#x,j#x] parquet


-- !query
SELECT *
  FROM CASE_TBL a, CASE2_TBL b
  WHERE COALESCE(a.f, b.i, b.j) = 2
-- !query analysis
Project [i#x, f#x, i#x, j#x]
+- Filter (coalesce(f#x, cast(i#x as double), cast(j#x as double)) = cast(2 as double))
   +- Join Inner
      :- SubqueryAlias a
      :  +- SubqueryAlias spark_catalog.default.case_tbl
      :     +- Relation spark_catalog.default.case_tbl[i#x,f#x] parquet
      +- SubqueryAlias b
         +- SubqueryAlias spark_catalog.default.case2_tbl
            +- Relation spark_catalog.default.case2_tbl[i#x,j#x] parquet


-- !query
SELECT '' AS Five, NULLIF(a.i,b.i) AS `NULLIF(a.i,b.i)`,
  NULLIF(b.i, 4) AS `NULLIF(b.i,4)`
  FROM CASE_TBL a, CASE2_TBL b
-- !query analysis
Project [ AS Five#x, nullif(i#x, i#x) AS NULLIF(a.i,b.i)#x, nullif(i#x, 4) AS NULLIF(b.i,4)#x]
+- Join Inner
   :- SubqueryAlias a
   :  +- SubqueryAlias spark_catalog.default.case_tbl
   :     +- Relation spark_catalog.default.case_tbl[i#x,f#x] parquet
   +- SubqueryAlias b
      +- SubqueryAlias spark_catalog.default.case2_tbl
         +- Relation spark_catalog.default.case2_tbl[i#x,j#x] parquet


-- !query
SELECT '' AS `Two`, *
  FROM CASE_TBL a, CASE2_TBL b
  WHERE COALESCE(f,b.i) = 2
-- !query analysis
Project [ AS Two#x, i#x, f#x, i#x, j#x]
+- Filter (coalesce(f#x, cast(i#x as double)) = cast(2 as double))
   +- Join Inner
      :- SubqueryAlias a
      :  +- SubqueryAlias spark_catalog.default.case_tbl
      :     +- Relation spark_catalog.default.case_tbl[i#x,f#x] parquet
      +- SubqueryAlias b
         +- SubqueryAlias spark_catalog.default.case2_tbl
            +- Relation spark_catalog.default.case2_tbl[i#x,j#x] parquet


-- !query
SELECT CASE
  (CASE vol('bar')
    WHEN 'foo' THEN 'it was foo!'
    WHEN vol(null) THEN 'null input'
    WHEN 'bar' THEN 'it was bar!' END
  )
  WHEN 'it was foo!' THEN 'foo recognized'
  WHEN 'it was bar!' THEN 'bar recognized'
  ELSE 'unrecognized' END
-- !query analysis
Project [CASE WHEN (CASE WHEN (vol(bar) = foo) THEN it was foo! WHEN (vol(bar) = vol(cast(null as string))) THEN null input WHEN (vol(bar) = bar) THEN it was bar! END = it was foo!) THEN foo recognized WHEN (CASE WHEN (vol(bar) = foo) THEN it was foo! WHEN (vol(bar) = vol(cast(null as string))) THEN null input WHEN (vol(bar) = bar) THEN it was bar! END = it was bar!) THEN bar recognized ELSE unrecognized END AS CASE WHEN (CASE WHEN (vol(bar) = foo) THEN it was foo! WHEN (vol(bar) = vol(NULL)) THEN null input WHEN (vol(bar) = bar) THEN it was bar! END = it was foo!) THEN foo recognized WHEN (CASE WHEN (vol(bar) = foo) THEN it was foo! WHEN (vol(bar) = vol(NULL)) THEN null input WHEN (vol(bar) = bar) THEN it was bar! END = it was bar!) THEN bar recognized ELSE unrecognized END#x]
+- OneRowRelation


-- !query
DROP TABLE CASE_TBL
-- !query analysis
DropTable false, false
+- ResolvedIdentifier V2SessionCatalog(spark_catalog), default.CASE_TBL


-- !query
DROP TABLE CASE2_TBL
-- !query analysis
DropTable false, false
+- ResolvedIdentifier V2SessionCatalog(spark_catalog), default.CASE2_TBL
