-- Automatically generated by SQLQueryTestSuite
-- !query
CREATE TABLE INT2_TBL(f1 smallint) USING parquet
-- !query analysis
CreateDataSourceTableCommand `spark_catalog`.`default`.`INT2_TBL`, false


-- !query
INSERT INTO INT2_TBL VALUES (smallint(trim('0   ')))
-- !query analysis
InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/int2_tbl, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/int2_tbl], Append, `spark_catalog`.`default`.`int2_tbl`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/int2_tbl), [f1]
+- Project [cast(col1#x as smallint) AS f1#x]
   +- LocalRelation [col1#x]


-- !query
INSERT INTO INT2_TBL VALUES (smallint(trim('  1234 ')))
-- !query analysis
InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/int2_tbl, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/int2_tbl], Append, `spark_catalog`.`default`.`int2_tbl`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/int2_tbl), [f1]
+- Project [cast(col1#x as smallint) AS f1#x]
   +- LocalRelation [col1#x]


-- !query
INSERT INTO INT2_TBL VALUES (smallint(trim('    -1234')))
-- !query analysis
InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/int2_tbl, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/int2_tbl], Append, `spark_catalog`.`default`.`int2_tbl`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/int2_tbl), [f1]
+- Project [cast(col1#x as smallint) AS f1#x]
   +- LocalRelation [col1#x]


-- !query
INSERT INTO INT2_TBL VALUES (smallint('32767'))
-- !query analysis
InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/int2_tbl, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/int2_tbl], Append, `spark_catalog`.`default`.`int2_tbl`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/int2_tbl), [f1]
+- Project [cast(col1#x as smallint) AS f1#x]
   +- LocalRelation [col1#x]


-- !query
INSERT INTO INT2_TBL VALUES (smallint('-32767'))
-- !query analysis
InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/int2_tbl, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/int2_tbl], Append, `spark_catalog`.`default`.`int2_tbl`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/int2_tbl), [f1]
+- Project [cast(col1#x as smallint) AS f1#x]
   +- LocalRelation [col1#x]


-- !query
SELECT '' AS five, * FROM INT2_TBL
-- !query analysis
Project [ AS five#x, f1#x]
+- SubqueryAlias spark_catalog.default.int2_tbl
   +- Relation spark_catalog.default.int2_tbl[f1#x] parquet


-- !query
SELECT '' AS four, i.* FROM INT2_TBL i WHERE i.f1 <> smallint('0')
-- !query analysis
Project [ AS four#x, f1#x]
+- Filter NOT (f1#x = cast(0 as smallint))
   +- SubqueryAlias i
      +- SubqueryAlias spark_catalog.default.int2_tbl
         +- Relation spark_catalog.default.int2_tbl[f1#x] parquet


-- !query
SELECT '' AS four, i.* FROM INT2_TBL i WHERE i.f1 <> int('0')
-- !query analysis
Project [ AS four#x, f1#x]
+- Filter NOT (cast(f1#x as int) = cast(0 as int))
   +- SubqueryAlias i
      +- SubqueryAlias spark_catalog.default.int2_tbl
         +- Relation spark_catalog.default.int2_tbl[f1#x] parquet


-- !query
SELECT '' AS one, i.* FROM INT2_TBL i WHERE i.f1 = smallint('0')
-- !query analysis
Project [ AS one#x, f1#x]
+- Filter (f1#x = cast(0 as smallint))
   +- SubqueryAlias i
      +- SubqueryAlias spark_catalog.default.int2_tbl
         +- Relation spark_catalog.default.int2_tbl[f1#x] parquet


-- !query
SELECT '' AS one, i.* FROM INT2_TBL i WHERE i.f1 = int('0')
-- !query analysis
Project [ AS one#x, f1#x]
+- Filter (cast(f1#x as int) = cast(0 as int))
   +- SubqueryAlias i
      +- SubqueryAlias spark_catalog.default.int2_tbl
         +- Relation spark_catalog.default.int2_tbl[f1#x] parquet


-- !query
SELECT '' AS two, i.* FROM INT2_TBL i WHERE i.f1 < smallint('0')
-- !query analysis
Project [ AS two#x, f1#x]
+- Filter (f1#x < cast(0 as smallint))
   +- SubqueryAlias i
      +- SubqueryAlias spark_catalog.default.int2_tbl
         +- Relation spark_catalog.default.int2_tbl[f1#x] parquet


-- !query
SELECT '' AS two, i.* FROM INT2_TBL i WHERE i.f1 < int('0')
-- !query analysis
Project [ AS two#x, f1#x]
+- Filter (cast(f1#x as int) < cast(0 as int))
   +- SubqueryAlias i
      +- SubqueryAlias spark_catalog.default.int2_tbl
         +- Relation spark_catalog.default.int2_tbl[f1#x] parquet


-- !query
SELECT '' AS three, i.* FROM INT2_TBL i WHERE i.f1 <= smallint('0')
-- !query analysis
Project [ AS three#x, f1#x]
+- Filter (f1#x <= cast(0 as smallint))
   +- SubqueryAlias i
      +- SubqueryAlias spark_catalog.default.int2_tbl
         +- Relation spark_catalog.default.int2_tbl[f1#x] parquet


-- !query
SELECT '' AS three, i.* FROM INT2_TBL i WHERE i.f1 <= int('0')
-- !query analysis
Project [ AS three#x, f1#x]
+- Filter (cast(f1#x as int) <= cast(0 as int))
   +- SubqueryAlias i
      +- SubqueryAlias spark_catalog.default.int2_tbl
         +- Relation spark_catalog.default.int2_tbl[f1#x] parquet


-- !query
SELECT '' AS two, i.* FROM INT2_TBL i WHERE i.f1 > smallint('0')
-- !query analysis
Project [ AS two#x, f1#x]
+- Filter (f1#x > cast(0 as smallint))
   +- SubqueryAlias i
      +- SubqueryAlias spark_catalog.default.int2_tbl
         +- Relation spark_catalog.default.int2_tbl[f1#x] parquet


-- !query
SELECT '' AS two, i.* FROM INT2_TBL i WHERE i.f1 > int('0')
-- !query analysis
Project [ AS two#x, f1#x]
+- Filter (cast(f1#x as int) > cast(0 as int))
   +- SubqueryAlias i
      +- SubqueryAlias spark_catalog.default.int2_tbl
         +- Relation spark_catalog.default.int2_tbl[f1#x] parquet


-- !query
SELECT '' AS three, i.* FROM INT2_TBL i WHERE i.f1 >= smallint('0')
-- !query analysis
Project [ AS three#x, f1#x]
+- Filter (f1#x >= cast(0 as smallint))
   +- SubqueryAlias i
      +- SubqueryAlias spark_catalog.default.int2_tbl
         +- Relation spark_catalog.default.int2_tbl[f1#x] parquet


-- !query
SELECT '' AS three, i.* FROM INT2_TBL i WHERE i.f1 >= int('0')
-- !query analysis
Project [ AS three#x, f1#x]
+- Filter (cast(f1#x as int) >= cast(0 as int))
   +- SubqueryAlias i
      +- SubqueryAlias spark_catalog.default.int2_tbl
         +- Relation spark_catalog.default.int2_tbl[f1#x] parquet


-- !query
SELECT '' AS one, i.* FROM INT2_TBL i WHERE (i.f1 % smallint('2')) = smallint('1')
-- !query analysis
Project [ AS one#x, f1#x]
+- Filter ((f1#x % cast(2 as smallint)) = cast(1 as smallint))
   +- SubqueryAlias i
      +- SubqueryAlias spark_catalog.default.int2_tbl
         +- Relation spark_catalog.default.int2_tbl[f1#x] parquet


-- !query
SELECT '' AS three, i.* FROM INT2_TBL i WHERE (i.f1 % int('2')) = smallint('0')
-- !query analysis
Project [ AS three#x, f1#x]
+- Filter ((cast(f1#x as int) % cast(2 as int)) = cast(cast(0 as smallint) as int))
   +- SubqueryAlias i
      +- SubqueryAlias spark_catalog.default.int2_tbl
         +- Relation spark_catalog.default.int2_tbl[f1#x] parquet


-- !query
SELECT '' AS five, i.f1, i.f1 * smallint('2') AS x FROM INT2_TBL i
WHERE abs(f1) < 16384
-- !query analysis
Project [ AS five#x, f1#x, (f1#x * cast(2 as smallint)) AS x#x]
+- Filter (cast(abs(f1#x) as int) < 16384)
   +- SubqueryAlias i
      +- SubqueryAlias spark_catalog.default.int2_tbl
         +- Relation spark_catalog.default.int2_tbl[f1#x] parquet


-- !query
SELECT '' AS five, i.f1, i.f1 * int('2') AS x FROM INT2_TBL i
-- !query analysis
Project [ AS five#x, f1#x, (cast(f1#x as int) * cast(2 as int)) AS x#x]
+- SubqueryAlias i
   +- SubqueryAlias spark_catalog.default.int2_tbl
      +- Relation spark_catalog.default.int2_tbl[f1#x] parquet


-- !query
SELECT '' AS five, i.f1, i.f1 + smallint('2') AS x FROM INT2_TBL i
WHERE f1 < 32766
-- !query analysis
Project [ AS five#x, f1#x, (f1#x + cast(2 as smallint)) AS x#x]
+- Filter (cast(f1#x as int) < 32766)
   +- SubqueryAlias i
      +- SubqueryAlias spark_catalog.default.int2_tbl
         +- Relation spark_catalog.default.int2_tbl[f1#x] parquet


-- !query
SELECT '' AS five, i.f1, i.f1 + int('2') AS x FROM INT2_TBL i
-- !query analysis
Project [ AS five#x, f1#x, (cast(f1#x as int) + cast(2 as int)) AS x#x]
+- SubqueryAlias i
   +- SubqueryAlias spark_catalog.default.int2_tbl
      +- Relation spark_catalog.default.int2_tbl[f1#x] parquet


-- !query
SELECT '' AS five, i.f1, i.f1 - smallint('2') AS x FROM INT2_TBL i
WHERE f1 > -32767
-- !query analysis
Project [ AS five#x, f1#x, (f1#x - cast(2 as smallint)) AS x#x]
+- Filter (cast(f1#x as int) > -32767)
   +- SubqueryAlias i
      +- SubqueryAlias spark_catalog.default.int2_tbl
         +- Relation spark_catalog.default.int2_tbl[f1#x] parquet


-- !query
SELECT '' AS five, i.f1, i.f1 - int('2') AS x FROM INT2_TBL i
-- !query analysis
Project [ AS five#x, f1#x, (cast(f1#x as int) - cast(2 as int)) AS x#x]
+- SubqueryAlias i
   +- SubqueryAlias spark_catalog.default.int2_tbl
      +- Relation spark_catalog.default.int2_tbl[f1#x] parquet


-- !query
SELECT '' AS five, i.f1, i.f1 / smallint('2') AS x FROM INT2_TBL i
-- !query analysis
Project [ AS five#x, f1#x, (cast(f1#x as double) / cast(cast(2 as smallint) as double)) AS x#x]
+- SubqueryAlias i
   +- SubqueryAlias spark_catalog.default.int2_tbl
      +- Relation spark_catalog.default.int2_tbl[f1#x] parquet


-- !query
SELECT '' AS five, i.f1, i.f1 / int('2') AS x FROM INT2_TBL i
-- !query analysis
Project [ AS five#x, f1#x, (cast(f1#x as double) / cast(cast(2 as int) as double)) AS x#x]
+- SubqueryAlias i
   +- SubqueryAlias spark_catalog.default.int2_tbl
      +- Relation spark_catalog.default.int2_tbl[f1#x] parquet


-- !query
SELECT string(shiftleft(smallint(-1), 15))
-- !query analysis
Project [cast(shiftleft(cast(cast(-1 as smallint) as int), 15) as string) AS shiftleft(-1, 15)#x]
+- OneRowRelation


-- !query
SELECT string(smallint(shiftleft(smallint(-1), 15))+1)
-- !query analysis
Project [cast((cast(cast(shiftleft(cast(cast(-1 as smallint) as int), 15) as smallint) as int) + 1) as string) AS (shiftleft(-1, 15) + 1)#x]
+- OneRowRelation


-- !query
SELECT smallint(-32768) % smallint(-1)
-- !query analysis
Project [(cast(-32768 as smallint) % cast(-1 as smallint)) AS (-32768 % -1)#x]
+- OneRowRelation


-- !query
SELECT x, smallint(x) AS int2_value
FROM (VALUES float(-2.5),
             float(-1.5),
             float(-0.5),
             float(0.0),
             float(0.5),
             float(1.5),
             float(2.5)) t(x)
-- !query analysis
Project [x#x, cast(x#x as smallint) AS int2_value#x]
+- SubqueryAlias t
   +- Project [col1#x AS x#x]
      +- LocalRelation [col1#x]


-- !query
SELECT x, smallint(x) AS int2_value
FROM (VALUES cast(-2.5 as decimal(38, 18)),
             cast(-1.5 as decimal(38, 18)),
             cast(-0.5 as decimal(38, 18)),
             cast(-0.0 as decimal(38, 18)),
             cast(0.5 as decimal(38, 18)),
             cast(1.5 as decimal(38, 18)),
             cast(2.5 as decimal(38, 18))) t(x)
-- !query analysis
Project [x#x, cast(x#x as smallint) AS int2_value#x]
+- SubqueryAlias t
   +- Project [col1#x AS x#x]
      +- LocalRelation [col1#x]


-- !query
DROP TABLE INT2_TBL
-- !query analysis
DropTable false, false
+- ResolvedIdentifier V2SessionCatalog(spark_catalog), default.INT2_TBL
