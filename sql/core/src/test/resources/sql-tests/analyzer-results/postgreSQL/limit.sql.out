-- Automatically generated by SQLQueryTestSuite
-- !query
SELECT '' AS two, unique1, unique2, stringu1
		FROM onek WHERE unique1 > 50
		ORDER BY unique1 LIMIT 2
-- !query analysis
GlobalLimit 2
+- LocalLimit 2
   +- Sort [unique1#x ASC NULLS FIRST], true
      +- Project [ AS two#x, unique1#x, unique2#x, stringu1#x]
         +- Filter (unique1#x > 50)
            +- SubqueryAlias spark_catalog.default.onek
               +- Relation spark_catalog.default.onek[unique1#x,unique2#x,two#x,four#x,ten#x,twenty#x,hundred#x,thousand#x,twothousand#x,fivethous#x,tenthous#x,odd#x,even#x,stringu1#x,stringu2#x,string4#x] parquet


-- !query
SELECT '' AS five, unique1, unique2, stringu1
		FROM onek WHERE unique1 > 60
		ORDER BY unique1 LIMIT 5
-- !query analysis
GlobalLimit 5
+- LocalLimit 5
   +- Sort [unique1#x ASC NULLS FIRST], true
      +- Project [ AS five#x, unique1#x, unique2#x, stringu1#x]
         +- Filter (unique1#x > 60)
            +- SubqueryAlias spark_catalog.default.onek
               +- Relation spark_catalog.default.onek[unique1#x,unique2#x,two#x,four#x,ten#x,twenty#x,hundred#x,thousand#x,twothousand#x,fivethous#x,tenthous#x,odd#x,even#x,stringu1#x,stringu2#x,string4#x] parquet


-- !query
SELECT '' AS two, unique1, unique2, stringu1
		FROM onek WHERE unique1 > 60 AND unique1 < 63
		ORDER BY unique1 LIMIT 5
-- !query analysis
GlobalLimit 5
+- LocalLimit 5
   +- Sort [unique1#x ASC NULLS FIRST], true
      +- Project [ AS two#x, unique1#x, unique2#x, stringu1#x]
         +- Filter ((unique1#x > 60) AND (unique1#x < 63))
            +- SubqueryAlias spark_catalog.default.onek
               +- Relation spark_catalog.default.onek[unique1#x,unique2#x,two#x,four#x,ten#x,twenty#x,hundred#x,thousand#x,twothousand#x,fivethous#x,tenthous#x,odd#x,even#x,stringu1#x,stringu2#x,string4#x] parquet


-- !query
SELECT '' AS three, unique1, unique2, stringu1
 		FROM onek WHERE unique1 > 100
 		ORDER BY unique1 LIMIT 3 OFFSET 20
-- !query analysis
GlobalLimit 3
+- LocalLimit 3
   +- Offset 20
      +- Sort [unique1#x ASC NULLS FIRST], true
         +- Project [ AS three#x, unique1#x, unique2#x, stringu1#x]
            +- Filter (unique1#x > 100)
               +- SubqueryAlias spark_catalog.default.onek
                  +- Relation spark_catalog.default.onek[unique1#x,unique2#x,two#x,four#x,ten#x,twenty#x,hundred#x,thousand#x,twothousand#x,fivethous#x,tenthous#x,odd#x,even#x,stringu1#x,stringu2#x,string4#x] parquet


-- !query
SELECT '' AS zero, unique1, unique2, stringu1
		FROM onek WHERE unique1 < 50
		ORDER BY unique1 DESC LIMIT 8 OFFSET 99
-- !query analysis
GlobalLimit 8
+- LocalLimit 8
   +- Offset 99
      +- Sort [unique1#x DESC NULLS LAST], true
         +- Project [ AS zero#x, unique1#x, unique2#x, stringu1#x]
            +- Filter (unique1#x < 50)
               +- SubqueryAlias spark_catalog.default.onek
                  +- Relation spark_catalog.default.onek[unique1#x,unique2#x,two#x,four#x,ten#x,twenty#x,hundred#x,thousand#x,twothousand#x,fivethous#x,tenthous#x,odd#x,even#x,stringu1#x,stringu2#x,string4#x] parquet


-- !query
SELECT '' AS eleven, unique1, unique2, stringu1
		FROM onek WHERE unique1 < 50
 		ORDER BY unique1 DESC LIMIT 20 OFFSET 39
-- !query analysis
GlobalLimit 20
+- LocalLimit 20
   +- Offset 39
      +- Sort [unique1#x DESC NULLS LAST], true
         +- Project [ AS eleven#x, unique1#x, unique2#x, stringu1#x]
            +- Filter (unique1#x < 50)
               +- SubqueryAlias spark_catalog.default.onek
                  +- Relation spark_catalog.default.onek[unique1#x,unique2#x,two#x,four#x,ten#x,twenty#x,hundred#x,thousand#x,twothousand#x,fivethous#x,tenthous#x,odd#x,even#x,stringu1#x,stringu2#x,string4#x] parquet


-- !query
SELECT '' AS ten, unique1, unique2, stringu1
 		FROM onek
 		ORDER BY unique1 OFFSET 990
-- !query analysis
Offset 990
+- Sort [unique1#x ASC NULLS FIRST], true
   +- Project [ AS ten#x, unique1#x, unique2#x, stringu1#x]
      +- SubqueryAlias spark_catalog.default.onek
         +- Relation spark_catalog.default.onek[unique1#x,unique2#x,two#x,four#x,ten#x,twenty#x,hundred#x,thousand#x,twothousand#x,fivethous#x,tenthous#x,odd#x,even#x,stringu1#x,stringu2#x,string4#x] parquet


-- !query
SELECT '' AS five, unique1, unique2, stringu1
 		FROM onek
 		ORDER BY unique1 LIMIT 5 OFFSET 900
-- !query analysis
GlobalLimit 5
+- LocalLimit 5
   +- Offset 900
      +- Sort [unique1#x ASC NULLS FIRST], true
         +- Project [ AS five#x, unique1#x, unique2#x, stringu1#x]
            +- SubqueryAlias spark_catalog.default.onek
               +- Relation spark_catalog.default.onek[unique1#x,unique2#x,two#x,four#x,ten#x,twenty#x,hundred#x,thousand#x,twothousand#x,fivethous#x,tenthous#x,odd#x,even#x,stringu1#x,stringu2#x,string4#x] parquet


-- !query
CREATE OR REPLACE TEMPORARY VIEW INT8_TBL AS SELECT * FROM
  (VALUES
    (123, 456),
    (123, 4567890123456789),
    (4567890123456789, 123),
    (4567890123456789, 4567890123456789),
    (4567890123456789, -4567890123456789))
  AS v(q1, q2)
-- !query analysis
CreateViewCommand `INT8_TBL`, SELECT * FROM
  (VALUES
    (123, 456),
    (123, 4567890123456789),
    (4567890123456789, 123),
    (4567890123456789, 4567890123456789),
    (4567890123456789, -4567890123456789))
  AS v(q1, q2), false, true, LocalTempView, UNSUPPORTED, true
   +- Project [q1#xL, q2#xL]
      +- SubqueryAlias v
         +- Project [col1#xL AS q1#xL, col2#xL AS q2#xL]
            +- LocalRelation [col1#xL, col2#xL]


-- !query
select * from int8_tbl limit (case when random() < 0.5 then bigint(null) end)
-- !query analysis
org.apache.spark.sql.catalyst.ExtendedAnalysisException
{
  "condition" : "INVALID_LIMIT_LIKE_EXPRESSION.IS_UNFOLDABLE",
  "sqlState" : "42K0E",
  "messageParameters" : {
    "expr" : "\"CASE WHEN (_nondeterministic < 0.5) THEN NULL END\"",
    "name" : "limit"
  },
  "queryContext" : [ {
    "objectType" : "",
    "objectName" : "",
    "startIndex" : 31,
    "stopIndex" : 76,
    "fragment" : "case when random() < 0.5 then bigint(null) end"
  } ]
}


-- !query
select * from int8_tbl offset (case when random() < 0.5 then bigint(null) end)
-- !query analysis
org.apache.spark.sql.catalyst.ExtendedAnalysisException
{
  "condition" : "INVALID_LIMIT_LIKE_EXPRESSION.IS_UNFOLDABLE",
  "sqlState" : "42K0E",
  "messageParameters" : {
    "expr" : "\"CASE WHEN (_nondeterministic < 0.5) THEN NULL END\"",
    "name" : "offset"
  },
  "queryContext" : [ {
    "objectType" : "",
    "objectName" : "",
    "startIndex" : 32,
    "stopIndex" : 77,
    "fragment" : "case when random() < 0.5 then bigint(null) end"
  } ]
}


-- !query
DROP VIEW INT8_TBL
-- !query analysis
DropTempViewCommand INT8_TBL


-- !query
select sum(tenthous) as s1, sum(tenthous) + random()*0 as s2
  from tenk1 group by thousand order by thousand limit 3
-- !query analysis
[Analyzer test output redacted due to nondeterminism]
