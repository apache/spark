-- Automatically generated by SQLQueryTestSuite
-- !query
BEGIN
  DECLARE cur CURSOR FOR SELECT 1;
END
-- !query analysis
org.apache.spark.sql.exceptions.SqlScriptingException
{
  "errorClass" : "UNSUPPORTED_FEATURE.SQL_CURSOR",
  "sqlState" : "0A000"
}


-- !query
BEGIN
  OPEN cur;
END
-- !query analysis
org.apache.spark.sql.exceptions.SqlScriptingException
{
  "errorClass" : "UNSUPPORTED_FEATURE.SQL_CURSOR",
  "sqlState" : "0A000"
}


-- !query
BEGIN
  DECLARE x INT;
  FETCH cur INTO x;
END
-- !query analysis
org.apache.spark.sql.exceptions.SqlScriptingException
{
  "errorClass" : "UNSUPPORTED_FEATURE.SQL_CURSOR",
  "sqlState" : "0A000"
}


-- !query
BEGIN
  CLOSE cur;
END
-- !query analysis
org.apache.spark.sql.exceptions.SqlScriptingException
{
  "errorClass" : "UNSUPPORTED_FEATURE.SQL_CURSOR",
  "sqlState" : "0A000"
}


-- !query
SET spark.sql.scripting.cursorEnabled=true
-- !query analysis
SetCommand (spark.sql.scripting.cursorEnabled,Some(true))


-- !query
SET spark.sql.scripting.continueHandlerEnabled=true
-- !query analysis
SetCommand (spark.sql.scripting.continueHandlerEnabled,Some(true))


-- !query
BEGIN
  DECLARE x INT DEFAULT 10;
  DECLARE x CURSOR FOR SELECT 1 AS col;
  OPEN x;
  FETCH x INTO x;
  VALUES (x); -- Should return 1
  CLOSE x;
END
-- !query analysis
org.apache.spark.sql.catalyst.ExtendedAnalysisException
{
  "errorClass" : "UNRESOLVED_COLUMN.WITHOUT_SUGGESTION",
  "sqlState" : "42703",
  "messageParameters" : {
    "objectName" : "`x`"
  },
  "queryContext" : [ {
    "objectType" : "",
    "objectName" : "",
    "startIndex" : 100,
    "stopIndex" : 100,
    "fragment" : "x"
  } ]
}


-- !query
BEGIN
  DECLARE c1 CURSOR FOR SELECT 1;
  DECLARE c1 CURSOR FOR SELECT 2;
END
-- !query analysis
org.apache.spark.sql.AnalysisException
{
  "errorClass" : "CURSOR_ALREADY_EXISTS",
  "sqlState" : "42723",
  "messageParameters" : {
    "cursorName" : "c1"
  }
}


-- !query
BEGIN
  DECLARE y INT;
  DECLARE c1 CURSOR FOR SELECT 1 AS val;
  BEGIN
    DECLARE x INT;
    DECLARE c1 CURSOR FOR SELECT 2 AS val;
    OPEN c1;  -- Opens inner c1
    FETCH c1 INTO x;
    VALUES (x); -- Should return 2
    CLOSE c1;
  END;
  OPEN c1;  -- Opens outer c1
  FETCH c1 INTO y;
  VALUES (y); -- Should return 1
  CLOSE c1;
END
-- !query analysis
org.apache.spark.sql.catalyst.ExtendedAnalysisException
{
  "errorClass" : "UNRESOLVED_COLUMN.WITHOUT_SUGGESTION",
  "sqlState" : "42703",
  "messageParameters" : {
    "objectName" : "`x`"
  },
  "queryContext" : [ {
    "objectType" : "",
    "objectName" : "",
    "startIndex" : 185,
    "stopIndex" : 185,
    "fragment" : "x"
  } ]
}


-- !query
BEGIN
  DECLARE c1 CURSOR FOR SELECT 1;
  OPEN c1;
  OPEN c1; -- Should fail
END
-- !query analysis
org.apache.spark.sql.AnalysisException
{
  "errorClass" : "CURSOR_ALREADY_OPEN",
  "sqlState" : "24502",
  "messageParameters" : {
    "cursorName" : "c1"
  }
}


-- !query
BEGIN
  DECLARE x INT;
  DECLARE c1 CURSOR FOR SELECT 1 AS val;
  OPEN c1;
  FETCH c1 INTO x;
  VALUES (x); -- Should return 1
  CLOSE c1;
  OPEN c1; -- Should succeed
  FETCH c1 INTO x;
  VALUES (x); -- Should return 1
  CLOSE c1;
END
-- !query analysis
org.apache.spark.sql.catalyst.ExtendedAnalysisException
{
  "errorClass" : "UNRESOLVED_COLUMN.WITHOUT_SUGGESTION",
  "sqlState" : "42703",
  "messageParameters" : {
    "objectName" : "`x`"
  },
  "queryContext" : [ {
    "objectType" : "",
    "objectName" : "",
    "startIndex" : 92,
    "stopIndex" : 92,
    "fragment" : "x"
  } ]
}


-- !query
BEGIN
  DECLARE c1 CURSOR FOR SELECT 1;
  CLOSE c1; -- Should fail
END
-- !query analysis
org.apache.spark.sql.AnalysisException
{
  "errorClass" : "CURSOR_NOT_OPEN",
  "sqlState" : "24501",
  "messageParameters" : {
    "cursorName" : "c1"
  }
}


-- !query
BEGIN
  DECLARE c1 CURSOR FOR SELECT 1;
  OPEN c1;
  CLOSE c1;
  CLOSE c1; -- Should fail
END
-- !query analysis
org.apache.spark.sql.AnalysisException
{
  "errorClass" : "CURSOR_NOT_OPEN",
  "sqlState" : "24501",
  "messageParameters" : {
    "cursorName" : "c1"
  }
}


-- !query
BEGIN
  DECLARE x INT;
  DECLARE c1 CURSOR FOR SELECT 1 AS val;
  FETCH c1 INTO x; -- Should fail
END
-- !query analysis
org.apache.spark.sql.catalyst.ExtendedAnalysisException
{
  "errorClass" : "UNRESOLVED_COLUMN.WITHOUT_SUGGESTION",
  "sqlState" : "42703",
  "messageParameters" : {
    "objectName" : "`x`"
  },
  "queryContext" : [ {
    "objectType" : "",
    "objectName" : "",
    "startIndex" : 81,
    "stopIndex" : 81,
    "fragment" : "x"
  } ]
}


-- !query
BEGIN
  DECLARE x INT;
  DECLARE c1 CURSOR FOR SELECT 1 AS val;
  OPEN c1;
  FETCH c1 INTO x;
  CLOSE c1;
  FETCH c1 INTO x; -- Should fail
END
-- !query analysis
org.apache.spark.sql.catalyst.ExtendedAnalysisException
{
  "errorClass" : "UNRESOLVED_COLUMN.WITHOUT_SUGGESTION",
  "sqlState" : "42703",
  "messageParameters" : {
    "objectName" : "`x`"
  },
  "queryContext" : [ {
    "objectType" : "",
    "objectName" : "",
    "startIndex" : 92,
    "stopIndex" : 92,
    "fragment" : "x"
  } ]
}


-- !query
BEGIN
  DECLARE step, x INT DEFAULT 0;
  REPEAT
    BEGIN
      DECLARE c1 CURSOR FOR SELECT step AS val;
      OPEN c1;
      FETCH c1 INTO x;
      SET step = step + 1;
    END;
  UNTIL step = 10 END REPEAT;
  VALUES(step);
END
-- !query analysis
org.apache.spark.sql.catalyst.ExtendedAnalysisException
{
  "errorClass" : "UNRESOLVED_COLUMN.WITHOUT_SUGGESTION",
  "sqlState" : "42703",
  "messageParameters" : {
    "objectName" : "`x`"
  },
  "queryContext" : [ {
    "objectType" : "",
    "objectName" : "",
    "startIndex" : 142,
    "stopIndex" : 142,
    "fragment" : "x"
  } ]
}


-- !query
BEGIN
  DECLARE x INT;
  DECLARE c1 CURSOR FOR SELECT 1 AS val;
  OPEN c1;
  BEGIN
    DECLARE y INT;
    DECLARE c1 CURSOR FOR SELECT 2 AS val;
    OPEN c1; -- This is the inner c1, should succeed
    FETCH c1 INTO y;
    VALUES (y); -- Should return 2
    CLOSE c1;
  END;
  FETCH c1 INTO x; -- This is the outer c1, should still be open
  VALUES (x); -- Should return 1
  CLOSE c1;
END
-- !query analysis
org.apache.spark.sql.catalyst.ExtendedAnalysisException
{
  "errorClass" : "UNRESOLVED_COLUMN.WITHOUT_SUGGESTION",
  "sqlState" : "42703",
  "messageParameters" : {
    "objectName" : "`y`"
  },
  "queryContext" : [ {
    "objectType" : "",
    "objectName" : "",
    "startIndex" : 217,
    "stopIndex" : 217,
    "fragment" : "y"
  } ]
}


-- !query
CREATE TABLE cursor_sensitivity_test (id INT, value STRING) USING parquet
-- !query analysis
CreateDataSourceTableCommand `spark_catalog`.`default`.`cursor_sensitivity_test`, false


-- !query
INSERT INTO cursor_sensitivity_test VALUES (1, 'row1'), (2, 'row2')
-- !query analysis
InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/cursor_sensitivity_test, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/cursor_sensitivity_test], Append, `spark_catalog`.`default`.`cursor_sensitivity_test`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/cursor_sensitivity_test), [id, value]
+- Project [col1#x AS id#x, col2#x AS value#x]
   +- LocalRelation [col1#x, col2#x]


-- !query
BEGIN
  DECLARE fetched_id INT;
  DECLARE fetched_value STRING;
  DECLARE row_count_first_open INT DEFAULT 0;
  DECLARE row_count_second_open INT DEFAULT 0;
  DECLARE nomorerows BOOLEAN DEFAULT false;

  DECLARE cur CURSOR FOR SELECT id, value FROM cursor_sensitivity_test ORDER BY id;

  DECLARE CONTINUE HANDLER FOR NOT FOUND SET nomorerows = true;

  INSERT INTO cursor_sensitivity_test VALUES (3, 'row3'), (4, 'row4');

  OPEN cur;

  INSERT INTO cursor_sensitivity_test VALUES (5, 'row5'), (6, 'row6');

  REPEAT
    FETCH cur INTO fetched_id, fetched_value;
    IF NOT nomorerows THEN
      SET row_count_first_open = row_count_first_open + 1;
    END IF;
  UNTIL nomorerows END REPEAT;

  CLOSE cur;

  SET nomorerows = false;
  OPEN cur;

  REPEAT
    FETCH cur INTO fetched_id, fetched_value;
    IF NOT nomorerows THEN
      SET row_count_second_open = row_count_second_open + 1;
    END IF;
  UNTIL nomorerows END REPEAT;

  VALUES (row_count_first_open, row_count_second_open);

  CLOSE cur;
END
-- !query analysis
org.apache.spark.sql.catalyst.ExtendedAnalysisException
{
  "errorClass" : "UNRESOLVED_COLUMN.WITHOUT_SUGGESTION",
  "sqlState" : "42703",
  "messageParameters" : {
    "objectName" : "`fetched_id`"
  },
  "queryContext" : [ {
    "objectType" : "",
    "objectName" : "",
    "startIndex" : 538,
    "stopIndex" : 562,
    "fragment" : "fetched_id, fetched_value"
  } ]
}


-- !query
DROP TABLE cursor_sensitivity_test
-- !query analysis
DropTable false, false
+- ResolvedIdentifier V2SessionCatalog(spark_catalog), default.cursor_sensitivity_test


-- !query
BEGIN
  DECLARE min_id INT DEFAULT 2;
  DECLARE max_id INT DEFAULT 4;
  DECLARE fetched_id INT;
  DECLARE fetched_value STRING;
  DECLARE nomorerows BOOLEAN DEFAULT false;
  DECLARE result STRING DEFAULT '';
  DECLARE cur CURSOR FOR SELECT id, value FROM VALUES(1, 'a'), (2, 'b'), (3, 'c'), (4, 'd'), (5, 'e') AS t(id, value) WHERE id >= ? AND id <= ?;
  DECLARE CONTINUE HANDLER FOR NOT FOUND SET nomorerows = true;

  OPEN cur USING min_id, max_id;

  REPEAT
    FETCH cur INTO fetched_id, fetched_value;
    IF NOT nomorerows THEN
      SET result = result || fetched_value;
    END IF;
  UNTIL nomorerows END REPEAT;

  CLOSE cur;
  VALUES (result);
END
-- !query analysis
org.apache.spark.sql.catalyst.ExtendedAnalysisException
{
  "errorClass" : "UNRESOLVED_COLUMN.WITHOUT_SUGGESTION",
  "sqlState" : "42703",
  "messageParameters" : {
    "objectName" : "`fetched_id`"
  },
  "queryContext" : [ {
    "objectType" : "",
    "objectName" : "",
    "startIndex" : 481,
    "stopIndex" : 505,
    "fragment" : "fetched_id, fetched_value"
  } ]
}


-- !query
BEGIN
  DECLARE search_value STRING DEFAULT 'c';
  DECLARE fetched_id INT;
  DECLARE fetched_value STRING;
  DECLARE nomorerows BOOLEAN DEFAULT false;
  DECLARE id_sum INT DEFAULT 0;
  DECLARE cur CURSOR FOR SELECT id, value FROM VALUES(1, 'a'), (2, 'b'), (3, 'c'), (4, 'c'), (5, 'e') AS t(id, value) WHERE value = :search_val;
  DECLARE CONTINUE HANDLER FOR NOT FOUND SET nomorerows = true;

  OPEN cur USING search_value AS search_val;

  REPEAT
    FETCH cur INTO fetched_id, fetched_value;
    IF NOT nomorerows THEN
      SET id_sum = id_sum + fetched_id;
    END IF;
  UNTIL nomorerows END REPEAT;

  CLOSE cur;
  VALUES (id_sum);
END
-- !query analysis
org.apache.spark.sql.catalyst.ExtendedAnalysisException
{
  "errorClass" : "UNRESOLVED_COLUMN.WITHOUT_SUGGESTION",
  "sqlState" : "42703",
  "messageParameters" : {
    "objectName" : "`fetched_id`"
  },
  "queryContext" : [ {
    "objectType" : "",
    "objectName" : "",
    "startIndex" : 468,
    "stopIndex" : 492,
    "fragment" : "fetched_id, fetched_value"
  } ]
}


-- !query
BEGIN
  DECLARE fetched_id INT;
  DECLARE nomorerows BOOLEAN DEFAULT false;
  DECLARE count1 INT DEFAULT 0;
  DECLARE count2 INT DEFAULT 0;
  DECLARE cur CURSOR FOR SELECT id FROM VALUES(1), (2), (3), (4), (5) AS t(id) WHERE id >= ? AND id <= ?;
  DECLARE CONTINUE HANDLER FOR NOT FOUND SET nomorerows = true;

  OPEN cur USING 2, 3;
  REPEAT
    FETCH cur INTO fetched_id;
    IF NOT nomorerows THEN
      SET count1 = count1 + 1;
    END IF;
  UNTIL nomorerows END REPEAT;
  CLOSE cur;

  SET nomorerows = false;
  OPEN cur USING 1, 5;
  REPEAT
    FETCH cur INTO fetched_id;
    IF NOT nomorerows THEN
      SET count2 = count2 + 1;
    END IF;
  UNTIL nomorerows END REPEAT;
  CLOSE cur;

  VALUES (count1, count2);
END
-- !query analysis
org.apache.spark.sql.catalyst.ExtendedAnalysisException
{
  "errorClass" : "UNRESOLVED_COLUMN.WITHOUT_SUGGESTION",
  "sqlState" : "42703",
  "messageParameters" : {
    "objectName" : "`fetched_id`"
  },
  "queryContext" : [ {
    "objectType" : "",
    "objectName" : "",
    "startIndex" : 363,
    "stopIndex" : 372,
    "fragment" : "fetched_id"
  } ]
}


-- !query
BEGIN
  DECLARE base INT DEFAULT 10;
  DECLARE fetched_id INT;
  DECLARE nomorerows BOOLEAN DEFAULT false;
  DECLARE sum INT DEFAULT 0;
  DECLARE cur CURSOR FOR SELECT id FROM VALUES(5), (10), (15), (20), (25) AS t(id) WHERE id > ?;
  DECLARE CONTINUE HANDLER FOR NOT FOUND SET nomorerows = true;

  OPEN cur USING base + 5;

  REPEAT
    FETCH cur INTO fetched_id;
    IF NOT nomorerows THEN
      SET sum = sum + fetched_id;
    END IF;
  UNTIL nomorerows END REPEAT;

  CLOSE cur;
  VALUES (sum); -- Should be 20 + 25 = 45
END
-- !query analysis
org.apache.spark.sql.catalyst.ExtendedAnalysisException
{
  "errorClass" : "UNRESOLVED_COLUMN.WITHOUT_SUGGESTION",
  "sqlState" : "42703",
  "messageParameters" : {
    "objectName" : "`fetched_id`"
  },
  "queryContext" : [ {
    "objectType" : "",
    "objectName" : "",
    "startIndex" : 355,
    "stopIndex" : 364,
    "fragment" : "fetched_id"
  } ]
}


-- !query
BEGIN
  DECLARE int_val INT;
  DECLARE str_val STRING;
  DECLARE date_val DATE;
  DECLARE bool_val BOOLEAN;

  DECLARE cur_int CURSOR FOR SELECT typeof(:p) as type, :p as val;
  OPEN cur_int USING 42 AS p;
  FETCH cur_int INTO str_val, int_val;
  CLOSE cur_int;
  VALUES ('INT', int_val);

  DECLARE cur_str CURSOR FOR SELECT typeof(:p) as type, :p as val;
  OPEN cur_str USING 'hello' AS p;
  FETCH cur_str INTO str_val, str_val;
  CLOSE cur_str;
  VALUES ('STRING', str_val);

  DECLARE cur_date CURSOR FOR SELECT typeof(:p) as type, :p as val;
  OPEN cur_date USING DATE '2023-12-25' AS p;
  FETCH cur_date INTO str_val, date_val;
  CLOSE cur_date;
  VALUES ('DATE', date_val);

  DECLARE cur_bool CURSOR FOR SELECT typeof(:p) as type, :p as val;
  OPEN cur_bool USING true AS p;
  FETCH cur_bool INTO str_val, bool_val;
  CLOSE cur_bool;
  VALUES ('BOOLEAN', bool_val);
END
-- !query analysis
org.apache.spark.sql.exceptions.SqlScriptingException
{
  "errorClass" : "INVALID_CURSOR_DECLARATION",
  "sqlState" : "42601"
}


-- !query
BEGIN
  DECLARE result INT;

  DECLARE cur_pos CURSOR FOR SELECT ? + ? AS sum;
  OPEN cur_pos USING 10, 20;
  FETCH cur_pos INTO result;
  CLOSE cur_pos;
  VALUES ('positional', result); -- Should be 30

  DECLARE cur_named CURSOR FOR SELECT :a + :b AS sum;
  OPEN cur_named USING 15 AS a, 25 AS b;
  FETCH cur_named INTO result;
  CLOSE cur_named;
  VALUES ('named', result); -- Should be 40

  DECLARE x INT DEFAULT 100;
  DECLARE cur_var CURSOR FOR SELECT ? + 1 AS val;
  OPEN cur_var USING x;
  FETCH cur_var INTO result;
  CLOSE cur_var;
  VALUES ('variable', result); -- Should be 101
END
-- !query analysis
org.apache.spark.sql.exceptions.SqlScriptingException
{
  "errorClass" : "INVALID_CURSOR_DECLARATION",
  "sqlState" : "42601"
}


-- !query
BEGIN
  DECLARE result INT;
  DECLARE base INT DEFAULT 5;

  DECLARE cur1 CURSOR FOR SELECT :p AS val;
  OPEN cur1 USING 5 + 10 AS p;
  FETCH cur1 INTO result;
  CLOSE cur1;
  VALUES ('arithmetic', result); -- Should be 15

  DECLARE cur2 CURSOR FOR SELECT :p AS val;
  OPEN cur2 USING base * 2 AS p;
  FETCH cur2 INTO result;
  CLOSE cur2;
  VALUES ('variable_expr', result); -- Should be 10
END
-- !query analysis
org.apache.spark.sql.exceptions.SqlScriptingException
{
  "errorClass" : "INVALID_CURSOR_DECLARATION",
  "sqlState" : "42601"
}


-- !query
BEGIN
  outer: BEGIN
    DECLARE x INT;
    DECLARE c1 CURSOR FOR SELECT 42 AS val;
    OPEN outer.c1;
    FETCH outer.c1 INTO x;
    VALUES (x); -- Should return 42
    CLOSE outer.c1;
  END;
END
-- !query analysis
org.apache.spark.sql.catalyst.ExtendedAnalysisException
{
  "errorClass" : "UNRESOLVED_COLUMN.WITHOUT_SUGGESTION",
  "sqlState" : "42703",
  "messageParameters" : {
    "objectName" : "`x`"
  },
  "queryContext" : [ {
    "objectType" : "",
    "objectName" : "",
    "startIndex" : 128,
    "stopIndex" : 128,
    "fragment" : "x"
  } ]
}


-- !query
BEGIN
  outer_lbl: BEGIN
    DECLARE x, y INT;
    DECLARE cur CURSOR FOR SELECT 1 AS val;

    inner_lbl: BEGIN
      DECLARE cur CURSOR FOR SELECT 2 AS val;

      OPEN outer_lbl.cur;  -- Opens outer cursor
      OPEN inner_lbl.cur;  -- Opens inner cursor

      FETCH cur INTO x;

      FETCH outer_lbl.cur INTO y;

      CLOSE inner_lbl.cur;
    END;

    CLOSE outer_lbl.cur;

    VALUES (x, y);
  END;
END
-- !query analysis
org.apache.spark.sql.catalyst.ExtendedAnalysisException
{
  "errorClass" : "UNRESOLVED_COLUMN.WITHOUT_SUGGESTION",
  "sqlState" : "42703",
  "messageParameters" : {
    "objectName" : "`x`"
  },
  "queryContext" : [ {
    "objectType" : "",
    "objectName" : "",
    "startIndex" : 281,
    "stopIndex" : 281,
    "fragment" : "x"
  } ]
}


-- !query
BEGIN
  lbl: BEGIN
    DECLARE min_val INT DEFAULT 3;
    DECLARE max_val INT DEFAULT 4;
    DECLARE fetched_id INT;
    DECLARE result STRING DEFAULT '';
    DECLARE cur CURSOR FOR SELECT id FROM VALUES(1), (2), (3), (4), (5) AS t(id) WHERE id >= ? AND id <= ?;

    OPEN lbl.cur USING min_val, max_val;

    FETCH lbl.cur INTO fetched_id;
    SET result = result || CAST(fetched_id AS STRING);
    FETCH lbl.cur INTO fetched_id;
    SET result = result || CAST(fetched_id AS STRING);

    CLOSE lbl.cur;
    VALUES (result); -- Should be '34'
  END;
END
-- !query analysis
org.apache.spark.sql.catalyst.ExtendedAnalysisException
{
  "errorClass" : "UNRESOLVED_COLUMN.WITHOUT_SUGGESTION",
  "sqlState" : "42703",
  "messageParameters" : {
    "objectName" : "`fetched_id`"
  },
  "queryContext" : [ {
    "objectType" : "",
    "objectName" : "",
    "startIndex" : 330,
    "stopIndex" : 339,
    "fragment" : "fetched_id"
  } ]
}


-- !query
BEGIN
  DECLARE x, y INT;
  DECLARE cur CURSOR FOR SELECT 1 AS a, 2 AS b;
  OPEN cur;
  FETCH cur INTO x, x;  -- Should fail - duplicate variable
END
-- !query analysis
org.apache.spark.sql.catalyst.ExtendedAnalysisException
{
  "errorClass" : "UNRESOLVED_COLUMN.WITHOUT_SUGGESTION",
  "sqlState" : "42703",
  "messageParameters" : {
    "objectName" : "`x`"
  },
  "queryContext" : [ {
    "objectType" : "",
    "objectName" : "",
    "startIndex" : 104,
    "stopIndex" : 107,
    "fragment" : "x, x"
  } ]
}


-- !query
BEGIN
  DECLARE int_var INT;
  DECLARE str_var STRING;
  DECLARE cur CURSOR FOR SELECT 100.7 AS double_val, 42 AS int_val;

  OPEN cur;
  FETCH cur INTO int_var, str_var;  -- double->int cast, int->string cast
  CLOSE cur;

  VALUES (int_var, str_var);  -- Should be (100, '42') with ANSI rounding
END
-- !query analysis
org.apache.spark.sql.catalyst.ExtendedAnalysisException
{
  "errorClass" : "UNRESOLVED_COLUMN.WITHOUT_SUGGESTION",
  "sqlState" : "42703",
  "messageParameters" : {
    "objectName" : "`int_var`"
  },
  "queryContext" : [ {
    "objectType" : "",
    "objectName" : "",
    "startIndex" : 154,
    "stopIndex" : 169,
    "fragment" : "int_var, str_var"
  } ]
}


-- !query
BEGIN
  DECLARE x INT;
  DECLARE cur CURSOR FOR SELECT 1, 2, 3;
  OPEN cur;
  FETCH cur INTO x;  -- Should fail - 1 target but 3 columns
END
-- !query analysis
org.apache.spark.sql.catalyst.ExtendedAnalysisException
{
  "errorClass" : "UNRESOLVED_COLUMN.WITHOUT_SUGGESTION",
  "sqlState" : "42703",
  "messageParameters" : {
    "objectName" : "`x`"
  },
  "queryContext" : [ {
    "objectType" : "",
    "objectName" : "",
    "startIndex" : 94,
    "stopIndex" : 94,
    "fragment" : "x"
  } ]
}


-- !query
BEGIN
  DECLARE x, y, z, w INT;
  DECLARE cur CURSOR FOR SELECT 1, 2;
  OPEN cur;
  FETCH cur INTO x, y, z, w;  -- Should fail - 4 targets but 2 columns
END
-- !query analysis
org.apache.spark.sql.catalyst.ExtendedAnalysisException
{
  "errorClass" : "UNRESOLVED_COLUMN.WITHOUT_SUGGESTION",
  "sqlState" : "42703",
  "messageParameters" : {
    "objectName" : "`x`"
  },
  "queryContext" : [ {
    "objectType" : "",
    "objectName" : "",
    "startIndex" : 100,
    "stopIndex" : 109,
    "fragment" : "x, y, z, w"
  } ]
}


-- !query
BEGIN
  DECLARE x INT;
  DECLARE cur INSENSITIVE CURSOR FOR SELECT 42 AS val;
  OPEN cur;
  FETCH cur INTO x;
  CLOSE cur;
  VALUES (x); -- Should return 42
END
-- !query analysis
org.apache.spark.sql.catalyst.ExtendedAnalysisException
{
  "errorClass" : "UNRESOLVED_COLUMN.WITHOUT_SUGGESTION",
  "sqlState" : "42703",
  "messageParameters" : {
    "objectName" : "`x`"
  },
  "queryContext" : [ {
    "objectType" : "",
    "objectName" : "",
    "startIndex" : 108,
    "stopIndex" : 108,
    "fragment" : "x"
  } ]
}


-- !query
BEGIN
  DECLARE y INT;
  DECLARE cur ASENSITIVE CURSOR FOR SELECT 99 AS val;
  OPEN cur;
  FETCH cur INTO y;
  CLOSE cur;
  VALUES (y); -- Should return 99
END
-- !query analysis
org.apache.spark.sql.catalyst.ExtendedAnalysisException
{
  "errorClass" : "UNRESOLVED_COLUMN.WITHOUT_SUGGESTION",
  "sqlState" : "42703",
  "messageParameters" : {
    "objectName" : "`y`"
  },
  "queryContext" : [ {
    "objectType" : "",
    "objectName" : "",
    "startIndex" : 107,
    "stopIndex" : 107,
    "fragment" : "y"
  } ]
}


-- !query
BEGIN
  DECLARE z INT;
  DECLARE cur CURSOR FOR SELECT 77 AS val FOR READ ONLY;
  OPEN cur;
  FETCH cur INTO z;
  CLOSE cur;
  VALUES (z); -- Should return 77
END
-- !query analysis
org.apache.spark.sql.catalyst.ExtendedAnalysisException
{
  "errorClass" : "UNRESOLVED_COLUMN.WITHOUT_SUGGESTION",
  "sqlState" : "42703",
  "messageParameters" : {
    "objectName" : "`z`"
  },
  "queryContext" : [ {
    "objectType" : "",
    "objectName" : "",
    "startIndex" : 110,
    "stopIndex" : 110,
    "fragment" : "z"
  } ]
}


-- !query
BEGIN
  DECLARE w INT;
  DECLARE cur INSENSITIVE CURSOR FOR SELECT 123 AS val FOR READ ONLY;
  OPEN cur;
  FETCH cur INTO w;
  CLOSE cur;
  VALUES (w); -- Should return 123
END
-- !query analysis
org.apache.spark.sql.catalyst.ExtendedAnalysisException
{
  "errorClass" : "UNRESOLVED_COLUMN.WITHOUT_SUGGESTION",
  "sqlState" : "42703",
  "messageParameters" : {
    "objectName" : "`w`"
  },
  "queryContext" : [ {
    "objectType" : "",
    "objectName" : "",
    "startIndex" : 123,
    "stopIndex" : 123,
    "fragment" : "w"
  } ]
}


-- !query
BEGIN
  DECLARE a INT;
  DECLARE cur CURSOR FOR SELECT 55 AS val;
  OPEN cur;
  FETCH NEXT cur INTO a;
  CLOSE cur;
  VALUES (a); -- Should return 55
END
-- !query analysis
org.apache.spark.sql.catalyst.ExtendedAnalysisException
{
  "errorClass" : "UNRESOLVED_COLUMN.WITHOUT_SUGGESTION",
  "sqlState" : "42703",
  "messageParameters" : {
    "objectName" : "`a`"
  },
  "queryContext" : [ {
    "objectType" : "",
    "objectName" : "",
    "startIndex" : 101,
    "stopIndex" : 101,
    "fragment" : "a"
  } ]
}


-- !query
BEGIN
  DECLARE b INT;
  DECLARE cur CURSOR FOR SELECT 66 AS val;
  OPEN cur;
  FETCH FROM cur INTO b;
  CLOSE cur;
  VALUES (b); -- Should return 66
END
-- !query analysis
org.apache.spark.sql.catalyst.ExtendedAnalysisException
{
  "errorClass" : "UNRESOLVED_COLUMN.WITHOUT_SUGGESTION",
  "sqlState" : "42703",
  "messageParameters" : {
    "objectName" : "`b`"
  },
  "queryContext" : [ {
    "objectType" : "",
    "objectName" : "",
    "startIndex" : 101,
    "stopIndex" : 101,
    "fragment" : "b"
  } ]
}


-- !query
BEGIN
  DECLARE c INT;
  DECLARE cur CURSOR FOR SELECT 88 AS val;
  OPEN cur;
  FETCH NEXT FROM cur INTO c;
  CLOSE cur;
  VALUES (c); -- Should return 88
END
-- !query analysis
org.apache.spark.sql.catalyst.ExtendedAnalysisException
{
  "errorClass" : "UNRESOLVED_COLUMN.WITHOUT_SUGGESTION",
  "sqlState" : "42703",
  "messageParameters" : {
    "objectName" : "`c`"
  },
  "queryContext" : [ {
    "objectType" : "",
    "objectName" : "",
    "startIndex" : 106,
    "stopIndex" : 106,
    "fragment" : "c"
  } ]
}


-- !query
BEGIN
  DECLARE person_record STRUCT<name STRING, age INT>;
  DECLARE cur CURSOR FOR SELECT 'Alice' AS name, 30 AS age;
  OPEN cur;
  FETCH cur INTO person_record;
  CLOSE cur;
  VALUES (person_record.name, person_record.age); -- Should return 'Alice', 30
END
-- !query analysis
org.apache.spark.sql.catalyst.ExtendedAnalysisException
{
  "errorClass" : "UNRESOLVED_COLUMN.WITHOUT_SUGGESTION",
  "sqlState" : "42703",
  "messageParameters" : {
    "objectName" : "`person_record`"
  },
  "queryContext" : [ {
    "objectType" : "",
    "objectName" : "",
    "startIndex" : 150,
    "stopIndex" : 162,
    "fragment" : "person_record"
  } ]
}


-- !query
BEGIN
  DECLARE record_var STRUCT<id INT, value STRING>;
  DECLARE cur CURSOR FOR SELECT 42.7 AS id, 100 AS value;
  OPEN cur;
  FETCH cur INTO record_var;
  CLOSE cur;
  VALUES (record_var.id, record_var.value); -- Should return 42, '100' (with casting)
END
-- !query analysis
org.apache.spark.sql.catalyst.ExtendedAnalysisException
{
  "errorClass" : "UNRESOLVED_COLUMN.WITHOUT_SUGGESTION",
  "sqlState" : "42703",
  "messageParameters" : {
    "objectName" : "`record_var`"
  },
  "queryContext" : [ {
    "objectType" : "",
    "objectName" : "",
    "startIndex" : 145,
    "stopIndex" : 154,
    "fragment" : "record_var"
  } ]
}


-- !query
BEGIN
  DECLARE record_var STRUCT<a INT, b INT>;
  DECLARE cur CURSOR FOR SELECT 1, 2, 3;
  OPEN cur;
  FETCH cur INTO record_var;  -- Should fail - 2 struct fields but 3 cursor columns
END
-- !query analysis
org.apache.spark.sql.catalyst.ExtendedAnalysisException
{
  "errorClass" : "UNRESOLVED_COLUMN.WITHOUT_SUGGESTION",
  "sqlState" : "42703",
  "messageParameters" : {
    "objectName" : "`record_var`"
  },
  "queryContext" : [ {
    "objectType" : "",
    "objectName" : "",
    "startIndex" : 120,
    "stopIndex" : 129,
    "fragment" : "record_var"
  } ]
}


-- !query
BEGIN
  DECLARE x INT;
  DECLARE cur CURSOR FOR SELECT 1, 2;
  OPEN cur;
  FETCH cur INTO x;  -- Should fail - single non-struct variable but 2 columns
END
-- !query analysis
org.apache.spark.sql.catalyst.ExtendedAnalysisException
{
  "errorClass" : "UNRESOLVED_COLUMN.WITHOUT_SUGGESTION",
  "sqlState" : "42703",
  "messageParameters" : {
    "objectName" : "`x`"
  },
  "queryContext" : [ {
    "objectType" : "",
    "objectName" : "",
    "startIndex" : 91,
    "stopIndex" : 91,
    "fragment" : "x"
  } ]
}


-- !query
BEGIN
  DECLARE complex_record STRUCT<id BIGINT, name STRING, value DOUBLE>;
  DECLARE cur CURSOR FOR SELECT 100 AS id, 'test' AS name, 99.5 AS value;
  OPEN cur;
  FETCH cur INTO complex_record;
  CLOSE cur;
  VALUES (complex_record); -- Should return struct(100, 'test', 99.5)
END
-- !query analysis
org.apache.spark.sql.catalyst.ExtendedAnalysisException
{
  "errorClass" : "UNRESOLVED_COLUMN.WITHOUT_SUGGESTION",
  "sqlState" : "42703",
  "messageParameters" : {
    "objectName" : "`complex_record`"
  },
  "queryContext" : [ {
    "objectType" : "",
    "objectName" : "",
    "startIndex" : 181,
    "stopIndex" : 194,
    "fragment" : "complex_record"
  } ]
}


-- !query
SET VAR session_x = 0;
SET VAR session_y = '';
BEGIN
  DECLARE cur CURSOR FOR SELECT 42 AS num, 'hello' AS text;
  OPEN cur;
  FETCH cur INTO session_x, session_y;
  CLOSE cur;
END;
SELECT session_x, session_y;  -- Should return 42, 'hello'
-- !query analysis
org.apache.spark.sql.catalyst.parser.ParseException
{
  "errorClass" : "INVALID_SET_SYNTAX",
  "sqlState" : "42000",
  "queryContext" : [ {
    "objectType" : "",
    "objectName" : "",
    "startIndex" : 1,
    "stopIndex" : 209,
    "fragment" : "SET VAR session_x = 0;\nSET VAR session_y = '';\nBEGIN\n  DECLARE cur CURSOR FOR SELECT 42 AS num, 'hello' AS text;\n  OPEN cur;\n  FETCH cur INTO session_x, session_y;\n  CLOSE cur;\nEND;\nSELECT session_x, session_y"
  } ]
}


-- !query
SET VAR session_var = 0;
BEGIN
  DECLARE local_var STRING;
  DECLARE cur CURSOR FOR SELECT 100 AS a, 'world' AS b;
  OPEN cur;
  FETCH cur INTO session_var, local_var;
  CLOSE cur;
  VALUES (session_var, local_var);  -- Should return 100, 'world'
END
-- !query analysis
org.apache.spark.sql.catalyst.parser.ParseException
{
  "errorClass" : "INVALID_SET_SYNTAX",
  "sqlState" : "42000",
  "queryContext" : [ {
    "objectType" : "",
    "objectName" : "",
    "startIndex" : 1,
    "stopIndex" : 250,
    "fragment" : "SET VAR session_var = 0;\nBEGIN\n  DECLARE local_var STRING;\n  DECLARE cur CURSOR FOR SELECT 100 AS a, 'world' AS b;\n  OPEN cur;\n  FETCH cur INTO session_var, local_var;\n  CLOSE cur;\n  VALUES (session_var, local_var);  -- Should return 100, 'world'\nEND"
  } ]
}


-- !query
SET VAR session_int = 0;
SET VAR session_str = '';
BEGIN
  DECLARE cur CURSOR FOR SELECT 99.9 AS double_val, 42 AS int_val;
  OPEN cur;
  FETCH cur INTO session_int, session_str;  -- double->int cast, int->string cast
  CLOSE cur;
END;
SELECT session_int, session_str;  -- Should return 99, '42' (with ANSI rounding)
-- !query analysis
org.apache.spark.sql.catalyst.parser.ParseException
{
  "errorClass" : "INVALID_SET_SYNTAX",
  "sqlState" : "42000",
  "queryContext" : [ {
    "objectType" : "",
    "objectName" : "",
    "startIndex" : 1,
    "stopIndex" : 267,
    "fragment" : "SET VAR session_int = 0;\nSET VAR session_str = '';\nBEGIN\n  DECLARE cur CURSOR FOR SELECT 99.9 AS double_val, 42 AS int_val;\n  OPEN cur;\n  FETCH cur INTO session_int, session_str;  -- double->int cast, int->string cast\n  CLOSE cur;\nEND;\nSELECT session_int, session_str"
  } ]
}


-- !query
SET VAR session_dup = 0;
BEGIN
  DECLARE cur CURSOR FOR SELECT 1, 2;
  OPEN cur;
  FETCH cur INTO session_dup, session_dup;  -- Should fail - duplicate session variable
END
-- !query analysis
org.apache.spark.sql.catalyst.parser.ParseException
{
  "errorClass" : "INVALID_SET_SYNTAX",
  "sqlState" : "42000",
  "queryContext" : [ {
    "objectType" : "",
    "objectName" : "",
    "startIndex" : 1,
    "stopIndex" : 172,
    "fragment" : "SET VAR session_dup = 0;\nBEGIN\n  DECLARE cur CURSOR FOR SELECT 1, 2;\n  OPEN cur;\n  FETCH cur INTO session_dup, session_dup;  -- Should fail - duplicate session variable\nEND"
  } ]
}


-- !query
SET VAR dup_var = 0;
BEGIN
  DECLARE dup_var INT;
  DECLARE cur CURSOR FOR SELECT 1, 2;
  OPEN cur;
  FETCH cur INTO dup_var, dup_var;  -- Should fail - duplicate variable name
END
-- !query analysis
org.apache.spark.sql.catalyst.parser.ParseException
{
  "errorClass" : "INVALID_SET_SYNTAX",
  "sqlState" : "42000",
  "queryContext" : [ {
    "objectType" : "",
    "objectName" : "",
    "startIndex" : 1,
    "stopIndex" : 180,
    "fragment" : "SET VAR dup_var = 0;\nBEGIN\n  DECLARE dup_var INT;\n  DECLARE cur CURSOR FOR SELECT 1, 2;\n  OPEN cur;\n  FETCH cur INTO dup_var, dup_var;  -- Should fail - duplicate variable name\nEND"
  } ]
}


-- !query
BEGIN
  outer: BEGIN
    DECLARE x INT;
    DECLARE cur CURSOR FOR SELECT 42 AS val;

    inner: BEGIN
      OPEN cur;  -- Open in inner scope
      FETCH cur INTO x;
    END;

    FETCH cur INTO x;  -- This should succeed
    VALUES (x);  -- Should return 42

  END;
END
-- !query analysis
org.apache.spark.sql.catalyst.parser.ParseException
{
  "errorClass" : "PARSE_SYNTAX_ERROR",
  "sqlState" : "42601",
  "messageParameters" : {
    "error" : "'inner'",
    "hint" : ""
  }
}


-- !query
BEGIN
  DECLARE y INT;

  scope1: BEGIN
    DECLARE cur CURSOR FOR SELECT 99 AS val;
    OPEN cur;
    FETCH cur INTO y;
  END;  -- cursor is implicitly closed here (exiting DECLARE scope)

  FETCH cur INTO y;
END
-- !query analysis
org.apache.spark.sql.catalyst.ExtendedAnalysisException
{
  "errorClass" : "UNRESOLVED_COLUMN.WITHOUT_SUGGESTION",
  "sqlState" : "42703",
  "messageParameters" : {
    "objectName" : "`y`"
  },
  "queryContext" : [ {
    "objectType" : "",
    "objectName" : "",
    "startIndex" : 119,
    "stopIndex" : 119,
    "fragment" : "y"
  } ]
}


-- !query
BEGIN
  DECLARE x INT DEFAULT 0;
  DECLARE result STRING DEFAULT '';
  DECLARE cur CURSOR FOR SELECT 42 AS val;

  OPEN cur;
  FETCH cur INTO x;  -- OK: gets value 42
  SET result = result || CAST(x AS STRING);

  FETCH cur INTO x;  -- Continues execution (no handler needed for completion conditions)

  SET result = result || '-after-fetch';
  CLOSE cur;

  VALUES (result);  -- Should return '42-after-fetch', proving execution continued
END
-- !query analysis
org.apache.spark.sql.catalyst.ExtendedAnalysisException
{
  "errorClass" : "UNRESOLVED_COLUMN.WITHOUT_SUGGESTION",
  "sqlState" : "42703",
  "messageParameters" : {
    "objectName" : "`x`"
  },
  "queryContext" : [ {
    "objectType" : "",
    "objectName" : "",
    "startIndex" : 143,
    "stopIndex" : 143,
    "fragment" : "x"
  } ]
}


-- !query
BEGIN
  DECLARE x INT;
  SET x = 1 / 0;  -- Should throw DIVIDE_BY_ZERO (SQLSTATE 22012), not continue
  VALUES ('This should not be reached');
END
-- !query analysis
org.apache.spark.SparkArithmeticException
{
  "errorClass" : "DIVIDE_BY_ZERO",
  "sqlState" : "22012",
  "messageParameters" : {
    "config" : "\"spark.sql.ansi.enabled\""
  },
  "queryContext" : [ {
    "objectType" : "",
    "objectName" : "",
    "startIndex" : 34,
    "stopIndex" : 38,
    "fragment" : "1 / 0"
  } ]
}


-- !query
BEGIN
  DECLARE x INT;
  DECLARE IDENTIFIER('my_cursor') CURSOR FOR SELECT 99 AS val;
  OPEN IDENTIFIER('my_cursor');
  FETCH IDENTIFIER('my_cursor') INTO x;
  CLOSE IDENTIFIER('my_cursor');
  VALUES (x); -- Should return 99
END
-- !query analysis
org.apache.spark.sql.catalyst.ExtendedAnalysisException
{
  "errorClass" : "UNRESOLVED_COLUMN.WITHOUT_SUGGESTION",
  "sqlState" : "42703",
  "messageParameters" : {
    "objectName" : "`x`"
  },
  "queryContext" : [ {
    "objectType" : "",
    "objectName" : "",
    "startIndex" : 156,
    "stopIndex" : 156,
    "fragment" : "x"
  } ]
}


-- !query
BEGIN
  DECLARE result INT;
  DECLARE IDENTIFIER('MixedCase') CURSOR FOR SELECT 42;
  OPEN IDENTIFIER('MixedCase');
  FETCH IDENTIFIER('MixedCase') INTO result;
  CLOSE IDENTIFIER('MixedCase');
  VALUES (result); -- Should return 42
END
-- !query analysis
org.apache.spark.sql.catalyst.ExtendedAnalysisException
{
  "errorClass" : "UNRESOLVED_COLUMN.WITHOUT_SUGGESTION",
  "sqlState" : "42703",
  "messageParameters" : {
    "objectName" : "`result`"
  },
  "queryContext" : [ {
    "objectType" : "",
    "objectName" : "",
    "startIndex" : 154,
    "stopIndex" : 159,
    "fragment" : "result"
  } ]
}


-- !query
BEGIN
  DECLARE IDENTIFIER('my_result') INT;
  DECLARE my_cursor CURSOR FOR SELECT 123 AS val;
  OPEN my_cursor;
  FETCH my_cursor INTO IDENTIFIER('my_result');
  CLOSE my_cursor;
  VALUES (IDENTIFIER('my_result')); -- Should return 123
END
-- !query analysis
org.apache.spark.sql.catalyst.ExtendedAnalysisException
{
  "errorClass" : "UNRESOLVED_COLUMN.WITHOUT_SUGGESTION",
  "sqlState" : "42703",
  "messageParameters" : {
    "objectName" : "`my_result`"
  },
  "queryContext" : [ {
    "objectType" : "",
    "objectName" : "",
    "startIndex" : 137,
    "stopIndex" : 159,
    "fragment" : "IDENTIFIER('my_result')"
  } ]
}


-- !query
BEGIN
  DECLARE IDENTIFIER('val1') INT;
  DECLARE IDENTIFIER('val2') STRING;
  DECLARE cur CURSOR FOR SELECT 42, 'test';
  OPEN cur;
  FETCH cur INTO IDENTIFIER('val1'), IDENTIFIER('val2');
  CLOSE cur;
  VALUES (IDENTIFIER('val1'), IDENTIFIER('val2')); -- Should return 42, 'test'
END
-- !query analysis
org.apache.spark.sql.catalyst.ExtendedAnalysisException
{
  "errorClass" : "UNRESOLVED_COLUMN.WITHOUT_SUGGESTION",
  "sqlState" : "42703",
  "messageParameters" : {
    "objectName" : "`val1`"
  },
  "queryContext" : [ {
    "objectType" : "",
    "objectName" : "",
    "startIndex" : 151,
    "stopIndex" : 188,
    "fragment" : "IDENTIFIER('val1'), IDENTIFIER('val2')"
  } ]
}


-- !query
BEGIN
  DECLARE n INT;
  DECLARE sum_result INT DEFAULT 0;
  DECLARE done BOOLEAN DEFAULT false;
  DECLARE cur CURSOR FOR
    WITH RECURSIVE numbers(n) AS (
      SELECT 1 AS n
      UNION ALL
      SELECT n + 1 FROM numbers WHERE n < 5
    )
    SELECT n FROM numbers;
  DECLARE CONTINUE HANDLER FOR NOT FOUND SET done = true;

  OPEN cur;
  REPEAT
    FETCH cur INTO n;
    IF NOT done THEN
      SET sum_result = sum_result + n;
    END IF;
  UNTIL done END REPEAT;
  CLOSE cur;

  VALUES (sum_result); -- Should return 15 (1+2+3+4+5)
END
-- !query analysis
org.apache.spark.sql.catalyst.ExtendedAnalysisException
{
  "errorClass" : "UNRESOLVED_COLUMN.WITHOUT_SUGGESTION",
  "sqlState" : "42703",
  "messageParameters" : {
    "objectName" : "`n`"
  },
  "queryContext" : [ {
    "objectType" : "",
    "objectName" : "",
    "startIndex" : 370,
    "stopIndex" : 370,
    "fragment" : "n"
  } ]
}


-- !query
CREATE TEMPORARY VIEW customers AS SELECT 1 AS id, 'Alice' AS name
UNION ALL SELECT 2, 'Bob'
UNION ALL SELECT 3, 'Charlie';

CREATE TEMPORARY VIEW orders AS SELECT 1 AS customer_id, 100 AS amount
UNION ALL SELECT 1, 200
UNION ALL SELECT 2, 150;

BEGIN
  DECLARE customer_name STRING;
  DECLARE total_amount INT;
  DECLARE result STRING DEFAULT '';
  DECLARE done BOOLEAN DEFAULT false;
  DECLARE cur CURSOR FOR
    SELECT c.name, COALESCE(SUM(o.amount), 0) AS total
    FROM customers c
    LEFT JOIN orders o ON c.id = o.customer_id
    WHERE c.id IN (SELECT customer_id FROM orders WHERE amount > 50)
    GROUP BY c.name
    ORDER BY total DESC;
  DECLARE CONTINUE HANDLER FOR NOT FOUND SET done = true;

  OPEN cur;
  REPEAT
    FETCH cur INTO customer_name, total_amount;
    IF NOT done THEN
      SET result = result || customer_name || ':' || CAST(total_amount AS STRING) || ',';
    END IF;
  UNTIL done END REPEAT;
  CLOSE cur;

  VALUES (result); -- Should return 'Alice:300,Bob:150,'
END;

DROP VIEW customers;
DROP VIEW orders
-- !query analysis
org.apache.spark.sql.catalyst.parser.ParseException
{
  "errorClass" : "PARSE_SYNTAX_ERROR",
  "sqlState" : "42601",
  "messageParameters" : {
    "error" : "'CREATE'",
    "hint" : ": extra input 'CREATE'"
  }
}


-- !query
BEGIN
  DECLARE result STRING DEFAULT '';
  DECLARE i INT;
  DECLARE done1 BOOLEAN DEFAULT false;
  DECLARE cur1 CURSOR FOR SELECT id FROM VALUES(1), (2) AS t(id);
  DECLARE CONTINUE HANDLER FOR NOT FOUND SET done1 = true;

  OPEN cur1;
  REPEAT
    FETCH cur1 INTO i;
    IF NOT done1 THEN
      DECLARE j INT;
      DECLARE done2 BOOLEAN DEFAULT false;
      DECLARE cur2 CURSOR FOR SELECT id FROM VALUES(10), (20) AS t(id);
      DECLARE CONTINUE HANDLER FOR NOT FOUND SET done2 = true;

      OPEN cur2;
      REPEAT
        FETCH cur2 INTO j;
        IF NOT done2 THEN
          DECLARE k INT;
          DECLARE done3 BOOLEAN DEFAULT false;
          DECLARE cur3 CURSOR FOR SELECT id FROM VALUES(100) AS t(id);
          DECLARE CONTINUE HANDLER FOR NOT FOUND SET done3 = true;

          OPEN cur3;
          REPEAT
            FETCH cur3 INTO k;
            IF NOT done3 THEN
              SET result = result || CAST(i AS STRING) || '-' ||
                          CAST(j AS STRING) || '-' ||
                          CAST(k AS STRING) || ',';
            END IF;
          UNTIL done3 END REPEAT;
          CLOSE cur3;
        END IF;
        SET done3 = false; -- Reset for next iteration
      UNTIL done2 END REPEAT;
      CLOSE cur2;
    END IF;
    SET done2 = false; -- Reset for next iteration
  UNTIL done1 END REPEAT;
  CLOSE cur1;

  VALUES (result); -- Should return '1-10-100,1-20-100,2-10-100,2-20-100,'
END
-- !query analysis
org.apache.spark.sql.exceptions.SqlScriptingException
{
  "errorClass" : "INVALID_VARIABLE_DECLARATION.NOT_ALLOWED_IN_SCOPE",
  "sqlState" : "42K0M",
  "messageParameters" : {
    "varName" : "`j`"
  }
}


-- !query
BEGIN
  DECLARE result INT;
  DECLARE cur CURSOR FOR SELECT ? + 10 AS val;

  OPEN cur USING 5, 99, 100;  -- Only first parameter (5) is used, others ignored
  FETCH cur INTO result;
  CLOSE cur;

  VALUES (result);  -- Should return 15 (5 + 10)
END
-- !query analysis
org.apache.spark.sql.catalyst.ExtendedAnalysisException
{
  "errorClass" : "UNRESOLVED_COLUMN.WITHOUT_SUGGESTION",
  "sqlState" : "42703",
  "messageParameters" : {
    "objectName" : "`result`"
  },
  "queryContext" : [ {
    "objectType" : "",
    "objectName" : "",
    "startIndex" : 176,
    "stopIndex" : 181,
    "fragment" : "result"
  } ]
}


-- !query
BEGIN
  DECLARE result INT;
  DECLARE cur CURSOR FOR SELECT ? + ? AS val;

  OPEN cur USING 10;  -- Only 1 parameter provided, but 2 needed
  FETCH cur INTO result;
  CLOSE cur;
END
-- !query analysis
org.apache.spark.sql.AnalysisException
{
  "errorClass" : "UNBOUND_SQL_PARAMETER",
  "sqlState" : "42P02",
  "messageParameters" : {
    "name" : "_11"
  },
  "queryContext" : [ {
    "objectType" : "",
    "objectName" : "",
    "startIndex" : 12,
    "stopIndex" : 12,
    "fragment" : "?"
  } ]
}


-- !query
BEGIN
  DECLARE result INT;
  DECLARE cur CURSOR FOR SELECT :x + :y AS val;

  OPEN cur USING (x AS x);  -- Missing parameter 'y'
  FETCH cur INTO result;
  CLOSE cur;
END
-- !query analysis
org.apache.spark.sql.catalyst.ExtendedAnalysisException
{
  "errorClass" : "UNRESOLVED_COLUMN.WITHOUT_SUGGESTION",
  "sqlState" : "42703",
  "messageParameters" : {
    "objectName" : "`x`"
  },
  "queryContext" : [ {
    "objectType" : "",
    "objectName" : "",
    "startIndex" : 96,
    "stopIndex" : 96,
    "fragment" : "x"
  } ]
}


-- !query
BEGIN
  DECLARE result INT;
  DECLARE cur CURSOR FOR SELECT :alpha + :beta AS val;

  OPEN cur USING (5 AS alpha, 10 AS gamma);  -- 'gamma' provided but 'beta' expected
  FETCH cur INTO result;
  CLOSE cur;
END
-- !query analysis
org.apache.spark.sql.AnalysisException
{
  "errorClass" : "UNBOUND_SQL_PARAMETER",
  "sqlState" : "42P02",
  "messageParameters" : {
    "name" : "beta"
  },
  "queryContext" : [ {
    "objectType" : "",
    "objectName" : "",
    "startIndex" : 17,
    "stopIndex" : 21,
    "fragment" : ":beta"
  } ]
}


-- !query
BEGIN
  DECLARE result INT;
  DECLARE cur CURSOR FOR SELECT ? + :named AS val;  -- Mixed positional and named

  OPEN cur USING 10, (20 AS named);
  FETCH cur INTO result;
  CLOSE cur;
END
-- !query analysis
org.apache.spark.sql.catalyst.parser.ParseException
{
  "errorClass" : "PARSE_SYNTAX_ERROR",
  "sqlState" : "42601",
  "messageParameters" : {
    "error" : "')'",
    "hint" : ""
  }
}


-- !query
BEGIN
  DECLARE x INT DEFAULT 100;
  DECLARE y INT DEFAULT 200;
  DECLARE result INT;
  DECLARE cur CURSOR FOR SELECT :x + :y AS val;

  OPEN cur USING (x AS x, y AS y);
  FETCH cur INTO result;
  CLOSE cur;

  VALUES (result);  -- Should return 300 (100 + 200)
END
-- !query analysis
org.apache.spark.sql.catalyst.ExtendedAnalysisException
{
  "errorClass" : "UNRESOLVED_COLUMN.WITHOUT_SUGGESTION",
  "sqlState" : "42703",
  "messageParameters" : {
    "objectName" : "`result`"
  },
  "queryContext" : [ {
    "objectType" : "",
    "objectName" : "",
    "startIndex" : 188,
    "stopIndex" : 193,
    "fragment" : "result"
  } ]
}


-- !query
BEGIN
  DECLARE base INT DEFAULT 10;
  DECLARE result INT;
  DECLARE cur CURSOR FOR SELECT :a * :b AS val;

  OPEN cur USING (base * 2 AS a, base + 5 AS b);  -- Expressions with aliases
  FETCH cur INTO result;
  CLOSE cur;

  VALUES (result);  -- Should return 300 (20 * 15)
END
-- !query analysis
org.apache.spark.sql.catalyst.ExtendedAnalysisException
{
  "errorClass" : "UNRESOLVED_COLUMN.WITHOUT_SUGGESTION",
  "sqlState" : "42703",
  "messageParameters" : {
    "objectName" : "`result`"
  },
  "queryContext" : [ {
    "objectType" : "",
    "objectName" : "",
    "startIndex" : 204,
    "stopIndex" : 209,
    "fragment" : "result"
  } ]
}


-- !query
BEGIN
  DECLARE x INT DEFAULT 5;
  DECLARE y INT DEFAULT 3;
  DECLARE result INT;
  DECLARE cur CURSOR FOR SELECT ? * ? + ? AS val;

  OPEN cur USING x * 2, y + 1, 10;  -- (5*2) * (3+1) + 10 = 10 * 4 + 10 = 50
  FETCH cur INTO result;
  CLOSE cur;

  VALUES (result);  -- Should return 50
END
-- !query analysis
org.apache.spark.sql.catalyst.ExtendedAnalysisException
{
  "errorClass" : "UNRESOLVED_COLUMN.WITHOUT_SUGGESTION",
  "sqlState" : "42703",
  "messageParameters" : {
    "objectName" : "`result`"
  },
  "queryContext" : [ {
    "objectType" : "",
    "objectName" : "",
    "startIndex" : 228,
    "stopIndex" : 233,
    "fragment" : "result"
  } ]
}


-- !query
BEGIN
  DECLARE result1 INT;
  DECLARE result2 INT;
  DECLARE cur CURSOR FOR SELECT ? * 10 AS val;

  OPEN cur USING 5;
  FETCH cur INTO result1;
  CLOSE cur;

  OPEN cur USING 8;
  FETCH cur INTO result2;
  CLOSE cur;

  VALUES (result1, result2);  -- Should return (50, 80)
END
-- !query analysis
org.apache.spark.sql.catalyst.ExtendedAnalysisException
{
  "errorClass" : "UNRESOLVED_COLUMN.WITHOUT_SUGGESTION",
  "sqlState" : "42703",
  "messageParameters" : {
    "objectName" : "`result1`"
  },
  "queryContext" : [ {
    "objectType" : "",
    "objectName" : "",
    "startIndex" : 138,
    "stopIndex" : 144,
    "fragment" : "result1"
  } ]
}


-- !query
BEGIN
  DECLARE result1 INT;
  DECLARE result2 INT;
  DECLARE cur CURSOR FOR SELECT :factor * 10 AS val;

  OPEN cur USING (3 AS factor);
  FETCH cur INTO result1;
  CLOSE cur;

  OPEN cur USING (7 AS factor);
  FETCH cur INTO result2;
  CLOSE cur;

  VALUES (result1, result2);  -- Should return (30, 70)
END
-- !query analysis
org.apache.spark.sql.catalyst.ExtendedAnalysisException
{
  "errorClass" : "UNRESOLVED_COLUMN.WITHOUT_SUGGESTION",
  "sqlState" : "42703",
  "messageParameters" : {
    "objectName" : "`result1`"
  },
  "queryContext" : [ {
    "objectType" : "",
    "objectName" : "",
    "startIndex" : 156,
    "stopIndex" : 162,
    "fragment" : "result1"
  } ]
}


-- !query
BEGIN
  DECLARE result INT;
  DECLARE cur CURSOR FOR SELECT :x + :y AS val;

  OPEN cur USING 10, (20 AS y);
  FETCH cur INTO result;
  CLOSE cur;
END
-- !query analysis
org.apache.spark.sql.catalyst.parser.ParseException
{
  "errorClass" : "PARSE_SYNTAX_ERROR",
  "sqlState" : "42601",
  "messageParameters" : {
    "error" : "')'",
    "hint" : ""
  }
}


-- !query
BEGIN
  DECLARE multiplier INT DEFAULT 4;
  DECLARE addend INT DEFAULT 6;
  DECLARE result INT;
  DECLARE cur CURSOR FOR SELECT :multiplier * 10 + :addend AS val;

  OPEN cur USING (multiplier AS multiplier, addend AS addend);
  FETCH cur INTO result;
  CLOSE cur;

  VALUES (result);  -- Should return 46 (4 * 10 + 6)
END
-- !query analysis
org.apache.spark.sql.catalyst.ExtendedAnalysisException
{
  "errorClass" : "UNRESOLVED_COLUMN.WITHOUT_SUGGESTION",
  "sqlState" : "42703",
  "messageParameters" : {
    "objectName" : "`result`"
  },
  "queryContext" : [ {
    "objectType" : "",
    "objectName" : "",
    "startIndex" : 245,
    "stopIndex" : 250,
    "fragment" : "result"
  } ]
}


-- !query
BEGIN
  DECLARE result INT;
  DECLARE my_cursor CURSOR FOR SELECT 42 AS val;
  OPEN MY_CURSOR;
  FETCH MY_CURSOR INTO result;
  CLOSE MY_CURSOR;
  VALUES (result);  -- Should return 42
END
-- !query analysis
org.apache.spark.sql.catalyst.ExtendedAnalysisException
{
  "errorClass" : "UNRESOLVED_COLUMN.WITHOUT_SUGGESTION",
  "sqlState" : "42703",
  "messageParameters" : {
    "objectName" : "`result`"
  },
  "queryContext" : [ {
    "objectType" : "",
    "objectName" : "",
    "startIndex" : 119,
    "stopIndex" : 124,
    "fragment" : "result"
  } ]
}


-- !query
BEGIN
  DECLARE result INT;
  DECLARE MyCursor CURSOR FOR SELECT 99 AS val;
  OPEN mycursor;
  FETCH mycursor INTO result;
  CLOSE mycursor;
  VALUES (result);  -- Should return 99
END
-- !query analysis
org.apache.spark.sql.catalyst.ExtendedAnalysisException
{
  "errorClass" : "UNRESOLVED_COLUMN.WITHOUT_SUGGESTION",
  "sqlState" : "42703",
  "messageParameters" : {
    "objectName" : "`result`"
  },
  "queryContext" : [ {
    "objectType" : "",
    "objectName" : "",
    "startIndex" : 116,
    "stopIndex" : 121,
    "fragment" : "result"
  } ]
}


-- !query
BEGIN
  outer_lbl: BEGIN
    DECLARE cur CURSOR FOR SELECT 123 AS val;
    DECLARE result INT;

    OPEN OUTER_LBL.cur;  -- Label in different case
    FETCH OUTER_LBL.CUR INTO result;  -- Both in different case
    CLOSE outer_lbl.CUR;  -- Cursor in different case

    VALUES (result);  -- Should return 123
  END;
END
-- !query analysis
org.apache.spark.SparkException
{
  "errorClass" : "INTERNAL_ERROR",
  "sqlState" : "XX000",
  "messageParameters" : {
    "message" : "Invalid state transition from CURSOR to VARIABLE"
  }
}


-- !query
BEGIN
  DECLARE result INT;
  DECLARE IDENTIFIER('MyCase') CURSOR FOR SELECT 42;
  OPEN IDENTIFIER('mycase');  -- Different case but should work (case-insensitive resolution)
  FETCH IDENTIFIER('MYCASE') INTO result;  -- Another case variation
  CLOSE IDENTIFIER('MyCaSe');  -- Yet another variation
  VALUES (result);  -- Should return 42
END
-- !query analysis
org.apache.spark.sql.catalyst.ExtendedAnalysisException
{
  "errorClass" : "UNRESOLVED_COLUMN.WITHOUT_SUGGESTION",
  "sqlState" : "42703",
  "messageParameters" : {
    "objectName" : "`result`"
  },
  "queryContext" : [ {
    "objectType" : "",
    "objectName" : "",
    "startIndex" : 210,
    "stopIndex" : 215,
    "fragment" : "result"
  } ]
}


-- !query
BEGIN
  DECLARE x INT;
  DECLARE cur CURSOR FOR SELECT 1 AS val;

  OPEN cur;
  FETCH cur INTO x;  -- Succeeds, x = 1
  VALUES ('After first fetch', x);

  FETCH cur INTO x;  -- No more rows, raises CURSOR_NO_MORE_ROWS (SQLSTATE 02000)

  VALUES ('After second fetch - script continued', x);  -- Should execute, x still = 1
  CLOSE cur;
END
-- !query analysis
org.apache.spark.sql.catalyst.ExtendedAnalysisException
{
  "errorClass" : "UNRESOLVED_COLUMN.WITHOUT_SUGGESTION",
  "sqlState" : "42703",
  "messageParameters" : {
    "objectName" : "`x`"
  },
  "queryContext" : [ {
    "objectType" : "",
    "objectName" : "",
    "startIndex" : 96,
    "stopIndex" : 96,
    "fragment" : "x"
  } ]
}


-- !query
BEGIN
  DECLARE x INT;
  DECLARE no_more_data BOOLEAN DEFAULT false;
  DECLARE cur CURSOR FOR SELECT 10 AS val;

  DECLARE CONTINUE HANDLER FOR CURSOR_NO_MORE_ROWS SET no_more_data = true;

  OPEN cur;
  FETCH cur INTO x;
  VALUES ('First fetch', x, no_more_data);  -- Should be (10, false)

  FETCH cur INTO x;  -- Triggers handler
  VALUES ('After no data', x, no_more_data);  -- Should be (10, true)

  CLOSE cur;
END
-- !query analysis
org.apache.spark.sql.catalyst.ExtendedAnalysisException
{
  "errorClass" : "UNRESOLVED_COLUMN.WITHOUT_SUGGESTION",
  "sqlState" : "42703",
  "messageParameters" : {
    "objectName" : "`x`"
  },
  "queryContext" : [ {
    "objectType" : "",
    "objectName" : "",
    "startIndex" : 220,
    "stopIndex" : 220,
    "fragment" : "x"
  } ]
}


-- !query
BEGIN
  DECLARE x INT;
  DECLARE found BOOLEAN DEFAULT true;
  DECLARE cur CURSOR FOR SELECT 20 AS val;

  DECLARE CONTINUE HANDLER FOR NOT FOUND SET found = false;

  OPEN cur;
  FETCH cur INTO x;
  VALUES ('First fetch', x, found);  -- Should be (20, true)

  FETCH cur INTO x;  -- No more rows, triggers NOT FOUND handler
  VALUES ('After NOT FOUND', x, found);  -- Should be (20, false)

  CLOSE cur;
END
-- !query analysis
org.apache.spark.sql.catalyst.ExtendedAnalysisException
{
  "errorClass" : "UNRESOLVED_COLUMN.WITHOUT_SUGGESTION",
  "sqlState" : "42703",
  "messageParameters" : {
    "objectName" : "`x`"
  },
  "queryContext" : [ {
    "objectType" : "",
    "objectName" : "",
    "startIndex" : 196,
    "stopIndex" : 196,
    "fragment" : "x"
  } ]
}


-- !query
BEGIN
  DECLARE x INT DEFAULT 0;
  DECLARE cur CURSOR FOR SELECT 30 AS val;

  DECLARE EXIT HANDLER FOR NOT FOUND
  BEGIN
    VALUES ('In EXIT handler', x);
    CLOSE cur;  -- Clean up in the handler
  END;

  OPEN cur;
  FETCH cur INTO x;
  VALUES ('First fetch', x);  -- Should execute (30)

  FETCH cur INTO x;  -- Triggers EXIT handler, which closes cursor and exits

  VALUES ('After handler');  -- Should NOT execute (handler exits)
END
-- !query analysis
org.apache.spark.sql.catalyst.ExtendedAnalysisException
{
  "errorClass" : "UNRESOLVED_COLUMN.WITHOUT_SUGGESTION",
  "sqlState" : "42703",
  "messageParameters" : {
    "objectName" : "`x`"
  },
  "queryContext" : [ {
    "objectType" : "",
    "objectName" : "",
    "startIndex" : 238,
    "stopIndex" : 238,
    "fragment" : "x"
  } ]
}


-- !query
BEGIN
  DECLARE x INT;
  DECLARE row_count INT DEFAULT 0;
  DECLARE done BOOLEAN DEFAULT false;
  DECLARE cur CURSOR FOR SELECT id FROM VALUES(1), (2), (3) AS t(id);

  DECLARE CONTINUE HANDLER FOR NOT FOUND SET done = true;

  OPEN cur;

  REPEAT
    FETCH cur INTO x;
    IF NOT done THEN
      SET row_count = row_count + 1;
      VALUES ('Fetched', x, row_count);
    END IF;
  UNTIL done END REPEAT;

  VALUES ('Total rows', row_count);  -- Should be 3
  CLOSE cur;
END
-- !query analysis
org.apache.spark.sql.catalyst.ExtendedAnalysisException
{
  "errorClass" : "UNRESOLVED_COLUMN.WITHOUT_SUGGESTION",
  "sqlState" : "42703",
  "messageParameters" : {
    "objectName" : "`x`"
  },
  "queryContext" : [ {
    "objectType" : "",
    "objectName" : "",
    "startIndex" : 268,
    "stopIndex" : 268,
    "fragment" : "x"
  } ]
}


-- !query
BEGIN
  DECLARE x INT;
  DECLARE specific_handler_ran BOOLEAN DEFAULT false;
  DECLARE generic_handler_ran BOOLEAN DEFAULT false;
  DECLARE cur CURSOR FOR SELECT 40 AS val;

  DECLARE CONTINUE HANDLER FOR CURSOR_NO_MORE_ROWS SET specific_handler_ran = true;

  DECLARE CONTINUE HANDLER FOR NOT FOUND SET generic_handler_ran = true;

  OPEN cur;
  FETCH cur INTO x;
  FETCH cur INTO x;  -- Should trigger specific handler, not generic

  VALUES ('Specific ran', specific_handler_ran);  -- Should be true
  VALUES ('Generic ran', generic_handler_ran);    -- Should be false

  CLOSE cur;
END
-- !query analysis
org.apache.spark.sql.catalyst.ExtendedAnalysisException
{
  "errorClass" : "UNRESOLVED_COLUMN.WITHOUT_SUGGESTION",
  "sqlState" : "42703",
  "messageParameters" : {
    "objectName" : "`x`"
  },
  "queryContext" : [ {
    "objectType" : "",
    "objectName" : "",
    "startIndex" : 363,
    "stopIndex" : 363,
    "fragment" : "x"
  } ]
}


-- !query
BEGIN
  DECLARE x INT;
  DECLARE outer_done BOOLEAN DEFAULT false;
  DECLARE outer_cur CURSOR FOR SELECT 50 AS val;

  DECLARE CONTINUE HANDLER FOR NOT FOUND SET outer_done = true;

  OPEN outer_cur;
  FETCH outer_cur INTO x;
  VALUES ('Outer fetch 1', x, outer_done);  -- Should be (50, false)

  BEGIN
    DECLARE y INT;
    DECLARE inner_done BOOLEAN DEFAULT false;
    DECLARE inner_cur CURSOR FOR SELECT 60 AS val;

    DECLARE CONTINUE HANDLER FOR NOT FOUND SET inner_done = true;

    OPEN inner_cur;
    FETCH inner_cur INTO y;
    FETCH inner_cur INTO y;  -- Triggers inner handler only

    VALUES ('Inner', y, inner_done, outer_done);  -- Should be (60, true, false)
    CLOSE inner_cur;
  END;

  FETCH outer_cur INTO x;  -- Triggers outer handler
  VALUES ('Outer fetch 2', x, outer_done);  -- Should be (50, true)

  CLOSE outer_cur;
END
-- !query analysis
org.apache.spark.sql.catalyst.ExtendedAnalysisException
{
  "errorClass" : "UNRESOLVED_COLUMN.WITHOUT_SUGGESTION",
  "sqlState" : "42703",
  "messageParameters" : {
    "objectName" : "`x`"
  },
  "queryContext" : [ {
    "objectType" : "",
    "objectName" : "",
    "startIndex" : 224,
    "stopIndex" : 224,
    "fragment" : "x"
  } ]
}
