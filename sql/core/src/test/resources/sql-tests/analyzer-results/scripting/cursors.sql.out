-- Automatically generated by SQLQueryTestSuite
-- !query
BEGIN
  DECLARE x INT DEFAULT 10;
  DECLARE x CURSOR FOR SELECT 1 AS col;
  OPEN x;
  FETCH x INTO x;
  VALUES (x); -- Should return 1
  CLOSE x;
END
-- !query analysis
LocalRelation [col1#x]


-- !query
BEGIN
  DECLARE c1 CURSOR FOR SELECT 1;
  DECLARE c1 CURSOR FOR SELECT 2;
END
-- !query analysis
org.apache.spark.sql.AnalysisException
{
  "errorClass" : "CURSOR_ALREADY_EXISTS",
  "sqlState" : "42723",
  "messageParameters" : {
    "cursorName" : "c1"
  }
}


-- !query
BEGIN
  DECLARE y INT;
  DECLARE c1 CURSOR FOR SELECT 1 AS val;
  BEGIN
    DECLARE x INT;
    DECLARE c1 CURSOR FOR SELECT 2 AS val;
    OPEN c1;  -- Opens inner c1
    FETCH c1 INTO x;
    VALUES (x); -- Should return 2
    CLOSE c1;
  END;
  OPEN c1;  -- Opens outer c1
  FETCH c1 INTO y;
  VALUES (y); -- Should return 1
  CLOSE c1;
END
-- !query analysis
LocalRelation [col1#x]


-- !query
BEGIN
  DECLARE c1 CURSOR FOR SELECT 1;
  OPEN c1;
  OPEN c1; -- Should fail
END
-- !query analysis
org.apache.spark.sql.AnalysisException
{
  "errorClass" : "CURSOR_ALREADY_OPEN",
  "sqlState" : "24502",
  "messageParameters" : {
    "cursorName" : "c1"
  }
}


-- !query
BEGIN
  DECLARE x INT;
  DECLARE c1 CURSOR FOR SELECT 1 AS val;
  OPEN c1;
  FETCH c1 INTO x;
  VALUES (x); -- Should return 1
  CLOSE c1;
  OPEN c1; -- Should succeed
  FETCH c1 INTO x;
  VALUES (x); -- Should return 1
  CLOSE c1;
END
-- !query analysis
LocalRelation [col1#x]


-- !query
BEGIN
  DECLARE c1 CURSOR FOR SELECT 1;
  CLOSE c1; -- Should fail
END
-- !query analysis
org.apache.spark.sql.AnalysisException
{
  "errorClass" : "CURSOR_NOT_OPEN",
  "sqlState" : "24501",
  "messageParameters" : {
    "cursorName" : "c1"
  }
}


-- !query
BEGIN
  DECLARE c1 CURSOR FOR SELECT 1;
  OPEN c1;
  CLOSE c1;
  CLOSE c1; -- Should fail
END
-- !query analysis
org.apache.spark.sql.AnalysisException
{
  "errorClass" : "CURSOR_NOT_OPEN",
  "sqlState" : "24501",
  "messageParameters" : {
    "cursorName" : "c1"
  }
}


-- !query
BEGIN
  DECLARE x INT;
  DECLARE c1 CURSOR FOR SELECT 1 AS val;
  FETCH c1 INTO x; -- Should fail
END
-- !query analysis
org.apache.spark.sql.AnalysisException
{
  "errorClass" : "CURSOR_NOT_OPEN",
  "sqlState" : "24501",
  "messageParameters" : {
    "cursorName" : "c1"
  }
}


-- !query
BEGIN
  DECLARE x INT;
  DECLARE c1 CURSOR FOR SELECT 1 AS val;
  OPEN c1;
  FETCH c1 INTO x;
  CLOSE c1;
  FETCH c1 INTO x; -- Should fail
END
-- !query analysis
org.apache.spark.sql.AnalysisException
{
  "errorClass" : "CURSOR_NOT_OPEN",
  "sqlState" : "24501",
  "messageParameters" : {
    "cursorName" : "c1"
  }
}


-- !query
BEGIN
  DECLARE step, x INT DEFAULT 0;
  REPEAT
    BEGIN
      DECLARE c1 CURSOR FOR SELECT step AS val;
      OPEN c1;
      FETCH c1 INTO x;
      SET step = step + 1;
    END;
  UNTIL step = 10 END REPEAT;
  VALUES(step);
END
-- !query analysis
org.apache.spark.sql.catalyst.analysis.UnresolvedException
{
  "errorClass" : "INTERNAL_ERROR",
  "sqlState" : "XX000",
  "messageParameters" : {
    "message" : "Invalid call to dataType on unresolved object"
  }
}


-- !query
BEGIN
  DECLARE x INT;
  DECLARE c1 CURSOR FOR SELECT 1 AS val;
  OPEN c1;
  BEGIN
    DECLARE y INT;
    DECLARE c1 CURSOR FOR SELECT 2 AS val;
    OPEN c1; -- This is the inner c1, should succeed
    FETCH c1 INTO y;
    VALUES (y); -- Should return 2
    CLOSE c1;
  END;
  FETCH c1 INTO x; -- This is the outer c1, should still be open
  VALUES (x); -- Should return 1
  CLOSE c1;
END
-- !query analysis
LocalRelation [col1#x]


-- !query
CREATE TABLE cursor_sensitivity_test (id INT, value STRING) USING parquet
-- !query analysis
CreateDataSourceTableCommand `spark_catalog`.`default`.`cursor_sensitivity_test`, false


-- !query
INSERT INTO cursor_sensitivity_test VALUES (1, 'row1'), (2, 'row2')
-- !query analysis
InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/cursor_sensitivity_test, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/cursor_sensitivity_test], Append, `spark_catalog`.`default`.`cursor_sensitivity_test`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/cursor_sensitivity_test), [id, value]
+- Project [col1#x AS id#x, col2#x AS value#x]
   +- LocalRelation [col1#x, col2#x]


-- !query
BEGIN
  DECLARE fetched_id INT;
  DECLARE fetched_value STRING;
  DECLARE row_count_first_open INT DEFAULT 0;
  DECLARE row_count_second_open INT DEFAULT 0;
  DECLARE nomorerows BOOLEAN DEFAULT false;

  DECLARE cur CURSOR FOR SELECT id, value FROM cursor_sensitivity_test ORDER BY id;

  DECLARE CONTINUE HANDLER FOR NOT FOUND SET nomorerows = true;

  INSERT INTO cursor_sensitivity_test VALUES (3, 'row3'), (4, 'row4');

  OPEN cur;

  INSERT INTO cursor_sensitivity_test VALUES (5, 'row5'), (6, 'row6');

  REPEAT
    FETCH cur INTO fetched_id, fetched_value;
    IF NOT nomorerows THEN
      SET row_count_first_open = row_count_first_open + 1;
    END IF;
  UNTIL nomorerows END REPEAT;

  CLOSE cur;

  SET nomorerows = false;
  OPEN cur;

  REPEAT
    FETCH cur INTO fetched_id, fetched_value;
    IF NOT nomorerows THEN
      SET row_count_second_open = row_count_second_open + 1;
    END IF;
  UNTIL nomorerows END REPEAT;

  VALUES (row_count_first_open, row_count_second_open);

  CLOSE cur;
END
-- !query analysis
org.apache.spark.sql.catalyst.analysis.UnresolvedException
{
  "errorClass" : "INTERNAL_ERROR",
  "sqlState" : "XX000",
  "messageParameters" : {
    "message" : "Invalid call to dataType on unresolved object"
  }
}


-- !query
DROP TABLE cursor_sensitivity_test
-- !query analysis
DropTable false, false
+- ResolvedIdentifier V2SessionCatalog(spark_catalog), default.cursor_sensitivity_test


-- !query
BEGIN
  DECLARE min_id INT DEFAULT 2;
  DECLARE max_id INT DEFAULT 4;
  DECLARE fetched_id INT;
  DECLARE fetched_value STRING;
  DECLARE nomorerows BOOLEAN DEFAULT false;
  DECLARE result STRING DEFAULT '';
  DECLARE cur CURSOR FOR SELECT id, value FROM VALUES(1, 'a'), (2, 'b'), (3, 'c'), (4, 'd'), (5, 'e') AS t(id, value) WHERE id >= ? AND id <= ?;
  DECLARE CONTINUE HANDLER FOR NOT FOUND SET nomorerows = true;

  OPEN cur USING min_id, max_id;

  REPEAT
    FETCH cur INTO fetched_id, fetched_value;
    IF NOT nomorerows THEN
      SET result = result || fetched_value;
    END IF;
  UNTIL nomorerows END REPEAT;

  CLOSE cur;
  VALUES (result);
END
-- !query analysis
org.apache.spark.sql.catalyst.analysis.UnresolvedException
{
  "errorClass" : "INTERNAL_ERROR",
  "sqlState" : "XX000",
  "messageParameters" : {
    "message" : "Invalid call to dataType on unresolved object"
  }
}


-- !query
BEGIN
  DECLARE search_value STRING DEFAULT 'c';
  DECLARE fetched_id INT;
  DECLARE fetched_value STRING;
  DECLARE nomorerows BOOLEAN DEFAULT false;
  DECLARE id_sum INT DEFAULT 0;
  DECLARE cur CURSOR FOR SELECT id, value FROM VALUES(1, 'a'), (2, 'b'), (3, 'c'), (4, 'c'), (5, 'e') AS t(id, value) WHERE value = :search_val;
  DECLARE CONTINUE HANDLER FOR NOT FOUND SET nomorerows = true;

  OPEN cur USING search_value AS search_val;

  REPEAT
    FETCH cur INTO fetched_id, fetched_value;
    IF NOT nomorerows THEN
      SET id_sum = id_sum + fetched_id;
    END IF;
  UNTIL nomorerows END REPEAT;

  CLOSE cur;
  VALUES (id_sum);
END
-- !query analysis
org.apache.spark.sql.catalyst.analysis.UnresolvedException
{
  "errorClass" : "INTERNAL_ERROR",
  "sqlState" : "XX000",
  "messageParameters" : {
    "message" : "Invalid call to dataType on unresolved object"
  }
}


-- !query
BEGIN
  DECLARE fetched_id INT;
  DECLARE nomorerows BOOLEAN DEFAULT false;
  DECLARE count1 INT DEFAULT 0;
  DECLARE count2 INT DEFAULT 0;
  DECLARE cur CURSOR FOR SELECT id FROM VALUES(1), (2), (3), (4), (5) AS t(id) WHERE id >= ? AND id <= ?;
  DECLARE CONTINUE HANDLER FOR NOT FOUND SET nomorerows = true;

  OPEN cur USING 2, 3;
  REPEAT
    FETCH cur INTO fetched_id;
    IF NOT nomorerows THEN
      SET count1 = count1 + 1;
    END IF;
  UNTIL nomorerows END REPEAT;
  CLOSE cur;

  SET nomorerows = false;
  OPEN cur USING 1, 5;
  REPEAT
    FETCH cur INTO fetched_id;
    IF NOT nomorerows THEN
      SET count2 = count2 + 1;
    END IF;
  UNTIL nomorerows END REPEAT;
  CLOSE cur;

  VALUES (count1, count2);
END
-- !query analysis
org.apache.spark.sql.catalyst.analysis.UnresolvedException
{
  "errorClass" : "INTERNAL_ERROR",
  "sqlState" : "XX000",
  "messageParameters" : {
    "message" : "Invalid call to dataType on unresolved object"
  }
}


-- !query
BEGIN
  DECLARE base INT DEFAULT 10;
  DECLARE fetched_id INT;
  DECLARE nomorerows BOOLEAN DEFAULT false;
  DECLARE sum INT DEFAULT 0;
  DECLARE cur CURSOR FOR SELECT id FROM VALUES(5), (10), (15), (20), (25) AS t(id) WHERE id > ?;
  DECLARE CONTINUE HANDLER FOR NOT FOUND SET nomorerows = true;

  OPEN cur USING base + 5;

  REPEAT
    FETCH cur INTO fetched_id;
    IF NOT nomorerows THEN
      SET sum = sum + fetched_id;
    END IF;
  UNTIL nomorerows END REPEAT;

  CLOSE cur;
  VALUES (sum); -- Should be 20 + 25 = 45
END
-- !query analysis
org.apache.spark.sql.catalyst.analysis.UnresolvedException
{
  "errorClass" : "INTERNAL_ERROR",
  "sqlState" : "XX000",
  "messageParameters" : {
    "message" : "Invalid call to dataType on unresolved object"
  }
}


-- !query
BEGIN
  outer: BEGIN
    DECLARE x INT;
    DECLARE c1 CURSOR FOR SELECT 42 AS val;
    OPEN outer.c1;
    FETCH outer.c1 INTO x;
    VALUES (x); -- Should return 42
    CLOSE outer.c1;
  END;
END
-- !query analysis
LocalRelation [col1#x]


-- !query
BEGIN
  outer_lbl: BEGIN
    DECLARE x, y INT;
    DECLARE cur CURSOR FOR SELECT 1 AS val;

    inner_lbl: BEGIN
      DECLARE cur CURSOR FOR SELECT 2 AS val;

      OPEN outer_lbl.cur;  -- Opens outer cursor
      OPEN inner_lbl.cur;  -- Opens inner cursor

      FETCH cur INTO x;

      FETCH outer_lbl.cur INTO y;

      CLOSE inner_lbl.cur;
    END;

    CLOSE outer_lbl.cur;

    VALUES (x, y);
  END;
END
-- !query analysis
LocalRelation [col1#x, col2#x]


-- !query
BEGIN
  lbl: BEGIN
    DECLARE min_val INT DEFAULT 3;
    DECLARE max_val INT DEFAULT 4;
    DECLARE fetched_id INT;
    DECLARE result STRING DEFAULT '';
    DECLARE cur CURSOR FOR SELECT id FROM VALUES(1), (2), (3), (4), (5) AS t(id) WHERE id >= ? AND id <= ?;

    OPEN lbl.cur USING min_val, max_val;

    FETCH lbl.cur INTO fetched_id;
    SET result = result || CAST(fetched_id AS STRING);
    FETCH lbl.cur INTO fetched_id;
    SET result = result || CAST(fetched_id AS STRING);

    CLOSE lbl.cur;
    VALUES (result); -- Should be '34'
  END;
END
-- !query analysis
org.apache.spark.sql.catalyst.analysis.UnresolvedException
{
  "errorClass" : "INTERNAL_ERROR",
  "sqlState" : "XX000",
  "messageParameters" : {
    "message" : "Invalid call to dataType on unresolved object"
  }
}


-- !query
BEGIN
  DECLARE x, y INT;
  DECLARE cur CURSOR FOR SELECT 1 AS a, 2 AS b;
  OPEN cur;
  FETCH cur INTO x, x;  -- Should fail - duplicate variable
END
-- !query analysis
org.apache.spark.sql.AnalysisException
{
  "errorClass" : "DUPLICATE_ASSIGNMENTS",
  "sqlState" : "42701",
  "messageParameters" : {
    "nameList" : "`x`"
  }
}


-- !query
BEGIN
  DECLARE int_var INT;
  DECLARE str_var STRING;
  DECLARE cur CURSOR FOR SELECT 100.7 AS double_val, 42 AS int_val;

  OPEN cur;
  FETCH cur INTO int_var, str_var;  -- double->int cast, int->string cast
  CLOSE cur;

  VALUES (int_var, str_var);  -- Should be (100, '42') with ANSI rounding
END
-- !query analysis
LocalRelation [col1#x, col2#x]


-- !query
BEGIN
  DECLARE x INT;
  DECLARE cur CURSOR FOR SELECT 1, 2, 3;
  OPEN cur;
  FETCH cur INTO x;  -- Should fail - 1 target but 3 columns
END
-- !query analysis
org.apache.spark.sql.AnalysisException
{
  "errorClass" : "ASSIGNMENT_ARITY_MISMATCH",
  "sqlState" : "42802",
  "messageParameters" : {
    "numExpr" : "3",
    "numTarget" : "1"
  }
}


-- !query
BEGIN
  DECLARE x, y, z, w INT;
  DECLARE cur CURSOR FOR SELECT 1, 2;
  OPEN cur;
  FETCH cur INTO x, y, z, w;  -- Should fail - 4 targets but 2 columns
END
-- !query analysis
org.apache.spark.sql.AnalysisException
{
  "errorClass" : "ASSIGNMENT_ARITY_MISMATCH",
  "sqlState" : "42802",
  "messageParameters" : {
    "numExpr" : "2",
    "numTarget" : "4"
  }
}


-- !query
BEGIN
  DECLARE x INT;
  DECLARE cur INSENSITIVE CURSOR FOR SELECT 42 AS val;
  OPEN cur;
  FETCH cur INTO x;
  CLOSE cur;
  VALUES (x); -- Should return 42
END
-- !query analysis
LocalRelation [col1#x]


-- !query
BEGIN
  DECLARE y INT;
  DECLARE cur ASENSITIVE CURSOR FOR SELECT 99 AS val;
  OPEN cur;
  FETCH cur INTO y;
  CLOSE cur;
  VALUES (y); -- Should return 99
END
-- !query analysis
LocalRelation [col1#x]


-- !query
BEGIN
  DECLARE z INT;
  DECLARE cur CURSOR FOR SELECT 77 AS val FOR READ ONLY;
  OPEN cur;
  FETCH cur INTO z;
  CLOSE cur;
  VALUES (z); -- Should return 77
END
-- !query analysis
LocalRelation [col1#x]


-- !query
BEGIN
  DECLARE w INT;
  DECLARE cur INSENSITIVE CURSOR FOR SELECT 123 AS val FOR READ ONLY;
  OPEN cur;
  FETCH cur INTO w;
  CLOSE cur;
  VALUES (w); -- Should return 123
END
-- !query analysis
LocalRelation [col1#x]


-- !query
BEGIN
  DECLARE a INT;
  DECLARE cur CURSOR FOR SELECT 55 AS val;
  OPEN cur;
  FETCH NEXT cur INTO a;
  CLOSE cur;
  VALUES (a); -- Should return 55
END
-- !query analysis
LocalRelation [col1#x]


-- !query
BEGIN
  DECLARE b INT;
  DECLARE cur CURSOR FOR SELECT 66 AS val;
  OPEN cur;
  FETCH FROM cur INTO b;
  CLOSE cur;
  VALUES (b); -- Should return 66
END
-- !query analysis
LocalRelation [col1#x]


-- !query
BEGIN
  DECLARE c INT;
  DECLARE cur CURSOR FOR SELECT 88 AS val;
  OPEN cur;
  FETCH NEXT FROM cur INTO c;
  CLOSE cur;
  VALUES (c); -- Should return 88
END
-- !query analysis
LocalRelation [col1#x]


-- !query
BEGIN
  DECLARE person_record STRUCT<name STRING, age INT>;
  DECLARE cur CURSOR FOR SELECT 'Alice' AS name, 30 AS age;
  OPEN cur;
  FETCH cur INTO person_record;
  CLOSE cur;
  VALUES (person_record.name, person_record.age); -- Should return 'Alice', 30
END
-- !query analysis
LocalRelation [col1#x, col2#x]


-- !query
BEGIN
  DECLARE record_var STRUCT<id INT, value STRING>;
  DECLARE cur CURSOR FOR SELECT 42.7 AS id, 100 AS value;
  OPEN cur;
  FETCH cur INTO record_var;
  CLOSE cur;
  VALUES (record_var.id, record_var.value); -- Should return 42, '100' (with casting)
END
-- !query analysis
LocalRelation [col1#x, col2#x]


-- !query
BEGIN
  DECLARE record_var STRUCT<a INT, b INT>;
  DECLARE cur CURSOR FOR SELECT 1, 2, 3;
  OPEN cur;
  FETCH cur INTO record_var;  -- Should fail - 2 struct fields but 3 cursor columns
END
-- !query analysis
org.apache.spark.sql.AnalysisException
{
  "errorClass" : "ASSIGNMENT_ARITY_MISMATCH",
  "sqlState" : "42802",
  "messageParameters" : {
    "numExpr" : "3",
    "numTarget" : "2"
  }
}


-- !query
BEGIN
  DECLARE x INT;
  DECLARE cur CURSOR FOR SELECT 1, 2;
  OPEN cur;
  FETCH cur INTO x;  -- Should fail - single non-struct variable but 2 columns
END
-- !query analysis
org.apache.spark.sql.AnalysisException
{
  "errorClass" : "ASSIGNMENT_ARITY_MISMATCH",
  "sqlState" : "42802",
  "messageParameters" : {
    "numExpr" : "2",
    "numTarget" : "1"
  }
}


-- !query
BEGIN
  DECLARE complex_record STRUCT<id BIGINT, name STRING, value DOUBLE>;
  DECLARE cur CURSOR FOR SELECT 100 AS id, 'test' AS name, 99.5 AS value;
  OPEN cur;
  FETCH cur INTO complex_record;
  CLOSE cur;
  VALUES (complex_record); -- Should return struct(100, 'test', 99.5)
END
-- !query analysis
LocalRelation [col1#x]


-- !query
SET VAR session_x = 0;
SET VAR session_y = '';
BEGIN
  DECLARE cur CURSOR FOR SELECT 42 AS num, 'hello' AS text;
  OPEN cur;
  FETCH cur INTO session_x, session_y;
  CLOSE cur;
END;
SELECT session_x, session_y;  -- Should return 42, 'hello'
-- !query analysis
org.apache.spark.sql.catalyst.parser.ParseException
{
  "errorClass" : "INVALID_SET_SYNTAX",
  "sqlState" : "42000",
  "queryContext" : [ {
    "objectType" : "",
    "objectName" : "",
    "startIndex" : 1,
    "stopIndex" : 209,
    "fragment" : "SET VAR session_x = 0;\nSET VAR session_y = '';\nBEGIN\n  DECLARE cur CURSOR FOR SELECT 42 AS num, 'hello' AS text;\n  OPEN cur;\n  FETCH cur INTO session_x, session_y;\n  CLOSE cur;\nEND;\nSELECT session_x, session_y"
  } ]
}


-- !query
SET VAR session_var = 0;
BEGIN
  DECLARE local_var STRING;
  DECLARE cur CURSOR FOR SELECT 100 AS a, 'world' AS b;
  OPEN cur;
  FETCH cur INTO session_var, local_var;
  CLOSE cur;
  VALUES (session_var, local_var);  -- Should return 100, 'world'
END
-- !query analysis
org.apache.spark.sql.catalyst.parser.ParseException
{
  "errorClass" : "INVALID_SET_SYNTAX",
  "sqlState" : "42000",
  "queryContext" : [ {
    "objectType" : "",
    "objectName" : "",
    "startIndex" : 1,
    "stopIndex" : 250,
    "fragment" : "SET VAR session_var = 0;\nBEGIN\n  DECLARE local_var STRING;\n  DECLARE cur CURSOR FOR SELECT 100 AS a, 'world' AS b;\n  OPEN cur;\n  FETCH cur INTO session_var, local_var;\n  CLOSE cur;\n  VALUES (session_var, local_var);  -- Should return 100, 'world'\nEND"
  } ]
}


-- !query
SET VAR session_int = 0;
SET VAR session_str = '';
BEGIN
  DECLARE cur CURSOR FOR SELECT 99.9 AS double_val, 42 AS int_val;
  OPEN cur;
  FETCH cur INTO session_int, session_str;  -- double->int cast, int->string cast
  CLOSE cur;
END;
SELECT session_int, session_str;  -- Should return 99, '42' (with ANSI rounding)
-- !query analysis
org.apache.spark.sql.catalyst.parser.ParseException
{
  "errorClass" : "INVALID_SET_SYNTAX",
  "sqlState" : "42000",
  "queryContext" : [ {
    "objectType" : "",
    "objectName" : "",
    "startIndex" : 1,
    "stopIndex" : 267,
    "fragment" : "SET VAR session_int = 0;\nSET VAR session_str = '';\nBEGIN\n  DECLARE cur CURSOR FOR SELECT 99.9 AS double_val, 42 AS int_val;\n  OPEN cur;\n  FETCH cur INTO session_int, session_str;  -- double->int cast, int->string cast\n  CLOSE cur;\nEND;\nSELECT session_int, session_str"
  } ]
}


-- !query
SET VAR session_dup = 0;
BEGIN
  DECLARE cur CURSOR FOR SELECT 1, 2;
  OPEN cur;
  FETCH cur INTO session_dup, session_dup;  -- Should fail - duplicate session variable
END
-- !query analysis
org.apache.spark.sql.catalyst.parser.ParseException
{
  "errorClass" : "INVALID_SET_SYNTAX",
  "sqlState" : "42000",
  "queryContext" : [ {
    "objectType" : "",
    "objectName" : "",
    "startIndex" : 1,
    "stopIndex" : 172,
    "fragment" : "SET VAR session_dup = 0;\nBEGIN\n  DECLARE cur CURSOR FOR SELECT 1, 2;\n  OPEN cur;\n  FETCH cur INTO session_dup, session_dup;  -- Should fail - duplicate session variable\nEND"
  } ]
}


-- !query
SET VAR dup_var = 0;
BEGIN
  DECLARE dup_var INT;
  DECLARE cur CURSOR FOR SELECT 1, 2;
  OPEN cur;
  FETCH cur INTO dup_var, dup_var;  -- Should fail - duplicate variable name
END
-- !query analysis
org.apache.spark.sql.catalyst.parser.ParseException
{
  "errorClass" : "INVALID_SET_SYNTAX",
  "sqlState" : "42000",
  "queryContext" : [ {
    "objectType" : "",
    "objectName" : "",
    "startIndex" : 1,
    "stopIndex" : 180,
    "fragment" : "SET VAR dup_var = 0;\nBEGIN\n  DECLARE dup_var INT;\n  DECLARE cur CURSOR FOR SELECT 1, 2;\n  OPEN cur;\n  FETCH cur INTO dup_var, dup_var;  -- Should fail - duplicate variable name\nEND"
  } ]
}


-- !query
BEGIN
  outer: BEGIN
    DECLARE x INT;
    DECLARE cur CURSOR FOR SELECT 42 AS val;

    inner: BEGIN
      OPEN cur;  -- Open in inner scope
      FETCH cur INTO x;
    END;

    FETCH cur INTO x;  -- This should succeed
    VALUES (x);  -- Should return 42

  END;
END
-- !query analysis
org.apache.spark.sql.catalyst.parser.ParseException
{
  "errorClass" : "PARSE_SYNTAX_ERROR",
  "sqlState" : "42601",
  "messageParameters" : {
    "error" : "'inner'",
    "hint" : ""
  }
}


-- !query
BEGIN
  DECLARE y INT;

  scope1: BEGIN
    DECLARE cur CURSOR FOR SELECT 99 AS val;
    OPEN cur;
    FETCH cur INTO y;
  END;  -- cursor is implicitly closed here (exiting DECLARE scope)

  FETCH cur INTO y;
END
-- !query analysis
org.apache.spark.sql.AnalysisException
{
  "errorClass" : "CURSOR_NOT_FOUND",
  "sqlState" : "42883",
  "messageParameters" : {
    "cursorName" : "cur"
  }
}


-- !query
BEGIN
  DECLARE x INT DEFAULT 0;
  DECLARE result STRING DEFAULT '';
  DECLARE cur CURSOR FOR SELECT 42 AS val;

  OPEN cur;
  FETCH cur INTO x;  -- OK: gets value 42
  SET result = result || CAST(x AS STRING);

  FETCH cur INTO x;  -- Continues execution (no handler needed for completion conditions)

  SET result = result || '-after-fetch';
  CLOSE cur;

  VALUES (result);  -- Should return '42-after-fetch', proving execution continued
END
-- !query analysis
LocalRelation [col1#x]


-- !query
BEGIN
  DECLARE x INT;
  SET x = 1 / 0;  -- Should throw DIVIDE_BY_ZERO (SQLSTATE 22012), not continue
  VALUES ('This should not be reached');
END
-- !query analysis
org.apache.spark.SparkArithmeticException
{
  "errorClass" : "DIVIDE_BY_ZERO",
  "sqlState" : "22012",
  "messageParameters" : {
    "config" : "\"spark.sql.ansi.enabled\""
  },
  "queryContext" : [ {
    "objectType" : "",
    "objectName" : "",
    "startIndex" : 34,
    "stopIndex" : 38,
    "fragment" : "1 / 0"
  } ]
}


-- !query
BEGIN
  DECLARE x INT;
  DECLARE IDENTIFIER('my_cursor') CURSOR FOR SELECT 99 AS val;
  OPEN IDENTIFIER('my_cursor');
  FETCH IDENTIFIER('my_cursor') INTO x;
  CLOSE IDENTIFIER('my_cursor');
  VALUES (x); -- Should return 99
END
-- !query analysis
org.apache.spark.sql.AnalysisException
{
  "errorClass" : "CURSOR_NOT_FOUND",
  "sqlState" : "42883",
  "messageParameters" : {
    "cursorName" : "IDENTIFIER('my_cursor')"
  }
}


-- !query
BEGIN
  DECLARE result INT;
  DECLARE IDENTIFIER('MixedCase') CURSOR FOR SELECT 42;
  OPEN IDENTIFIER('MixedCase');
  FETCH IDENTIFIER('MixedCase') INTO result;
  CLOSE IDENTIFIER('MixedCase');
  VALUES (result); -- Should return 42
END
-- !query analysis
org.apache.spark.sql.AnalysisException
{
  "errorClass" : "CURSOR_NOT_FOUND",
  "sqlState" : "42883",
  "messageParameters" : {
    "cursorName" : "IDENTIFIER('MixedCase')"
  }
}


-- !query
BEGIN
  DECLARE n INT;
  DECLARE sum_result INT DEFAULT 0;
  DECLARE done BOOLEAN DEFAULT false;
  DECLARE cur CURSOR FOR
    WITH RECURSIVE numbers(n) AS (
      SELECT 1 AS n
      UNION ALL
      SELECT n + 1 FROM numbers WHERE n < 5
    )
    SELECT n FROM numbers;
  DECLARE CONTINUE HANDLER FOR NOT FOUND SET done = true;

  OPEN cur;
  REPEAT
    FETCH cur INTO n;
    IF NOT done THEN
      SET sum_result = sum_result + n;
    END IF;
  UNTIL done END REPEAT;
  CLOSE cur;

  VALUES (sum_result); -- Should return 15 (1+2+3+4+5)
END
-- !query analysis
org.apache.spark.sql.catalyst.analysis.UnresolvedException
{
  "errorClass" : "INTERNAL_ERROR",
  "sqlState" : "XX000",
  "messageParameters" : {
    "message" : "Invalid call to dataType on unresolved object"
  }
}


-- !query
CREATE TEMPORARY VIEW customers AS SELECT 1 AS id, 'Alice' AS name
UNION ALL SELECT 2, 'Bob'
UNION ALL SELECT 3, 'Charlie';

CREATE TEMPORARY VIEW orders AS SELECT 1 AS customer_id, 100 AS amount
UNION ALL SELECT 1, 200
UNION ALL SELECT 2, 150;

BEGIN
  DECLARE customer_name STRING;
  DECLARE total_amount INT;
  DECLARE result STRING DEFAULT '';
  DECLARE done BOOLEAN DEFAULT false;
  DECLARE cur CURSOR FOR
    SELECT c.name, COALESCE(SUM(o.amount), 0) AS total
    FROM customers c
    LEFT JOIN orders o ON c.id = o.customer_id
    WHERE c.id IN (SELECT customer_id FROM orders WHERE amount > 50)
    GROUP BY c.name
    ORDER BY total DESC;
  DECLARE CONTINUE HANDLER FOR NOT FOUND SET done = true;

  OPEN cur;
  REPEAT
    FETCH cur INTO customer_name, total_amount;
    IF NOT done THEN
      SET result = result || customer_name || ':' || CAST(total_amount AS STRING) || ',';
    END IF;
  UNTIL done END REPEAT;
  CLOSE cur;

  VALUES (result); -- Should return 'Alice:300,Bob:150,'
END;

DROP VIEW customers;
DROP VIEW orders
-- !query analysis
org.apache.spark.sql.catalyst.parser.ParseException
{
  "errorClass" : "PARSE_SYNTAX_ERROR",
  "sqlState" : "42601",
  "messageParameters" : {
    "error" : "'CREATE'",
    "hint" : ": extra input 'CREATE'"
  }
}


-- !query
BEGIN
  DECLARE result STRING DEFAULT '';
  DECLARE i INT;
  DECLARE done1 BOOLEAN DEFAULT false;
  DECLARE cur1 CURSOR FOR SELECT id FROM VALUES(1), (2) AS t(id);
  DECLARE CONTINUE HANDLER FOR NOT FOUND SET done1 = true;

  OPEN cur1;
  REPEAT
    FETCH cur1 INTO i;
    IF NOT done1 THEN
      DECLARE j INT;
      DECLARE done2 BOOLEAN DEFAULT false;
      DECLARE cur2 CURSOR FOR SELECT id FROM VALUES(10), (20) AS t(id);
      DECLARE CONTINUE HANDLER FOR NOT FOUND SET done2 = true;

      OPEN cur2;
      REPEAT
        FETCH cur2 INTO j;
        IF NOT done2 THEN
          DECLARE k INT;
          DECLARE done3 BOOLEAN DEFAULT false;
          DECLARE cur3 CURSOR FOR SELECT id FROM VALUES(100) AS t(id);
          DECLARE CONTINUE HANDLER FOR NOT FOUND SET done3 = true;

          OPEN cur3;
          REPEAT
            FETCH cur3 INTO k;
            IF NOT done3 THEN
              SET result = result || CAST(i AS STRING) || '-' ||
                          CAST(j AS STRING) || '-' ||
                          CAST(k AS STRING) || ',';
            END IF;
          UNTIL done3 END REPEAT;
          CLOSE cur3;
        END IF;
        SET done3 = false; -- Reset for next iteration
      UNTIL done2 END REPEAT;
      CLOSE cur2;
    END IF;
    SET done2 = false; -- Reset for next iteration
  UNTIL done1 END REPEAT;
  CLOSE cur1;

  VALUES (result); -- Should return '1-10-100,1-20-100,2-10-100,2-20-100,'
END
-- !query analysis
org.apache.spark.sql.exceptions.SqlScriptingException
{
  "errorClass" : "INVALID_VARIABLE_DECLARATION.NOT_ALLOWED_IN_SCOPE",
  "sqlState" : "42K0M",
  "messageParameters" : {
    "varName" : "`j`"
  }
}
