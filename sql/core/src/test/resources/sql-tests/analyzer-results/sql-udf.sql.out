-- Automatically generated by SQLQueryTestSuite
-- !query
CREATE FUNCTION foo1a0() RETURNS INT RETURN 1
-- !query analysis
org.apache.spark.sql.catalyst.analysis.FunctionAlreadyExistsException
{
  "errorClass" : "ROUTINE_ALREADY_EXISTS",
  "sqlState" : "42723",
  "messageParameters" : {
    "existingRoutineType" : "routine",
    "newRoutineType" : "routine",
    "routineName" : "`default`.`foo1a0`"
  }
}


-- !query
SELECT foo1a0()
-- !query analysis
Project [spark_catalog.default.foo1a0() AS spark_catalog.default.foo1a0()#x]
+- Project
   +- OneRowRelation


-- !query
SELECT foo1a0(1)
-- !query analysis
org.apache.spark.sql.AnalysisException
{
  "errorClass" : "WRONG_NUM_ARGS.WITHOUT_SUGGESTION",
  "sqlState" : "42605",
  "messageParameters" : {
    "actualNum" : "1",
    "docroot" : "https://spark.apache.org/docs/latest",
    "expectedNum" : "0",
    "functionName" : "`spark_catalog`.`default`.`foo1a0`"
  },
  "queryContext" : [ {
    "objectType" : "",
    "objectName" : "",
    "startIndex" : 8,
    "stopIndex" : 16,
    "fragment" : "foo1a0(1)"
  } ]
}


-- !query
CREATE FUNCTION foo1a1(a INT) RETURNS INT RETURN 1
-- !query analysis
org.apache.spark.sql.catalyst.analysis.FunctionAlreadyExistsException
{
  "errorClass" : "ROUTINE_ALREADY_EXISTS",
  "sqlState" : "42723",
  "messageParameters" : {
    "existingRoutineType" : "routine",
    "newRoutineType" : "routine",
    "routineName" : "`default`.`foo1a1`"
  }
}


-- !query
SELECT foo1a1(1)
-- !query analysis
Project [spark_catalog.default.foo1a1(a#x) AS spark_catalog.default.foo1a1(1)#x]
+- Project [cast(1 as int) AS a#x]
   +- OneRowRelation


-- !query
SELECT foo1a1(1, 2)
-- !query analysis
org.apache.spark.sql.AnalysisException
{
  "errorClass" : "WRONG_NUM_ARGS.WITHOUT_SUGGESTION",
  "sqlState" : "42605",
  "messageParameters" : {
    "actualNum" : "2",
    "docroot" : "https://spark.apache.org/docs/latest",
    "expectedNum" : "1",
    "functionName" : "`spark_catalog`.`default`.`foo1a1`"
  },
  "queryContext" : [ {
    "objectType" : "",
    "objectName" : "",
    "startIndex" : 8,
    "stopIndex" : 19,
    "fragment" : "foo1a1(1, 2)"
  } ]
}


-- !query
CREATE FUNCTION foo1a2(a INT, b INT, c INT, d INT) RETURNS INT RETURN 1
-- !query analysis
org.apache.spark.sql.catalyst.analysis.FunctionAlreadyExistsException
{
  "errorClass" : "ROUTINE_ALREADY_EXISTS",
  "sqlState" : "42723",
  "messageParameters" : {
    "existingRoutineType" : "routine",
    "newRoutineType" : "routine",
    "routineName" : "`default`.`foo1a2`"
  }
}


-- !query
SELECT foo1a2(1, 2, 3, 4)
-- !query analysis
Project [spark_catalog.default.foo1a2(a#x, b#x, c#x, d#x) AS spark_catalog.default.foo1a2(1, 2, 3, 4)#x]
+- Project [cast(1 as int) AS a#x, cast(2 as int) AS b#x, cast(3 as int) AS c#x, cast(4 as int) AS d#x]
   +- OneRowRelation


-- !query
CREATE FUNCTION foo1b0() RETURNS TABLE (c1 INT) RETURN SELECT 1
-- !query analysis
org.apache.spark.sql.catalyst.analysis.FunctionAlreadyExistsException
{
  "errorClass" : "ROUTINE_ALREADY_EXISTS",
  "sqlState" : "42723",
  "messageParameters" : {
    "existingRoutineType" : "routine",
    "newRoutineType" : "routine",
    "routineName" : "`default`.`foo1b0`"
  }
}


-- !query
SELECT * FROM foo1b0()
-- !query analysis
Project [c1#x]
+- SQLFunctionNode spark_catalog.default.foo1b0
   +- SubqueryAlias foo1b0
      +- Project [cast(1#x as int) AS c1#x]
         +- Project [1 AS 1#x]
            +- OneRowRelation


-- !query
CREATE FUNCTION foo1b1(a INT) RETURNS TABLE (c1 INT) RETURN SELECT 1
-- !query analysis
org.apache.spark.sql.catalyst.analysis.FunctionAlreadyExistsException
{
  "errorClass" : "ROUTINE_ALREADY_EXISTS",
  "sqlState" : "42723",
  "messageParameters" : {
    "existingRoutineType" : "routine",
    "newRoutineType" : "routine",
    "routineName" : "`default`.`foo1b1`"
  }
}


-- !query
SELECT * FROM foo1b1(1)
-- !query analysis
Project [c1#x]
+- SQLFunctionNode spark_catalog.default.foo1b1
   +- SubqueryAlias foo1b1
      +- Project [cast(1#x as int) AS c1#x]
         +- Project [1 AS 1#x]
            +- OneRowRelation


-- !query
CREATE FUNCTION foo1b2(a INT, b INT, c INT, d INT) RETURNS TABLE(c1 INT) RETURN SELECT 1
-- !query analysis
org.apache.spark.sql.catalyst.analysis.FunctionAlreadyExistsException
{
  "errorClass" : "ROUTINE_ALREADY_EXISTS",
  "sqlState" : "42723",
  "messageParameters" : {
    "existingRoutineType" : "routine",
    "newRoutineType" : "routine",
    "routineName" : "`default`.`foo1b2`"
  }
}


-- !query
SELECT * FROM foo1b2(1, 2, 3, 4)
-- !query analysis
Project [c1#x]
+- SQLFunctionNode spark_catalog.default.foo1b2
   +- SubqueryAlias foo1b2
      +- Project [cast(1#x as int) AS c1#x]
         +- Project [1 AS 1#x]
            +- OneRowRelation


-- !query
CREATE FUNCTION foo1c1(duplicate INT, DUPLICATE INT) RETURNS INT RETURN 1
-- !query analysis
org.apache.spark.sql.AnalysisException
{
  "errorClass" : "DUPLICATE_ROUTINE_PARAMETER_NAMES",
  "sqlState" : "42734",
  "messageParameters" : {
    "names" : "`duplicate`",
    "routineName" : "foo1c1"
  }
}


-- !query
CREATE FUNCTION foo1c2(a INT, b INT, thisisaduplicate INT, c INT, d INT, e INT, f INT, thisIsaDuplicate INT, g INT)
    RETURNS TABLE (a INT) RETURN SELECT 1
-- !query analysis
org.apache.spark.sql.AnalysisException
{
  "errorClass" : "DUPLICATE_ROUTINE_PARAMETER_NAMES",
  "sqlState" : "42734",
  "messageParameters" : {
    "names" : "`thisisaduplicate`",
    "routineName" : "foo1c2"
  }
}


-- !query
CREATE OR REPLACE FUNCTION foo1d1(a INT DEFAULT NULL) RETURNS INT RETURN a
-- !query analysis
org.apache.spark.sql.catalyst.parser.ParseException
{
  "errorClass" : "PARSE_SYNTAX_ERROR",
  "sqlState" : "42601",
  "messageParameters" : {
    "error" : "'DEFAULT'",
    "hint" : ""
  },
  "queryContext" : [ {
    "objectType" : "",
    "objectName" : "",
    "startIndex" : 1,
    "stopIndex" : 74,
    "fragment" : "CREATE OR REPLACE FUNCTION foo1d1(a INT DEFAULT NULL) RETURNS INT RETURN a"
  } ]
}


-- !query
CREATE OR REPLACE FUNCTION foo1d1(a INT, b INT DEFAULT 7) RETURNS TABLE(a INT, b INT) RETURN SELECT a, b
-- !query analysis
org.apache.spark.sql.catalyst.parser.ParseException
{
  "errorClass" : "PARSE_SYNTAX_ERROR",
  "sqlState" : "42601",
  "messageParameters" : {
    "error" : "'DEFAULT'",
    "hint" : ""
  },
  "queryContext" : [ {
    "objectType" : "",
    "objectName" : "",
    "startIndex" : 1,
    "stopIndex" : 104,
    "fragment" : "CREATE OR REPLACE FUNCTION foo1d1(a INT, b INT DEFAULT 7) RETURNS TABLE(a INT, b INT) RETURN SELECT a, b"
  } ]
}


-- !query
CREATE FUNCTION foo2_1a(a INT) RETURNS INT RETURN a
-- !query analysis
org.apache.spark.sql.catalyst.analysis.FunctionAlreadyExistsException
{
  "errorClass" : "ROUTINE_ALREADY_EXISTS",
  "sqlState" : "42723",
  "messageParameters" : {
    "existingRoutineType" : "routine",
    "newRoutineType" : "routine",
    "routineName" : "`default`.`foo2_1a`"
  }
}


-- !query
SELECT foo2_1a(5)
-- !query analysis
Project [spark_catalog.default.foo2_1a(a#x) AS spark_catalog.default.foo2_1a(5)#x]
+- Project [cast(5 as int) AS a#x]
   +- OneRowRelation


-- !query
CREATE FUNCTION foo2_1b(a INT, b INT) RETURNS INT RETURN a + b
-- !query analysis
org.apache.spark.sql.catalyst.analysis.FunctionAlreadyExistsException
{
  "errorClass" : "ROUTINE_ALREADY_EXISTS",
  "sqlState" : "42723",
  "messageParameters" : {
    "existingRoutineType" : "routine",
    "newRoutineType" : "routine",
    "routineName" : "`default`.`foo2_1b`"
  }
}


-- !query
SELECT foo2_1b(5, 6)
-- !query analysis
Project [spark_catalog.default.foo2_1b(a#x, b#x) AS spark_catalog.default.foo2_1b(5, 6)#x]
+- Project [cast(5 as int) AS a#x, cast(6 as int) AS b#x]
   +- OneRowRelation


-- !query
CREATE FUNCTION foo2_1c(a INT, b INT) RETURNS INT RETURN 10 * (a + b) + 100 * (a -b)
-- !query analysis
org.apache.spark.sql.catalyst.analysis.FunctionAlreadyExistsException
{
  "errorClass" : "ROUTINE_ALREADY_EXISTS",
  "sqlState" : "42723",
  "messageParameters" : {
    "existingRoutineType" : "routine",
    "newRoutineType" : "routine",
    "routineName" : "`default`.`foo2_1c`"
  }
}


-- !query
SELECT foo2_1c(5, 6)
-- !query analysis
Project [spark_catalog.default.foo2_1c(a#x, b#x) AS spark_catalog.default.foo2_1c(5, 6)#x]
+- Project [cast(5 as int) AS a#x, cast(6 as int) AS b#x]
   +- OneRowRelation


-- !query
CREATE FUNCTION foo2_1d(a INT, b INT) RETURNS INT RETURN ABS(a) - LENGTH(CAST(b AS VARCHAR(10)))
-- !query analysis
org.apache.spark.sql.catalyst.analysis.FunctionAlreadyExistsException
{
  "errorClass" : "ROUTINE_ALREADY_EXISTS",
  "sqlState" : "42723",
  "messageParameters" : {
    "existingRoutineType" : "routine",
    "newRoutineType" : "routine",
    "routineName" : "`default`.`foo2_1d`"
  }
}


-- !query
SELECT foo2_1d(-5, 6)
-- !query analysis
Project [spark_catalog.default.foo2_1d(a#x, b#x) AS spark_catalog.default.foo2_1d(-5, 6)#x]
+- Project [cast(-5 as int) AS a#x, cast(6 as int) AS b#x]
   +- OneRowRelation


-- !query
CREATE FUNCTION foo2_2a(a INT) RETURNS INT RETURN SELECT a
-- !query analysis
org.apache.spark.sql.catalyst.analysis.FunctionAlreadyExistsException
{
  "errorClass" : "ROUTINE_ALREADY_EXISTS",
  "sqlState" : "42723",
  "messageParameters" : {
    "existingRoutineType" : "routine",
    "newRoutineType" : "routine",
    "routineName" : "`default`.`foo2_2a`"
  }
}


-- !query
SELECT foo2_2a(5)
-- !query analysis
Project [spark_catalog.default.foo2_2a(a#x) AS spark_catalog.default.foo2_2a(5)#x]
+- Project [cast(5 as int) AS a#x]
   +- OneRowRelation


-- !query
CREATE FUNCTION foo2_2b(a INT) RETURNS INT RETURN 1 + (SELECT a)
-- !query analysis
org.apache.spark.sql.catalyst.analysis.FunctionAlreadyExistsException
{
  "errorClass" : "ROUTINE_ALREADY_EXISTS",
  "sqlState" : "42723",
  "messageParameters" : {
    "existingRoutineType" : "routine",
    "newRoutineType" : "routine",
    "routineName" : "`default`.`foo2_2b`"
  }
}


-- !query
SELECT foo2_2b(5)
-- !query analysis
Project [spark_catalog.default.foo2_2b(a#x) AS spark_catalog.default.foo2_2b(5)#x]
:  +- Project [outer(a#x)]
:     +- OneRowRelation
+- Project [cast(5 as int) AS a#x]
   +- OneRowRelation


-- !query
CREATE FUNCTION foo2_2c(a INT) RETURNS INT RETURN 1 + (SELECT (SELECT a))
-- !query analysis
org.apache.spark.sql.catalyst.ExtendedAnalysisException
{
  "errorClass" : "UNRESOLVED_COLUMN.WITHOUT_SUGGESTION",
  "sqlState" : "42703",
  "messageParameters" : {
    "objectName" : "`a`"
  },
  "queryContext" : [ {
    "objectType" : "",
    "objectName" : "",
    "startIndex" : 21,
    "stopIndex" : 21,
    "fragment" : "a"
  } ]
}


-- !query
CREATE FUNCTION foo2_2d(a INT) RETURNS INT RETURN 1 + (SELECT (SELECT (SELECT (SELECT a))))
-- !query analysis
org.apache.spark.sql.catalyst.ExtendedAnalysisException
{
  "errorClass" : "UNRESOLVED_COLUMN.WITHOUT_SUGGESTION",
  "sqlState" : "42703",
  "messageParameters" : {
    "objectName" : "`a`"
  },
  "queryContext" : [ {
    "objectType" : "",
    "objectName" : "",
    "startIndex" : 37,
    "stopIndex" : 37,
    "fragment" : "a"
  } ]
}


-- !query
CREATE FUNCTION foo2_2e(a INT) RETURNS INT RETURN
SELECT a FROM (VALUES 1) AS V(c1) WHERE c1 = 2
UNION ALL
SELECT a + 1 FROM (VALUES 1) AS V(c1)
-- !query analysis
org.apache.spark.sql.catalyst.analysis.FunctionAlreadyExistsException
{
  "errorClass" : "ROUTINE_ALREADY_EXISTS",
  "sqlState" : "42723",
  "messageParameters" : {
    "existingRoutineType" : "routine",
    "newRoutineType" : "routine",
    "routineName" : "`default`.`foo2_2e`"
  }
}


-- !query
CREATE FUNCTION foo2_2f(a INT) RETURNS INT RETURN
SELECT a FROM (VALUES 1) AS V(c1)
EXCEPT
SELECT a + 1 FROM (VALUES 1) AS V(a)
-- !query analysis
org.apache.spark.sql.catalyst.analysis.FunctionAlreadyExistsException
{
  "errorClass" : "ROUTINE_ALREADY_EXISTS",
  "sqlState" : "42723",
  "messageParameters" : {
    "existingRoutineType" : "routine",
    "newRoutineType" : "routine",
    "routineName" : "`default`.`foo2_2f`"
  }
}


-- !query
CREATE FUNCTION foo2_2g(a INT) RETURNS INT RETURN
SELECT a FROM (VALUES 1) AS V(c1)
INTERSECT
SELECT a FROM (VALUES 1) AS V(a)
-- !query analysis
org.apache.spark.sql.catalyst.analysis.FunctionAlreadyExistsException
{
  "errorClass" : "ROUTINE_ALREADY_EXISTS",
  "sqlState" : "42723",
  "messageParameters" : {
    "existingRoutineType" : "routine",
    "newRoutineType" : "routine",
    "routineName" : "`default`.`foo2_2g`"
  }
}


-- !query
DROP TABLE IF EXISTS t1
-- !query analysis
DropTable true, false
+- ResolvedIdentifier V2SessionCatalog(spark_catalog), default.t1


-- !query
DROP TABLE IF EXISTS t2
-- !query analysis
DropTable true, false
+- ResolvedIdentifier V2SessionCatalog(spark_catalog), default.t2


-- !query
DROP TABLE IF EXISTS ts
-- !query analysis
DropTable true, false
+- ResolvedIdentifier V2SessionCatalog(spark_catalog), default.ts


-- !query
DROP TABLE IF EXISTS tm
-- !query analysis
DropTable true, false
+- ResolvedIdentifier V2SessionCatalog(spark_catalog), default.tm


-- !query
DROP TABLE IF EXISTS ta
-- !query analysis
DropTable true, false
+- ResolvedIdentifier V2SessionCatalog(spark_catalog), default.ta


-- !query
DROP TABLE IF EXISTS V1
-- !query analysis
DropTable true, false
+- ResolvedIdentifier V2SessionCatalog(spark_catalog), default.V1


-- !query
DROP TABLE IF EXISTS V2
-- !query analysis
DropTable true, false
+- ResolvedIdentifier V2SessionCatalog(spark_catalog), default.V2


-- !query
DROP VIEW IF EXISTS t1
-- !query analysis
DropTableCommand `spark_catalog`.`default`.`t1`, true, true, false


-- !query
DROP VIEW IF EXISTS t2
-- !query analysis
DropTableCommand `spark_catalog`.`default`.`t2`, true, true, false


-- !query
DROP VIEW IF EXISTS ts
-- !query analysis
DropTableCommand `spark_catalog`.`default`.`ts`, true, true, false


-- !query
DROP VIEW IF EXISTS tm
-- !query analysis
DropTableCommand `spark_catalog`.`default`.`tm`, true, true, false


-- !query
DROP VIEW IF EXISTS ta
-- !query analysis
DropTableCommand `spark_catalog`.`default`.`ta`, true, true, false


-- !query
DROP VIEW IF EXISTS V1
-- !query analysis
DropTableCommand `spark_catalog`.`default`.`V1`, true, true, false


-- !query
DROP VIEW IF EXISTS V2
-- !query analysis
DropTableCommand `spark_catalog`.`default`.`V2`, true, true, false


-- !query
CREATE FUNCTION foo2_3(a INT, b INT) RETURNS INT RETURN a + b
-- !query analysis
org.apache.spark.sql.catalyst.analysis.FunctionAlreadyExistsException
{
  "errorClass" : "ROUTINE_ALREADY_EXISTS",
  "sqlState" : "42723",
  "messageParameters" : {
    "existingRoutineType" : "routine",
    "newRoutineType" : "routine",
    "routineName" : "`default`.`foo2_3`"
  }
}


-- !query
CREATE VIEW V1(c1, c2) AS VALUES (1, 2), (3, 4), (5, 6)
-- !query analysis
CreateViewCommand `spark_catalog`.`default`.`V1`, [(c1,None), (c2,None)], VALUES (1, 2), (3, 4), (5, 6), false, false, PersistedView, COMPENSATION, true
   +- LocalRelation [col1#x, col2#x]


-- !query
CREATE VIEW V2(c1, c2) AS VALUES (-1, -2), (-3, -4), (-5, -6)
-- !query analysis
CreateViewCommand `spark_catalog`.`default`.`V2`, [(c1,None), (c2,None)], VALUES (-1, -2), (-3, -4), (-5, -6), false, false, PersistedView, COMPENSATION, true
   +- LocalRelation [col1#x, col2#x]


-- !query
SELECT foo2_3(c1, c2), foo2_3(c2, 1), foo2_3(c1, c2) - foo2_3(c2, c1 - 1) FROM V1 ORDER BY 1, 2, 3
-- !query analysis
Sort [spark_catalog.default.foo2_3(c1, c2)#x ASC NULLS FIRST, spark_catalog.default.foo2_3(c2, 1)#x ASC NULLS FIRST, (spark_catalog.default.foo2_3(c1, c2) - spark_catalog.default.foo2_3(c2, (c1 - 1)))#x ASC NULLS FIRST], true
+- Project [spark_catalog.default.foo2_3(a#x, b#x) AS spark_catalog.default.foo2_3(c1, c2)#x, spark_catalog.default.foo2_3(a#x, b#x) AS spark_catalog.default.foo2_3(c2, 1)#x, (spark_catalog.default.foo2_3(a#x, b#x) - spark_catalog.default.foo2_3(a#x, b#x)) AS (spark_catalog.default.foo2_3(c1, c2) - spark_catalog.default.foo2_3(c2, (c1 - 1)))#x]
   +- Project [c1#x, c2#x, cast(c1#x as int) AS a#x, cast(c2#x as int) AS b#x, cast(c2#x as int) AS a#x, cast(1 as int) AS b#x, cast(c1#x as int) AS a#x, cast(c2#x as int) AS b#x, cast(c2#x as int) AS a#x, cast((c1#x - 1) as int) AS b#x]
      +- SubqueryAlias spark_catalog.default.v1
         +- View (`spark_catalog`.`default`.`v1`, [c1#x, c2#x])
            +- Project [cast(col1#x as int) AS c1#x, cast(col2#x as int) AS c2#x]
               +- LocalRelation [col1#x, col2#x]


-- !query
SELECT * FROM V1 WHERE foo2_3(c1, 0) = c1 AND foo2_3(c1, c2) < 8
-- !query analysis
Project [c1#x, c2#x]
+- Project [c1#x, c2#x]
   +- Filter ((spark_catalog.default.foo2_3(a#x, b#x) = c1#x) AND (spark_catalog.default.foo2_3(a#x, b#x) < 8))
      +- Project [c1#x, c2#x, cast(c1#x as int) AS a#x, cast(0 as int) AS b#x, cast(c1#x as int) AS a#x, cast(c2#x as int) AS b#x]
         +- SubqueryAlias spark_catalog.default.v1
            +- View (`spark_catalog`.`default`.`v1`, [c1#x, c2#x])
               +- Project [cast(col1#x as int) AS c1#x, cast(col2#x as int) AS c2#x]
                  +- LocalRelation [col1#x, col2#x]


-- !query
SELECT foo2_3(SUM(c1), SUM(c2)), SUM(c1) + SUM(c2), SUM(foo2_3(c1, c2) + foo2_3(c2, c1) - foo2_3(c2, c1))
FROM V1
-- !query analysis
Project [spark_catalog.default.foo2_3(a#x, b#x) AS spark_catalog.default.foo2_3(sum(c1), sum(c2))#x, (sum(c1) + sum(c2))#xL, sum(((spark_catalog.default.foo2_3(c1, c2) + spark_catalog.default.foo2_3(c2, c1)) - spark_catalog.default.foo2_3(c2, c1)))#xL]
+- Project [sum(c1)#xL, sum(c2)#xL, (sum(c1) + sum(c2))#xL, sum(((spark_catalog.default.foo2_3(c1, c2) + spark_catalog.default.foo2_3(c2, c1)) - spark_catalog.default.foo2_3(c2, c1)))#xL, cast(sum(c1)#xL as int) AS a#x, cast(sum(c2)#xL as int) AS b#x]
   +- Aggregate [sum(c1#x) AS sum(c1)#xL, sum(c2#x) AS sum(c2)#xL, (sum(c1#x) + sum(c2#x)) AS (sum(c1) + sum(c2))#xL, sum(((spark_catalog.default.foo2_3(a#x, b#x) + spark_catalog.default.foo2_3(a#x, b#x)) - spark_catalog.default.foo2_3(a#x, b#x))) AS sum(((spark_catalog.default.foo2_3(c1, c2) + spark_catalog.default.foo2_3(c2, c1)) - spark_catalog.default.foo2_3(c2, c1)))#xL]
      +- Project [c1#x, c2#x, cast(c1#x as int) AS a#x, cast(c2#x as int) AS b#x, cast(c2#x as int) AS a#x, cast(c1#x as int) AS b#x, cast(c2#x as int) AS a#x, cast(c1#x as int) AS b#x]
         +- SubqueryAlias spark_catalog.default.v1
            +- View (`spark_catalog`.`default`.`v1`, [c1#x, c2#x])
               +- Project [cast(col1#x as int) AS c1#x, cast(col2#x as int) AS c2#x]
                  +- LocalRelation [col1#x, col2#x]


-- !query
CREATE FUNCTION foo2_4a(a ARRAY<STRING>) RETURNS STRING RETURN
SELECT array_sort(a, (i, j) -> rank[i] - rank[j])[0] FROM (SELECT MAP('a', 1, 'b', 2) rank)
-- !query analysis
org.apache.spark.sql.catalyst.analysis.FunctionAlreadyExistsException
{
  "errorClass" : "ROUTINE_ALREADY_EXISTS",
  "sqlState" : "42723",
  "messageParameters" : {
    "existingRoutineType" : "routine",
    "newRoutineType" : "routine",
    "routineName" : "`default`.`foo2_4a`"
  }
}


-- !query
SELECT foo2_4a(ARRAY('a', 'b'))
-- !query analysis
Project [spark_catalog.default.foo2_4a(a#x) AS spark_catalog.default.foo2_4a(array(a, b))#x]
:  +- Project [array_sort(outer(a#x), lambdafunction((rank#x[lambda i#x] - rank#x[lambda j#x]), lambda i#x, lambda j#x, false), false)[0] AS array_sort(outer(foo2_4a.a), lambdafunction((rank[namedlambdavariable()] - rank[namedlambdavariable()]), namedlambdavariable(), namedlambdavariable()))[0]#x]
:     +- SubqueryAlias __auto_generated_subquery_name
:        +- Project [map(a, 1, b, 2) AS rank#x]
:           +- OneRowRelation
+- Project [cast(array(a, b) as array<string>) AS a#x]
   +- OneRowRelation


-- !query
CREATE FUNCTION foo2_4b(m MAP<STRING, STRING>, k STRING) RETURNS STRING RETURN
SELECT v || ' ' || v FROM (SELECT upper(m[k]) AS v)
-- !query analysis
org.apache.spark.sql.catalyst.analysis.FunctionAlreadyExistsException
{
  "errorClass" : "ROUTINE_ALREADY_EXISTS",
  "sqlState" : "42723",
  "messageParameters" : {
    "existingRoutineType" : "routine",
    "newRoutineType" : "routine",
    "routineName" : "`default`.`foo2_4b`"
  }
}


-- !query
SELECT foo2_4b(map('a', 'hello', 'b', 'world'), 'a')
-- !query analysis
Project [spark_catalog.default.foo2_4b(m#x, k#x) AS spark_catalog.default.foo2_4b(map(a, hello, b, world), a)#x]
:  +- Project [concat(concat(v#x,  ), v#x) AS concat(concat(v,  ), v)#x]
:     +- SubqueryAlias __auto_generated_subquery_name
:        +- Project [upper(outer(m#x)[outer(k#x)]) AS v#x]
:           +- OneRowRelation
+- Project [cast(map(a, hello, b, world) as map<string,string>) AS m#x, cast(a as string) AS k#x]
   +- OneRowRelation


-- !query
DROP VIEW V2
-- !query analysis
DropTableCommand `spark_catalog`.`default`.`V2`, false, true, false


-- !query
DROP VIEW V1
-- !query analysis
DropTableCommand `spark_catalog`.`default`.`V1`, false, true, false


-- !query
CREATE VIEW t1(c1, c2) AS VALUES (0, 1), (0, 2), (1, 2)
-- !query analysis
CreateViewCommand `spark_catalog`.`default`.`t1`, [(c1,None), (c2,None)], VALUES (0, 1), (0, 2), (1, 2), false, false, PersistedView, COMPENSATION, true
   +- LocalRelation [col1#x, col2#x]


-- !query
CREATE VIEW t2(c1, c2) AS VALUES (0, 2), (0, 3)
-- !query analysis
CreateViewCommand `spark_catalog`.`default`.`t2`, [(c1,None), (c2,None)], VALUES (0, 2), (0, 3), false, false, PersistedView, COMPENSATION, true
   +- LocalRelation [col1#x, col2#x]


-- !query
CREATE FUNCTION foo4_0() RETURNS TABLE (x INT) RETURN SELECT 1
-- !query analysis
org.apache.spark.sql.catalyst.analysis.FunctionAlreadyExistsException
{
  "errorClass" : "ROUTINE_ALREADY_EXISTS",
  "sqlState" : "42723",
  "messageParameters" : {
    "existingRoutineType" : "routine",
    "newRoutineType" : "routine",
    "routineName" : "`default`.`foo4_0`"
  }
}


-- !query
CREATE FUNCTION foo4_1(x INT) RETURNS TABLE (a INT) RETURN SELECT x
-- !query analysis
org.apache.spark.sql.catalyst.analysis.FunctionAlreadyExistsException
{
  "errorClass" : "ROUTINE_ALREADY_EXISTS",
  "sqlState" : "42723",
  "messageParameters" : {
    "existingRoutineType" : "routine",
    "newRoutineType" : "routine",
    "routineName" : "`default`.`foo4_1`"
  }
}


-- !query
CREATE FUNCTION foo4_2(x INT) RETURNS TABLE (a INT) RETURN SELECT c2 FROM t2 WHERE c1 = x
-- !query analysis
org.apache.spark.sql.catalyst.analysis.FunctionAlreadyExistsException
{
  "errorClass" : "ROUTINE_ALREADY_EXISTS",
  "sqlState" : "42723",
  "messageParameters" : {
    "existingRoutineType" : "routine",
    "newRoutineType" : "routine",
    "routineName" : "`default`.`foo4_2`"
  }
}


-- !query
CREATE FUNCTION foo4_3(x INT) RETURNS TABLE (a INT, cnt INT) RETURN SELECT c1, COUNT(*) FROM t2 WHERE c1 = x GROUP BY c1
-- !query analysis
org.apache.spark.sql.catalyst.analysis.FunctionAlreadyExistsException
{
  "errorClass" : "ROUTINE_ALREADY_EXISTS",
  "sqlState" : "42723",
  "messageParameters" : {
    "existingRoutineType" : "routine",
    "newRoutineType" : "routine",
    "routineName" : "`default`.`foo4_3`"
  }
}


-- !query
SELECT * FROM foo4_0()
-- !query analysis
Project [x#x]
+- SQLFunctionNode spark_catalog.default.foo4_0
   +- SubqueryAlias foo4_0
      +- Project [cast(1#x as int) AS x#x]
         +- Project [1 AS 1#x]
            +- OneRowRelation


-- !query
SELECT * FROM foo4_1(1)
-- !query analysis
Project [a#x]
+- SQLFunctionNode spark_catalog.default.foo4_1
   +- SubqueryAlias foo4_1
      +- Project [cast(x#x as int) AS a#x]
         +- Project [cast(1 as int) AS x#x]
            +- OneRowRelation


-- !query
SELECT * FROM foo4_2(2)
-- !query analysis
Project [a#x]
+- SQLFunctionNode spark_catalog.default.foo4_2
   +- SubqueryAlias foo4_2
      +- Project [cast(c2#x as int) AS a#x]
         +- Project [c2#x]
            +- Filter (c1#x = cast(2 as int))
               +- SubqueryAlias spark_catalog.default.t2
                  +- View (`spark_catalog`.`default`.`t2`, [c1#x, c2#x])
                     +- Project [cast(col1#x as int) AS c1#x, cast(col2#x as int) AS c2#x]
                        +- LocalRelation [col1#x, col2#x]


-- !query
SELECT * FROM foo4_3(0)
-- !query analysis
Project [a#x, cnt#x]
+- SQLFunctionNode spark_catalog.default.foo4_3
   +- SubqueryAlias foo4_3
      +- Project [cast(c1#x as int) AS a#x, cast(count(1)#xL as int) AS cnt#x]
         +- Aggregate [c1#x], [c1#x, count(1) AS count(1)#xL]
            +- Filter (c1#x = cast(0 as int))
               +- SubqueryAlias spark_catalog.default.t2
                  +- View (`spark_catalog`.`default`.`t2`, [c1#x, c2#x])
                     +- Project [cast(col1#x as int) AS c1#x, cast(col2#x as int) AS c2#x]
                        +- LocalRelation [col1#x, col2#x]


-- !query
SELECT * FROM foo4_1(rand(0) * 0)
-- !query analysis
[Analyzer test output redacted due to nondeterminism]


-- !query
SELECT * FROM foo4_1(x => 1)
-- !query analysis
Project [a#x]
+- SQLFunctionNode spark_catalog.default.foo4_1
   +- SubqueryAlias foo4_1
      +- Project [cast(x#x as int) AS a#x]
         +- Project [cast(1 as int) AS x#x]
            +- OneRowRelation


-- !query
SELECT * FROM t1, LATERAL foo4_1(c1)
-- !query analysis
Project [c1#x, c2#x, a#x]
+- LateralJoin lateral-subquery#x [c1#x], Inner
   :  +- SQLFunctionNode spark_catalog.default.foo4_1
   :     +- SubqueryAlias foo4_1
   :        +- Project [cast(x#x as int) AS a#x]
   :           +- Project [cast(outer(c1#x) as int) AS x#x]
   :              +- OneRowRelation
   +- SubqueryAlias spark_catalog.default.t1
      +- View (`spark_catalog`.`default`.`t1`, [c1#x, c2#x])
         +- Project [cast(col1#x as int) AS c1#x, cast(col2#x as int) AS c2#x]
            +- LocalRelation [col1#x, col2#x]


-- !query
SELECT * FROM t1, LATERAL foo4_2(c1)
-- !query analysis
Project [c1#x, c2#x, a#x]
+- LateralJoin lateral-subquery#x [c1#x], Inner
   :  +- SQLFunctionNode spark_catalog.default.foo4_2
   :     +- SubqueryAlias foo4_2
   :        +- Project [cast(c2#x as int) AS a#x]
   :           +- Project [c2#x]
   :              +- Filter (c1#x = cast(outer(c1#x) as int))
   :                 +- SubqueryAlias spark_catalog.default.t2
   :                    +- View (`spark_catalog`.`default`.`t2`, [c1#x, c2#x])
   :                       +- Project [cast(col1#x as int) AS c1#x, cast(col2#x as int) AS c2#x]
   :                          +- LocalRelation [col1#x, col2#x]
   +- SubqueryAlias spark_catalog.default.t1
      +- View (`spark_catalog`.`default`.`t1`, [c1#x, c2#x])
         +- Project [cast(col1#x as int) AS c1#x, cast(col2#x as int) AS c2#x]
            +- LocalRelation [col1#x, col2#x]


-- !query
SELECT * FROM t1 JOIN LATERAL foo4_2(c1) ON t1.c2 = foo4_2.a
-- !query analysis
Project [c1#x, c2#x, a#x]
+- LateralJoin lateral-subquery#x [c1#x], Inner, (c2#x = a#x)
   :  +- SQLFunctionNode spark_catalog.default.foo4_2
   :     +- SubqueryAlias foo4_2
   :        +- Project [cast(c2#x as int) AS a#x]
   :           +- Project [c2#x]
   :              +- Filter (c1#x = cast(outer(c1#x) as int))
   :                 +- SubqueryAlias spark_catalog.default.t2
   :                    +- View (`spark_catalog`.`default`.`t2`, [c1#x, c2#x])
   :                       +- Project [cast(col1#x as int) AS c1#x, cast(col2#x as int) AS c2#x]
   :                          +- LocalRelation [col1#x, col2#x]
   +- SubqueryAlias spark_catalog.default.t1
      +- View (`spark_catalog`.`default`.`t1`, [c1#x, c2#x])
         +- Project [cast(col1#x as int) AS c1#x, cast(col2#x as int) AS c2#x]
            +- LocalRelation [col1#x, col2#x]


-- !query
SELECT * FROM t1, LATERAL foo4_3(c1)
-- !query analysis
Project [c1#x, c2#x, a#x, cnt#x]
+- LateralJoin lateral-subquery#x [c1#x], Inner
   :  +- SQLFunctionNode spark_catalog.default.foo4_3
   :     +- SubqueryAlias foo4_3
   :        +- Project [cast(c1#x as int) AS a#x, cast(count(1)#xL as int) AS cnt#x]
   :           +- LateralJoin lateral-subquery#x [x#x], Inner
   :              :  +- Aggregate [c1#x], [c1#x, count(1) AS count(1)#xL]
   :              :     +- Filter (c1#x = outer(x#x))
   :              :        +- SubqueryAlias spark_catalog.default.t2
   :              :           +- View (`spark_catalog`.`default`.`t2`, [c1#x, c2#x])
   :              :              +- Project [cast(col1#x as int) AS c1#x, cast(col2#x as int) AS c2#x]
   :              :                 +- LocalRelation [col1#x, col2#x]
   :              +- Project [cast(outer(c1#x) as int) AS x#x]
   :                 +- OneRowRelation
   +- SubqueryAlias spark_catalog.default.t1
      +- View (`spark_catalog`.`default`.`t1`, [c1#x, c2#x])
         +- Project [cast(col1#x as int) AS c1#x, cast(col2#x as int) AS c2#x]
            +- LocalRelation [col1#x, col2#x]


-- !query
SELECT * FROM t1, LATERAL (SELECT cnt FROM foo4_3(c1))
-- !query analysis
Project [c1#x, c2#x, cnt#x]
+- LateralJoin lateral-subquery#x [c1#x], Inner
   :  +- SubqueryAlias __auto_generated_subquery_name
   :     +- Project [cnt#x]
   :        +- SQLFunctionNode spark_catalog.default.foo4_3
   :           +- SubqueryAlias foo4_3
   :              +- Project [cast(c1#x as int) AS a#x, cast(count(1)#xL as int) AS cnt#x]
   :                 +- LateralJoin lateral-subquery#x [x#x], Inner
   :                    :  +- Aggregate [c1#x], [c1#x, count(1) AS count(1)#xL]
   :                    :     +- Filter (c1#x = outer(x#x))
   :                    :        +- SubqueryAlias spark_catalog.default.t2
   :                    :           +- View (`spark_catalog`.`default`.`t2`, [c1#x, c2#x])
   :                    :              +- Project [cast(col1#x as int) AS c1#x, cast(col2#x as int) AS c2#x]
   :                    :                 +- LocalRelation [col1#x, col2#x]
   :                    +- Project [cast(outer(c1#x) as int) AS x#x]
   :                       +- OneRowRelation
   +- SubqueryAlias spark_catalog.default.t1
      +- View (`spark_catalog`.`default`.`t1`, [c1#x, c2#x])
         +- Project [cast(col1#x as int) AS c1#x, cast(col2#x as int) AS c2#x]
            +- LocalRelation [col1#x, col2#x]


-- !query
SELECT * FROM t1, LATERAL foo4_1(c1 + rand(0) * 0)
-- !query analysis
org.apache.spark.sql.catalyst.ExtendedAnalysisException
{
  "errorClass" : "UNSUPPORTED_SUBQUERY_EXPRESSION_CATEGORY.NON_DETERMINISTIC_LATERAL_SUBQUERIES",
  "sqlState" : "0A000",
  "messageParameters" : {
    "treeNode" : "LateralJoin lateral-subquery#x [c1#x], Inner\n:  +- SQLFunctionNode spark_catalog.default.foo4_1\n:     +- SubqueryAlias foo4_1\n:        +- Project [cast(x#x as int) AS a#x]\n:           +- LateralJoin lateral-subquery#x [x#x], Inner\n:              :  +- Project [outer(x#x) AS x#x]\n:              :     +- OneRowRelation\n:              +- Project [cast((cast(outer(c1#x) as double) + (rand(number) * cast(0 as double))) as int) AS x#x]\n:                 +- OneRowRelation\n+- SubqueryAlias spark_catalog.default.t1\n   +- View (`spark_catalog`.`default`.`t1`, [c1#x, c2#x])\n      +- Project [cast(col1#x as int) AS c1#x, cast(col2#x as int) AS c2#x]\n         +- LocalRelation [col1#x, col2#x]\n"
  },
  "queryContext" : [ {
    "objectType" : "",
    "objectName" : "",
    "startIndex" : 10,
    "stopIndex" : 50,
    "fragment" : "FROM t1, LATERAL foo4_1(c1 + rand(0) * 0)"
  } ]
}


-- !query
SELECT * FROM t1 JOIN foo4_1(1) AS foo4_1(x) ON t1.c1 = foo4_1.x
-- !query analysis
Project [c1#x, c2#x, x#x]
+- Join Inner, (c1#x = x#x)
   :- SubqueryAlias spark_catalog.default.t1
   :  +- View (`spark_catalog`.`default`.`t1`, [c1#x, c2#x])
   :     +- Project [cast(col1#x as int) AS c1#x, cast(col2#x as int) AS c2#x]
   :        +- LocalRelation [col1#x, col2#x]
   +- SubqueryAlias foo4_1
      +- Project [a#x AS x#x]
         +- SQLFunctionNode spark_catalog.default.foo4_1
            +- SubqueryAlias foo4_1
               +- Project [cast(x#x as int) AS a#x]
                  +- Project [cast(1 as int) AS x#x]
                     +- OneRowRelation


-- !query
SELECT * FROM t1, LATERAL foo4_1(c1), LATERAL foo4_2(foo4_1.a + c1)
-- !query analysis
Project [c1#x, c2#x, a#x, a#x]
+- LateralJoin lateral-subquery#x [a#x && c1#x], Inner
   :  +- SQLFunctionNode spark_catalog.default.foo4_2
   :     +- SubqueryAlias foo4_2
   :        +- Project [cast(c2#x as int) AS a#x]
   :           +- Project [c2#x]
   :              +- Filter (c1#x = cast((outer(a#x) + outer(c1#x)) as int))
   :                 +- SubqueryAlias spark_catalog.default.t2
   :                    +- View (`spark_catalog`.`default`.`t2`, [c1#x, c2#x])
   :                       +- Project [cast(col1#x as int) AS c1#x, cast(col2#x as int) AS c2#x]
   :                          +- LocalRelation [col1#x, col2#x]
   +- LateralJoin lateral-subquery#x [c1#x], Inner
      :  +- SQLFunctionNode spark_catalog.default.foo4_1
      :     +- SubqueryAlias foo4_1
      :        +- Project [cast(x#x as int) AS a#x]
      :           +- Project [cast(outer(c1#x) as int) AS x#x]
      :              +- OneRowRelation
      +- SubqueryAlias spark_catalog.default.t1
         +- View (`spark_catalog`.`default`.`t1`, [c1#x, c2#x])
            +- Project [cast(col1#x as int) AS c1#x, cast(col2#x as int) AS c2#x]
               +- LocalRelation [col1#x, col2#x]


-- !query
SELECT (SELECT MAX(a) FROM foo4_1(c1)) FROM t1
-- !query analysis
Project [scalar-subquery#x [c1#x] AS scalarsubquery(c1)#x]
:  +- Aggregate [max(a#x) AS max(a)#x]
:     +- SQLFunctionNode spark_catalog.default.foo4_1
:        +- SubqueryAlias foo4_1
:           +- Project [cast(x#x as int) AS a#x]
:              +- Project [cast(outer(c1#x) as int) AS x#x]
:                 +- OneRowRelation
+- SubqueryAlias spark_catalog.default.t1
   +- View (`spark_catalog`.`default`.`t1`, [c1#x, c2#x])
      +- Project [cast(col1#x as int) AS c1#x, cast(col2#x as int) AS c2#x]
         +- LocalRelation [col1#x, col2#x]


-- !query
SELECT (SELECT MAX(a) FROM foo4_1(c1) WHERE a = c2) FROM t1
-- !query analysis
Project [scalar-subquery#x [c2#x && c1#x] AS scalarsubquery(c2, c1)#x]
:  +- Aggregate [max(a#x) AS max(a)#x]
:     +- Filter (a#x = outer(c2#x))
:        +- SQLFunctionNode spark_catalog.default.foo4_1
:           +- SubqueryAlias foo4_1
:              +- Project [cast(x#x as int) AS a#x]
:                 +- Project [cast(outer(c1#x) as int) AS x#x]
:                    +- OneRowRelation
+- SubqueryAlias spark_catalog.default.t1
   +- View (`spark_catalog`.`default`.`t1`, [c1#x, c2#x])
      +- Project [cast(col1#x as int) AS c1#x, cast(col2#x as int) AS c2#x]
         +- LocalRelation [col1#x, col2#x]


-- !query
SELECT (SELECT MAX(cnt) FROM foo4_3(c1)) FROM t1
-- !query analysis
Project [scalar-subquery#x [c1#x] AS scalarsubquery(c1)#x]
:  +- Aggregate [max(cnt#x) AS max(cnt)#x]
:     +- SQLFunctionNode spark_catalog.default.foo4_3
:        +- SubqueryAlias foo4_3
:           +- Project [cast(c1#x as int) AS a#x, cast(count(1)#xL as int) AS cnt#x]
:              +- LateralJoin lateral-subquery#x [x#x], Inner
:                 :  +- Aggregate [c1#x], [c1#x, count(1) AS count(1)#xL]
:                 :     +- Filter (c1#x = outer(x#x))
:                 :        +- SubqueryAlias spark_catalog.default.t2
:                 :           +- View (`spark_catalog`.`default`.`t2`, [c1#x, c2#x])
:                 :              +- Project [cast(col1#x as int) AS c1#x, cast(col2#x as int) AS c2#x]
:                 :                 +- LocalRelation [col1#x, col2#x]
:                 +- Project [cast(outer(c1#x) as int) AS x#x]
:                    +- OneRowRelation
+- SubqueryAlias spark_catalog.default.t1
   +- View (`spark_catalog`.`default`.`t1`, [c1#x, c2#x])
      +- Project [cast(col1#x as int) AS c1#x, cast(col2#x as int) AS c2#x]
         +- LocalRelation [col1#x, col2#x]


-- !query
DROP VIEW t1
-- !query analysis
DropTableCommand `spark_catalog`.`default`.`t1`, false, true, false


-- !query
DROP VIEW t2
-- !query analysis
DropTableCommand `spark_catalog`.`default`.`t2`, false, true, false
