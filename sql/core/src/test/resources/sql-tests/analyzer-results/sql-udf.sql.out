-- Automatically generated by SQLQueryTestSuite
-- !query
CREATE FUNCTION foo1a0() RETURNS INT RETURN 1
-- !query analysis
CreateSQLFunctionCommand spark_catalog.default.foo1a0, INT, 1, false, false, false, false


-- !query
SELECT foo1a0()
-- !query analysis
Project [spark_catalog.default.foo1a0() AS spark_catalog.default.foo1a0()#x]
+- Project
   +- OneRowRelation


-- !query
SELECT foo1a0(1)
-- !query analysis
org.apache.spark.sql.AnalysisException
{
  "errorClass" : "WRONG_NUM_ARGS.WITHOUT_SUGGESTION",
  "sqlState" : "42605",
  "messageParameters" : {
    "actualNum" : "1",
    "docroot" : "https://spark.apache.org/docs/latest",
    "expectedNum" : "0",
    "functionName" : "`spark_catalog`.`default`.`foo1a0`"
  },
  "queryContext" : [ {
    "objectType" : "",
    "objectName" : "",
    "startIndex" : 8,
    "stopIndex" : 16,
    "fragment" : "foo1a0(1)"
  } ]
}


-- !query
CREATE FUNCTION foo1a1(a INT) RETURNS INT RETURN 1
-- !query analysis
CreateSQLFunctionCommand spark_catalog.default.foo1a1, a INT, INT, 1, false, false, false, false


-- !query
SELECT foo1a1(1)
-- !query analysis
Project [spark_catalog.default.foo1a1(a#x) AS spark_catalog.default.foo1a1(1)#x]
+- Project [cast(1 as int) AS a#x]
   +- OneRowRelation


-- !query
SELECT foo1a1(1, 2)
-- !query analysis
org.apache.spark.sql.AnalysisException
{
  "errorClass" : "WRONG_NUM_ARGS.WITHOUT_SUGGESTION",
  "sqlState" : "42605",
  "messageParameters" : {
    "actualNum" : "2",
    "docroot" : "https://spark.apache.org/docs/latest",
    "expectedNum" : "1",
    "functionName" : "`spark_catalog`.`default`.`foo1a1`"
  },
  "queryContext" : [ {
    "objectType" : "",
    "objectName" : "",
    "startIndex" : 8,
    "stopIndex" : 19,
    "fragment" : "foo1a1(1, 2)"
  } ]
}


-- !query
CREATE FUNCTION foo1a2(a INT, b INT, c INT, d INT) RETURNS INT RETURN 1
-- !query analysis
CreateSQLFunctionCommand spark_catalog.default.foo1a2, a INT, b INT, c INT, d INT, INT, 1, false, false, false, false


-- !query
SELECT foo1a2(1, 2, 3, 4)
-- !query analysis
Project [spark_catalog.default.foo1a2(a#x, b#x, c#x, d#x) AS spark_catalog.default.foo1a2(1, 2, 3, 4)#x]
+- Project [cast(1 as int) AS a#x, cast(2 as int) AS b#x, cast(3 as int) AS c#x, cast(4 as int) AS d#x]
   +- OneRowRelation


-- !query
CREATE FUNCTION foo1b0() RETURNS TABLE (c1 INT) RETURN SELECT 1
-- !query analysis
CreateSQLFunctionCommand spark_catalog.default.foo1b0, c1 INT, SELECT 1, true, false, false, false


-- !query
SELECT * FROM foo1b0()
-- !query analysis
Project [c1#x]
+- SQLFunctionNode spark_catalog.default.foo1b0
   +- SubqueryAlias foo1b0
      +- Project [cast(1#x as int) AS c1#x]
         +- Project [1 AS 1#x]
            +- OneRowRelation


-- !query
CREATE FUNCTION foo1b1(a INT) RETURNS TABLE (c1 INT) RETURN SELECT 1
-- !query analysis
CreateSQLFunctionCommand spark_catalog.default.foo1b1, a INT, c1 INT, SELECT 1, true, false, false, false


-- !query
SELECT * FROM foo1b1(1)
-- !query analysis
Project [c1#x]
+- SQLFunctionNode spark_catalog.default.foo1b1
   +- SubqueryAlias foo1b1
      +- Project [cast(1#x as int) AS c1#x]
         +- Project [1 AS 1#x]
            +- OneRowRelation


-- !query
CREATE FUNCTION foo1b2(a INT, b INT, c INT, d INT) RETURNS TABLE(c1 INT) RETURN SELECT 1
-- !query analysis
CreateSQLFunctionCommand spark_catalog.default.foo1b2, a INT, b INT, c INT, d INT, c1 INT, SELECT 1, true, false, false, false


-- !query
SELECT * FROM foo1b2(1, 2, 3, 4)
-- !query analysis
Project [c1#x]
+- SQLFunctionNode spark_catalog.default.foo1b2
   +- SubqueryAlias foo1b2
      +- Project [cast(1#x as int) AS c1#x]
         +- Project [1 AS 1#x]
            +- OneRowRelation


-- !query
CREATE FUNCTION foo1c1(duplicate INT, DUPLICATE INT) RETURNS INT RETURN 1
-- !query analysis
org.apache.spark.sql.AnalysisException
{
  "errorClass" : "DUPLICATE_ROUTINE_PARAMETER_NAMES",
  "sqlState" : "42734",
  "messageParameters" : {
    "names" : "`duplicate`",
    "routineName" : "foo1c1"
  }
}


-- !query
CREATE FUNCTION foo1c2(a INT, b INT, thisisaduplicate INT, c INT, d INT, e INT, f INT, thisIsaDuplicate INT, g INT)
    RETURNS TABLE (a INT) RETURN SELECT 1
-- !query analysis
org.apache.spark.sql.AnalysisException
{
  "errorClass" : "DUPLICATE_ROUTINE_PARAMETER_NAMES",
  "sqlState" : "42734",
  "messageParameters" : {
    "names" : "`thisisaduplicate`",
    "routineName" : "foo1c2"
  }
}


-- !query
CREATE OR REPLACE FUNCTION foo1d1(a INT DEFAULT NULL) RETURNS INT RETURN a
-- !query analysis
CreateSQLFunctionCommand spark_catalog.default.foo1d1, a INT DEFAULT NULL, INT, a, false, false, false, true


-- !query
SELECT foo1d1(5), foo1d1()
-- !query analysis
Project [spark_catalog.default.foo1d1(a#x) AS spark_catalog.default.foo1d1(5)#x, spark_catalog.default.foo1d1(a#x) AS spark_catalog.default.foo1d1()#x]
+- Project [cast(5 as int) AS a#x, cast(cast(null as int) as int) AS a#x]
   +- OneRowRelation


-- !query
CREATE OR REPLACE FUNCTION foo1d1(a INT DEFAULT 10) RETURNS INT RETURN a
-- !query analysis
CreateSQLFunctionCommand spark_catalog.default.foo1d1, a INT DEFAULT 10, INT, a, false, false, false, true


-- !query
SELECT foo1d1(5), foo1d1()
-- !query analysis
Project [spark_catalog.default.foo1d1(a#x) AS spark_catalog.default.foo1d1(5)#x, spark_catalog.default.foo1d1(a#x) AS spark_catalog.default.foo1d1()#x]
+- Project [cast(5 as int) AS a#x, cast(cast(10 as int) as int) AS a#x]
   +- OneRowRelation


-- !query
CREATE OR REPLACE FUNCTION foo1d1(a INT DEFAULT length(substr(current_database(), 1, 1))) RETURNS INT RETURN a
-- !query analysis
CreateSQLFunctionCommand spark_catalog.default.foo1d1, a INT DEFAULT length(substr(current_database(), 1, 1)), INT, a, false, false, false, true


-- !query
SELECT foo1d1(5), foo1d1()
-- !query analysis
Project [spark_catalog.default.foo1d1(a#x) AS spark_catalog.default.foo1d1(5)#x, spark_catalog.default.foo1d1(a#x) AS spark_catalog.default.foo1d1()#x]
+- Project [cast(5 as int) AS a#x, cast(cast(length(substr(current_schema(), 1, 1)) as int) as int) AS a#x]
   +- OneRowRelation


-- !query
CREATE OR REPLACE FUNCTION foo1d1(a INT DEFAULT '5' || length(substr(current_database(), 1, 1)))
  RETURNS INT RETURN a
-- !query analysis
CreateSQLFunctionCommand spark_catalog.default.foo1d1, a INT DEFAULT '5' || length(substr(current_database(), 1, 1)), INT, a, false, false, false, true


-- !query
SELECT foo1d1(5), foo1d1()
-- !query analysis
Project [spark_catalog.default.foo1d1(a#x) AS spark_catalog.default.foo1d1(5)#x, spark_catalog.default.foo1d1(a#x) AS spark_catalog.default.foo1d1()#x]
+- Project [cast(5 as int) AS a#x, cast(cast(concat(5, cast(length(substr(current_schema(), 1, 1)) as string)) as int) as int) AS a#x]
   +- OneRowRelation


-- !query
CREATE OR REPLACE FUNCTION foo1d1(a INT DEFAULT RAND()::INT) RETURNS INT RETURN a
-- !query analysis
CreateSQLFunctionCommand spark_catalog.default.foo1d1, a INT DEFAULT RAND()::INT, INT, a, false, false, false, true


-- !query
SELECT foo1d1(5), foo1d1()
-- !query analysis
[Analyzer test output redacted due to nondeterminism]


-- !query
CREATE OR REPLACE FUNCTION foo1d1(a INT DEFAULT array(55, 17))
  RETURNS INT RETURN a
-- !query analysis
org.apache.spark.sql.catalyst.ExtendedAnalysisException
{
  "errorClass" : "DATATYPE_MISMATCH.CAST_WITHOUT_SUGGESTION",
  "sqlState" : "42K09",
  "messageParameters" : {
    "sqlExpr" : "\"array(55, 17)\"",
    "srcType" : "\"ARRAY<INT>\"",
    "targetType" : "\"INT\""
  },
  "queryContext" : [ {
    "objectType" : "",
    "objectName" : "",
    "startIndex" : 1,
    "stopIndex" : 85,
    "fragment" : "CREATE OR REPLACE FUNCTION foo1d1(a INT DEFAULT array(55, 17))\n  RETURNS INT RETURN a"
  } ]
}


-- !query
CREATE OR REPLACE FUNCTION foo1d1(a INT DEFAULT (SELECT max(c1) FROM VALUES (1) AS T(c1)))
  RETURNS INT RETURN a
-- !query analysis
org.apache.spark.sql.AnalysisException
{
  "errorClass" : "USER_DEFINED_FUNCTIONS.NOT_A_VALID_DEFAULT_EXPRESSION",
  "sqlState" : "42601",
  "messageParameters" : {
    "functionName" : "foo1d1",
    "parameterName" : "a"
  }
}


-- !query
CREATE OR REPLACE FUNCTION foo1d2(a INT, b INT DEFAULT 7, c INT DEFAULT 8, d INT DEFAULT 9 COMMENT 'test')
  RETURNS STRING RETURN a || ' ' || b || ' ' || c || ' ' || d
-- !query analysis
CreateSQLFunctionCommand spark_catalog.default.foo1d2, a INT, b INT DEFAULT 7, c INT DEFAULT 8, d INT DEFAULT 9 COMMENT 'test', STRING, a || ' ' || b || ' ' || c || ' ' || d, false, false, false, true


-- !query
SELECT foo1d2(1, 2, 3, 4), foo1d2(1, 2, 3), foo1d2(1, 2), foo1d2(1)
-- !query analysis
Project [spark_catalog.default.foo1d2(a#x, b#x, c#x, d#x) AS spark_catalog.default.foo1d2(1, 2, 3, 4)#x, spark_catalog.default.foo1d2(a#x, b#x, c#x, d#x) AS spark_catalog.default.foo1d2(1, 2, 3)#x, spark_catalog.default.foo1d2(a#x, b#x, c#x, d#x) AS spark_catalog.default.foo1d2(1, 2)#x, spark_catalog.default.foo1d2(a#x, b#x, c#x, d#x) AS spark_catalog.default.foo1d2(1)#x]
+- Project [cast(1 as int) AS a#x, cast(2 as int) AS b#x, cast(3 as int) AS c#x, cast(4 as int) AS d#x, cast(1 as int) AS a#x, cast(2 as int) AS b#x, cast(3 as int) AS c#x, cast(cast(9 as int) as int) AS d#x, cast(1 as int) AS a#x, cast(2 as int) AS b#x, cast(cast(8 as int) as int) AS c#x, cast(cast(9 as int) as int) AS d#x, cast(1 as int) AS a#x, cast(cast(7 as int) as int) AS b#x, cast(cast(8 as int) as int) AS c#x, cast(cast(9 as int) as int) AS d#x]
   +- OneRowRelation


-- !query
SELECT foo1d2()
-- !query analysis
org.apache.spark.sql.AnalysisException
{
  "errorClass" : "WRONG_NUM_ARGS.WITHOUT_SUGGESTION",
  "sqlState" : "42605",
  "messageParameters" : {
    "actualNum" : "0",
    "docroot" : "https://spark.apache.org/docs/latest",
    "expectedNum" : "4",
    "functionName" : "`spark_catalog`.`default`.`foo1d2`"
  },
  "queryContext" : [ {
    "objectType" : "",
    "objectName" : "",
    "startIndex" : 8,
    "stopIndex" : 15,
    "fragment" : "foo1d2()"
  } ]
}


-- !query
SELECT foo1d2(1, 2, 3, 4, 5)
-- !query analysis
org.apache.spark.sql.AnalysisException
{
  "errorClass" : "WRONG_NUM_ARGS.WITHOUT_SUGGESTION",
  "sqlState" : "42605",
  "messageParameters" : {
    "actualNum" : "5",
    "docroot" : "https://spark.apache.org/docs/latest",
    "expectedNum" : "4",
    "functionName" : "`spark_catalog`.`default`.`foo1d2`"
  },
  "queryContext" : [ {
    "objectType" : "",
    "objectName" : "",
    "startIndex" : 8,
    "stopIndex" : 28,
    "fragment" : "foo1d2(1, 2, 3, 4, 5)"
  } ]
}


-- !query
CREATE OR REPLACE FUNCTION foo1d2(a INT DEFAULT 5, b INT , c INT DEFAULT 8, d INT DEFAULT 9 COMMENT 'test')
  RETURNS STRING RETURN a || ' ' || b || ' ' || c || ' ' || d
-- !query analysis
org.apache.spark.sql.AnalysisException
{
  "errorClass" : "USER_DEFINED_FUNCTIONS.NOT_A_VALID_DEFAULT_PARAMETER_POSITION",
  "sqlState" : "42601",
  "messageParameters" : {
    "functionName" : "foo1d2",
    "nextParameterName" : "b",
    "parameterName" : "a"
  }
}


-- !query
CREATE OR REPLACE FUNCTION foo1d2(a INT, b INT DEFAULT 7, c INT DEFAULT 8, d INT COMMENT 'test')
  RETURNS STRING RETURN a || ' ' || b || ' ' || c || ' ' || d
-- !query analysis
org.apache.spark.sql.AnalysisException
{
  "errorClass" : "USER_DEFINED_FUNCTIONS.NOT_A_VALID_DEFAULT_PARAMETER_POSITION",
  "sqlState" : "42601",
  "messageParameters" : {
    "functionName" : "foo1d2",
    "nextParameterName" : "d",
    "parameterName" : "c"
  }
}


-- !query
CREATE OR REPLACE TEMPORARY FUNCTION foo1d3(a INT DEFAULT 7 COMMENT 'hello') RETURNS INT RETURN a
-- !query analysis
CreateSQLFunctionCommand foo1d3, a INT DEFAULT 7 COMMENT 'hello', INT, a, false, true, false, true


-- !query
SELECT foo1d3(5), foo1d3()
-- !query analysis
Project [foo1d3(a#x) AS foo1d3(5)#x, foo1d3(a#x) AS foo1d3()#x]
+- Project [cast(5 as int) AS a#x, cast(cast(7 as int) as int) AS a#x]
   +- OneRowRelation


-- !query
CREATE OR REPLACE FUNCTION foo1d4(a INT, b INT DEFAULT a) RETURNS INT RETURN a + b
-- !query analysis
org.apache.spark.sql.AnalysisException
{
  "errorClass" : "UNRESOLVED_COLUMN.WITHOUT_SUGGESTION",
  "sqlState" : "42703",
  "messageParameters" : {
    "objectName" : "`a`"
  },
  "queryContext" : [ {
    "objectType" : "",
    "objectName" : "",
    "startIndex" : 1,
    "stopIndex" : 1,
    "fragment" : "a"
  } ]
}


-- !query
CREATE OR REPLACE FUNCTION foo1d4(a INT, b INT DEFAULT 3) RETURNS INT RETURN a + b
-- !query analysis
CreateSQLFunctionCommand spark_catalog.default.foo1d4, a INT, b INT DEFAULT 3, INT, a + b, false, false, false, true


-- !query
CREATE OR REPLACE FUNCTION foo1d5(a INT, b INT DEFAULT foo1d4(6)) RETURNS INT RETURN a + b
-- !query analysis
CreateSQLFunctionCommand spark_catalog.default.foo1d5, a INT, b INT DEFAULT foo1d4(6), INT, a + b, false, false, false, true


-- !query
SELECT foo1d5(10), foo1d5(10, 2)
-- !query analysis
Project [spark_catalog.default.foo1d5(a#x, b#x) AS spark_catalog.default.foo1d5(10)#x, spark_catalog.default.foo1d5(a#x, b#x) AS spark_catalog.default.foo1d5(10, 2)#x]
+- Project [cast(10 as int) AS a#x, cast(cast(spark_catalog.default.foo1d4(a#x, b#x) as int) as int) AS b#x, cast(10 as int) AS a#x, cast(2 as int) AS b#x]
   +- Project [cast(6 as int) AS a#x, cast(cast(3 as int) as int) AS b#x]
      +- OneRowRelation


-- !query
CREATE OR REPLACE FUNCTION foo1d5(a INT, b INT) RETURNS INT RETURN a + foo1d4(b)
-- !query analysis
CreateSQLFunctionCommand spark_catalog.default.foo1d5, a INT, b INT, INT, a + foo1d4(b), false, false, false, true


-- !query
SELECT foo1d5(10, 2)
-- !query analysis
Project [spark_catalog.default.foo1d5(a#x, b#x) AS spark_catalog.default.foo1d5(10, 2)#x]
+- Project [a#x, b#x, cast(b#x as int) AS a#x, cast(cast(3 as int) as int) AS b#x]
   +- Project [cast(10 as int) AS a#x, cast(2 as int) AS b#x]
      +- OneRowRelation


-- !query
CREATE OR REPLACE FUNCTION foo1d6(a INT, b INT DEFAULT 7) RETURNS TABLE(a INT, b INT) RETURN SELECT a, b
-- !query analysis
CreateSQLFunctionCommand spark_catalog.default.foo1d6, a INT, b INT DEFAULT 7, a INT, b INT, SELECT a, b, true, false, false, true


-- !query
SELECT * FROM foo1d6(5)
-- !query analysis
Project [a#x, b#x]
+- SQLFunctionNode spark_catalog.default.foo1d6
   +- SubqueryAlias foo1d6
      +- Project [cast(a#x as int) AS a#x, cast(b#x as int) AS b#x]
         +- Project [cast(5 as int) AS a#x, cast(7 as int) AS b#x]
            +- OneRowRelation


-- !query
SELECT * FROM foo1d6(5, 2)
-- !query analysis
Project [a#x, b#x]
+- SQLFunctionNode spark_catalog.default.foo1d6
   +- SubqueryAlias foo1d6
      +- Project [cast(a#x as int) AS a#x, cast(b#x as int) AS b#x]
         +- Project [cast(5 as int) AS a#x, cast(2 as int) AS b#x]
            +- OneRowRelation


-- !query
CREATE FUNCTION foo1e1(x INT NOT NULL, y INT) RETURNS INT RETURN 1
-- !query analysis
org.apache.spark.sql.AnalysisException
{
  "errorClass" : "USER_DEFINED_FUNCTIONS.NOT_NULL_ON_FUNCTION_PARAMETERS",
  "sqlState" : "42601",
  "messageParameters" : {
    "input" : "x INT NOT NULL, y INT"
  }
}


-- !query
CREATE FUNCTION foo1e2(x INT, y INT NOT NULL) RETURNS TABLE (x INT) RETURN SELECT 1
-- !query analysis
org.apache.spark.sql.AnalysisException
{
  "errorClass" : "USER_DEFINED_FUNCTIONS.NOT_NULL_ON_FUNCTION_PARAMETERS",
  "sqlState" : "42601",
  "messageParameters" : {
    "input" : "x INT, y INT NOT NULL"
  }
}


-- !query
CREATE FUNCTION foo1e3(x INT, y INT) RETURNS TABLE (x INT NOT NULL) RETURN SELECT 1
-- !query analysis
org.apache.spark.sql.AnalysisException
{
  "errorClass" : "USER_DEFINED_FUNCTIONS.NOT_NULL_ON_FUNCTION_PARAMETERS",
  "sqlState" : "42601",
  "messageParameters" : {
    "input" : "x INT NOT NULL"
  }
}


-- !query
CREATE FUNCTION foo1f1(x INT, y INT GENERATED ALWAYS AS (x + 10)) RETURNS INT RETURN y + 1
-- !query analysis
org.apache.spark.sql.catalyst.parser.ParseException
{
  "errorClass" : "INVALID_SQL_SYNTAX.CREATE_FUNC_WITH_GENERATED_COLUMNS_AS_PARAMETERS",
  "sqlState" : "42000",
  "queryContext" : [ {
    "objectType" : "",
    "objectName" : "",
    "startIndex" : 1,
    "stopIndex" : 90,
    "fragment" : "CREATE FUNCTION foo1f1(x INT, y INT GENERATED ALWAYS AS (x + 10)) RETURNS INT RETURN y + 1"
  } ]
}


-- !query
CREATE FUNCTION foo1f2(id BIGINT GENERATED ALWAYS AS IDENTITY) RETURNS BIGINT RETURN id + 1
-- !query analysis
org.apache.spark.sql.catalyst.parser.ParseException
{
  "errorClass" : "INVALID_SQL_SYNTAX.CREATE_FUNC_WITH_GENERATED_COLUMNS_AS_PARAMETERS",
  "sqlState" : "42000",
  "queryContext" : [ {
    "objectType" : "",
    "objectName" : "",
    "startIndex" : 1,
    "stopIndex" : 91,
    "fragment" : "CREATE FUNCTION foo1f2(id BIGINT GENERATED ALWAYS AS IDENTITY) RETURNS BIGINT RETURN id + 1"
  } ]
}


-- !query
CREATE FUNCTION foo1g1(x INT, y INT UNIQUE) RETURNS INT RETURN y + 1
-- !query analysis
org.apache.spark.sql.catalyst.parser.ParseException
{
  "errorClass" : "INVALID_SQL_SYNTAX.CREATE_FUNC_WITH_COLUMN_CONSTRAINTS",
  "sqlState" : "42000",
  "queryContext" : [ {
    "objectType" : "",
    "objectName" : "",
    "startIndex" : 1,
    "stopIndex" : 68,
    "fragment" : "CREATE FUNCTION foo1g1(x INT, y INT UNIQUE) RETURNS INT RETURN y + 1"
  } ]
}


-- !query
CREATE FUNCTION foo1g2(id BIGINT CHECK (true)) RETURNS BIGINT RETURN id + 1
-- !query analysis
org.apache.spark.sql.catalyst.parser.ParseException
{
  "errorClass" : "INVALID_SQL_SYNTAX.CREATE_FUNC_WITH_COLUMN_CONSTRAINTS",
  "sqlState" : "42000",
  "queryContext" : [ {
    "objectType" : "",
    "objectName" : "",
    "startIndex" : 1,
    "stopIndex" : 75,
    "fragment" : "CREATE FUNCTION foo1g2(id BIGINT CHECK (true)) RETURNS BIGINT RETURN id + 1"
  } ]
}


-- !query
CREATE FUNCTION foo2a0() RETURNS TABLE() RETURN SELECT 1
-- !query analysis
org.apache.spark.sql.catalyst.parser.ParseException
{
  "errorClass" : "PARSE_SYNTAX_ERROR",
  "sqlState" : "42601",
  "messageParameters" : {
    "error" : "')'",
    "hint" : ""
  }
}


-- !query
CREATE FUNCTION foo2a2() RETURNS TABLE(c1 INT, c2 INT) RETURN SELECT 1, 2
-- !query analysis
CreateSQLFunctionCommand spark_catalog.default.foo2a2, c1 INT, c2 INT, SELECT 1, 2, true, false, false, false


-- !query
SELECT * FROM foo2a2()
-- !query analysis
Project [c1#x, c2#x]
+- SQLFunctionNode spark_catalog.default.foo2a2
   +- SubqueryAlias foo2a2
      +- Project [cast(1#x as int) AS c1#x, cast(2#x as int) AS c2#x]
         +- Project [1 AS 1#x, 2 AS 2#x]
            +- OneRowRelation


-- !query
CREATE FUNCTION foo2a4() RETURNS TABLE(c1 INT, c2 INT, c3 INT, c4 INT) RETURN SELECT 1, 2, 3, 4
-- !query analysis
CreateSQLFunctionCommand spark_catalog.default.foo2a4, c1 INT, c2 INT, c3 INT, c4 INT, SELECT 1, 2, 3, 4, true, false, false, false


-- !query
SELECT * FROM foo2a2()
-- !query analysis
Project [c1#x, c2#x]
+- SQLFunctionNode spark_catalog.default.foo2a2
   +- SubqueryAlias foo2a2
      +- Project [cast(1#x as int) AS c1#x, cast(2#x as int) AS c2#x]
         +- Project [1 AS 1#x, 2 AS 2#x]
            +- OneRowRelation


-- !query
CREATE FUNCTION foo2b1() RETURNS TABLE(DuPLiCatE INT, duplicate INT) RETURN SELECT 1, 2
-- !query analysis
org.apache.spark.sql.AnalysisException
{
  "errorClass" : "DUPLICATE_ROUTINE_RETURNS_COLUMNS",
  "sqlState" : "42711",
  "messageParameters" : {
    "columns" : "`duplicate`",
    "routineName" : "foo2b1"
  }
}


-- !query
CREATE FUNCTION foo2b2() RETURNS TABLE(a INT, b INT, duplicate INT, c INT, d INT, e INT, DUPLICATE INT)
RETURN SELECT 1, 2, 3, 4, 5, 6, 7
-- !query analysis
org.apache.spark.sql.AnalysisException
{
  "errorClass" : "DUPLICATE_ROUTINE_RETURNS_COLUMNS",
  "sqlState" : "42711",
  "messageParameters" : {
    "columns" : "`duplicate`",
    "routineName" : "foo2b2"
  }
}


-- !query
CREATE FUNCTION foo2c1() RETURNS TABLE(c1 INT DEFAULT 5) RETURN SELECT 1, 2
-- !query analysis
org.apache.spark.sql.catalyst.parser.ParseException
{
  "errorClass" : "PARSE_SYNTAX_ERROR",
  "sqlState" : "42601",
  "messageParameters" : {
    "error" : "'DEFAULT'",
    "hint" : ""
  }
}


-- !query
CREATE FUNCTION foo31() RETURNS INT RETURN (SELECT 1, 2)
-- !query analysis
org.apache.spark.sql.catalyst.ExtendedAnalysisException
{
  "errorClass" : "INVALID_SUBQUERY_EXPRESSION.SCALAR_SUBQUERY_RETURN_MORE_THAN_ONE_OUTPUT_COLUMN",
  "sqlState" : "42823",
  "messageParameters" : {
    "number" : "2"
  },
  "queryContext" : [ {
    "objectType" : "",
    "objectName" : "",
    "startIndex" : 1,
    "stopIndex" : 56,
    "fragment" : "CREATE FUNCTION foo31() RETURNS INT RETURN (SELECT 1, 2)"
  } ]
}


-- !query
CREATE FUNCTION foo32() RETURNS TABLE(a INT) RETURN SELECT 1, 2
-- !query analysis
org.apache.spark.sql.AnalysisException
{
  "errorClass" : "USER_DEFINED_FUNCTIONS.RETURN_COLUMN_COUNT_MISMATCH",
  "sqlState" : "42601",
  "messageParameters" : {
    "name" : "spark_catalog.default.foo32",
    "outputSize" : "2",
    "returnParamSize" : "1"
  }
}


-- !query
CREATE FUNCTION foo33() RETURNS TABLE(a INT, b INT) RETURN SELECT 1
-- !query analysis
org.apache.spark.sql.AnalysisException
{
  "errorClass" : "USER_DEFINED_FUNCTIONS.RETURN_COLUMN_COUNT_MISMATCH",
  "sqlState" : "42601",
  "messageParameters" : {
    "name" : "spark_catalog.default.foo33",
    "outputSize" : "1",
    "returnParamSize" : "2"
  }
}


-- !query
CREATE FUNCTION foo41() RETURNS INT RETURN SELECT 1
-- !query analysis
CreateSQLFunctionCommand spark_catalog.default.foo41, INT, SELECT 1, false, false, false, false


-- !query
CREATE FUNCTION foo42() RETURNS TABLE(a INT) RETURN 1
-- !query analysis
org.apache.spark.sql.AnalysisException
{
  "errorClass" : "USER_DEFINED_FUNCTIONS.SQL_TABLE_UDF_BODY_MUST_BE_A_QUERY",
  "sqlState" : "42601",
  "messageParameters" : {
    "name" : "foo42"
  }
}


-- !query
CREATE FUNCTION foo51() RETURNS INT RETURN (SELECT a FROM VALUES(1), (2) AS T(a))
-- !query analysis
CreateSQLFunctionCommand spark_catalog.default.foo51, INT, (SELECT a FROM VALUES(1), (2) AS T(a)), false, false, false, false


-- !query
SELECT foo51()
-- !query analysis
Project [spark_catalog.default.foo51() AS spark_catalog.default.foo51()#x]
:  +- Project [a#x]
:     +- SubqueryAlias T
:        +- LocalRelation [a#x]
+- Project
   +- OneRowRelation


-- !query
CREATE FUNCTION foo52() RETURNS INT RETURN (SELECT 1 FROM VALUES(1) WHERE 1 = 0)
-- !query analysis
CreateSQLFunctionCommand spark_catalog.default.foo52, INT, (SELECT 1 FROM VALUES(1) WHERE 1 = 0), false, false, false, false


-- !query
SELECT foo52()
-- !query analysis
Project [spark_catalog.default.foo52() AS spark_catalog.default.foo52()#x]
:  +- Project [1 AS 1#x]
:     +- Filter (1 = 0)
:        +- LocalRelation [col1#x]
+- Project
   +- OneRowRelation


-- !query
CREATE FUNCTION foo6c(` a` INT, a INT, `a b` INT) RETURNS INT RETURN 1
-- !query analysis
CreateSQLFunctionCommand spark_catalog.default.foo6c, ` a` INT, a INT, `a b` INT, INT, 1, false, false, false, false


-- !query
SELECT foo6c(1, 2, 3)
-- !query analysis
Project [spark_catalog.default.foo6c( a#x, a#x, a b#x) AS spark_catalog.default.foo6c(1, 2, 3)#x]
+- Project [cast(1 as int) AS  a#x, cast(2 as int) AS a#x, cast(3 as int) AS a b#x]
   +- OneRowRelation


-- !query
CREATE FUNCTION foo6d() RETURNS TABLE(` a` INT, a INT, `a b` INT) RETURN SELECT 1, 2, 3
-- !query analysis
CreateSQLFunctionCommand spark_catalog.default.foo6d, ` a` INT, a INT, `a b` INT, SELECT 1, 2, 3, true, false, false, false


-- !query
SELECT * FROM foo6d()
-- !query analysis
Project [ a#x, a#x, a b#x]
+- SQLFunctionNode spark_catalog.default.foo6d
   +- SubqueryAlias foo6d
      +- Project [cast(1#x as int) AS  a#x, cast(2#x as int) AS a#x, cast(3#x as int) AS a b#x]
         +- Project [1 AS 1#x, 2 AS 2#x, 3 AS 3#x]
            +- OneRowRelation


-- !query
CREATE FUNCTION foo7a(a STRING, b STRING, c STRING) RETURNS STRING RETURN
SELECT 'Foo.a: ' || a ||  ' Foo.a: ' || foo7a.a
       || ' T.b: ' ||  b || ' Foo.b: ' || foo7a.b
       || ' T.c: ' || c || ' T.c: ' || t.c FROM VALUES('t.b', 't.c') AS T(b, c)
-- !query analysis
CreateSQLFunctionCommand spark_catalog.default.foo7a, a STRING, b STRING, c STRING, STRING, SELECT 'Foo.a: ' || a ||  ' Foo.a: ' || foo7a.a
       || ' T.b: ' ||  b || ' Foo.b: ' || foo7a.b
       || ' T.c: ' || c || ' T.c: ' || t.c FROM VALUES('t.b', 't.c') AS T(b, c), false, false, false, false


-- !query
SELECT foo7a('Foo.a', 'Foo.b', 'Foo.c')
-- !query analysis
Project [spark_catalog.default.foo7a(a#x, b#x, c#x) AS spark_catalog.default.foo7a(Foo.a, Foo.b, Foo.c)#x]
:  +- Project [concat(concat(concat(concat(concat(concat(concat(concat(concat(concat(concat(Foo.a: , outer(a#x)),  Foo.a: ), outer(a#x)),  T.b: ), b#x),  Foo.b: ), outer(b#x)),  T.c: ), c#x),  T.c: ), c#x) AS concat(concat(concat(concat(concat(concat(concat(concat(concat(concat(concat(Foo.a: , outer(foo7a.a)),  Foo.a: ), outer(foo7a.a)),  T.b: ), b),  Foo.b: ), outer(foo7a.b)),  T.c: ), c),  T.c: ), c)#x]
:     +- SubqueryAlias T
:        +- LocalRelation [b#x, c#x]
+- Project [cast(Foo.a as string) AS a#x, cast(Foo.b as string) AS b#x, cast(Foo.c as string) AS c#x]
   +- OneRowRelation


-- !query
CREATE FUNCTION foo7at(a STRING, b STRING, c STRING) RETURNS TABLE (a STRING, b STRING, c STRING, d STRING, e STRING) RETURN
SELECT CONCAT('Foo.a: ', a), CONCAT('Foo.b: ', foo7at.b), CONCAT('T.b: ', b),
       CONCAT('Foo.c: ', foo7at.c), CONCAT('T.c: ', c)
FROM VALUES ('t.b', 't.c') AS T(b, c)
-- !query analysis
CreateSQLFunctionCommand spark_catalog.default.foo7at, a STRING, b STRING, c STRING, a STRING, b STRING, c STRING, d STRING, e STRING, SELECT CONCAT('Foo.a: ', a), CONCAT('Foo.b: ', foo7at.b), CONCAT('T.b: ', b),
       CONCAT('Foo.c: ', foo7at.c), CONCAT('T.c: ', c)
FROM VALUES ('t.b', 't.c') AS T(b, c), true, false, false, false


-- !query
SELECT * FROM foo7at('Foo.a', 'Foo.b', 'Foo.c')
-- !query analysis
Project [a#x, b#x, c#x, d#x, e#x]
+- SQLFunctionNode spark_catalog.default.foo7at
   +- SubqueryAlias foo7at
      +- Project [cast(concat(Foo.a: , outer(foo7at.a))#x as string) AS a#x, cast(concat(Foo.b: , outer(foo7at.b))#x as string) AS b#x, cast(concat(T.b: , b)#x as string) AS c#x, cast(concat(Foo.c: , outer(foo7at.c))#x as string) AS d#x, cast(concat(T.c: , c)#x as string) AS e#x]
         +- Project [concat(Foo.a: , cast(Foo.a as string)) AS concat(Foo.a: , outer(foo7at.a))#x, concat(Foo.b: , cast(Foo.b as string)) AS concat(Foo.b: , outer(foo7at.b))#x, concat(T.b: , b#x) AS concat(T.b: , b)#x, concat(Foo.c: , cast(Foo.c as string)) AS concat(Foo.c: , outer(foo7at.c))#x, concat(T.c: , c#x) AS concat(T.c: , c)#x]
            +- SubqueryAlias T
               +- LocalRelation [b#x, c#x]


-- !query
CREATE FUNCTION foo9a(a BOOLEAN) RETURNS BOOLEAN RETURN NOT a
-- !query analysis
CreateSQLFunctionCommand spark_catalog.default.foo9a, a BOOLEAN, BOOLEAN, NOT a, false, false, false, false


-- !query
SELECT foo9a(true)
-- !query analysis
Project [spark_catalog.default.foo9a(a#x) AS spark_catalog.default.foo9a(true)#x]
+- Project [cast(true as boolean) AS a#x]
   +- OneRowRelation


-- !query
SELECT foo9a(5)
-- !query analysis
Project [spark_catalog.default.foo9a(a#x) AS spark_catalog.default.foo9a(5)#x]
+- Project [cast(5 as boolean) AS a#x]
   +- OneRowRelation


-- !query
SELECT foo9a('Nonsense')
-- !query analysis
Project [spark_catalog.default.foo9a(a#x) AS spark_catalog.default.foo9a(Nonsense)#x]
+- Project [cast(Nonsense as boolean) AS a#x]
   +- OneRowRelation


-- !query
CREATE FUNCTION foo9b(a BYTE) RETURNS BYTE RETURN CAST(a AS SHORT) + 1
-- !query analysis
CreateSQLFunctionCommand spark_catalog.default.foo9b, a BYTE, BYTE, CAST(a AS SHORT) + 1, false, false, false, false


-- !query
SELECT foo9b(126)
-- !query analysis
Project [spark_catalog.default.foo9b(a#x) AS spark_catalog.default.foo9b(126)#x]
+- Project [cast(126 as tinyint) AS a#x]
   +- OneRowRelation


-- !query
SELECT foo9b(127)
-- !query analysis
Project [spark_catalog.default.foo9b(a#x) AS spark_catalog.default.foo9b(127)#x]
+- Project [cast(127 as tinyint) AS a#x]
   +- OneRowRelation


-- !query
SELECT foo9b(128)
-- !query analysis
Project [spark_catalog.default.foo9b(a#x) AS spark_catalog.default.foo9b(128)#x]
+- Project [cast(128 as tinyint) AS a#x]
   +- OneRowRelation


-- !query
CREATE FUNCTION foo9c(a SHORT) RETURNS SHORT RETURN CAST(a AS INTEGER) + 1
-- !query analysis
CreateSQLFunctionCommand spark_catalog.default.foo9c, a SHORT, SHORT, CAST(a AS INTEGER) + 1, false, false, false, false


-- !query
SELECT foo9c(32766)
-- !query analysis
Project [spark_catalog.default.foo9c(a#x) AS spark_catalog.default.foo9c(32766)#x]
+- Project [cast(32766 as smallint) AS a#x]
   +- OneRowRelation


-- !query
SELECT foo9c(32767)
-- !query analysis
Project [spark_catalog.default.foo9c(a#x) AS spark_catalog.default.foo9c(32767)#x]
+- Project [cast(32767 as smallint) AS a#x]
   +- OneRowRelation


-- !query
SELECT foo9c(32768)
-- !query analysis
Project [spark_catalog.default.foo9c(a#x) AS spark_catalog.default.foo9c(32768)#x]
+- Project [cast(32768 as smallint) AS a#x]
   +- OneRowRelation


-- !query
CREATE FUNCTION foo9d(a INTEGER) RETURNS INTEGER RETURN CAST(a AS BIGINT) + 1
-- !query analysis
CreateSQLFunctionCommand spark_catalog.default.foo9d, a INTEGER, INTEGER, CAST(a AS BIGINT) + 1, false, false, false, false


-- !query
SELECT foo9d(2147483647 - 1)
-- !query analysis
Project [spark_catalog.default.foo9d(a#x) AS spark_catalog.default.foo9d((2147483647 - 1))#x]
+- Project [cast((2147483647 - 1) as int) AS a#x]
   +- OneRowRelation


-- !query
SELECT foo9d(2147483647)
-- !query analysis
Project [spark_catalog.default.foo9d(a#x) AS spark_catalog.default.foo9d(2147483647)#x]
+- Project [cast(2147483647 as int) AS a#x]
   +- OneRowRelation


-- !query
SELECT foo9d(2147483647 + 1)
-- !query analysis
Project [spark_catalog.default.foo9d(a#x) AS spark_catalog.default.foo9d((2147483647 + 1))#x]
+- Project [cast((2147483647 + 1) as int) AS a#x]
   +- OneRowRelation


-- !query
CREATE FUNCTION foo9e(a BIGINT) RETURNS BIGINT RETURN CAST(a AS DECIMAL(20, 0)) + 1
-- !query analysis
CreateSQLFunctionCommand spark_catalog.default.foo9e, a BIGINT, BIGINT, CAST(a AS DECIMAL(20, 0)) + 1, false, false, false, false


-- !query
SELECT foo9e(9223372036854775807 - 1)
-- !query analysis
Project [spark_catalog.default.foo9e(a#xL) AS spark_catalog.default.foo9e((9223372036854775807 - 1))#xL]
+- Project [cast((9223372036854775807 - cast(1 as bigint)) as bigint) AS a#xL]
   +- OneRowRelation


-- !query
SELECT foo9e(9223372036854775807)
-- !query analysis
Project [spark_catalog.default.foo9e(a#xL) AS spark_catalog.default.foo9e(9223372036854775807)#xL]
+- Project [cast(9223372036854775807 as bigint) AS a#xL]
   +- OneRowRelation


-- !query
SELECT foo9e(9223372036854775807.0 + 1)
-- !query analysis
Project [spark_catalog.default.foo9e(a#xL) AS spark_catalog.default.foo9e((9223372036854775807.0 + 1))#xL]
+- Project [cast((9223372036854775807.0 + cast(1 as decimal(1,0))) as bigint) AS a#xL]
   +- OneRowRelation


-- !query
CREATE FUNCTION foo9f(a DECIMAL( 5, 2 )) RETURNS DECIMAL (5, 2) RETURN CAST(a AS DECIMAL(6, 2)) + 1
-- !query analysis
CreateSQLFunctionCommand spark_catalog.default.foo9f, a DECIMAL( 5, 2 ), DECIMAL (5, 2), CAST(a AS DECIMAL(6, 2)) + 1, false, false, false, false


-- !query
SELECT foo9f(999 - 1)
-- !query analysis
Project [spark_catalog.default.foo9f(a#x) AS spark_catalog.default.foo9f((999 - 1))#x]
+- Project [cast((999 - 1) as decimal(5,2)) AS a#x]
   +- OneRowRelation


-- !query
SELECT foo9f(999)
-- !query analysis
Project [spark_catalog.default.foo9f(a#x) AS spark_catalog.default.foo9f(999)#x]
+- Project [cast(999 as decimal(5,2)) AS a#x]
   +- OneRowRelation


-- !query
SELECT foo9f(999 + 1)
-- !query analysis
Project [spark_catalog.default.foo9f(a#x) AS spark_catalog.default.foo9f((999 + 1))#x]
+- Project [cast((999 + 1) as decimal(5,2)) AS a#x]
   +- OneRowRelation


-- !query
CREATE FUNCTION foo9g(a FLOAT, b String) RETURNS FLOAT RETURN b || CAST(a AS String)
-- !query analysis
CreateSQLFunctionCommand spark_catalog.default.foo9g, a FLOAT, b String, FLOAT, b || CAST(a AS String), false, false, false, false


-- !query
SELECT foo9g(123.23, '7')
-- !query analysis
Project [spark_catalog.default.foo9g(a#x, b#x) AS spark_catalog.default.foo9g(123.23, 7)#x]
+- Project [cast(123.23 as float) AS a#x, cast(7 as string) AS b#x]
   +- OneRowRelation


-- !query
SELECT foo9g('hello', '7')
-- !query analysis
Project [spark_catalog.default.foo9g(a#x, b#x) AS spark_catalog.default.foo9g(hello, 7)#x]
+- Project [cast(hello as float) AS a#x, cast(7 as string) AS b#x]
   +- OneRowRelation


-- !query
SELECT foo9g(123.23, 'q')
-- !query analysis
Project [spark_catalog.default.foo9g(a#x, b#x) AS spark_catalog.default.foo9g(123.23, q)#x]
+- Project [cast(123.23 as float) AS a#x, cast(q as string) AS b#x]
   +- OneRowRelation


-- !query
CREATE FUNCTION foo9h(a DOUBLE, b String) RETURNS DOUBLE RETURN b || CAST(a AS String)
-- !query analysis
CreateSQLFunctionCommand spark_catalog.default.foo9h, a DOUBLE, b String, DOUBLE, b || CAST(a AS String), false, false, false, false


-- !query
SELECT foo9h(123.23, '7')
-- !query analysis
Project [spark_catalog.default.foo9h(a#x, b#x) AS spark_catalog.default.foo9h(123.23, 7)#x]
+- Project [cast(123.23 as double) AS a#x, cast(7 as string) AS b#x]
   +- OneRowRelation


-- !query
SELECT foo9h('hello', '7')
-- !query analysis
Project [spark_catalog.default.foo9h(a#x, b#x) AS spark_catalog.default.foo9h(hello, 7)#x]
+- Project [cast(hello as double) AS a#x, cast(7 as string) AS b#x]
   +- OneRowRelation


-- !query
SELECT foo9h(123.23, 'q')
-- !query analysis
Project [spark_catalog.default.foo9h(a#x, b#x) AS spark_catalog.default.foo9h(123.23, q)#x]
+- Project [cast(123.23 as double) AS a#x, cast(q as string) AS b#x]
   +- OneRowRelation


-- !query
CREATE FUNCTION foo9i(a VARCHAR(10), b VARCHAR(10)) RETURNS VARCHAR(12) RETURN a || b
-- !query analysis
org.apache.spark.sql.AnalysisException
{
  "errorClass" : "UNSUPPORTED_CHAR_OR_VARCHAR_AS_STRING",
  "sqlState" : "0A000"
}


-- !query
CREATE FUNCTION foo9j(a STRING, b STRING) RETURNS STRING RETURN a || b
-- !query analysis
CreateSQLFunctionCommand spark_catalog.default.foo9j, a STRING, b STRING, STRING, a || b, false, false, false, false


-- !query
SELECT foo9j('1234567890', '12')
-- !query analysis
Project [spark_catalog.default.foo9j(a#x, b#x) AS spark_catalog.default.foo9j(1234567890, 12)#x]
+- Project [cast(1234567890 as string) AS a#x, cast(12 as string) AS b#x]
   +- OneRowRelation


-- !query
SELECT foo9j(12345678901, '12')
-- !query analysis
Project [spark_catalog.default.foo9j(a#x, b#x) AS spark_catalog.default.foo9j(12345678901, 12)#x]
+- Project [cast(12345678901 as string) AS a#x, cast(12 as string) AS b#x]
   +- OneRowRelation


-- !query
CREATE FUNCTION foo9l(a DATE, b INTERVAL) RETURNS DATE RETURN a + b
-- !query analysis
CreateSQLFunctionCommand spark_catalog.default.foo9l, a DATE, b INTERVAL, DATE, a + b, false, false, false, false


-- !query
SELECT foo9l(DATE '2020-02-02', INTERVAL '1' YEAR)
-- !query analysis
org.apache.spark.sql.catalyst.ExtendedAnalysisException
{
  "errorClass" : "DATATYPE_MISMATCH.CAST_WITHOUT_SUGGESTION",
  "sqlState" : "42K09",
  "messageParameters" : {
    "sqlExpr" : "\"INTERVAL '1' YEAR\"",
    "srcType" : "\"INTERVAL YEAR\"",
    "targetType" : "\"INTERVAL\""
  },
  "queryContext" : [ {
    "objectType" : "",
    "objectName" : "",
    "startIndex" : 8,
    "stopIndex" : 50,
    "fragment" : "foo9l(DATE '2020-02-02', INTERVAL '1' YEAR)"
  } ]
}


-- !query
SELECT foo9l('2020-02-02', INTERVAL '1' YEAR)
-- !query analysis
org.apache.spark.sql.catalyst.ExtendedAnalysisException
{
  "errorClass" : "DATATYPE_MISMATCH.CAST_WITHOUT_SUGGESTION",
  "sqlState" : "42K09",
  "messageParameters" : {
    "sqlExpr" : "\"INTERVAL '1' YEAR\"",
    "srcType" : "\"INTERVAL YEAR\"",
    "targetType" : "\"INTERVAL\""
  },
  "queryContext" : [ {
    "objectType" : "",
    "objectName" : "",
    "startIndex" : 8,
    "stopIndex" : 45,
    "fragment" : "foo9l('2020-02-02', INTERVAL '1' YEAR)"
  } ]
}


-- !query
SELECT foo9l(DATE '-7', INTERVAL '1' YEAR)
-- !query analysis
org.apache.spark.sql.catalyst.parser.ParseException
{
  "errorClass" : "INVALID_TYPED_LITERAL",
  "sqlState" : "42604",
  "messageParameters" : {
    "value" : "'-7'",
    "valueType" : "\"DATE\""
  },
  "queryContext" : [ {
    "objectType" : "",
    "objectName" : "",
    "startIndex" : 14,
    "stopIndex" : 22,
    "fragment" : "DATE '-7'"
  } ]
}


-- !query
SELECT foo9l(DATE '2020-02-02', INTERVAL '9999999' YEAR)
-- !query analysis
org.apache.spark.sql.catalyst.ExtendedAnalysisException
{
  "errorClass" : "DATATYPE_MISMATCH.CAST_WITHOUT_SUGGESTION",
  "sqlState" : "42K09",
  "messageParameters" : {
    "sqlExpr" : "\"INTERVAL '9999999' YEAR\"",
    "srcType" : "\"INTERVAL YEAR\"",
    "targetType" : "\"INTERVAL\""
  },
  "queryContext" : [ {
    "objectType" : "",
    "objectName" : "",
    "startIndex" : 8,
    "stopIndex" : 56,
    "fragment" : "foo9l(DATE '2020-02-02', INTERVAL '9999999' YEAR)"
  } ]
}


-- !query
CREATE FUNCTION foo9m(a TIMESTAMP, b INTERVAL) RETURNS TIMESTAMP RETURN a + b
-- !query analysis
CreateSQLFunctionCommand spark_catalog.default.foo9m, a TIMESTAMP, b INTERVAL, TIMESTAMP, a + b, false, false, false, false


-- !query
SELECT foo9m(TIMESTAMP'2020-02-02 12:15:16.123', INTERVAL '1' YEAR)
-- !query analysis
org.apache.spark.sql.catalyst.ExtendedAnalysisException
{
  "errorClass" : "DATATYPE_MISMATCH.CAST_WITHOUT_SUGGESTION",
  "sqlState" : "42K09",
  "messageParameters" : {
    "sqlExpr" : "\"INTERVAL '1' YEAR\"",
    "srcType" : "\"INTERVAL YEAR\"",
    "targetType" : "\"INTERVAL\""
  },
  "queryContext" : [ {
    "objectType" : "",
    "objectName" : "",
    "startIndex" : 8,
    "stopIndex" : 67,
    "fragment" : "foo9m(TIMESTAMP'2020-02-02 12:15:16.123', INTERVAL '1' YEAR)"
  } ]
}


-- !query
SELECT foo9m('2020-02-02 12:15:16.123', INTERVAL '1' YEAR)
-- !query analysis
org.apache.spark.sql.catalyst.ExtendedAnalysisException
{
  "errorClass" : "DATATYPE_MISMATCH.CAST_WITHOUT_SUGGESTION",
  "sqlState" : "42K09",
  "messageParameters" : {
    "sqlExpr" : "\"INTERVAL '1' YEAR\"",
    "srcType" : "\"INTERVAL YEAR\"",
    "targetType" : "\"INTERVAL\""
  },
  "queryContext" : [ {
    "objectType" : "",
    "objectName" : "",
    "startIndex" : 8,
    "stopIndex" : 58,
    "fragment" : "foo9m('2020-02-02 12:15:16.123', INTERVAL '1' YEAR)"
  } ]
}


-- !query
SELECT foo9m(TIMESTAMP'2020-02-02 12:15:16.123', INTERVAL '999999' YEAR)
-- !query analysis
org.apache.spark.sql.catalyst.ExtendedAnalysisException
{
  "errorClass" : "DATATYPE_MISMATCH.CAST_WITHOUT_SUGGESTION",
  "sqlState" : "42K09",
  "messageParameters" : {
    "sqlExpr" : "\"INTERVAL '999999' YEAR\"",
    "srcType" : "\"INTERVAL YEAR\"",
    "targetType" : "\"INTERVAL\""
  },
  "queryContext" : [ {
    "objectType" : "",
    "objectName" : "",
    "startIndex" : 8,
    "stopIndex" : 72,
    "fragment" : "foo9m(TIMESTAMP'2020-02-02 12:15:16.123', INTERVAL '999999' YEAR)"
  } ]
}


-- !query
CREATE FUNCTION foo9n(a ARRAY<INTEGER>) RETURNS ARRAY<INTEGER> RETURN a
-- !query analysis
CreateSQLFunctionCommand spark_catalog.default.foo9n, a ARRAY<INTEGER>, ARRAY<INTEGER>, a, false, false, false, false


-- !query
SELECT foo9n(ARRAY(1, 2, 3))
-- !query analysis
Project [spark_catalog.default.foo9n(a#x) AS spark_catalog.default.foo9n(array(1, 2, 3))#x]
+- Project [cast(array(1, 2, 3) as array<int>) AS a#x]
   +- OneRowRelation


-- !query
SELECT foo9n(from_json('[1, 2, 3]', 'array<int>'))
-- !query analysis
Project [spark_catalog.default.foo9n(a#x) AS spark_catalog.default.foo9n(from_json([1, 2, 3]))#x]
+- Project [cast(from_json(ArrayType(IntegerType,true), [1, 2, 3], Some(America/Los_Angeles), false) as array<int>) AS a#x]
   +- OneRowRelation


-- !query
CREATE FUNCTION foo9o(a MAP<STRING, INTEGER>) RETURNS MAP<STRING, INTEGER> RETURN a
-- !query analysis
CreateSQLFunctionCommand spark_catalog.default.foo9o, a MAP<STRING, INTEGER>, MAP<STRING, INTEGER>, a, false, false, false, false


-- !query
SELECT foo9o(MAP('hello', 1, 'world', 2))
-- !query analysis
Project [spark_catalog.default.foo9o(a#x) AS spark_catalog.default.foo9o(map(hello, 1, world, 2))#x]
+- Project [cast(map(hello, 1, world, 2) as map<string,int>) AS a#x]
   +- OneRowRelation


-- !query
SELECT foo9o(from_json('{"hello":1, "world":2}', 'map<string,int>'))
-- !query analysis
Project [spark_catalog.default.foo9o(a#x) AS spark_catalog.default.foo9o(entries)#x]
+- Project [cast(from_json(MapType(StringType,IntegerType,true), {"hello":1, "world":2}, Some(America/Los_Angeles), false) as map<string,int>) AS a#x]
   +- OneRowRelation


-- !query
CREATE FUNCTION foo9p(a STRUCT<a1: INTEGER, a2: STRING>) RETURNS STRUCT<a1: INTEGER, a2: STRING> RETURN a
-- !query analysis
CreateSQLFunctionCommand spark_catalog.default.foo9p, a STRUCT<a1: INTEGER, a2: STRING>, STRUCT<a1: INTEGER, a2: STRING>, a, false, false, false, false


-- !query
SELECT foo9p(STRUCT(1, 'hello'))
-- !query analysis
Project [spark_catalog.default.foo9p(a#x) AS spark_catalog.default.foo9p(struct(1, hello))#x]
+- Project [cast(struct(col1, 1, col2, hello) as struct<a1:int,a2:string>) AS a#x]
   +- OneRowRelation


-- !query
SELECT foo9p(from_json('{1:"hello"}', 'struct<a1:int, a2:string>'))
-- !query analysis
Project [spark_catalog.default.foo9p(a#x) AS spark_catalog.default.foo9p(from_json({1:"hello"}))#x]
+- Project [cast(from_json(StructField(a1,IntegerType,true), StructField(a2,StringType,true), {1:"hello"}, Some(America/Los_Angeles), false) as struct<a1:int,a2:string>) AS a#x]
   +- OneRowRelation


-- !query
CREATE FUNCTION foo9q(a ARRAY<STRUCT<a1: INT, a2: STRING>>) RETURNS ARRAY<STRUCT<a1: INT, a2: STRING>> RETURN a
-- !query analysis
CreateSQLFunctionCommand spark_catalog.default.foo9q, a ARRAY<STRUCT<a1: INT, a2: STRING>>, ARRAY<STRUCT<a1: INT, a2: STRING>>, a, false, false, false, false


-- !query
SELECT foo9q(ARRAY(STRUCT(1, 'hello'), STRUCT(2, 'world')))
-- !query analysis
Project [spark_catalog.default.foo9q(a#x) AS spark_catalog.default.foo9q(array(struct(1, hello), struct(2, world)))#x]
+- Project [cast(array(struct(col1, 1, col2, hello), struct(col1, 2, col2, world)) as array<struct<a1:int,a2:string>>) AS a#x]
   +- OneRowRelation


-- !query
SELECT foo9q(ARRAY(NAMED_STRUCT('x', 1, 'y', 'hello'), NAMED_STRUCT('x', 2, 'y', 'world')))
-- !query analysis
Project [spark_catalog.default.foo9q(a#x) AS spark_catalog.default.foo9q(array(named_struct(x, 1, y, hello), named_struct(x, 2, y, world)))#x]
+- Project [cast(array(named_struct(x, 1, y, hello), named_struct(x, 2, y, world)) as array<struct<a1:int,a2:string>>) AS a#x]
   +- OneRowRelation


-- !query
SELECT foo9q(from_json('[{1:"hello"}, {2:"world"}]', 'array<struct<a1:int,a2:string>>'))
-- !query analysis
Project [spark_catalog.default.foo9q(a#x) AS spark_catalog.default.foo9q(from_json([{1:"hello"}, {2:"world"}]))#x]
+- Project [cast(from_json(ArrayType(StructType(StructField(a1,IntegerType,true),StructField(a2,StringType,true)),true), [{1:"hello"}, {2:"world"}], Some(America/Los_Angeles), false) as array<struct<a1:int,a2:string>>) AS a#x]
   +- OneRowRelation


-- !query
CREATE FUNCTION foo9r(a ARRAY<MAP<STRING, INT>>) RETURNS ARRAY<MAP<STRING, INT>> RETURN a
-- !query analysis
CreateSQLFunctionCommand spark_catalog.default.foo9r, a ARRAY<MAP<STRING, INT>>, ARRAY<MAP<STRING, INT>>, a, false, false, false, false


-- !query
SELECT foo9r(ARRAY(MAP('hello', 1), MAP('world', 2)))
-- !query analysis
Project [spark_catalog.default.foo9r(a#x) AS spark_catalog.default.foo9r(array(map(hello, 1), map(world, 2)))#x]
+- Project [cast(array(map(hello, 1), map(world, 2)) as array<map<string,int>>) AS a#x]
   +- OneRowRelation


-- !query
SELECT foo9r(from_json('[{"hello":1}, {"world":2}]', 'array<map<string,int>>'))
-- !query analysis
Project [spark_catalog.default.foo9r(a#x) AS spark_catalog.default.foo9r(from_json([{"hello":1}, {"world":2}]))#x]
+- Project [cast(from_json(ArrayType(MapType(StringType,IntegerType,true),true), [{"hello":1}, {"world":2}], Some(America/Los_Angeles), false) as array<map<string,int>>) AS a#x]
   +- OneRowRelation


-- !query
CREATE OR REPLACE FUNCTION foo1_10(a INT) RETURNS INT RETURN a + 2
-- !query analysis
CreateSQLFunctionCommand spark_catalog.default.foo1_10, a INT, INT, a + 2, false, false, false, true


-- !query
CREATE OR REPLACE FUNCTION bar1_10(b INT) RETURNS STRING RETURN foo1_10(TRY_CAST(b AS STRING))
-- !query analysis
CreateSQLFunctionCommand spark_catalog.default.bar1_10, b INT, STRING, foo1_10(TRY_CAST(b AS STRING)), false, false, false, true


-- !query
SELECT bar1_10(3)
-- !query analysis
Project [spark_catalog.default.bar1_10(b#x) AS spark_catalog.default.bar1_10(3)#x]
+- Project [b#x, cast(try_cast(b#x as string) as int) AS a#x]
   +- Project [cast(3 as int) AS b#x]
      +- OneRowRelation


-- !query
CREATE OR REPLACE FUNCTION foo1_11a() RETURN 42
-- !query analysis
CreateSQLFunctionCommand spark_catalog.default.foo1_11a, , 42, false, false, false, true


-- !query
SELECT foo1_11a()
-- !query analysis
Project [spark_catalog.default.foo1_11a() AS spark_catalog.default.foo1_11a()#x]
+- Project
   +- OneRowRelation


-- !query
CREATE OR REPLACE FUNCTION foo1_11b() RETURN 'hello world'
-- !query analysis
CreateSQLFunctionCommand spark_catalog.default.foo1_11b, , 'hello world', false, false, false, true


-- !query
SELECT foo1_11b()
-- !query analysis
Project [spark_catalog.default.foo1_11b() AS spark_catalog.default.foo1_11b()#x]
+- Project
   +- OneRowRelation


-- !query
CREATE OR REPLACE FUNCTION foo1_11c(a INT, b INT) RETURN a + b
-- !query analysis
CreateSQLFunctionCommand spark_catalog.default.foo1_11c, a INT, b INT, , a + b, false, false, false, true


-- !query
SELECT foo1_11c(3, 5)
-- !query analysis
Project [spark_catalog.default.foo1_11c(a#x, b#x) AS spark_catalog.default.foo1_11c(3, 5)#x]
+- Project [cast(3 as int) AS a#x, cast(5 as int) AS b#x]
   +- OneRowRelation


-- !query
CREATE OR REPLACE FUNCTION foo1_11d(a DOUBLE, b INT) RETURN a * b + 1.5
-- !query analysis
CreateSQLFunctionCommand spark_catalog.default.foo1_11d, a DOUBLE, b INT, , a * b + 1.5, false, false, false, true


-- !query
SELECT foo1_11d(3.0, 5)
-- !query analysis
Project [spark_catalog.default.foo1_11d(a#x, b#x) AS spark_catalog.default.foo1_11d(3.0, 5)#x]
+- Project [cast(3.0 as double) AS a#x, cast(5 as int) AS b#x]
   +- OneRowRelation


-- !query
CREATE OR REPLACE FUNCTION foo1_11e(a INT) RETURN a > 10
-- !query analysis
CreateSQLFunctionCommand spark_catalog.default.foo1_11e, a INT, , a > 10, false, false, false, true


-- !query
SELECT foo1_11e(15), foo1_11e(5)
-- !query analysis
Project [spark_catalog.default.foo1_11e(a#x) AS spark_catalog.default.foo1_11e(15)#x, spark_catalog.default.foo1_11e(a#x) AS spark_catalog.default.foo1_11e(5)#x]
+- Project [cast(15 as int) AS a#x, cast(5 as int) AS a#x]
   +- OneRowRelation


-- !query
CREATE OR REPLACE FUNCTION foo1_11f(d DATE) RETURN d + INTERVAL '1' DAY
-- !query analysis
CreateSQLFunctionCommand spark_catalog.default.foo1_11f, d DATE, , d + INTERVAL '1' DAY, false, false, false, true


-- !query
SELECT foo1_11f(DATE '2024-01-01')
-- !query analysis
[Analyzer test output redacted due to nondeterminism]


-- !query
CREATE OR REPLACE FUNCTION foo1_11g(n INT) RETURN ARRAY(1, 2, n)
-- !query analysis
CreateSQLFunctionCommand spark_catalog.default.foo1_11g, n INT, , ARRAY(1, 2, n), false, false, false, true


-- !query
SELECT foo1_11g(5)
-- !query analysis
Project [spark_catalog.default.foo1_11g(n#x) AS spark_catalog.default.foo1_11g(5)#x]
+- Project [cast(5 as int) AS n#x]
   +- OneRowRelation


-- !query
CREATE OR REPLACE FUNCTION foo1_11h(a INT, b STRING) RETURN STRUCT(a, b)
-- !query analysis
CreateSQLFunctionCommand spark_catalog.default.foo1_11h, a INT, b STRING, , STRUCT(a, b), false, false, false, true


-- !query
SELECT foo1_11h(1, 'test')
-- !query analysis
Project [spark_catalog.default.foo1_11h(a#x, b#x) AS spark_catalog.default.foo1_11h(1, test)#x]
+- Project [cast(1 as int) AS a#x, cast(test as string) AS b#x]
   +- OneRowRelation


-- !query
CREATE OR REPLACE FUNCTION foo1_11i(x INT) RETURN (SELECT x * 2)
-- !query analysis
CreateSQLFunctionCommand spark_catalog.default.foo1_11i, x INT, , (SELECT x * 2), false, false, false, true


-- !query
SELECT foo1_11i(5)
-- !query analysis
Project [spark_catalog.default.foo1_11i(x#x) AS spark_catalog.default.foo1_11i(5)#x]
+- Project [cast(5 as int) AS x#x]
   +- OneRowRelation


-- !query
CREATE OR REPLACE FUNCTION foo1_11j(s STRING) RETURN UPPER(s)
-- !query analysis
CreateSQLFunctionCommand spark_catalog.default.foo1_11j, s STRING, , UPPER(s), false, false, false, true


-- !query
SELECT foo1_11j('hello')
-- !query analysis
Project [spark_catalog.default.foo1_11j(s#x) AS spark_catalog.default.foo1_11j(hello)#x]
+- Project [cast(hello as string) AS s#x]
   +- OneRowRelation


-- !query
CREATE OR REPLACE FUNCTION foo1_11k(a INT, b STRING) RETURN CONCAT(CAST(a AS STRING), '_', b)
-- !query analysis
CreateSQLFunctionCommand spark_catalog.default.foo1_11k, a INT, b STRING, , CONCAT(CAST(a AS STRING), '_', b), false, false, false, true


-- !query
SELECT foo1_11k(123, 'test')
-- !query analysis
Project [spark_catalog.default.foo1_11k(a#x, b#x) AS spark_catalog.default.foo1_11k(123, test)#x]
+- Project [cast(123 as int) AS a#x, cast(test as string) AS b#x]
   +- OneRowRelation


-- !query
CREATE OR REPLACE FUNCTION foo1_11l() RETURNS TABLE RETURN SELECT 1 as id, 'hello' as name
-- !query analysis
CreateSQLFunctionCommand spark_catalog.default.foo1_11l, TABLE, SELECT 1 as id, 'hello' as name, true, false, false, true


-- !query
SELECT * FROM foo1_11l()
-- !query analysis
Project [id#x, name#x]
+- SQLFunctionNode spark_catalog.default.foo1_11l
   +- SubqueryAlias foo1_11l
      +- Project [cast(id#x as int) AS id#x, cast(name#x as string) AS name#x]
         +- Project [1 AS id#x, hello AS name#x]
            +- OneRowRelation


-- !query
CREATE OR REPLACE FUNCTION foo1_11m(a INT, b STRING) RETURNS TABLE RETURN SELECT a * 2 as doubled, UPPER(b) as upper_name
-- !query analysis
CreateSQLFunctionCommand spark_catalog.default.foo1_11m, a INT, b STRING, TABLE, SELECT a * 2 as doubled, UPPER(b) as upper_name, true, false, false, true


-- !query
SELECT * FROM foo1_11m(5, 'world')
-- !query analysis
Project [doubled#x, upper_name#x]
+- SQLFunctionNode spark_catalog.default.foo1_11m
   +- SubqueryAlias foo1_11m
      +- Project [cast(doubled#x as int) AS doubled#x, cast(upper_name#x as string) AS upper_name#x]
         +- Project [(cast(5 as int) * 2) AS doubled#x, upper(cast(world as string)) AS upper_name#x]
            +- OneRowRelation


-- !query
CREATE OR REPLACE FUNCTION foo1_11n(arr ARRAY<INT>) RETURNS TABLE RETURN SELECT size(arr) as array_size, arr[0] as first_element
-- !query analysis
CreateSQLFunctionCommand spark_catalog.default.foo1_11n, arr ARRAY<INT>, TABLE, SELECT size(arr) as array_size, arr[0] as first_element, true, false, false, true


-- !query
SELECT * FROM foo1_11n(ARRAY(1, 2, 3))
-- !query analysis
Project [array_size#x, first_element#x]
+- SQLFunctionNode spark_catalog.default.foo1_11n
   +- SubqueryAlias foo1_11n
      +- Project [cast(array_size#x as int) AS array_size#x, cast(first_element#x as int) AS first_element#x]
         +- Project [size(cast(array(1, 2, 3) as array<int>), false) AS array_size#x, cast(array(1, 2, 3) as array<int>)[0] AS first_element#x]
            +- OneRowRelation


-- !query
CREATE OR REPLACE FUNCTION foo1_11o(id INT, name STRING) RETURNS TABLE RETURN SELECT STRUCT(id, name) as person_info, id + 100 as modified_id
-- !query analysis
CreateSQLFunctionCommand spark_catalog.default.foo1_11o, id INT, name STRING, TABLE, SELECT STRUCT(id, name) as person_info, id + 100 as modified_id, true, false, false, true


-- !query
SELECT * FROM foo1_11o(1, 'Alice')
-- !query analysis
Project [person_info#x, modified_id#x]
+- SQLFunctionNode spark_catalog.default.foo1_11o
   +- SubqueryAlias foo1_11o
      +- Project [cast(person_info#x as struct<id:int,name:string>) AS person_info#x, cast(modified_id#x as int) AS modified_id#x]
         +- Project [struct(id, cast(1 as int), name, cast(Alice as string)) AS person_info#x, (cast(1 as int) + 100) AS modified_id#x]
            +- OneRowRelation


-- !query
CREATE FUNCTION foo2_1a(a INT) RETURNS INT RETURN a
-- !query analysis
CreateSQLFunctionCommand spark_catalog.default.foo2_1a, a INT, INT, a, false, false, false, false


-- !query
SELECT foo2_1a(5)
-- !query analysis
Project [spark_catalog.default.foo2_1a(a#x) AS spark_catalog.default.foo2_1a(5)#x]
+- Project [cast(5 as int) AS a#x]
   +- OneRowRelation


-- !query
CREATE FUNCTION foo2_1b(a INT, b INT) RETURNS INT RETURN a + b
-- !query analysis
CreateSQLFunctionCommand spark_catalog.default.foo2_1b, a INT, b INT, INT, a + b, false, false, false, false


-- !query
SELECT foo2_1b(5, 6)
-- !query analysis
Project [spark_catalog.default.foo2_1b(a#x, b#x) AS spark_catalog.default.foo2_1b(5, 6)#x]
+- Project [cast(5 as int) AS a#x, cast(6 as int) AS b#x]
   +- OneRowRelation


-- !query
CREATE FUNCTION foo2_1c(a INT, b INT) RETURNS INT RETURN 10 * (a + b) + 100 * (a -b)
-- !query analysis
CreateSQLFunctionCommand spark_catalog.default.foo2_1c, a INT, b INT, INT, 10 * (a + b) + 100 * (a -b), false, false, false, false


-- !query
SELECT foo2_1c(5, 6)
-- !query analysis
Project [spark_catalog.default.foo2_1c(a#x, b#x) AS spark_catalog.default.foo2_1c(5, 6)#x]
+- Project [cast(5 as int) AS a#x, cast(6 as int) AS b#x]
   +- OneRowRelation


-- !query
CREATE FUNCTION foo2_1d(a INT, b INT) RETURNS INT RETURN ABS(a) - LENGTH(CAST(b AS VARCHAR(10)))
-- !query analysis
CreateSQLFunctionCommand spark_catalog.default.foo2_1d, a INT, b INT, INT, ABS(a) - LENGTH(CAST(b AS VARCHAR(10))), false, false, false, false


-- !query
SELECT foo2_1d(-5, 6)
-- !query analysis
Project [spark_catalog.default.foo2_1d(a#x, b#x) AS spark_catalog.default.foo2_1d(-5, 6)#x]
+- Project [cast(-5 as int) AS a#x, cast(6 as int) AS b#x]
   +- OneRowRelation


-- !query
CREATE FUNCTION foo2_2a(a INT) RETURNS INT RETURN SELECT a
-- !query analysis
CreateSQLFunctionCommand spark_catalog.default.foo2_2a, a INT, INT, SELECT a, false, false, false, false


-- !query
SELECT foo2_2a(5)
-- !query analysis
Project [spark_catalog.default.foo2_2a(a#x) AS spark_catalog.default.foo2_2a(5)#x]
+- Project [cast(5 as int) AS a#x]
   +- OneRowRelation


-- !query
CREATE FUNCTION foo2_2b(a INT) RETURNS INT RETURN 1 + (SELECT a)
-- !query analysis
CreateSQLFunctionCommand spark_catalog.default.foo2_2b, a INT, INT, 1 + (SELECT a), false, false, false, false


-- !query
SELECT foo2_2b(5)
-- !query analysis
Project [spark_catalog.default.foo2_2b(a#x) AS spark_catalog.default.foo2_2b(5)#x]
:  +- Project [outer(a#x)]
:     +- OneRowRelation
+- Project [cast(5 as int) AS a#x]
   +- OneRowRelation


-- !query
CREATE FUNCTION foo2_2c(a INT) RETURNS INT RETURN 1 + (SELECT (SELECT a))
-- !query analysis
org.apache.spark.sql.catalyst.ExtendedAnalysisException
{
  "errorClass" : "UNRESOLVED_COLUMN.WITHOUT_SUGGESTION",
  "sqlState" : "42703",
  "messageParameters" : {
    "objectName" : "`a`"
  },
  "queryContext" : [ {
    "objectType" : "",
    "objectName" : "",
    "startIndex" : 21,
    "stopIndex" : 21,
    "fragment" : "a"
  } ]
}


-- !query
CREATE FUNCTION foo2_2d(a INT) RETURNS INT RETURN 1 + (SELECT (SELECT (SELECT (SELECT a))))
-- !query analysis
org.apache.spark.sql.catalyst.ExtendedAnalysisException
{
  "errorClass" : "UNRESOLVED_COLUMN.WITHOUT_SUGGESTION",
  "sqlState" : "42703",
  "messageParameters" : {
    "objectName" : "`a`"
  },
  "queryContext" : [ {
    "objectType" : "",
    "objectName" : "",
    "startIndex" : 37,
    "stopIndex" : 37,
    "fragment" : "a"
  } ]
}


-- !query
CREATE FUNCTION foo2_2e(a INT) RETURNS INT RETURN
SELECT a FROM (VALUES 1) AS V(c1) WHERE c1 = 2
UNION ALL
SELECT a + 1 FROM (VALUES 1) AS V(c1)
-- !query analysis
CreateSQLFunctionCommand spark_catalog.default.foo2_2e, a INT, INT, SELECT a FROM (VALUES 1) AS V(c1) WHERE c1 = 2
UNION ALL
SELECT a + 1 FROM (VALUES 1) AS V(c1), false, false, false, false


-- !query
CREATE FUNCTION foo2_2f(a INT) RETURNS INT RETURN
SELECT a FROM (VALUES 1) AS V(c1)
EXCEPT
SELECT a + 1 FROM (VALUES 1) AS V(a)
-- !query analysis
CreateSQLFunctionCommand spark_catalog.default.foo2_2f, a INT, INT, SELECT a FROM (VALUES 1) AS V(c1)
EXCEPT
SELECT a + 1 FROM (VALUES 1) AS V(a), false, false, false, false


-- !query
CREATE FUNCTION foo2_2g(a INT) RETURNS INT RETURN
SELECT a FROM (VALUES 1) AS V(c1)
INTERSECT
SELECT a FROM (VALUES 1) AS V(a)
-- !query analysis
CreateSQLFunctionCommand spark_catalog.default.foo2_2g, a INT, INT, SELECT a FROM (VALUES 1) AS V(c1)
INTERSECT
SELECT a FROM (VALUES 1) AS V(a), false, false, false, false


-- !query
DROP TABLE IF EXISTS t1
-- !query analysis
DropTable true, false
+- ResolvedIdentifier V2SessionCatalog(spark_catalog), default.t1


-- !query
DROP TABLE IF EXISTS t2
-- !query analysis
DropTable true, false
+- ResolvedIdentifier V2SessionCatalog(spark_catalog), default.t2


-- !query
DROP TABLE IF EXISTS ts
-- !query analysis
org.apache.spark.sql.AnalysisException
{
  "errorClass" : "WRONG_COMMAND_FOR_OBJECT_TYPE",
  "sqlState" : "42809",
  "messageParameters" : {
    "alternative" : "DROP VIEW",
    "foundType" : "VIEW",
    "objectName" : "spark_catalog.default.ts",
    "operation" : "DROP TABLE",
    "requiredType" : "EXTERNAL or MANAGED"
  }
}


-- !query
DROP TABLE IF EXISTS tm
-- !query analysis
org.apache.spark.sql.AnalysisException
{
  "errorClass" : "WRONG_COMMAND_FOR_OBJECT_TYPE",
  "sqlState" : "42809",
  "messageParameters" : {
    "alternative" : "DROP VIEW",
    "foundType" : "VIEW",
    "objectName" : "spark_catalog.default.tm",
    "operation" : "DROP TABLE",
    "requiredType" : "EXTERNAL or MANAGED"
  }
}


-- !query
DROP TABLE IF EXISTS ta
-- !query analysis
org.apache.spark.sql.AnalysisException
{
  "errorClass" : "WRONG_COMMAND_FOR_OBJECT_TYPE",
  "sqlState" : "42809",
  "messageParameters" : {
    "alternative" : "DROP VIEW",
    "foundType" : "VIEW",
    "objectName" : "spark_catalog.default.ta",
    "operation" : "DROP TABLE",
    "requiredType" : "EXTERNAL or MANAGED"
  }
}


-- !query
DROP TABLE IF EXISTS V1
-- !query analysis
DropTable true, false
+- ResolvedIdentifier V2SessionCatalog(spark_catalog), default.V1


-- !query
DROP TABLE IF EXISTS V2
-- !query analysis
DropTable true, false
+- ResolvedIdentifier V2SessionCatalog(spark_catalog), default.V2


-- !query
DROP VIEW IF EXISTS t1
-- !query analysis
DropTableCommand `spark_catalog`.`default`.`t1`, true, true, false


-- !query
DROP VIEW IF EXISTS t2
-- !query analysis
DropTableCommand `spark_catalog`.`default`.`t2`, true, true, false


-- !query
DROP VIEW IF EXISTS ts
-- !query analysis
DropTableCommand `spark_catalog`.`default`.`ts`, true, true, false


-- !query
DROP VIEW IF EXISTS tm
-- !query analysis
DropTableCommand `spark_catalog`.`default`.`tm`, true, true, false


-- !query
DROP VIEW IF EXISTS ta
-- !query analysis
DropTableCommand `spark_catalog`.`default`.`ta`, true, true, false


-- !query
DROP VIEW IF EXISTS V1
-- !query analysis
DropTableCommand `spark_catalog`.`default`.`V1`, true, true, false


-- !query
DROP VIEW IF EXISTS V2
-- !query analysis
DropTableCommand `spark_catalog`.`default`.`V2`, true, true, false


-- !query
CREATE FUNCTION foo2_3(a INT, b INT) RETURNS INT RETURN a + b
-- !query analysis
CreateSQLFunctionCommand spark_catalog.default.foo2_3, a INT, b INT, INT, a + b, false, false, false, false


-- !query
CREATE VIEW V1(c1, c2) AS VALUES (1, 2), (3, 4), (5, 6)
-- !query analysis
CreateViewCommand `spark_catalog`.`default`.`V1`, [(c1,None), (c2,None)], VALUES (1, 2), (3, 4), (5, 6), false, false, PersistedView, COMPENSATION, true
   +- LocalRelation [col1#x, col2#x]


-- !query
CREATE VIEW V2(c1, c2) AS VALUES (-1, -2), (-3, -4), (-5, -6)
-- !query analysis
CreateViewCommand `spark_catalog`.`default`.`V2`, [(c1,None), (c2,None)], VALUES (-1, -2), (-3, -4), (-5, -6), false, false, PersistedView, COMPENSATION, true
   +- LocalRelation [col1#x, col2#x]


-- !query
SELECT foo2_3(c1, c2), foo2_3(c2, 1), foo2_3(c1, c2) - foo2_3(c2, c1 - 1) FROM V1 ORDER BY 1, 2, 3
-- !query analysis
Sort [spark_catalog.default.foo2_3(c1, c2)#x ASC NULLS FIRST, spark_catalog.default.foo2_3(c2, 1)#x ASC NULLS FIRST, (spark_catalog.default.foo2_3(c1, c2) - spark_catalog.default.foo2_3(c2, (c1 - 1)))#x ASC NULLS FIRST], true
+- Project [spark_catalog.default.foo2_3(a#x, b#x) AS spark_catalog.default.foo2_3(c1, c2)#x, spark_catalog.default.foo2_3(a#x, b#x) AS spark_catalog.default.foo2_3(c2, 1)#x, (spark_catalog.default.foo2_3(a#x, b#x) - spark_catalog.default.foo2_3(a#x, b#x)) AS (spark_catalog.default.foo2_3(c1, c2) - spark_catalog.default.foo2_3(c2, (c1 - 1)))#x]
   +- Project [c1#x, c2#x, cast(c1#x as int) AS a#x, cast(c2#x as int) AS b#x, cast(c2#x as int) AS a#x, cast(1 as int) AS b#x, cast(c1#x as int) AS a#x, cast(c2#x as int) AS b#x, cast(c2#x as int) AS a#x, cast((c1#x - 1) as int) AS b#x]
      +- SubqueryAlias spark_catalog.default.v1
         +- View (`spark_catalog`.`default`.`v1`, [c1#x, c2#x])
            +- Project [cast(col1#x as int) AS c1#x, cast(col2#x as int) AS c2#x]
               +- LocalRelation [col1#x, col2#x]


-- !query
SELECT * FROM V1 WHERE foo2_3(c1, 0) = c1 AND foo2_3(c1, c2) < 8
-- !query analysis
Project [c1#x, c2#x]
+- Project [c1#x, c2#x]
   +- Filter ((spark_catalog.default.foo2_3(a#x, b#x) = c1#x) AND (spark_catalog.default.foo2_3(a#x, b#x) < 8))
      +- Project [c1#x, c2#x, cast(c1#x as int) AS a#x, cast(0 as int) AS b#x, cast(c1#x as int) AS a#x, cast(c2#x as int) AS b#x]
         +- SubqueryAlias spark_catalog.default.v1
            +- View (`spark_catalog`.`default`.`v1`, [c1#x, c2#x])
               +- Project [cast(col1#x as int) AS c1#x, cast(col2#x as int) AS c2#x]
                  +- LocalRelation [col1#x, col2#x]


-- !query
SELECT foo2_3(SUM(c1), SUM(c2)), SUM(c1) + SUM(c2), SUM(foo2_3(c1, c2) + foo2_3(c2, c1) - foo2_3(c2, c1))
FROM V1
-- !query analysis
Project [spark_catalog.default.foo2_3(a#x, b#x) AS spark_catalog.default.foo2_3(sum(c1), sum(c2))#x, (sum(c1) + sum(c2))#xL, sum(((spark_catalog.default.foo2_3(c1, c2) + spark_catalog.default.foo2_3(c2, c1)) - spark_catalog.default.foo2_3(c2, c1)))#xL]
+- Project [sum(c1)#xL, sum(c2)#xL, (sum(c1) + sum(c2))#xL, sum(((spark_catalog.default.foo2_3(c1, c2) + spark_catalog.default.foo2_3(c2, c1)) - spark_catalog.default.foo2_3(c2, c1)))#xL, cast(sum(c1)#xL as int) AS a#x, cast(sum(c2)#xL as int) AS b#x]
   +- Aggregate [sum(c1#x) AS sum(c1)#xL, sum(c2#x) AS sum(c2)#xL, (sum(c1#x) + sum(c2#x)) AS (sum(c1) + sum(c2))#xL, sum(((spark_catalog.default.foo2_3(a#x, b#x) + spark_catalog.default.foo2_3(a#x, b#x)) - spark_catalog.default.foo2_3(a#x, b#x))) AS sum(((spark_catalog.default.foo2_3(c1, c2) + spark_catalog.default.foo2_3(c2, c1)) - spark_catalog.default.foo2_3(c2, c1)))#xL]
      +- Project [c1#x, c2#x, cast(c1#x as int) AS a#x, cast(c2#x as int) AS b#x, cast(c2#x as int) AS a#x, cast(c1#x as int) AS b#x, cast(c2#x as int) AS a#x, cast(c1#x as int) AS b#x]
         +- SubqueryAlias spark_catalog.default.v1
            +- View (`spark_catalog`.`default`.`v1`, [c1#x, c2#x])
               +- Project [cast(col1#x as int) AS c1#x, cast(col2#x as int) AS c2#x]
                  +- LocalRelation [col1#x, col2#x]


-- !query
CREATE FUNCTION foo2_4a(a ARRAY<STRING>) RETURNS STRING RETURN
SELECT array_sort(a, (i, j) -> rank[i] - rank[j])[0] FROM (SELECT MAP('a', 1, 'b', 2) rank)
-- !query analysis
CreateSQLFunctionCommand spark_catalog.default.foo2_4a, a ARRAY<STRING>, STRING, SELECT array_sort(a, (i, j) -> rank[i] - rank[j])[0] FROM (SELECT MAP('a', 1, 'b', 2) rank), false, false, false, false


-- !query
SELECT foo2_4a(ARRAY('a', 'b'))
-- !query analysis
Project [spark_catalog.default.foo2_4a(a#x) AS spark_catalog.default.foo2_4a(array(a, b))#x]
:  +- Project [array_sort(outer(a#x), lambdafunction((rank#x[lambda i#x] - rank#x[lambda j#x]), lambda i#x, lambda j#x, false), false)[0] AS array_sort(outer(foo2_4a.a), lambdafunction((rank[namedlambdavariable()] - rank[namedlambdavariable()]), namedlambdavariable(), namedlambdavariable()))[0]#x]
:     +- SubqueryAlias __auto_generated_subquery_name
:        +- Project [map(a, 1, b, 2) AS rank#x]
:           +- OneRowRelation
+- Project [cast(array(a, b) as array<string>) AS a#x]
   +- OneRowRelation


-- !query
CREATE FUNCTION foo2_4b(m MAP<STRING, STRING>, k STRING) RETURNS STRING RETURN
SELECT v || ' ' || v FROM (SELECT upper(m[k]) AS v)
-- !query analysis
CreateSQLFunctionCommand spark_catalog.default.foo2_4b, m MAP<STRING, STRING>, k STRING, STRING, SELECT v || ' ' || v FROM (SELECT upper(m[k]) AS v), false, false, false, false


-- !query
SELECT foo2_4b(map('a', 'hello', 'b', 'world'), 'a')
-- !query analysis
Project [spark_catalog.default.foo2_4b(m#x, k#x) AS spark_catalog.default.foo2_4b(map(a, hello, b, world), a)#x]
:  +- Project [concat(concat(v#x,  ), v#x) AS concat(concat(v,  ), v)#x]
:     +- SubqueryAlias __auto_generated_subquery_name
:        +- Project [upper(outer(m#x)[outer(k#x)]) AS v#x]
:           +- OneRowRelation
+- Project [cast(map(a, hello, b, world) as map<string,string>) AS m#x, cast(a as string) AS k#x]
   +- OneRowRelation


-- !query
DROP VIEW V2
-- !query analysis
DropTableCommand `spark_catalog`.`default`.`V2`, false, true, false


-- !query
DROP VIEW V1
-- !query analysis
DropTableCommand `spark_catalog`.`default`.`V1`, false, true, false


-- !query
CREATE VIEW t1(c1, c2) AS VALUES (0, 1), (0, 2), (1, 2)
-- !query analysis
CreateViewCommand `spark_catalog`.`default`.`t1`, [(c1,None), (c2,None)], VALUES (0, 1), (0, 2), (1, 2), false, false, PersistedView, COMPENSATION, true
   +- LocalRelation [col1#x, col2#x]


-- !query
CREATE VIEW t2(c1, c2) AS VALUES (0, 2), (0, 3)
-- !query analysis
CreateViewCommand `spark_catalog`.`default`.`t2`, [(c1,None), (c2,None)], VALUES (0, 2), (0, 3), false, false, PersistedView, COMPENSATION, true
   +- LocalRelation [col1#x, col2#x]


-- !query
CREATE VIEW ts(x) AS VALUES NAMED_STRUCT('a', 1, 'b', 2)
-- !query analysis
CreateViewCommand `spark_catalog`.`default`.`ts`, [(x,None)], VALUES NAMED_STRUCT('a', 1, 'b', 2), false, false, PersistedView, COMPENSATION, true
   +- LocalRelation [col1#x]


-- !query
CREATE VIEW tm(x) AS VALUES MAP('a', 1, 'b', 2)
-- !query analysis
CreateViewCommand `spark_catalog`.`default`.`tm`, [(x,None)], VALUES MAP('a', 1, 'b', 2), false, false, PersistedView, COMPENSATION, true
   +- LocalRelation [col1#x]


-- !query
CREATE VIEW ta(x) AS VALUES ARRAY(1, 2, 3)
-- !query analysis
CreateViewCommand `spark_catalog`.`default`.`ta`, [(x,None)], VALUES ARRAY(1, 2, 3), false, false, PersistedView, COMPENSATION, true
   +- LocalRelation [col1#x]


-- !query
CREATE FUNCTION foo3_1a(a DOUBLE, b DOUBLE) RETURNS DOUBLE RETURN a * b
-- !query analysis
CreateSQLFunctionCommand spark_catalog.default.foo3_1a, a DOUBLE, b DOUBLE, DOUBLE, a * b, false, false, false, false


-- !query
CREATE FUNCTION foo3_1b(x INT) RETURNS INT RETURN x
-- !query analysis
CreateSQLFunctionCommand spark_catalog.default.foo3_1b, x INT, INT, x, false, false, false, false


-- !query
CREATE FUNCTION foo3_1c(x INT) RETURNS INT RETURN SELECT x
-- !query analysis
CreateSQLFunctionCommand spark_catalog.default.foo3_1c, x INT, INT, SELECT x, false, false, false, false


-- !query
CREATE FUNCTION foo3_1d(x INT) RETURNS INT RETURN (SELECT SUM(c2) FROM t2 WHERE c1 = x)
-- !query analysis
CreateSQLFunctionCommand spark_catalog.default.foo3_1d, x INT, INT, (SELECT SUM(c2) FROM t2 WHERE c1 = x), false, false, false, false


-- !query
CREATE FUNCTION foo3_1e() RETURNS INT RETURN foo3_1d(0)
-- !query analysis
CreateSQLFunctionCommand spark_catalog.default.foo3_1e, INT, foo3_1d(0), false, false, false, false


-- !query
CREATE FUNCTION foo3_1f() RETURNS INT RETURN SELECT SUM(c2) FROM t2 WHERE c1 = 0
-- !query analysis
CreateSQLFunctionCommand spark_catalog.default.foo3_1f, INT, SELECT SUM(c2) FROM t2 WHERE c1 = 0, false, false, false, false


-- !query
CREATE FUNCTION foo3_1g(x INT) RETURNS INT RETURN SELECT (SELECT x)
-- !query analysis
CreateSQLFunctionCommand spark_catalog.default.foo3_1g, x INT, INT, SELECT (SELECT x), false, false, false, false


-- !query
SELECT a, b, foo3_1a(a + 1, b + 1) FROM t1 AS t(a, b)
-- !query analysis
Project [a#x, b#x, spark_catalog.default.foo3_1a(a#x, b#x) AS spark_catalog.default.foo3_1a((a + 1), (b + 1))#x]
+- Project [a#x, b#x, cast((a#x + 1) as double) AS a#x, cast((b#x + 1) as double) AS b#x]
   +- SubqueryAlias t
      +- Project [c1#x AS a#x, c2#x AS b#x]
         +- SubqueryAlias spark_catalog.default.t1
            +- View (`spark_catalog`.`default`.`t1`, [c1#x, c2#x])
               +- Project [cast(col1#x as int) AS c1#x, cast(col2#x as int) AS c2#x]
                  +- LocalRelation [col1#x, col2#x]


-- !query
SELECT x, foo3_1c(x) FROM t1 AS t(x, y)
-- !query analysis
Project [x#x, spark_catalog.default.foo3_1c(x#x) AS spark_catalog.default.foo3_1c(x)#x]
+- Project [x#x, y#x, cast(x#x as int) AS x#x]
   +- SubqueryAlias t
      +- Project [c1#x AS x#x, c2#x AS y#x]
         +- SubqueryAlias spark_catalog.default.t1
            +- View (`spark_catalog`.`default`.`t1`, [c1#x, c2#x])
               +- Project [cast(col1#x as int) AS c1#x, cast(col2#x as int) AS c2#x]
                  +- LocalRelation [col1#x, col2#x]


-- !query
SELECT c1, foo3_1d(c1) FROM t1
-- !query analysis
Project [c1#x, spark_catalog.default.foo3_1d(x#x) AS spark_catalog.default.foo3_1d(c1)#x]
:  +- Aggregate [sum(c2#x) AS sum(c2)#xL]
:     +- Filter (c1#x = outer(x#x))
:        +- SubqueryAlias spark_catalog.default.t2
:           +- View (`spark_catalog`.`default`.`t2`, [c1#x, c2#x])
:              +- Project [cast(col1#x as int) AS c1#x, cast(col2#x as int) AS c2#x]
:                 +- LocalRelation [col1#x, col2#x]
+- Project [c1#x, c2#x, cast(c1#x as int) AS x#x]
   +- SubqueryAlias spark_catalog.default.t1
      +- View (`spark_catalog`.`default`.`t1`, [c1#x, c2#x])
         +- Project [cast(col1#x as int) AS c1#x, cast(col2#x as int) AS c2#x]
            +- LocalRelation [col1#x, col2#x]


-- !query
SELECT c1, foo3_1a(foo3_1b(c1), foo3_1b(c1)) FROM t1
-- !query analysis
Project [c1#x, spark_catalog.default.foo3_1a(a#x, b#x) AS spark_catalog.default.foo3_1a(spark_catalog.default.foo3_1b(c1), spark_catalog.default.foo3_1b(c1))#x]
+- Project [c1#x, c2#x, x#x, x#x, cast(spark_catalog.default.foo3_1b(x#x) as double) AS a#x, cast(spark_catalog.default.foo3_1b(x#x) as double) AS b#x]
   +- Project [c1#x, c2#x, cast(c1#x as int) AS x#x, cast(c1#x as int) AS x#x]
      +- SubqueryAlias spark_catalog.default.t1
         +- View (`spark_catalog`.`default`.`t1`, [c1#x, c2#x])
            +- Project [cast(col1#x as int) AS c1#x, cast(col2#x as int) AS c2#x]
               +- LocalRelation [col1#x, col2#x]


-- !query
SELECT c1, foo3_1d(foo3_1c(foo3_1b(c1))) FROM t1
-- !query analysis
Project [c1#x, spark_catalog.default.foo3_1d(x#x) AS spark_catalog.default.foo3_1d(spark_catalog.default.foo3_1c(spark_catalog.default.foo3_1b(c1)))#x]
:  +- Aggregate [sum(c2#x) AS sum(c2)#xL]
:     +- Filter (c1#x = outer(x#x))
:        +- SubqueryAlias spark_catalog.default.t2
:           +- View (`spark_catalog`.`default`.`t2`, [c1#x, c2#x])
:              +- Project [cast(col1#x as int) AS c1#x, cast(col2#x as int) AS c2#x]
:                 +- LocalRelation [col1#x, col2#x]
+- Project [c1#x, c2#x, x#x, x#x, cast(spark_catalog.default.foo3_1c(x#x) as int) AS x#x]
   +- Project [c1#x, c2#x, x#x, cast(spark_catalog.default.foo3_1b(x#x) as int) AS x#x]
      +- Project [c1#x, c2#x, cast(c1#x as int) AS x#x]
         +- SubqueryAlias spark_catalog.default.t1
            +- View (`spark_catalog`.`default`.`t1`, [c1#x, c2#x])
               +- Project [cast(col1#x as int) AS c1#x, cast(col2#x as int) AS c2#x]
                  +- LocalRelation [col1#x, col2#x]


-- !query
SELECT c1, foo3_1a(foo3_1c(foo3_1b(c1)), foo3_1d(foo3_1b(c1))) FROM t1
-- !query analysis
Project [c1#x, spark_catalog.default.foo3_1a(a#x, b#x) AS spark_catalog.default.foo3_1a(spark_catalog.default.foo3_1c(spark_catalog.default.foo3_1b(c1)), spark_catalog.default.foo3_1d(spark_catalog.default.foo3_1b(c1)))#x]
+- Project [c1#x, c2#x, x#x, x#x, x#x, x#x, cast(spark_catalog.default.foo3_1c(x#x) as double) AS a#x, cast(spark_catalog.default.foo3_1d(x#x) as double) AS b#x]
   :  +- Aggregate [sum(c2#x) AS sum(c2)#xL]
   :     +- Filter (c1#x = outer(x#x))
   :        +- SubqueryAlias spark_catalog.default.t2
   :           +- View (`spark_catalog`.`default`.`t2`, [c1#x, c2#x])
   :              +- Project [cast(col1#x as int) AS c1#x, cast(col2#x as int) AS c2#x]
   :                 +- LocalRelation [col1#x, col2#x]
   +- Project [c1#x, c2#x, x#x, x#x, cast(spark_catalog.default.foo3_1b(x#x) as int) AS x#x, cast(spark_catalog.default.foo3_1b(x#x) as int) AS x#x]
      +- Project [c1#x, c2#x, cast(c1#x as int) AS x#x, cast(c1#x as int) AS x#x]
         +- SubqueryAlias spark_catalog.default.t1
            +- View (`spark_catalog`.`default`.`t1`, [c1#x, c2#x])
               +- Project [cast(col1#x as int) AS c1#x, cast(col2#x as int) AS c2#x]
                  +- LocalRelation [col1#x, col2#x]


-- !query
SELECT foo3_1c(foo3_1e()) FROM t1
-- !query analysis
Project [spark_catalog.default.foo3_1c(x#x) AS spark_catalog.default.foo3_1c(spark_catalog.default.foo3_1e())#x]
+- Project [c1#x, c2#x, x#x, cast(spark_catalog.default.foo3_1e() as int) AS x#x]
   :  +- Aggregate [sum(c2#x) AS sum(c2)#xL]
   :     +- Filter (c1#x = outer(x#x))
   :        +- SubqueryAlias spark_catalog.default.t2
   :           +- View (`spark_catalog`.`default`.`t2`, [c1#x, c2#x])
   :              +- Project [cast(col1#x as int) AS c1#x, cast(col2#x as int) AS c2#x]
   :                 +- LocalRelation [col1#x, col2#x]
   +- Project [c1#x, c2#x, cast(0 as int) AS x#x]
      +- Project [c1#x, c2#x]
         +- SubqueryAlias spark_catalog.default.t1
            +- View (`spark_catalog`.`default`.`t1`, [c1#x, c2#x])
               +- Project [cast(col1#x as int) AS c1#x, cast(col2#x as int) AS c2#x]
                  +- LocalRelation [col1#x, col2#x]


-- !query
SELECT foo3_1a(MAX(c1), MAX(c2)) FROM t1
-- !query analysis
Project [spark_catalog.default.foo3_1a(a#x, b#x) AS spark_catalog.default.foo3_1a(max(c1), max(c2))#x]
+- Project [max(c1)#x, max(c2)#x, cast(max(c1)#x as double) AS a#x, cast(max(c2)#x as double) AS b#x]
   +- Aggregate [max(c1#x) AS max(c1)#x, max(c2#x) AS max(c2)#x]
      +- SubqueryAlias spark_catalog.default.t1
         +- View (`spark_catalog`.`default`.`t1`, [c1#x, c2#x])
            +- Project [cast(col1#x as int) AS c1#x, cast(col2#x as int) AS c2#x]
               +- LocalRelation [col1#x, col2#x]


-- !query
SELECT foo3_1a(MAX(c1), c2) FROM t1 GROUP BY c2
-- !query analysis
Project [spark_catalog.default.foo3_1a(a#x, b#x) AS spark_catalog.default.foo3_1a(max(c1), c2)#x]
+- Project [max(c1)#x, c2#x, cast(max(c1)#x as double) AS a#x, cast(c2#x as double) AS b#x]
   +- Aggregate [c2#x], [max(c1#x) AS max(c1)#x, c2#x]
      +- SubqueryAlias spark_catalog.default.t1
         +- View (`spark_catalog`.`default`.`t1`, [c1#x, c2#x])
            +- Project [cast(col1#x as int) AS c1#x, cast(col2#x as int) AS c2#x]
               +- LocalRelation [col1#x, col2#x]


-- !query
SELECT foo3_1a(c1, c2) FROM t1 GROUP BY c1, c2
-- !query analysis
Project [spark_catalog.default.foo3_1a(a#x, b#x) AS spark_catalog.default.foo3_1a(c1, c2)#x]
+- Project [c1#x, c2#x, cast(c1#x as double) AS a#x, cast(c2#x as double) AS b#x]
   +- Aggregate [c1#x, c2#x], [c1#x, c2#x]
      +- SubqueryAlias spark_catalog.default.t1
         +- View (`spark_catalog`.`default`.`t1`, [c1#x, c2#x])
            +- Project [cast(col1#x as int) AS c1#x, cast(col2#x as int) AS c2#x]
               +- LocalRelation [col1#x, col2#x]


-- !query
SELECT MAX(foo3_1a(c1, c2)) FROM t1 GROUP BY c1, c2
-- !query analysis
Project [max(spark_catalog.default.foo3_1a(c1, c2))#x]
+- Aggregate [c1#x, c2#x], [max(spark_catalog.default.foo3_1a(a#x, b#x)) AS max(spark_catalog.default.foo3_1a(c1, c2))#x]
   +- Project [c1#x, c2#x, cast(c1#x as double) AS a#x, cast(c2#x as double) AS b#x]
      +- SubqueryAlias spark_catalog.default.t1
         +- View (`spark_catalog`.`default`.`t1`, [c1#x, c2#x])
            +- Project [cast(col1#x as int) AS c1#x, cast(col2#x as int) AS c2#x]
               +- LocalRelation [col1#x, col2#x]


-- !query
SELECT MAX(c1) + foo3_1b(MAX(c1)) FROM t1 GROUP BY c2
-- !query analysis
Project [(max(c1)#x + spark_catalog.default.foo3_1b(x#x)) AS (max(c1) + spark_catalog.default.foo3_1b(max(c1)))#x]
+- Project [max(c1)#x, max(c1)#x, cast(max(c1)#x as int) AS x#x]
   +- Aggregate [c2#x], [max(c1#x) AS max(c1)#x, max(c1#x) AS max(c1)#x]
      +- SubqueryAlias spark_catalog.default.t1
         +- View (`spark_catalog`.`default`.`t1`, [c1#x, c2#x])
            +- Project [cast(col1#x as int) AS c1#x, cast(col2#x as int) AS c2#x]
               +- LocalRelation [col1#x, col2#x]


-- !query
SELECT c1, SUM(foo3_1c(c2)) FROM t1 GROUP BY c1
-- !query analysis
Project [c1#x, sum(spark_catalog.default.foo3_1c(c2))#xL]
+- Aggregate [c1#x], [c1#x, sum(spark_catalog.default.foo3_1c(x#x)) AS sum(spark_catalog.default.foo3_1c(c2))#xL]
   +- Project [c1#x, c2#x, cast(c2#x as int) AS x#x]
      +- SubqueryAlias spark_catalog.default.t1
         +- View (`spark_catalog`.`default`.`t1`, [c1#x, c2#x])
            +- Project [cast(col1#x as int) AS c1#x, cast(col2#x as int) AS c2#x]
               +- LocalRelation [col1#x, col2#x]


-- !query
SELECT c1, SUM(foo3_1d(c2)) FROM t1 GROUP BY c1
-- !query analysis
Project [c1#x, sum(spark_catalog.default.foo3_1d(c2))#xL]
+- Aggregate [c1#x], [c1#x, sum(spark_catalog.default.foo3_1d(x#x)) AS sum(spark_catalog.default.foo3_1d(c2))#xL]
   :  +- Aggregate [sum(c2#x) AS sum(c2)#xL]
   :     +- Filter (c1#x = outer(x#x))
   :        +- SubqueryAlias spark_catalog.default.t2
   :           +- View (`spark_catalog`.`default`.`t2`, [c1#x, c2#x])
   :              +- Project [cast(col1#x as int) AS c1#x, cast(col2#x as int) AS c2#x]
   :                 +- LocalRelation [col1#x, col2#x]
   +- Project [c1#x, c2#x, cast(c2#x as int) AS x#x]
      +- SubqueryAlias spark_catalog.default.t1
         +- View (`spark_catalog`.`default`.`t1`, [c1#x, c2#x])
            +- Project [cast(col1#x as int) AS c1#x, cast(col2#x as int) AS c2#x]
               +- LocalRelation [col1#x, col2#x]


-- !query
SELECT foo3_1c(c1), foo3_1d(c1) FROM t1 GROUP BY c1
-- !query analysis
Project [spark_catalog.default.foo3_1c(x#x) AS spark_catalog.default.foo3_1c(c1)#x, spark_catalog.default.foo3_1d(x#x) AS spark_catalog.default.foo3_1d(c1)#x]
:  +- Aggregate [sum(c2#x) AS sum(c2)#xL]
:     +- Filter (c1#x = outer(x#x))
:        +- SubqueryAlias spark_catalog.default.t2
:           +- View (`spark_catalog`.`default`.`t2`, [c1#x, c2#x])
:              +- Project [cast(col1#x as int) AS c1#x, cast(col2#x as int) AS c2#x]
:                 +- LocalRelation [col1#x, col2#x]
+- Project [c1#x, c1#x, cast(c1#x as int) AS x#x, cast(c1#x as int) AS x#x]
   +- Aggregate [c1#x], [c1#x, c1#x]
      +- SubqueryAlias spark_catalog.default.t1
         +- View (`spark_catalog`.`default`.`t1`, [c1#x, c2#x])
            +- Project [cast(col1#x as int) AS c1#x, cast(col2#x as int) AS c2#x]
               +- LocalRelation [col1#x, col2#x]


-- !query
SELECT foo3_1a(SUM(c1), rand(0) * 0) FROM t1
-- !query analysis
[Analyzer test output redacted due to nondeterminism]


-- !query
SELECT foo3_1a(SUM(c1) + rand(0) * 0, SUM(c2)) FROM t1
-- !query analysis
[Analyzer test output redacted due to nondeterminism]


-- !query
SELECT foo3_1b(SUM(c1) + rand(0) * 0) FROM t1
-- !query analysis
[Analyzer test output redacted due to nondeterminism]


-- !query
SELECT foo3_1b(SUM(1) + rand(0) * 0) FROM t1 GROUP BY c2
-- !query analysis
[Analyzer test output redacted due to nondeterminism]


-- !query
SELECT foo3_1c(SUM(c2) + rand(0) * 0) FROM t1 GROUP by c1
-- !query analysis
[Analyzer test output redacted due to nondeterminism]


-- !query
SELECT foo3_1b(foo3_1b(MAX(c2))) FROM t1
-- !query analysis
Project [spark_catalog.default.foo3_1b(x#x) AS spark_catalog.default.foo3_1b(spark_catalog.default.foo3_1b(max(c2)))#x]
+- Project [max(c2)#x, x#x, cast(spark_catalog.default.foo3_1b(x#x) as int) AS x#x]
   +- Project [max(c2)#x, cast(max(c2)#x as int) AS x#x]
      +- Aggregate [max(c2#x) AS max(c2)#x]
         +- SubqueryAlias spark_catalog.default.t1
            +- View (`spark_catalog`.`default`.`t1`, [c1#x, c2#x])
               +- Project [cast(col1#x as int) AS c1#x, cast(col2#x as int) AS c2#x]
                  +- LocalRelation [col1#x, col2#x]


-- !query
SELECT foo3_1b(MAX(foo3_1b(c2))) FROM t1
-- !query analysis
Project [spark_catalog.default.foo3_1b(x#x) AS spark_catalog.default.foo3_1b(max(spark_catalog.default.foo3_1b(c2)))#x]
+- Project [max(spark_catalog.default.foo3_1b(c2))#x, cast(max(spark_catalog.default.foo3_1b(c2))#x as int) AS x#x]
   +- Aggregate [max(spark_catalog.default.foo3_1b(x#x)) AS max(spark_catalog.default.foo3_1b(c2))#x]
      +- Project [c1#x, c2#x, cast(c2#x as int) AS x#x]
         +- SubqueryAlias spark_catalog.default.t1
            +- View (`spark_catalog`.`default`.`t1`, [c1#x, c2#x])
               +- Project [cast(col1#x as int) AS c1#x, cast(col2#x as int) AS c2#x]
                  +- LocalRelation [col1#x, col2#x]


-- !query
SELECT foo3_1a(foo3_1b(c1), MAX(c2)) FROM t1 GROUP BY c1
-- !query analysis
Project [spark_catalog.default.foo3_1a(a#x, b#x) AS spark_catalog.default.foo3_1a(spark_catalog.default.foo3_1b(c1), max(c2))#x]
+- Project [c1#x, max(c2)#x, x#x, cast(spark_catalog.default.foo3_1b(x#x) as double) AS a#x, cast(max(c2)#x as double) AS b#x]
   +- Project [c1#x, max(c2)#x, cast(c1#x as int) AS x#x]
      +- Aggregate [c1#x], [c1#x, max(c2#x) AS max(c2)#x]
         +- SubqueryAlias spark_catalog.default.t1
            +- View (`spark_catalog`.`default`.`t1`, [c1#x, c2#x])
               +- Project [cast(col1#x as int) AS c1#x, cast(col2#x as int) AS c2#x]
                  +- LocalRelation [col1#x, col2#x]


-- !query
SELECT c1, foo3_1b(c1) FROM t1 GROUP BY c1
-- !query analysis
Project [c1#x, spark_catalog.default.foo3_1b(x#x) AS spark_catalog.default.foo3_1b(c1)#x]
+- Project [c1#x, c1#x, cast(c1#x as int) AS x#x]
   +- Aggregate [c1#x], [c1#x, c1#x]
      +- SubqueryAlias spark_catalog.default.t1
         +- View (`spark_catalog`.`default`.`t1`, [c1#x, c2#x])
            +- Project [cast(col1#x as int) AS c1#x, cast(col2#x as int) AS c2#x]
               +- LocalRelation [col1#x, col2#x]


-- !query
SELECT c1, foo3_1b(c1 + 1) FROM t1 GROUP BY c1
-- !query analysis
Project [c1#x, spark_catalog.default.foo3_1b(x#x) AS spark_catalog.default.foo3_1b((c1 + 1))#x]
+- Project [c1#x, (c1 + 1)#x, cast((c1 + 1)#x as int) AS x#x]
   +- Aggregate [c1#x], [c1#x, (c1#x + 1) AS (c1 + 1)#x]
      +- SubqueryAlias spark_catalog.default.t1
         +- View (`spark_catalog`.`default`.`t1`, [c1#x, c2#x])
            +- Project [cast(col1#x as int) AS c1#x, cast(col2#x as int) AS c2#x]
               +- LocalRelation [col1#x, col2#x]


-- !query
SELECT c1, foo3_1b(c1 + rand(0) * 0) FROM t1 GROUP BY c1
-- !query analysis
[Analyzer test output redacted due to nondeterminism]


-- !query
SELECT c1, foo3_1a(c1, MIN(c2)) FROM t1 GROUP BY c1
-- !query analysis
Project [c1#x, spark_catalog.default.foo3_1a(a#x, b#x) AS spark_catalog.default.foo3_1a(c1, min(c2))#x]
+- Project [c1#x, c1#x, min(c2)#x, cast(c1#x as double) AS a#x, cast(min(c2)#x as double) AS b#x]
   +- Aggregate [c1#x], [c1#x, c1#x, min(c2#x) AS min(c2)#x]
      +- SubqueryAlias spark_catalog.default.t1
         +- View (`spark_catalog`.`default`.`t1`, [c1#x, c2#x])
            +- Project [cast(col1#x as int) AS c1#x, cast(col2#x as int) AS c2#x]
               +- LocalRelation [col1#x, col2#x]


-- !query
SELECT c1, foo3_1a(c1 + 1, MIN(c2 + 1)) FROM t1 GROUP BY c1
-- !query analysis
Project [c1#x, spark_catalog.default.foo3_1a(a#x, b#x) AS spark_catalog.default.foo3_1a((c1 + 1), min((c2 + 1)))#x]
+- Project [c1#x, (c1 + 1)#x, min((c2 + 1))#x, cast((c1 + 1)#x as double) AS a#x, cast(min((c2 + 1))#x as double) AS b#x]
   +- Aggregate [c1#x], [c1#x, (c1#x + 1) AS (c1 + 1)#x, min((c2#x + 1)) AS min((c2 + 1))#x]
      +- SubqueryAlias spark_catalog.default.t1
         +- View (`spark_catalog`.`default`.`t1`, [c1#x, c2#x])
            +- Project [cast(col1#x as int) AS c1#x, cast(col2#x as int) AS c2#x]
               +- LocalRelation [col1#x, col2#x]


-- !query
SELECT c1, c2, foo3_1a(c1, c2) FROM t1 GROUP BY c1, c2
-- !query analysis
Project [c1#x, c2#x, spark_catalog.default.foo3_1a(a#x, b#x) AS spark_catalog.default.foo3_1a(c1, c2)#x]
+- Project [c1#x, c2#x, c1#x, c2#x, cast(c1#x as double) AS a#x, cast(c2#x as double) AS b#x]
   +- Aggregate [c1#x, c2#x], [c1#x, c2#x, c1#x, c2#x]
      +- SubqueryAlias spark_catalog.default.t1
         +- View (`spark_catalog`.`default`.`t1`, [c1#x, c2#x])
            +- Project [cast(col1#x as int) AS c1#x, cast(col2#x as int) AS c2#x]
               +- LocalRelation [col1#x, col2#x]


-- !query
SELECT c1, c2, foo3_1a(1, 2) FROM t1 GROUP BY c1, c2
-- !query analysis
Project [c1#x, c2#x, spark_catalog.default.foo3_1a(a#x, b#x) AS spark_catalog.default.foo3_1a(1, 2)#x]
+- Project [c1#x, c2#x, 1#x, 2#x, cast(1#x as double) AS a#x, cast(2#x as double) AS b#x]
   +- Aggregate [c1#x, c2#x], [c1#x, c2#x, 1 AS 1#x, 2 AS 2#x]
      +- SubqueryAlias spark_catalog.default.t1
         +- View (`spark_catalog`.`default`.`t1`, [c1#x, c2#x])
            +- Project [cast(col1#x as int) AS c1#x, cast(col2#x as int) AS c2#x]
               +- LocalRelation [col1#x, col2#x]


-- !query
SELECT c1 + c2, foo3_1b(c1 + c2 + 1) FROM t1 GROUP BY c1 + c2
-- !query analysis
Project [(c1 + c2)#x, spark_catalog.default.foo3_1b(x#x) AS spark_catalog.default.foo3_1b(((c1 + c2) + 1))#x]
+- Project [(c1 + c2)#x, ((c1 + c2) + 1)#x, cast(((c1 + c2) + 1)#x as int) AS x#x]
   +- Aggregate [(c1#x + c2#x)], [(c1#x + c2#x) AS (c1 + c2)#x, ((c1#x + c2#x) + 1) AS ((c1 + c2) + 1)#x]
      +- SubqueryAlias spark_catalog.default.t1
         +- View (`spark_catalog`.`default`.`t1`, [c1#x, c2#x])
            +- Project [cast(col1#x as int) AS c1#x, cast(col2#x as int) AS c2#x]
               +- LocalRelation [col1#x, col2#x]


-- !query
SELECT COUNT(*) + foo3_1b(c1) + foo3_1b(SUM(c2)) + SUM(foo3_1b(c2)) FROM t1 GROUP BY c1
-- !query analysis
Project [(((count(1)#xL + cast(spark_catalog.default.foo3_1b(x#x) as bigint)) + cast(spark_catalog.default.foo3_1b(x#x) as bigint)) + sum(spark_catalog.default.foo3_1b(c2))#xL) AS (((count(1) + spark_catalog.default.foo3_1b(c1)) + spark_catalog.default.foo3_1b(sum(c2))) + sum(spark_catalog.default.foo3_1b(c2)))#xL]
+- Project [count(1)#xL, c1#x, sum(c2)#xL, sum(spark_catalog.default.foo3_1b(c2))#xL, cast(c1#x as int) AS x#x, cast(sum(c2)#xL as int) AS x#x]
   +- Aggregate [c1#x], [count(1) AS count(1)#xL, c1#x, sum(c2#x) AS sum(c2)#xL, sum(spark_catalog.default.foo3_1b(x#x)) AS sum(spark_catalog.default.foo3_1b(c2))#xL]
      +- Project [c1#x, c2#x, cast(c2#x as int) AS x#x]
         +- SubqueryAlias spark_catalog.default.t1
            +- View (`spark_catalog`.`default`.`t1`, [c1#x, c2#x])
               +- Project [cast(col1#x as int) AS c1#x, cast(col2#x as int) AS c2#x]
                  +- LocalRelation [col1#x, col2#x]


-- !query
SELECT c1, COUNT(*), foo3_1b(SUM(c2)) FROM t1 GROUP BY c1 HAVING COUNT(*) > 0
-- !query analysis
Filter (count(1)#xL > cast(0 as bigint))
+- Project [c1#x, count(1)#xL, spark_catalog.default.foo3_1b(x#x) AS spark_catalog.default.foo3_1b(sum(c2))#x]
   +- Project [c1#x, count(1)#xL, sum(c2)#xL, cast(sum(c2)#xL as int) AS x#x]
      +- Aggregate [c1#x], [c1#x, count(1) AS count(1)#xL, sum(c2#x) AS sum(c2)#xL]
         +- SubqueryAlias spark_catalog.default.t1
            +- View (`spark_catalog`.`default`.`t1`, [c1#x, c2#x])
               +- Project [cast(col1#x as int) AS c1#x, cast(col2#x as int) AS c2#x]
                  +- LocalRelation [col1#x, col2#x]


-- !query
SELECT c1, COUNT(*), foo3_1b(SUM(c2)) FROM t1 GROUP BY c1 HAVING foo3_1b(SUM(c2)) > 0
-- !query analysis
Filter (spark_catalog.default.foo3_1b(sum(c2))#x > 0)
+- Project [c1#x, count(1)#xL, spark_catalog.default.foo3_1b(x#x) AS spark_catalog.default.foo3_1b(sum(c2))#x]
   +- Project [c1#x, count(1)#xL, sum(c2)#xL, cast(sum(c2)#xL as int) AS x#x]
      +- Aggregate [c1#x], [c1#x, count(1) AS count(1)#xL, sum(c2#x) AS sum(c2)#xL]
         +- SubqueryAlias spark_catalog.default.t1
            +- View (`spark_catalog`.`default`.`t1`, [c1#x, c2#x])
               +- Project [cast(col1#x as int) AS c1#x, cast(col2#x as int) AS c2#x]
                  +- LocalRelation [col1#x, col2#x]


-- !query
SELECT c1, COUNT(*), foo3_1b(SUM(c2)) FROM t1 GROUP BY c1 HAVING SUM(foo3_1b(c2)) > 0
-- !query analysis
Project [c1#x, count(1)#xL, spark_catalog.default.foo3_1b(sum(c2))#x]
+- Filter (sum(spark_catalog.default.foo3_1b(c2))#xL > cast(0 as bigint))
   +- Project [c1#x, count(1)#xL, spark_catalog.default.foo3_1b(x#x) AS spark_catalog.default.foo3_1b(sum(c2))#x, sum(spark_catalog.default.foo3_1b(c2))#xL]
      +- Project [c1#x, count(1)#xL, sum(c2)#xL, sum(spark_catalog.default.foo3_1b(c2))#xL, cast(sum(c2)#xL as int) AS x#x]
         +- Aggregate [c1#x], [c1#x, count(1) AS count(1)#xL, sum(c2#x) AS sum(c2)#xL, sum(spark_catalog.default.foo3_1b(x#x)) AS sum(spark_catalog.default.foo3_1b(c2))#xL]
            +- Project [c1#x, c2#x, cast(c2#x as int) AS x#x]
               +- SubqueryAlias spark_catalog.default.t1
                  +- View (`spark_catalog`.`default`.`t1`, [c1#x, c2#x])
                     +- Project [cast(col1#x as int) AS c1#x, cast(col2#x as int) AS c2#x]
                        +- LocalRelation [col1#x, col2#x]


-- !query
SELECT foo3_1b(c1), MIN(c2) FROM t1 GROUP BY 1
-- !query analysis
Project [spark_catalog.default.foo3_1b(c1)#x, min(c2)#x]
+- Aggregate [spark_catalog.default.foo3_1b#x], [spark_catalog.default.foo3_1b#x AS spark_catalog.default.foo3_1b(c1)#x, min(c2#x) AS min(c2)#x]
   +- Project [c1#x, c2#x, spark_catalog.default.foo3_1b(x#x) AS spark_catalog.default.foo3_1b#x]
      +- Project [c1#x, c2#x, cast(c1#x as int) AS x#x]
         +- SubqueryAlias spark_catalog.default.t1
            +- View (`spark_catalog`.`default`.`t1`, [c1#x, c2#x])
               +- Project [cast(col1#x as int) AS c1#x, cast(col2#x as int) AS c2#x]
                  +- LocalRelation [col1#x, col2#x]


-- !query
SELECT foo3_1a(c1 + rand(0) * 0, c2) FROM t1 GROUP BY 1
-- !query analysis
[Analyzer test output redacted due to nondeterminism]


-- !query
SELECT c1, c2, foo3_1a(c1, c2) FROM t1 GROUP BY c1, c2, 3
-- !query analysis
Project [c1#x, c2#x, spark_catalog.default.foo3_1a(c1, c2)#x]
+- Aggregate [c1#x, c2#x, spark_catalog.default.foo3_1a#x], [c1#x, c2#x, spark_catalog.default.foo3_1a#x AS spark_catalog.default.foo3_1a(c1, c2)#x]
   +- Project [c1#x, c2#x, spark_catalog.default.foo3_1a(a#x, b#x) AS spark_catalog.default.foo3_1a#x]
      +- Project [c1#x, c2#x, cast(c1#x as double) AS a#x, cast(c2#x as double) AS b#x]
         +- SubqueryAlias spark_catalog.default.t1
            +- View (`spark_catalog`.`default`.`t1`, [c1#x, c2#x])
               +- Project [cast(col1#x as int) AS c1#x, cast(col2#x as int) AS c2#x]
                  +- LocalRelation [col1#x, col2#x]


-- !query
SELECT c1, (SELECT c1), (SELECT foo3_1b(c1)), SUM(c2) FROM t1 GROUP BY 1, 2, 3
-- !query analysis
Aggregate [c1#x, scalar-subquery#x [c1#x], scalar-subquery#x [c1#x]], [c1#x, scalar-subquery#x [c1#x] AS scalarsubquery(c1)#x, scalar-subquery#x [c1#x] AS scalarsubquery(c1)#x, sum(c2#x) AS sum(c2)#xL]
:  :- Project [outer(c1#x)]
:  :  +- OneRowRelation
:  :- Project [spark_catalog.default.foo3_1b(x#x) AS spark_catalog.default.foo3_1b(outer(spark_catalog.default.t1.c1))#x]
:  :  +- Project [cast(outer(c1#x) as int) AS x#x]
:  :     +- OneRowRelation
:  :- Project [outer(c1#x)]
:  :  +- OneRowRelation
:  +- Project [spark_catalog.default.foo3_1b(x#x) AS spark_catalog.default.foo3_1b(outer(spark_catalog.default.t1.c1))#x]
:     +- Project [cast(outer(c1#x) as int) AS x#x]
:        +- OneRowRelation
+- SubqueryAlias spark_catalog.default.t1
   +- View (`spark_catalog`.`default`.`t1`, [c1#x, c2#x])
      +- Project [cast(col1#x as int) AS c1#x, cast(col2#x as int) AS c2#x]
         +- LocalRelation [col1#x, col2#x]


-- !query
SELECT c1, SUM(c2) + foo3_1a(MIN(c2), MAX(c2)) + (SELECT SUM(c2)) FROM t1 GROUP BY c1
-- !query analysis
org.apache.spark.sql.catalyst.ExtendedAnalysisException
{
  "errorClass" : "UNSUPPORTED_SUBQUERY_EXPRESSION_CATEGORY.CORRELATED_REFERENCE",
  "sqlState" : "0A000",
  "messageParameters" : {
    "sqlExprs" : "\"sum(c2) AS `sum(outer(spark_catalog.default.t1.c2))`\""
  },
  "queryContext" : [ {
    "objectType" : "",
    "objectName" : "",
    "startIndex" : 51,
    "stopIndex" : 64,
    "fragment" : "SELECT SUM(c2)"
  } ]
}


-- !query
SELECT foo3_1b(SUM(c1)) + (SELECT foo3_1b(SUM(c1))) FROM t1
-- !query analysis
org.apache.spark.sql.catalyst.ExtendedAnalysisException
{
  "errorClass" : "UNSUPPORTED_SUBQUERY_EXPRESSION_CATEGORY.CORRELATED_REFERENCE",
  "sqlState" : "0A000",
  "messageParameters" : {
    "sqlExprs" : "\"sum(c1) AS `sum(outer(spark_catalog.default.t1.c1))`\""
  },
  "queryContext" : [ {
    "objectType" : "",
    "objectName" : "",
    "startIndex" : 27,
    "stopIndex" : 51,
    "fragment" : "(SELECT foo3_1b(SUM(c1)))"
  } ]
}


-- !query
SELECT SUM(foo3_1b(SUM(c1))) FROM t1
-- !query analysis
org.apache.spark.sql.AnalysisException
{
  "errorClass" : "NESTED_AGGREGATE_FUNCTION",
  "sqlState" : "42607",
  "queryContext" : [ {
    "objectType" : "",
    "objectName" : "",
    "startIndex" : 12,
    "stopIndex" : 27,
    "fragment" : "foo3_1b(SUM(c1))"
  } ]
}


-- !query
SELECT foo3_1b(SUM(c1)) + (SELECT SUM(SUM(c1))) FROM t1
-- !query analysis
org.apache.spark.sql.catalyst.ExtendedAnalysisException
{
  "errorClass" : "NESTED_AGGREGATE_FUNCTION",
  "sqlState" : "42607",
  "queryContext" : [ {
    "objectType" : "",
    "objectName" : "",
    "startIndex" : 39,
    "stopIndex" : 45,
    "fragment" : "SUM(c1)"
  } ]
}


-- !query
SELECT foo3_1b(SUM(c1) + SUM(SUM(c1))) FROM t1
-- !query analysis
org.apache.spark.sql.catalyst.ExtendedAnalysisException
{
  "errorClass" : "NESTED_AGGREGATE_FUNCTION",
  "sqlState" : "42607",
  "queryContext" : [ {
    "objectType" : "",
    "objectName" : "",
    "startIndex" : 30,
    "stopIndex" : 36,
    "fragment" : "SUM(c1)"
  } ]
}


-- !query
SELECT foo3_1b(SUM(c1 + rand(0) * 0)) FROM t1
-- !query analysis
org.apache.spark.sql.catalyst.ExtendedAnalysisException
{
  "errorClass" : "AGGREGATE_FUNCTION_WITH_NONDETERMINISTIC_EXPRESSION",
  "sqlState" : "42845",
  "messageParameters" : {
    "sqlExpr" : "\"sum((c1 + (rand(0) * 0)))\""
  },
  "queryContext" : [ {
    "objectType" : "",
    "objectName" : "",
    "startIndex" : 20,
    "stopIndex" : 35,
    "fragment" : "c1 + rand(0) * 0"
  } ]
}


-- !query
SELECT SUM(foo3_1b(c1) + rand(0) * 0) FROM t1
-- !query analysis
org.apache.spark.sql.catalyst.ExtendedAnalysisException
{
  "errorClass" : "AGGREGATE_FUNCTION_WITH_NONDETERMINISTIC_EXPRESSION",
  "sqlState" : "42845",
  "messageParameters" : {
    "sqlExpr" : "\"sum((spark_catalog.default.foo3_1b(foo3_1b.x) + (rand(0) * 0)))\""
  },
  "queryContext" : [ {
    "objectType" : "",
    "objectName" : "",
    "startIndex" : 12,
    "stopIndex" : 36,
    "fragment" : "foo3_1b(c1) + rand(0) * 0"
  } ]
}


-- !query
SELECT SUM(foo3_1b(c1 + rand(0) * 0)) FROM t1
-- !query analysis
[Analyzer test output redacted due to nondeterminism]


-- !query
SELECT foo3_1b(SUM(c1) + foo3_1b(SUM(c1))) FROM t1
-- !query analysis
Project [spark_catalog.default.foo3_1b(x#x) AS spark_catalog.default.foo3_1b((sum(c1) + spark_catalog.default.foo3_1b(sum(c1))))#x]
+- Project [sum(c1)#xL, sum(c1)#xL, x#x, cast((sum(c1)#xL + cast(spark_catalog.default.foo3_1b(x#x) as bigint)) as int) AS x#x]
   +- Project [sum(c1)#xL, sum(c1)#xL, cast(sum(c1)#xL as int) AS x#x]
      +- Aggregate [sum(c1#x) AS sum(c1)#xL, sum(c1#x) AS sum(c1)#xL]
         +- SubqueryAlias spark_catalog.default.t1
            +- View (`spark_catalog`.`default`.`t1`, [c1#x, c2#x])
               +- Project [cast(col1#x as int) AS c1#x, cast(col2#x as int) AS c2#x]
                  +- LocalRelation [col1#x, col2#x]


-- !query
SELECT foo3_1b(SUM(c2) + foo3_1b(SUM(c1))) AS foo FROM t1 HAVING foo > 0
-- !query analysis
Filter (foo#x > 0)
+- Project [spark_catalog.default.foo3_1b(x#x) AS foo#x]
   +- Project [sum(c2)#xL, sum(c1)#xL, x#x, cast((sum(c2)#xL + cast(spark_catalog.default.foo3_1b(x#x) as bigint)) as int) AS x#x]
      +- Project [sum(c2)#xL, sum(c1)#xL, cast(sum(c1)#xL as int) AS x#x]
         +- Aggregate [sum(c2#x) AS sum(c2)#xL, sum(c1#x) AS sum(c1)#xL]
            +- SubqueryAlias spark_catalog.default.t1
               +- View (`spark_catalog`.`default`.`t1`, [c1#x, c2#x])
                  +- Project [cast(col1#x as int) AS c1#x, cast(col2#x as int) AS c2#x]
                     +- LocalRelation [col1#x, col2#x]


-- !query
SELECT c1, COUNT(*), foo3_1b(SUM(c2) + foo3_1b(SUM(c2))) FROM t1 GROUP BY c1 HAVING COUNT(*) > 0
-- !query analysis
Filter (count(1)#xL > cast(0 as bigint))
+- Project [c1#x, count(1)#xL, spark_catalog.default.foo3_1b(x#x) AS spark_catalog.default.foo3_1b((sum(c2) + spark_catalog.default.foo3_1b(sum(c2))))#x]
   +- Project [c1#x, count(1)#xL, sum(c2)#xL, sum(c2)#xL, x#x, cast((sum(c2)#xL + cast(spark_catalog.default.foo3_1b(x#x) as bigint)) as int) AS x#x]
      +- Project [c1#x, count(1)#xL, sum(c2)#xL, sum(c2)#xL, cast(sum(c2)#xL as int) AS x#x]
         +- Aggregate [c1#x], [c1#x, count(1) AS count(1)#xL, sum(c2#x) AS sum(c2)#xL, sum(c2#x) AS sum(c2)#xL]
            +- SubqueryAlias spark_catalog.default.t1
               +- View (`spark_catalog`.`default`.`t1`, [c1#x, c2#x])
                  +- Project [cast(col1#x as int) AS c1#x, cast(col2#x as int) AS c2#x]
                     +- LocalRelation [col1#x, col2#x]


-- !query
SELECT foo3_1a(c1, MAX(c2)) FROM t1 GROUP BY c1, 1
-- !query analysis
org.apache.spark.sql.AnalysisException
{
  "errorClass" : "GROUP_BY_POS_AGGREGATE",
  "sqlState" : "42903",
  "messageParameters" : {
    "aggExpr" : "spark_catalog.default.foo3_1a(spark_catalog.default.t1.c1, max(spark_catalog.default.t1.c2)) AS `spark_catalog.default.foo3_1a(c1, max(c2))`",
    "index" : "1"
  },
  "queryContext" : [ {
    "objectType" : "",
    "objectName" : "",
    "startIndex" : 50,
    "stopIndex" : 50,
    "fragment" : "1"
  } ]
}


-- !query
WITH cte AS (SELECT foo3_1a(c1, c2) FROM t1)
SELECT * FROM cte
-- !query analysis
WithCTE
:- CTERelationDef xxxx, false
:  +- SubqueryAlias cte
:     +- Project [spark_catalog.default.foo3_1a(a#x, b#x) AS spark_catalog.default.foo3_1a(c1, c2)#x]
:        +- Project [c1#x, c2#x, cast(c1#x as double) AS a#x, cast(c2#x as double) AS b#x]
:           +- SubqueryAlias spark_catalog.default.t1
:              +- View (`spark_catalog`.`default`.`t1`, [c1#x, c2#x])
:                 +- Project [cast(col1#x as int) AS c1#x, cast(col2#x as int) AS c2#x]
:                    +- LocalRelation [col1#x, col2#x]
+- Project [spark_catalog.default.foo3_1a(c1, c2)#x]
   +- SubqueryAlias cte
      +- CTERelationRef xxxx, true, [spark_catalog.default.foo3_1a(c1, c2)#x], false, false


-- !query
SELECT SUM(c2) FROM t1 GROUP BY foo3_1b(c1)
-- !query analysis
Project [sum(c2)#xL]
+- Aggregate [spark_catalog.default.foo3_1b#x], [sum(c2#x) AS sum(c2)#xL]
   +- Project [c1#x, c2#x, spark_catalog.default.foo3_1b(x#x) AS spark_catalog.default.foo3_1b#x]
      +- Project [c1#x, c2#x, cast(c1#x as int) AS x#x]
         +- SubqueryAlias spark_catalog.default.t1
            +- View (`spark_catalog`.`default`.`t1`, [c1#x, c2#x])
               +- Project [cast(col1#x as int) AS c1#x, cast(col2#x as int) AS c2#x]
                  +- LocalRelation [col1#x, col2#x]


-- !query
SELECT foo3_1b(c1), SUM(c2) FROM t1 GROUP BY 1
-- !query analysis
Project [spark_catalog.default.foo3_1b(c1)#x, sum(c2)#xL]
+- Aggregate [spark_catalog.default.foo3_1b#x], [spark_catalog.default.foo3_1b#x AS spark_catalog.default.foo3_1b(c1)#x, sum(c2#x) AS sum(c2)#xL]
   +- Project [c1#x, c2#x, spark_catalog.default.foo3_1b(x#x) AS spark_catalog.default.foo3_1b#x]
      +- Project [c1#x, c2#x, cast(c1#x as int) AS x#x]
         +- SubqueryAlias spark_catalog.default.t1
            +- View (`spark_catalog`.`default`.`t1`, [c1#x, c2#x])
               +- Project [cast(col1#x as int) AS c1#x, cast(col2#x as int) AS c2#x]
                  +- LocalRelation [col1#x, col2#x]


-- !query
SELECT foo3_1b(c1), c2, GROUPING(foo3_1b(c1)), SUM(c1) FROM t1 GROUP BY ROLLUP(foo3_1b(c1), c2)
-- !query analysis
Aggregate [spark_catalog.default.foo3_1b(c1)#x, c2#x, spark_grouping_id#xL], [spark_catalog.default.foo3_1b(c1)#x AS spark_catalog.default.foo3_1b(c1)#x, c2#x, cast((shiftright(spark_grouping_id#xL, 1) & 1) as tinyint) AS grouping(spark_catalog.default.foo3_1b(c1))#x, sum(c1#x) AS sum(c1)#xL]
+- Expand [[c1#x, c2#x, spark_catalog.default.foo3_1b(c1)#x, c2#x, 0], [c1#x, c2#x, spark_catalog.default.foo3_1b(c1)#x, null, 1], [c1#x, c2#x, null, null, 3]], [c1#x, c2#x, spark_catalog.default.foo3_1b(c1)#x, c2#x, spark_grouping_id#xL]
   +- Project [c1#x, c2#x, spark_catalog.default.foo3_1b(x#x) AS spark_catalog.default.foo3_1b(c1)#x, c2#x AS c2#x]
      +- Project [c1#x, c2#x, cast(c1#x as int) AS x#x]
         +- SubqueryAlias spark_catalog.default.t1
            +- View (`spark_catalog`.`default`.`t1`, [c1#x, c2#x])
               +- Project [cast(col1#x as int) AS c1#x, cast(col2#x as int) AS c2#x]
                  +- LocalRelation [col1#x, col2#x]


-- !query
SELECT c1, SUM(c2) FROM t1 GROUP BY c1 HAVING foo3_1b(SUM(c2)) > 1
-- !query analysis
Project [c1#x, sum(c2)#xL]
+- Filter (spark_catalog.default.foo3_1b(x#x) > 1)
   +- Project [c1#x, sum(c2)#xL, cast(sum(c2)#xL as int) AS x#x]
      +- Aggregate [c1#x], [c1#x, sum(c2#x) AS sum(c2)#xL]
         +- SubqueryAlias spark_catalog.default.t1
            +- View (`spark_catalog`.`default`.`t1`, [c1#x, c2#x])
               +- Project [cast(col1#x as int) AS c1#x, cast(col2#x as int) AS c2#x]
                  +- LocalRelation [col1#x, col2#x]


-- !query
SELECT c1, SUM(c2) FROM t1 GROUP BY CUBE(c1) HAVING foo3_1b(GROUPING(c1)) = 0
-- !query analysis
Project [c1#x, sum(c2)#xL]
+- Project [c1#x, sum(c2)#xL, spark_grouping_id#xL]
   +- Filter (spark_catalog.default.foo3_1b(x#x) = 0)
      +- Project [c1#x, sum(c2)#xL, spark_grouping_id#xL, cast(cast((shiftright(spark_grouping_id#xL, 0) & 1) as tinyint) as int) AS x#x]
         +- Aggregate [c1#x, spark_grouping_id#xL], [c1#x, sum(c2#x) AS sum(c2)#xL, spark_grouping_id#xL]
            +- Expand [[c1#x, c2#x, c1#x, 0], [c1#x, c2#x, null, 1]], [c1#x, c2#x, c1#x, spark_grouping_id#xL]
               +- Project [c1#x, c2#x, c1#x AS c1#x]
                  +- SubqueryAlias spark_catalog.default.t1
                     +- View (`spark_catalog`.`default`.`t1`, [c1#x, c2#x])
                        +- Project [cast(col1#x as int) AS c1#x, cast(col2#x as int) AS c2#x]
                           +- LocalRelation [col1#x, col2#x]


-- !query
SELECT * FROM t1 JOIN t2 ON foo3_1a(t1.c1, t2.c2) >= 2
-- !query analysis
Project [c1#x, c2#x, c1#x, c2#x]
+- Join Inner, (spark_catalog.default.foo3_1a(a#x, b#x) >= cast(2 as double))
   :- SubqueryAlias spark_catalog.default.t1
   :  +- View (`spark_catalog`.`default`.`t1`, [c1#x, c2#x])
   :     +- Project [cast(col1#x as int) AS c1#x, cast(col2#x as int) AS c2#x]
   :        +- LocalRelation [col1#x, col2#x]
   +- SubqueryAlias spark_catalog.default.t2
      +- View (`spark_catalog`.`default`.`t2`, [c1#x, c2#x])
         +- Project [cast(col1#x as int) AS c1#x, cast(col2#x as int) AS c2#x]
            +- LocalRelation [col1#x, col2#x]


-- !query
SELECT * FROM t1 JOIN t2 ON foo3_1b(t1.c2) = foo3_1b(t2.c2)
-- !query analysis
Project [c1#x, c2#x, c1#x, c2#x]
+- Join Inner, (spark_catalog.default.foo3_1b(x#x) = spark_catalog.default.foo3_1b(x#x))
   :- SubqueryAlias spark_catalog.default.t1
   :  +- View (`spark_catalog`.`default`.`t1`, [c1#x, c2#x])
   :     +- Project [cast(col1#x as int) AS c1#x, cast(col2#x as int) AS c2#x]
   :        +- LocalRelation [col1#x, col2#x]
   +- SubqueryAlias spark_catalog.default.t2
      +- View (`spark_catalog`.`default`.`t2`, [c1#x, c2#x])
         +- Project [cast(col1#x as int) AS c1#x, cast(col2#x as int) AS c2#x]
            +- LocalRelation [col1#x, col2#x]


-- !query
SELECT * FROM t1 JOIN t2 ON foo3_1b(t1.c1 + t2.c1 + 2) > 2
-- !query analysis
Project [c1#x, c2#x, c1#x, c2#x]
+- Join Inner, (spark_catalog.default.foo3_1b(x#x) > 2)
   :- SubqueryAlias spark_catalog.default.t1
   :  +- View (`spark_catalog`.`default`.`t1`, [c1#x, c2#x])
   :     +- Project [cast(col1#x as int) AS c1#x, cast(col2#x as int) AS c2#x]
   :        +- LocalRelation [col1#x, col2#x]
   +- SubqueryAlias spark_catalog.default.t2
      +- View (`spark_catalog`.`default`.`t2`, [c1#x, c2#x])
         +- Project [cast(col1#x as int) AS c1#x, cast(col2#x as int) AS c2#x]
            +- LocalRelation [col1#x, col2#x]


-- !query
SELECT * FROM t1 JOIN t2 ON foo3_1a(foo3_1b(t1.c1), t2.c2) >= 2
-- !query analysis
Project [c1#x, c2#x, c1#x, c2#x]
+- Join Inner, (spark_catalog.default.foo3_1a(a#x, b#x) >= cast(2 as double))
   :- SubqueryAlias spark_catalog.default.t1
   :  +- View (`spark_catalog`.`default`.`t1`, [c1#x, c2#x])
   :     +- Project [cast(col1#x as int) AS c1#x, cast(col2#x as int) AS c2#x]
   :        +- LocalRelation [col1#x, col2#x]
   +- SubqueryAlias spark_catalog.default.t2
      +- View (`spark_catalog`.`default`.`t2`, [c1#x, c2#x])
         +- Project [cast(col1#x as int) AS c1#x, cast(col2#x as int) AS c2#x]
            +- LocalRelation [col1#x, col2#x]


-- !query
SELECT * FROM t1 JOIN t2 ON foo3_1f() > 0
-- !query analysis
Project [c1#x, c2#x, c1#x, c2#x]
+- Join Inner, (spark_catalog.default.foo3_1f() > 0)
   :  +- Aggregate [sum(c2#x) AS sum(c2)#xL]
   :     +- Filter (c1#x = 0)
   :        +- SubqueryAlias spark_catalog.default.t2
   :           +- View (`spark_catalog`.`default`.`t2`, [c1#x, c2#x])
   :              +- Project [cast(col1#x as int) AS c1#x, cast(col2#x as int) AS c2#x]
   :                 +- LocalRelation [col1#x, col2#x]
   :- SubqueryAlias spark_catalog.default.t1
   :  +- View (`spark_catalog`.`default`.`t1`, [c1#x, c2#x])
   :     +- Project [cast(col1#x as int) AS c1#x, cast(col2#x as int) AS c2#x]
   :        +- LocalRelation [col1#x, col2#x]
   +- SubqueryAlias spark_catalog.default.t2
      +- View (`spark_catalog`.`default`.`t2`, [c1#x, c2#x])
         +- Project [cast(col1#x as int) AS c1#x, cast(col2#x as int) AS c2#x]
            +- LocalRelation [col1#x, col2#x]


-- !query
SELECT * FROM t1 JOIN t2 ON foo3_1b(t1.c1 + rand(0) * 0) > 1
-- !query analysis
org.apache.spark.sql.catalyst.ExtendedAnalysisException
{
  "errorClass" : "INVALID_NON_DETERMINISTIC_EXPRESSIONS",
  "sqlState" : "42K0E",
  "messageParameters" : {
    "sqlExprs" : "\"(spark_catalog.default.foo3_1b(foo3_1b.x) > 1)\""
  }
}


-- !query
SELECT * FROM t1 JOIN t2 ON foo3_1c(t1.c1) = 2
-- !query analysis
Project [c1#x, c2#x, c1#x, c2#x]
+- Join Inner, (spark_catalog.default.foo3_1c(x#x) = 2)
   :- SubqueryAlias spark_catalog.default.t1
   :  +- View (`spark_catalog`.`default`.`t1`, [c1#x, c2#x])
   :     +- Project [cast(col1#x as int) AS c1#x, cast(col2#x as int) AS c2#x]
   :        +- LocalRelation [col1#x, col2#x]
   +- SubqueryAlias spark_catalog.default.t2
      +- View (`spark_catalog`.`default`.`t2`, [c1#x, c2#x])
         +- Project [cast(col1#x as int) AS c1#x, cast(col2#x as int) AS c2#x]
            +- LocalRelation [col1#x, col2#x]


-- !query
SELECT * FROM t1 JOIN t2 ON foo3_1g(t1.c1) = 2
-- !query analysis
org.apache.spark.sql.catalyst.ExtendedAnalysisException
{
  "errorClass" : "UNSUPPORTED_SUBQUERY_EXPRESSION_CATEGORY.UNSUPPORTED_CORRELATED_SCALAR_SUBQUERY",
  "sqlState" : "0A000",
  "messageParameters" : {
    "treeNode" : "Join Inner, (spark_catalog.default.foo3_1g(x#x) = 2)\n:  +- Project [outer(x#x)]\n:     +- OneRowRelation\n:- SubqueryAlias spark_catalog.default.t1\n:  +- View (`spark_catalog`.`default`.`t1`, [c1#x, c2#x])\n:     +- Project [cast(col1#x as int) AS c1#x, cast(col2#x as int) AS c2#x]\n:        +- LocalRelation [col1#x, col2#x]\n+- SubqueryAlias spark_catalog.default.t2\n   +- View (`spark_catalog`.`default`.`t2`, [c1#x, c2#x])\n      +- Project [cast(col1#x as int) AS c1#x, cast(col2#x as int) AS c2#x]\n         +- LocalRelation [col1#x, col2#x]\n"
  }
}


-- !query
SELECT * FROM t1 ORDER BY foo3_1b(c1)
-- !query analysis
org.apache.spark.sql.AnalysisException
{
  "errorClass" : "UNSUPPORTED_SQL_UDF_USAGE",
  "sqlState" : "0A000",
  "messageParameters" : {
    "functionName" : "`spark_catalog`.`default`.`foo3_1b`",
    "nodeName" : "Sort"
  },
  "queryContext" : [ {
    "objectType" : "",
    "objectName" : "",
    "startIndex" : 27,
    "stopIndex" : 37,
    "fragment" : "foo3_1b(c1)"
  } ]
}


-- !query
SELECT * FROM t1 LIMIT foo3_1b(1)
-- !query analysis
org.apache.spark.sql.AnalysisException
{
  "errorClass" : "UNSUPPORTED_SQL_UDF_USAGE",
  "sqlState" : "0A000",
  "messageParameters" : {
    "functionName" : "`spark_catalog`.`default`.`foo3_1b`",
    "nodeName" : "GlobalLimit"
  },
  "queryContext" : [ {
    "objectType" : "",
    "objectName" : "",
    "startIndex" : 24,
    "stopIndex" : 33,
    "fragment" : "foo3_1b(1)"
  } ]
}


-- !query
SELECT * FROM ta LATERAL VIEW EXPLODE(ARRAY(foo3_1b(x[0]), foo3_1b(x[1]))) AS t
-- !query analysis
org.apache.spark.sql.AnalysisException
{
  "errorClass" : "UNSUPPORTED_SQL_UDF_USAGE",
  "sqlState" : "0A000",
  "messageParameters" : {
    "functionName" : "`spark_catalog`.`default`.`foo3_1b`",
    "nodeName" : "Generate"
  },
  "queryContext" : [ {
    "objectType" : "",
    "objectName" : "",
    "startIndex" : 45,
    "stopIndex" : 57,
    "fragment" : "foo3_1b(x[0])"
  } ]
}


-- !query
SELECT CASE WHEN foo3_1b(rand(0) * 0 < 1 THEN 1 ELSE -1 END
-- !query analysis
org.apache.spark.sql.catalyst.parser.ParseException
{
  "errorClass" : "PARSE_SYNTAX_ERROR",
  "sqlState" : "42601",
  "messageParameters" : {
    "error" : "'foo3_1b'",
    "hint" : ""
  }
}


-- !query
SELECT (SELECT SUM(c2) FROM t2 WHERE c1 = foo3_1b(t1.c1)) FROM t1
-- !query analysis
Project [scalar-subquery#x [c1#x] AS scalarsubquery(c1)#xL]
:  +- Aggregate [sum(c2#x) AS sum(c2)#xL]
:     +- Project [c1#x, c2#x]
:        +- Filter (c1#x = spark_catalog.default.foo3_1b(x#x))
:           +- Project [c1#x, c2#x, cast(outer(c1#x) as int) AS x#x]
:              +- SubqueryAlias spark_catalog.default.t2
:                 +- View (`spark_catalog`.`default`.`t2`, [c1#x, c2#x])
:                    +- Project [cast(col1#x as int) AS c1#x, cast(col2#x as int) AS c2#x]
:                       +- LocalRelation [col1#x, col2#x]
+- SubqueryAlias spark_catalog.default.t1
   +- View (`spark_catalog`.`default`.`t1`, [c1#x, c2#x])
      +- Project [cast(col1#x as int) AS c1#x, cast(col2#x as int) AS c2#x]
         +- LocalRelation [col1#x, col2#x]


-- !query
SELECT foo3_1b((SELECT SUM(c1) FROM t1))
-- !query analysis
Project [spark_catalog.default.foo3_1b(x#x) AS spark_catalog.default.foo3_1b(scalarsubquery())#x]
+- Project [cast(scalar-subquery#x [] as int) AS x#x]
   :  +- Aggregate [sum(c1#x) AS sum(c1)#xL]
   :     +- SubqueryAlias spark_catalog.default.t1
   :        +- View (`spark_catalog`.`default`.`t1`, [c1#x, c2#x])
   :           +- Project [cast(col1#x as int) AS c1#x, cast(col2#x as int) AS c2#x]
   :              +- LocalRelation [col1#x, col2#x]
   +- OneRowRelation


-- !query
SELECT foo3_1a(c1, (SELECT MIN(c1) FROM t1)) FROM t1
-- !query analysis
Project [spark_catalog.default.foo3_1a(a#x, b#x) AS spark_catalog.default.foo3_1a(c1, scalarsubquery())#x]
+- Project [c1#x, c2#x, cast(c1#x as double) AS a#x, cast(scalar-subquery#x [] as double) AS b#x]
   :  +- Aggregate [min(c1#x) AS min(c1)#x]
   :     +- SubqueryAlias spark_catalog.default.t1
   :        +- View (`spark_catalog`.`default`.`t1`, [c1#x, c2#x])
   :           +- Project [cast(col1#x as int) AS c1#x, cast(col2#x as int) AS c2#x]
   :              +- LocalRelation [col1#x, col2#x]
   +- SubqueryAlias spark_catalog.default.t1
      +- View (`spark_catalog`.`default`.`t1`, [c1#x, c2#x])
         +- Project [cast(col1#x as int) AS c1#x, cast(col2#x as int) AS c2#x]
            +- LocalRelation [col1#x, col2#x]


-- !query
SELECT foo3_1b((SELECT SUM(c1))) FROM t1
-- !query analysis
org.apache.spark.sql.catalyst.ExtendedAnalysisException
{
  "errorClass" : "UNSUPPORTED_SUBQUERY_EXPRESSION_CATEGORY.CORRELATED_REFERENCE",
  "sqlState" : "0A000",
  "messageParameters" : {
    "sqlExprs" : "\"sum(c1) AS `sum(outer(spark_catalog.default.t1.c1))`\""
  },
  "queryContext" : [ {
    "objectType" : "",
    "objectName" : "",
    "startIndex" : 17,
    "stopIndex" : 30,
    "fragment" : "SELECT SUM(c1)"
  } ]
}


-- !query
SELECT foo3_1b((SELECT SUM(c1) FROM t1 WHERE c2 = t2.c2)) FROM t2
-- !query analysis
Project [spark_catalog.default.foo3_1b(x#x) AS spark_catalog.default.foo3_1b(scalarsubquery(c2))#x]
+- Project [c1#x, c2#x, cast(scalar-subquery#x [c2#x] as int) AS x#x]
   :  +- Aggregate [sum(c1#x) AS sum(c1)#xL]
   :     +- Filter (c2#x = outer(c2#x))
   :        +- SubqueryAlias spark_catalog.default.t1
   :           +- View (`spark_catalog`.`default`.`t1`, [c1#x, c2#x])
   :              +- Project [cast(col1#x as int) AS c1#x, cast(col2#x as int) AS c2#x]
   :                 +- LocalRelation [col1#x, col2#x]
   +- SubqueryAlias spark_catalog.default.t2
      +- View (`spark_catalog`.`default`.`t2`, [c1#x, c2#x])
         +- Project [cast(col1#x as int) AS c1#x, cast(col2#x as int) AS c2#x]
            +- LocalRelation [col1#x, col2#x]


-- !query
SELECT c2, AVG(foo3_1b((SELECT COUNT(*) FROM t1 WHERE c2 = t2.c2))) OVER (PARTITION BY c1) AS r FROM t2
-- !query analysis
Project [c2#x, r#x]
+- Project [c2#x, _w0#x, c1#x, r#x, r#x]
   +- Window [avg(_w0#x) windowspecdefinition(c1#x, specifiedwindowframe(RowFrame, unboundedpreceding$(), unboundedfollowing$())) AS r#x], [c1#x]
      +- Project [c2#x, spark_catalog.default.foo3_1b(x#x) AS _w0#x, c1#x]
         +- Project [c1#x, c2#x, cast(scalar-subquery#x [c2#x] as int) AS x#x]
            :  +- Aggregate [count(1) AS count(1)#xL]
            :     +- Filter (c2#x = outer(c2#x))
            :        +- SubqueryAlias spark_catalog.default.t1
            :           +- View (`spark_catalog`.`default`.`t1`, [c1#x, c2#x])
            :              +- Project [cast(col1#x as int) AS c1#x, cast(col2#x as int) AS c2#x]
            :                 +- LocalRelation [col1#x, col2#x]
            +- SubqueryAlias spark_catalog.default.t2
               +- View (`spark_catalog`.`default`.`t2`, [c1#x, c2#x])
                  +- Project [cast(col1#x as int) AS c1#x, cast(col2#x as int) AS c2#x]
                     +- LocalRelation [col1#x, col2#x]


-- !query
CREATE FUNCTION foo3_1x(x STRUCT<a: INT, b: INT>) RETURNS INT RETURN x.a + x.b
-- !query analysis
CreateSQLFunctionCommand spark_catalog.default.foo3_1x, x STRUCT<a: INT, b: INT>, INT, x.a + x.b, false, false, false, false


-- !query
CREATE FUNCTION foo3_1y(x ARRAY<INT>) RETURNS INT RETURN aggregate(x, BIGINT(0), (x, y) -> x + y)
-- !query analysis
CreateSQLFunctionCommand spark_catalog.default.foo3_1y, x ARRAY<INT>, INT, aggregate(x, BIGINT(0), (x, y) -> x + y), false, false, false, false


-- !query
SELECT foo3_1a(x.a, x.b) FROM ts
-- !query analysis
Project [spark_catalog.default.foo3_1a(a#x, b#x) AS spark_catalog.default.foo3_1a(x.a, x.b)#x]
+- Project [x#x, cast(x#x.a as double) AS a#x, cast(x#x.b as double) AS b#x]
   +- SubqueryAlias spark_catalog.default.ts
      +- View (`spark_catalog`.`default`.`ts`, [x#x])
         +- Project [cast(col1#x as struct<a:int,b:int>) AS x#x]
            +- LocalRelation [col1#x]


-- !query
SELECT foo3_1x(x) FROM ts
-- !query analysis
Project [spark_catalog.default.foo3_1x(x#x) AS spark_catalog.default.foo3_1x(x)#x]
+- Project [x#x, cast(x#x as struct<a:int,b:int>) AS x#x]
   +- SubqueryAlias spark_catalog.default.ts
      +- View (`spark_catalog`.`default`.`ts`, [x#x])
         +- Project [cast(col1#x as struct<a:int,b:int>) AS x#x]
            +- LocalRelation [col1#x]


-- !query
SELECT foo3_1a(x['a'], x['b']) FROM tm
-- !query analysis
Project [spark_catalog.default.foo3_1a(a#x, b#x) AS spark_catalog.default.foo3_1a(x[a], x[b])#x]
+- Project [x#x, cast(x#x[a] as double) AS a#x, cast(x#x[b] as double) AS b#x]
   +- SubqueryAlias spark_catalog.default.tm
      +- View (`spark_catalog`.`default`.`tm`, [x#x])
         +- Project [cast(col1#x as map<string,int>) AS x#x]
            +- LocalRelation [col1#x]


-- !query
SELECT foo3_1a(x[0], x[1]) FROM ta
-- !query analysis
Project [spark_catalog.default.foo3_1a(a#x, b#x) AS spark_catalog.default.foo3_1a(x[0], x[1])#x]
+- Project [x#x, cast(x#x[0] as double) AS a#x, cast(x#x[1] as double) AS b#x]
   +- SubqueryAlias spark_catalog.default.ta
      +- View (`spark_catalog`.`default`.`ta`, [x#x])
         +- Project [cast(col1#x as array<int>) AS x#x]
            +- LocalRelation [col1#x]


-- !query
SELECT foo3_1y(x) FROM ta
-- !query analysis
Project [spark_catalog.default.foo3_1y(x#x) AS spark_catalog.default.foo3_1y(x)#x]
+- Project [x#x, cast(x#x as array<int>) AS x#x]
   +- SubqueryAlias spark_catalog.default.ta
      +- View (`spark_catalog`.`default`.`ta`, [x#x])
         +- Project [cast(col1#x as array<int>) AS x#x]
            +- LocalRelation [col1#x]


-- !query
CREATE FUNCTION foo3_2a() RETURNS INT RETURN FLOOR(RAND() * 6) + 1
-- !query analysis
CreateSQLFunctionCommand spark_catalog.default.foo3_2a, INT, FLOOR(RAND() * 6) + 1, false, false, false, false


-- !query
SELECT CASE WHEN foo3_2a() > 6 THEN FALSE ELSE TRUE END
-- !query analysis
[Analyzer test output redacted due to nondeterminism]


-- !query
SELECT * FROM t1 JOIN t2 ON foo3_2a() = 1
-- !query analysis
org.apache.spark.sql.catalyst.ExtendedAnalysisException
{
  "errorClass" : "INVALID_NON_DETERMINISTIC_EXPRESSIONS",
  "sqlState" : "42K0E",
  "messageParameters" : {
    "sqlExprs" : "\"(spark_catalog.default.foo3_2a() = 1)\""
  }
}


-- !query
CREATE FUNCTION foo3_2b1(x INT) RETURNS BOOLEAN RETURN x IN (SELECT 1)
-- !query analysis
CreateSQLFunctionCommand spark_catalog.default.foo3_2b1, x INT, BOOLEAN, x IN (SELECT 1), false, false, false, false


-- !query
SELECT * FROM t1 WHERE foo3_2b1(c1)
-- !query analysis
Project [c1#x, c2#x]
+- Project [c1#x, c2#x]
   +- Filter spark_catalog.default.foo3_2b1(x#x)
      :  +- Project [1 AS 1#x]
      :     +- OneRowRelation
      +- Project [c1#x, c2#x, cast(c1#x as int) AS x#x]
         +- SubqueryAlias spark_catalog.default.t1
            +- View (`spark_catalog`.`default`.`t1`, [c1#x, c2#x])
               +- Project [cast(col1#x as int) AS c1#x, cast(col2#x as int) AS c2#x]
                  +- LocalRelation [col1#x, col2#x]


-- !query
CREATE FUNCTION foo3_2b2(x INT) RETURNS INT RETURN IF(x IN (SELECT 1), 1, 0)
-- !query analysis
CreateSQLFunctionCommand spark_catalog.default.foo3_2b2, x INT, INT, IF(x IN (SELECT 1), 1, 0), false, false, false, false


-- !query
SELECT * FROM t1 WHERE foo3_2b2(c1) = 0
-- !query analysis
Project [c1#x, c2#x]
+- Project [c1#x, c2#x]
   +- Filter (spark_catalog.default.foo3_2b2(x#x) = 0)
      :  +- Project [1 AS 1#x]
      :     +- OneRowRelation
      +- Project [c1#x, c2#x, cast(c1#x as int) AS x#x]
         +- SubqueryAlias spark_catalog.default.t1
            +- View (`spark_catalog`.`default`.`t1`, [c1#x, c2#x])
               +- Project [cast(col1#x as int) AS c1#x, cast(col2#x as int) AS c2#x]
                  +- LocalRelation [col1#x, col2#x]


-- !query
SELECT foo3_2b2(c1) FROM t1
-- !query analysis
Project [spark_catalog.default.foo3_2b2(x#x) AS spark_catalog.default.foo3_2b2(c1)#x]
:  +- Project [1 AS 1#x]
:     +- OneRowRelation
+- Project [c1#x, c2#x, cast(c1#x as int) AS x#x]
   +- SubqueryAlias spark_catalog.default.t1
      +- View (`spark_catalog`.`default`.`t1`, [c1#x, c2#x])
         +- Project [cast(col1#x as int) AS c1#x, cast(col2#x as int) AS c2#x]
            +- LocalRelation [col1#x, col2#x]


-- !query
CREATE FUNCTION foo3_2b3(x INT) RETURNS BOOLEAN RETURN x IN (SELECT c1 FROM t2)
-- !query analysis
CreateSQLFunctionCommand spark_catalog.default.foo3_2b3, x INT, BOOLEAN, x IN (SELECT c1 FROM t2), false, false, false, false


-- !query
SELECT * FROM t1 WHERE foo3_2b3(c1)
-- !query analysis
Project [c1#x, c2#x]
+- Project [c1#x, c2#x]
   +- Filter spark_catalog.default.foo3_2b3(x#x)
      :  +- Project [c1#x]
      :     +- SubqueryAlias spark_catalog.default.t2
      :        +- View (`spark_catalog`.`default`.`t2`, [c1#x, c2#x])
      :           +- Project [cast(col1#x as int) AS c1#x, cast(col2#x as int) AS c2#x]
      :              +- LocalRelation [col1#x, col2#x]
      +- Project [c1#x, c2#x, cast(c1#x as int) AS x#x]
         +- SubqueryAlias spark_catalog.default.t1
            +- View (`spark_catalog`.`default`.`t1`, [c1#x, c2#x])
               +- Project [cast(col1#x as int) AS c1#x, cast(col2#x as int) AS c2#x]
                  +- LocalRelation [col1#x, col2#x]


-- !query
CREATE FUNCTION foo3_2b4(x INT) RETURNS BOOLEAN RETURN x NOT IN (SELECT c2 FROM t2 WHERE x = c1)
-- !query analysis
CreateSQLFunctionCommand spark_catalog.default.foo3_2b4, x INT, BOOLEAN, x NOT IN (SELECT c2 FROM t2 WHERE x = c1), false, false, false, false


-- !query
SELECT * FROM t1 WHERE foo3_2b4(c1)
-- !query analysis
Project [c1#x, c2#x]
+- Project [c1#x, c2#x]
   +- Filter spark_catalog.default.foo3_2b4(x#x)
      :  +- Project [c2#x]
      :     +- Filter (outer(x#x) = c1#x)
      :        +- SubqueryAlias spark_catalog.default.t2
      :           +- View (`spark_catalog`.`default`.`t2`, [c1#x, c2#x])
      :              +- Project [cast(col1#x as int) AS c1#x, cast(col2#x as int) AS c2#x]
      :                 +- LocalRelation [col1#x, col2#x]
      +- Project [c1#x, c2#x, cast(c1#x as int) AS x#x]
         +- SubqueryAlias spark_catalog.default.t1
            +- View (`spark_catalog`.`default`.`t1`, [c1#x, c2#x])
               +- Project [cast(col1#x as int) AS c1#x, cast(col2#x as int) AS c2#x]
                  +- LocalRelation [col1#x, col2#x]


-- !query
CREATE FUNCTION foo3_2b5(x INT) RETURNS BOOLEAN RETURN SUM(1) + IF(x IN (SELECT 1), 1, 0)
-- !query analysis
org.apache.spark.sql.AnalysisException
{
  "errorClass" : "USER_DEFINED_FUNCTIONS.CANNOT_CONTAIN_COMPLEX_FUNCTIONS",
  "sqlState" : "42601",
  "messageParameters" : {
    "queryText" : "SUM(1) + IF(x IN (SELECT 1), 1, 0)"
  }
}


-- !query
CREATE FUNCTION foo3_2b5(x INT) RETURNS BOOLEAN RETURN y IN (SELECT 1)
-- !query analysis
org.apache.spark.sql.catalyst.ExtendedAnalysisException
{
  "errorClass" : "UNRESOLVED_COLUMN.WITH_SUGGESTION",
  "sqlState" : "42703",
  "messageParameters" : {
    "objectName" : "`y`",
    "proposal" : "`x`"
  },
  "queryContext" : [ {
    "objectType" : "",
    "objectName" : "",
    "startIndex" : 1,
    "stopIndex" : 1,
    "fragment" : "y"
  } ]
}


-- !query
CREATE FUNCTION foo3_2b5(x INT) RETURNS BOOLEAN RETURN x IN (SELECT x WHERE x = 1)
-- !query analysis
CreateSQLFunctionCommand spark_catalog.default.foo3_2b5, x INT, BOOLEAN, x IN (SELECT x WHERE x = 1), false, false, false, false


-- !query
CREATE FUNCTION foo3_2c1(x INT) RETURNS BOOLEAN RETURN EXISTS(SELECT 1)
-- !query analysis
CreateSQLFunctionCommand spark_catalog.default.foo3_2c1, x INT, BOOLEAN, EXISTS(SELECT 1), false, false, false, false


-- !query
SELECT * FROM t1 WHERE foo3_2c1(c1)
-- !query analysis
Project [c1#x, c2#x]
+- Project [c1#x, c2#x]
   +- Filter spark_catalog.default.foo3_2c1(x#x)
      :  +- Project [1 AS 1#x]
      :     +- OneRowRelation
      +- Project [c1#x, c2#x, cast(c1#x as int) AS x#x]
         +- SubqueryAlias spark_catalog.default.t1
            +- View (`spark_catalog`.`default`.`t1`, [c1#x, c2#x])
               +- Project [cast(col1#x as int) AS c1#x, cast(col2#x as int) AS c2#x]
                  +- LocalRelation [col1#x, col2#x]


-- !query
CREATE FUNCTION foo3_2c2(x INT) RETURNS BOOLEAN RETURN NOT EXISTS(SELECT * FROM t2 WHERE c1 = x)
-- !query analysis
CreateSQLFunctionCommand spark_catalog.default.foo3_2c2, x INT, BOOLEAN, NOT EXISTS(SELECT * FROM t2 WHERE c1 = x), false, false, false, false


-- !query
SELECT * FROM t1 WHERE foo3_2c2(c1)
-- !query analysis
Project [c1#x, c2#x]
+- Project [c1#x, c2#x]
   +- Filter spark_catalog.default.foo3_2c2(x#x)
      :  +- Project [c1#x, c2#x]
      :     +- Filter (c1#x = outer(x#x))
      :        +- SubqueryAlias spark_catalog.default.t2
      :           +- View (`spark_catalog`.`default`.`t2`, [c1#x, c2#x])
      :              +- Project [cast(col1#x as int) AS c1#x, cast(col2#x as int) AS c2#x]
      :                 +- LocalRelation [col1#x, col2#x]
      +- Project [c1#x, c2#x, cast(c1#x as int) AS x#x]
         +- SubqueryAlias spark_catalog.default.t1
            +- View (`spark_catalog`.`default`.`t1`, [c1#x, c2#x])
               +- Project [cast(col1#x as int) AS c1#x, cast(col2#x as int) AS c2#x]
                  +- LocalRelation [col1#x, col2#x]


-- !query
CREATE FUNCTION foo3_2d1(x INT) RETURNS INT RETURN SELECT (SELECT x)
-- !query analysis
CreateSQLFunctionCommand spark_catalog.default.foo3_2d1, x INT, INT, SELECT (SELECT x), false, false, false, false


-- !query
CREATE FUNCTION foo3_2d2(x INT) RETURNS INT RETURN SELECT (SELECT 1 WHERE EXISTS (SELECT * FROM t2 WHERE c1 = x))
-- !query analysis
org.apache.spark.sql.catalyst.ExtendedAnalysisException
{
  "errorClass" : "UNRESOLVED_COLUMN.WITH_SUGGESTION",
  "sqlState" : "42703",
  "messageParameters" : {
    "objectName" : "`x`",
    "proposal" : "`c1`, `c2`"
  },
  "queryContext" : [ {
    "objectType" : "",
    "objectName" : "",
    "startIndex" : 60,
    "stopIndex" : 60,
    "fragment" : "x"
  } ]
}


-- !query
CREATE FUNCTION foo3_2e1(
    occurrences ARRAY<STRUCT<start_time: TIMESTAMP, occurrence_id: STRING>>,
    instance_start_time TIMESTAMP
) RETURNS STRING RETURN
WITH t AS (
    SELECT transform(occurrences, x -> named_struct(
        'diff', abs(unix_millis(x.start_time) - unix_millis(instance_start_time)),
        'id', x.occurrence_id
    )) AS diffs
)
SELECT CASE WHEN occurrences IS NULL OR size(occurrences) = 0
       THEN NULL
       ELSE sort_array(diffs)[0].id END AS id
FROM t
-- !query analysis
CreateSQLFunctionCommand spark_catalog.default.foo3_2e1, occurrences ARRAY<STRUCT<start_time: TIMESTAMP, occurrence_id: STRING>>,
    instance_start_time TIMESTAMP, STRING, WITH t AS (
    SELECT transform(occurrences, x -> named_struct(
        'diff', abs(unix_millis(x.start_time) - unix_millis(instance_start_time)),
        'id', x.occurrence_id
    )) AS diffs
)
SELECT CASE WHEN occurrences IS NULL OR size(occurrences) = 0
       THEN NULL
       ELSE sort_array(diffs)[0].id END AS id
FROM t, false, false, false, false


-- !query
SELECT foo3_2e1(
    ARRAY(STRUCT('2022-01-01 10:11:12', '1'), STRUCT('2022-01-01 10:11:15', '2')),
    '2022-01-01')
-- !query analysis
Project [spark_catalog.default.foo3_2e1(occurrences#x, instance_start_time#x) AS spark_catalog.default.foo3_2e1(array(struct(2022-01-01 10:11:12, 1), struct(2022-01-01 10:11:15, 2)), 2022-01-01)#x]
:  +- WithCTE
:     :- CTERelationDef xxxx, false
:     :  +- SubqueryAlias t
:     :     +- Project [transform(outer(occurrences#x), lambdafunction(named_struct(diff, abs((unix_millis(lambda x#x.start_time) - unix_millis(outer(instance_start_time#x)))), id, lambda x#x.occurrence_id), lambda x#x, false)) AS diffs#x]
:     :        +- OneRowRelation
:     +- Project [CASE WHEN (isnull(outer(occurrences#x)) OR (size(outer(occurrences#x), false) = 0)) THEN cast(null as string) ELSE sort_array(diffs#x, true)[0].id END AS id#x]
:        +- SubqueryAlias t
:           +- CTERelationRef xxxx, true, [diffs#x], false, false, 1
+- Project [cast(array(struct(col1, 2022-01-01 10:11:12, col2, 1), struct(col1, 2022-01-01 10:11:15, col2, 2)) as array<struct<start_time:timestamp,occurrence_id:string>>) AS occurrences#x, cast(2022-01-01 as timestamp) AS instance_start_time#x]
   +- OneRowRelation


-- !query
SET spark.sql.ansi.enabled=true
-- !query analysis
SetCommand (spark.sql.ansi.enabled,Some(true))


-- !query
CREATE FUNCTION foo3_3a(x INT) RETURNS DOUBLE RETURN 1 / x
-- !query analysis
CreateSQLFunctionCommand spark_catalog.default.foo3_3a, x INT, DOUBLE, 1 / x, false, false, false, false


-- !query
CREATE FUNCTION foo3_3at(x INT) RETURNS TABLE (a DOUBLE) RETURN SELECT 1 / x
-- !query analysis
CreateSQLFunctionCommand spark_catalog.default.foo3_3at, x INT, a DOUBLE, SELECT 1 / x, true, false, false, false


-- !query
CREATE TEMPORARY FUNCTION foo3_3b(x INT) RETURNS DOUBLE RETURN 1 / x
-- !query analysis
CreateSQLFunctionCommand foo3_3b, x INT, DOUBLE, 1 / x, false, true, false, false


-- !query
SET spark.sql.ansi.enabled=false
-- !query analysis
SetCommand (spark.sql.ansi.enabled,Some(false))


-- !query
SELECT foo3_3a(0)
-- !query analysis
Project [spark_catalog.default.foo3_3a(x#x) AS spark_catalog.default.foo3_3a(0)#x]
+- Project [cast(0 as int) AS x#x]
   +- OneRowRelation


-- !query
SELECT foo3_3b(0)
-- !query analysis
Project [foo3_3b(x#x) AS foo3_3b(0)#x]
+- Project [cast(0 as int) AS x#x]
   +- OneRowRelation


-- !query
SELECT * FROM foo3_3at(0)
-- !query analysis
Project [a#x]
+- SQLFunctionNode spark_catalog.default.foo3_3at
   +- SubqueryAlias foo3_3at
      +- Project [cast((1 / outer(foo3_3at.x))#x as double) AS a#x]
         +- Project [(cast(1 as double) / cast(cast(0 as int) as double)) AS (1 / outer(foo3_3at.x))#x]
            +- OneRowRelation


-- !query
CREATE OR REPLACE FUNCTION foo3_3a(x INT) RETURNS DOUBLE RETURN 1 / x
-- !query analysis
CreateSQLFunctionCommand spark_catalog.default.foo3_3a, x INT, DOUBLE, 1 / x, false, false, false, true


-- !query
CREATE OR REPLACE FUNCTION foo3_3at(x INT) RETURNS TABLE (a DOUBLE) RETURN SELECT 1 / x
-- !query analysis
CreateSQLFunctionCommand spark_catalog.default.foo3_3at, x INT, a DOUBLE, SELECT 1 / x, true, false, false, true


-- !query
CREATE OR REPLACE TEMPORARY FUNCTION foo3_3b(x INT) RETURNS DOUBLE RETURN 1 / x
-- !query analysis
CreateSQLFunctionCommand foo3_3b, x INT, DOUBLE, 1 / x, false, true, false, true


-- !query
SELECT foo3_3a(0)
-- !query analysis
Project [spark_catalog.default.foo3_3a(x#x) AS spark_catalog.default.foo3_3a(0)#x]
+- Project [cast(0 as int) AS x#x]
   +- OneRowRelation


-- !query
SELECT foo3_3b(0)
-- !query analysis
Project [foo3_3b(x#x) AS foo3_3b(0)#x]
+- Project [cast(0 as int) AS x#x]
   +- OneRowRelation


-- !query
SELECT * FROM foo3_3at(0)
-- !query analysis
Project [a#x]
+- SQLFunctionNode spark_catalog.default.foo3_3at
   +- SubqueryAlias foo3_3at
      +- Project [cast((1 / outer(foo3_3at.x))#x as double) AS a#x]
         +- Project [(cast(1 as double) / cast(cast(0 as int) as double)) AS (1 / outer(foo3_3at.x))#x]
            +- OneRowRelation


-- !query
CREATE FUNCTION foo3_3c() RETURNS INT RETURN CAST('a' AS INT)
-- !query analysis
CreateSQLFunctionCommand spark_catalog.default.foo3_3c, INT, CAST('a' AS INT), false, false, false, false


-- !query
CREATE FUNCTION foo3_3ct() RETURNS TABLE (a INT) RETURN SELECT CAST('a' AS INT)
-- !query analysis
CreateSQLFunctionCommand spark_catalog.default.foo3_3ct, a INT, SELECT CAST('a' AS INT), true, false, false, false


-- !query
CREATE FUNCTION foo3_3d() RETURNS INT RETURN 'a' + 1
-- !query analysis
CreateSQLFunctionCommand spark_catalog.default.foo3_3d, INT, 'a' + 1, false, false, false, false


-- !query
CREATE FUNCTION foo3_3dt() RETURNS TABLE (a INT) RETURN SELECT 'a' + 1
-- !query analysis
CreateSQLFunctionCommand spark_catalog.default.foo3_3dt, a INT, SELECT 'a' + 1, true, false, false, false


-- !query
SELECT foo3_3c()
-- !query analysis
Project [spark_catalog.default.foo3_3c() AS spark_catalog.default.foo3_3c()#x]
+- Project
   +- OneRowRelation


-- !query
SELECT foo3_3d()
-- !query analysis
Project [spark_catalog.default.foo3_3d() AS spark_catalog.default.foo3_3d()#x]
+- Project
   +- OneRowRelation


-- !query
SELECT * FROM foo3_3ct()
-- !query analysis
Project [a#x]
+- SQLFunctionNode spark_catalog.default.foo3_3ct
   +- SubqueryAlias foo3_3ct
      +- Project [cast(CAST(a AS INT)#x as int) AS a#x]
         +- Project [cast(a as int) AS CAST(a AS INT)#x]
            +- OneRowRelation


-- !query
SELECT * FROM foo3_3dt()
-- !query analysis
Project [a#x]
+- SQLFunctionNode spark_catalog.default.foo3_3dt
   +- SubqueryAlias foo3_3dt
      +- Project [cast((a + 1)#x as int) AS a#x]
         +- Project [(cast(a as double) + cast(1 as double)) AS (a + 1)#x]
            +- OneRowRelation


-- !query
SET spark.sql.ansi.enabled=true
-- !query analysis
SetCommand (spark.sql.ansi.enabled,Some(true))


-- !query
SELECT foo3_3c()
-- !query analysis
Project [spark_catalog.default.foo3_3c() AS spark_catalog.default.foo3_3c()#x]
+- Project
   +- OneRowRelation


-- !query
SELECT foo3_3d()
-- !query analysis
Project [spark_catalog.default.foo3_3d() AS spark_catalog.default.foo3_3d()#x]
+- Project
   +- OneRowRelation


-- !query
SELECT * FROM foo3_3ct()
-- !query analysis
Project [a#x]
+- SQLFunctionNode spark_catalog.default.foo3_3ct
   +- SubqueryAlias foo3_3ct
      +- Project [cast(CAST(a AS INT)#x as int) AS a#x]
         +- Project [cast(a as int) AS CAST(a AS INT)#x]
            +- OneRowRelation


-- !query
SELECT * FROM foo3_3dt()
-- !query analysis
Project [a#x]
+- SQLFunctionNode spark_catalog.default.foo3_3dt
   +- SubqueryAlias foo3_3dt
      +- Project [cast((a + 1)#x as int) AS a#x]
         +- Project [(cast(a as double) + cast(1 as double)) AS (a + 1)#x]
            +- OneRowRelation


-- !query
RESET spark.sql.ansi.enabled
-- !query analysis
ResetCommand spark.sql.ansi.enabled


-- !query
CREATE FUNCTION foo3_14a() RETURNS INT RETURN 1
-- !query analysis
CreateSQLFunctionCommand spark_catalog.default.foo3_14a, INT, 1, false, false, false, false


-- !query
CREATE FUNCTION foo3_14b() RETURNS TABLE (a INT) RETURN SELECT 1
-- !query analysis
CreateSQLFunctionCommand spark_catalog.default.foo3_14b, a INT, SELECT 1, true, false, false, false


-- !query
SELECT * FROM foo3_14a()
-- !query analysis
org.apache.spark.sql.AnalysisException
{
  "errorClass" : "NOT_A_TABLE_FUNCTION",
  "sqlState" : "42887",
  "messageParameters" : {
    "functionName" : "`spark_catalog`.`default`.`foo3_14a`"
  },
  "queryContext" : [ {
    "objectType" : "",
    "objectName" : "",
    "startIndex" : 15,
    "stopIndex" : 24,
    "fragment" : "foo3_14a()"
  } ]
}


-- !query
SELECT foo3_14b()
-- !query analysis
org.apache.spark.sql.AnalysisException
{
  "errorClass" : "NOT_A_SCALAR_FUNCTION",
  "sqlState" : "42887",
  "messageParameters" : {
    "functionName" : "`spark_catalog`.`default`.`foo3_14b`"
  },
  "queryContext" : [ {
    "objectType" : "",
    "objectName" : "",
    "startIndex" : 8,
    "stopIndex" : 17,
    "fragment" : "foo3_14b()"
  } ]
}


-- !query
CREATE FUNCTION foo4_0() RETURNS TABLE (x INT) RETURN SELECT 1
-- !query analysis
CreateSQLFunctionCommand spark_catalog.default.foo4_0, x INT, SELECT 1, true, false, false, false


-- !query
CREATE FUNCTION foo4_1(x INT) RETURNS TABLE (a INT) RETURN SELECT x
-- !query analysis
CreateSQLFunctionCommand spark_catalog.default.foo4_1, x INT, a INT, SELECT x, true, false, false, false


-- !query
CREATE FUNCTION foo4_2(x INT) RETURNS TABLE (a INT) RETURN SELECT c2 FROM t2 WHERE c1 = x
-- !query analysis
CreateSQLFunctionCommand spark_catalog.default.foo4_2, x INT, a INT, SELECT c2 FROM t2 WHERE c1 = x, true, false, false, false


-- !query
CREATE FUNCTION foo4_3(x INT) RETURNS TABLE (a INT, cnt INT) RETURN SELECT c1, COUNT(*) FROM t2 WHERE c1 = x GROUP BY c1
-- !query analysis
CreateSQLFunctionCommand spark_catalog.default.foo4_3, x INT, a INT, cnt INT, SELECT c1, COUNT(*) FROM t2 WHERE c1 = x GROUP BY c1, true, false, false, false


-- !query
SELECT * FROM foo4_0()
-- !query analysis
Project [x#x]
+- SQLFunctionNode spark_catalog.default.foo4_0
   +- SubqueryAlias foo4_0
      +- Project [cast(1#x as int) AS x#x]
         +- Project [1 AS 1#x]
            +- OneRowRelation


-- !query
SELECT * FROM foo4_1(1)
-- !query analysis
Project [a#x]
+- SQLFunctionNode spark_catalog.default.foo4_1
   +- SubqueryAlias foo4_1
      +- Project [cast(x#x as int) AS a#x]
         +- Project [cast(1 as int) AS x#x]
            +- OneRowRelation


-- !query
SELECT * FROM foo4_2(2)
-- !query analysis
Project [a#x]
+- SQLFunctionNode spark_catalog.default.foo4_2
   +- SubqueryAlias foo4_2
      +- Project [cast(c2#x as int) AS a#x]
         +- Project [c2#x]
            +- Filter (c1#x = cast(2 as int))
               +- SubqueryAlias spark_catalog.default.t2
                  +- View (`spark_catalog`.`default`.`t2`, [c1#x, c2#x])
                     +- Project [cast(col1#x as int) AS c1#x, cast(col2#x as int) AS c2#x]
                        +- LocalRelation [col1#x, col2#x]


-- !query
SELECT * FROM foo4_3(0)
-- !query analysis
Project [a#x, cnt#x]
+- SQLFunctionNode spark_catalog.default.foo4_3
   +- SubqueryAlias foo4_3
      +- Project [cast(c1#x as int) AS a#x, cast(count(1)#xL as int) AS cnt#x]
         +- Aggregate [c1#x], [c1#x, count(1) AS count(1)#xL]
            +- Filter (c1#x = cast(0 as int))
               +- SubqueryAlias spark_catalog.default.t2
                  +- View (`spark_catalog`.`default`.`t2`, [c1#x, c2#x])
                     +- Project [cast(col1#x as int) AS c1#x, cast(col2#x as int) AS c2#x]
                        +- LocalRelation [col1#x, col2#x]


-- !query
SELECT * FROM foo4_1(rand(0) * 0)
-- !query analysis
[Analyzer test output redacted due to nondeterminism]


-- !query
SELECT * FROM foo4_1(x => 1)
-- !query analysis
Project [a#x]
+- SQLFunctionNode spark_catalog.default.foo4_1
   +- SubqueryAlias foo4_1
      +- Project [cast(x#x as int) AS a#x]
         +- Project [cast(1 as int) AS x#x]
            +- OneRowRelation


-- !query
SELECT * FROM t1, LATERAL foo4_1(c1)
-- !query analysis
Project [c1#x, c2#x, a#x]
+- LateralJoin lateral-subquery#x [c1#x], Inner
   :  +- SQLFunctionNode spark_catalog.default.foo4_1
   :     +- SubqueryAlias foo4_1
   :        +- Project [cast(x#x as int) AS a#x]
   :           +- Project [cast(outer(c1#x) as int) AS x#x]
   :              +- OneRowRelation
   +- SubqueryAlias spark_catalog.default.t1
      +- View (`spark_catalog`.`default`.`t1`, [c1#x, c2#x])
         +- Project [cast(col1#x as int) AS c1#x, cast(col2#x as int) AS c2#x]
            +- LocalRelation [col1#x, col2#x]


-- !query
SELECT * FROM t1, LATERAL foo4_2(c1)
-- !query analysis
Project [c1#x, c2#x, a#x]
+- LateralJoin lateral-subquery#x [c1#x], Inner
   :  +- SQLFunctionNode spark_catalog.default.foo4_2
   :     +- SubqueryAlias foo4_2
   :        +- Project [cast(c2#x as int) AS a#x]
   :           +- Project [c2#x]
   :              +- Filter (c1#x = cast(outer(c1#x) as int))
   :                 +- SubqueryAlias spark_catalog.default.t2
   :                    +- View (`spark_catalog`.`default`.`t2`, [c1#x, c2#x])
   :                       +- Project [cast(col1#x as int) AS c1#x, cast(col2#x as int) AS c2#x]
   :                          +- LocalRelation [col1#x, col2#x]
   +- SubqueryAlias spark_catalog.default.t1
      +- View (`spark_catalog`.`default`.`t1`, [c1#x, c2#x])
         +- Project [cast(col1#x as int) AS c1#x, cast(col2#x as int) AS c2#x]
            +- LocalRelation [col1#x, col2#x]


-- !query
SELECT * FROM t1 JOIN LATERAL foo4_2(c1) ON t1.c2 = foo4_2.a
-- !query analysis
Project [c1#x, c2#x, a#x]
+- LateralJoin lateral-subquery#x [c1#x], Inner, (c2#x = a#x)
   :  +- SQLFunctionNode spark_catalog.default.foo4_2
   :     +- SubqueryAlias foo4_2
   :        +- Project [cast(c2#x as int) AS a#x]
   :           +- Project [c2#x]
   :              +- Filter (c1#x = cast(outer(c1#x) as int))
   :                 +- SubqueryAlias spark_catalog.default.t2
   :                    +- View (`spark_catalog`.`default`.`t2`, [c1#x, c2#x])
   :                       +- Project [cast(col1#x as int) AS c1#x, cast(col2#x as int) AS c2#x]
   :                          +- LocalRelation [col1#x, col2#x]
   +- SubqueryAlias spark_catalog.default.t1
      +- View (`spark_catalog`.`default`.`t1`, [c1#x, c2#x])
         +- Project [cast(col1#x as int) AS c1#x, cast(col2#x as int) AS c2#x]
            +- LocalRelation [col1#x, col2#x]


-- !query
SELECT * FROM t1, LATERAL foo4_3(c1)
-- !query analysis
Project [c1#x, c2#x, a#x, cnt#x]
+- LateralJoin lateral-subquery#x [c1#x], Inner
   :  +- SQLFunctionNode spark_catalog.default.foo4_3
   :     +- SubqueryAlias foo4_3
   :        +- Project [cast(c1#x as int) AS a#x, cast(count(1)#xL as int) AS cnt#x]
   :           +- LateralJoin lateral-subquery#x [x#x], Inner
   :              :  +- Aggregate [c1#x], [c1#x, count(1) AS count(1)#xL]
   :              :     +- Filter (c1#x = outer(x#x))
   :              :        +- SubqueryAlias spark_catalog.default.t2
   :              :           +- View (`spark_catalog`.`default`.`t2`, [c1#x, c2#x])
   :              :              +- Project [cast(col1#x as int) AS c1#x, cast(col2#x as int) AS c2#x]
   :              :                 +- LocalRelation [col1#x, col2#x]
   :              +- Project [cast(outer(c1#x) as int) AS x#x]
   :                 +- OneRowRelation
   +- SubqueryAlias spark_catalog.default.t1
      +- View (`spark_catalog`.`default`.`t1`, [c1#x, c2#x])
         +- Project [cast(col1#x as int) AS c1#x, cast(col2#x as int) AS c2#x]
            +- LocalRelation [col1#x, col2#x]


-- !query
SELECT * FROM t1, LATERAL (SELECT cnt FROM foo4_3(c1))
-- !query analysis
Project [c1#x, c2#x, cnt#x]
+- LateralJoin lateral-subquery#x [c1#x], Inner
   :  +- SubqueryAlias __auto_generated_subquery_name
   :     +- Project [cnt#x]
   :        +- SQLFunctionNode spark_catalog.default.foo4_3
   :           +- SubqueryAlias foo4_3
   :              +- Project [cast(c1#x as int) AS a#x, cast(count(1)#xL as int) AS cnt#x]
   :                 +- LateralJoin lateral-subquery#x [x#x], Inner
   :                    :  +- Aggregate [c1#x], [c1#x, count(1) AS count(1)#xL]
   :                    :     +- Filter (c1#x = outer(x#x))
   :                    :        +- SubqueryAlias spark_catalog.default.t2
   :                    :           +- View (`spark_catalog`.`default`.`t2`, [c1#x, c2#x])
   :                    :              +- Project [cast(col1#x as int) AS c1#x, cast(col2#x as int) AS c2#x]
   :                    :                 +- LocalRelation [col1#x, col2#x]
   :                    +- Project [cast(outer(c1#x) as int) AS x#x]
   :                       +- OneRowRelation
   +- SubqueryAlias spark_catalog.default.t1
      +- View (`spark_catalog`.`default`.`t1`, [c1#x, c2#x])
         +- Project [cast(col1#x as int) AS c1#x, cast(col2#x as int) AS c2#x]
            +- LocalRelation [col1#x, col2#x]


-- !query
SELECT * FROM t1, LATERAL foo4_1(c1 + rand(0) * 0)
-- !query analysis
org.apache.spark.sql.catalyst.ExtendedAnalysisException
{
  "errorClass" : "UNSUPPORTED_SUBQUERY_EXPRESSION_CATEGORY.NON_DETERMINISTIC_LATERAL_SUBQUERIES",
  "sqlState" : "0A000",
  "messageParameters" : {
    "treeNode" : "LateralJoin lateral-subquery#x [c1#x], Inner\n:  +- SQLFunctionNode spark_catalog.default.foo4_1\n:     +- SubqueryAlias foo4_1\n:        +- Project [cast(x#x as int) AS a#x]\n:           +- LateralJoin lateral-subquery#x [x#x], Inner\n:              :  +- Project [outer(x#x) AS x#x]\n:              :     +- OneRowRelation\n:              +- Project [cast((cast(outer(c1#x) as double) + (rand(number) * cast(0 as double))) as int) AS x#x]\n:                 +- OneRowRelation\n+- SubqueryAlias spark_catalog.default.t1\n   +- View (`spark_catalog`.`default`.`t1`, [c1#x, c2#x])\n      +- Project [cast(col1#x as int) AS c1#x, cast(col2#x as int) AS c2#x]\n         +- LocalRelation [col1#x, col2#x]\n"
  },
  "queryContext" : [ {
    "objectType" : "",
    "objectName" : "",
    "startIndex" : 10,
    "stopIndex" : 50,
    "fragment" : "FROM t1, LATERAL foo4_1(c1 + rand(0) * 0)"
  } ]
}


-- !query
SELECT * FROM t1 JOIN foo4_1(1) AS foo4_1(x) ON t1.c1 = foo4_1.x
-- !query analysis
Project [c1#x, c2#x, x#x]
+- Join Inner, (c1#x = x#x)
   :- SubqueryAlias spark_catalog.default.t1
   :  +- View (`spark_catalog`.`default`.`t1`, [c1#x, c2#x])
   :     +- Project [cast(col1#x as int) AS c1#x, cast(col2#x as int) AS c2#x]
   :        +- LocalRelation [col1#x, col2#x]
   +- SubqueryAlias foo4_1
      +- Project [a#x AS x#x]
         +- SQLFunctionNode spark_catalog.default.foo4_1
            +- SubqueryAlias foo4_1
               +- Project [cast(x#x as int) AS a#x]
                  +- Project [cast(1 as int) AS x#x]
                     +- OneRowRelation


-- !query
SELECT * FROM t1, LATERAL foo4_1(c1), LATERAL foo4_2(foo4_1.a + c1)
-- !query analysis
Project [c1#x, c2#x, a#x, a#x]
+- LateralJoin lateral-subquery#x [a#x && c1#x], Inner
   :  +- SQLFunctionNode spark_catalog.default.foo4_2
   :     +- SubqueryAlias foo4_2
   :        +- Project [cast(c2#x as int) AS a#x]
   :           +- Project [c2#x]
   :              +- Filter (c1#x = cast((outer(a#x) + outer(c1#x)) as int))
   :                 +- SubqueryAlias spark_catalog.default.t2
   :                    +- View (`spark_catalog`.`default`.`t2`, [c1#x, c2#x])
   :                       +- Project [cast(col1#x as int) AS c1#x, cast(col2#x as int) AS c2#x]
   :                          +- LocalRelation [col1#x, col2#x]
   +- LateralJoin lateral-subquery#x [c1#x], Inner
      :  +- SQLFunctionNode spark_catalog.default.foo4_1
      :     +- SubqueryAlias foo4_1
      :        +- Project [cast(x#x as int) AS a#x]
      :           +- Project [cast(outer(c1#x) as int) AS x#x]
      :              +- OneRowRelation
      +- SubqueryAlias spark_catalog.default.t1
         +- View (`spark_catalog`.`default`.`t1`, [c1#x, c2#x])
            +- Project [cast(col1#x as int) AS c1#x, cast(col2#x as int) AS c2#x]
               +- LocalRelation [col1#x, col2#x]


-- !query
SELECT (SELECT MAX(a) FROM foo4_1(c1)) FROM t1
-- !query analysis
Project [scalar-subquery#x [c1#x] AS scalarsubquery(c1)#x]
:  +- Aggregate [max(a#x) AS max(a)#x]
:     +- SQLFunctionNode spark_catalog.default.foo4_1
:        +- SubqueryAlias foo4_1
:           +- Project [cast(x#x as int) AS a#x]
:              +- Project [cast(outer(c1#x) as int) AS x#x]
:                 +- OneRowRelation
+- SubqueryAlias spark_catalog.default.t1
   +- View (`spark_catalog`.`default`.`t1`, [c1#x, c2#x])
      +- Project [cast(col1#x as int) AS c1#x, cast(col2#x as int) AS c2#x]
         +- LocalRelation [col1#x, col2#x]


-- !query
SELECT (SELECT MAX(a) FROM foo4_1(c1) WHERE a = c2) FROM t1
-- !query analysis
Project [scalar-subquery#x [c2#x && c1#x] AS scalarsubquery(c2, c1)#x]
:  +- Aggregate [max(a#x) AS max(a)#x]
:     +- Filter (a#x = outer(c2#x))
:        +- SQLFunctionNode spark_catalog.default.foo4_1
:           +- SubqueryAlias foo4_1
:              +- Project [cast(x#x as int) AS a#x]
:                 +- Project [cast(outer(c1#x) as int) AS x#x]
:                    +- OneRowRelation
+- SubqueryAlias spark_catalog.default.t1
   +- View (`spark_catalog`.`default`.`t1`, [c1#x, c2#x])
      +- Project [cast(col1#x as int) AS c1#x, cast(col2#x as int) AS c2#x]
         +- LocalRelation [col1#x, col2#x]


-- !query
SELECT (SELECT MAX(cnt) FROM foo4_3(c1)) FROM t1
-- !query analysis
Project [scalar-subquery#x [c1#x] AS scalarsubquery(c1)#x]
:  +- Aggregate [max(cnt#x) AS max(cnt)#x]
:     +- SQLFunctionNode spark_catalog.default.foo4_3
:        +- SubqueryAlias foo4_3
:           +- Project [cast(c1#x as int) AS a#x, cast(count(1)#xL as int) AS cnt#x]
:              +- LateralJoin lateral-subquery#x [x#x], Inner
:                 :  +- Aggregate [c1#x], [c1#x, count(1) AS count(1)#xL]
:                 :     +- Filter (c1#x = outer(x#x))
:                 :        +- SubqueryAlias spark_catalog.default.t2
:                 :           +- View (`spark_catalog`.`default`.`t2`, [c1#x, c2#x])
:                 :              +- Project [cast(col1#x as int) AS c1#x, cast(col2#x as int) AS c2#x]
:                 :                 +- LocalRelation [col1#x, col2#x]
:                 +- Project [cast(outer(c1#x) as int) AS x#x]
:                    +- OneRowRelation
+- SubqueryAlias spark_catalog.default.t1
   +- View (`spark_catalog`.`default`.`t1`, [c1#x, c2#x])
      +- Project [cast(col1#x as int) AS c1#x, cast(col2#x as int) AS c2#x]
         +- LocalRelation [col1#x, col2#x]


-- !query
DROP VIEW t1
-- !query analysis
DropTableCommand `spark_catalog`.`default`.`t1`, false, true, false


-- !query
DROP VIEW t2
-- !query analysis
DropTableCommand `spark_catalog`.`default`.`t2`, false, true, false


-- !query
DROP FUNCTION IF EXISTS foo1a0
-- !query analysis
DropFunctionCommand spark_catalog.default.foo1a0, true, false


-- !query
DROP FUNCTION IF EXISTS foo1a1
-- !query analysis
DropFunctionCommand spark_catalog.default.foo1a1, true, false


-- !query
DROP FUNCTION IF EXISTS foo1a2
-- !query analysis
DropFunctionCommand spark_catalog.default.foo1a2, true, false


-- !query
DROP FUNCTION IF EXISTS foo1b0
-- !query analysis
DropFunctionCommand spark_catalog.default.foo1b0, true, false


-- !query
DROP FUNCTION IF EXISTS foo1b1
-- !query analysis
DropFunctionCommand spark_catalog.default.foo1b1, true, false


-- !query
DROP FUNCTION IF EXISTS foo1b2
-- !query analysis
DropFunctionCommand spark_catalog.default.foo1b2, true, false


-- !query
DROP FUNCTION IF EXISTS foo1c1
-- !query analysis
NoopCommand DROP FUNCTION, [foo1c1]


-- !query
DROP FUNCTION IF EXISTS foo1c2
-- !query analysis
NoopCommand DROP FUNCTION, [foo1c2]


-- !query
DROP FUNCTION IF EXISTS foo1d1
-- !query analysis
DropFunctionCommand spark_catalog.default.foo1d1, true, false


-- !query
DROP FUNCTION IF EXISTS foo1d2
-- !query analysis
DropFunctionCommand spark_catalog.default.foo1d2, true, false


-- !query
DROP FUNCTION IF EXISTS foo1d4
-- !query analysis
DropFunctionCommand spark_catalog.default.foo1d4, true, false


-- !query
DROP FUNCTION IF EXISTS foo1d5
-- !query analysis
DropFunctionCommand spark_catalog.default.foo1d5, true, false


-- !query
DROP FUNCTION IF EXISTS foo1d6
-- !query analysis
DropFunctionCommand spark_catalog.default.foo1d6, true, false


-- !query
DROP FUNCTION IF EXISTS foo1e1
-- !query analysis
NoopCommand DROP FUNCTION, [foo1e1]


-- !query
DROP FUNCTION IF EXISTS foo1e2
-- !query analysis
NoopCommand DROP FUNCTION, [foo1e2]


-- !query
DROP FUNCTION IF EXISTS foo1e3
-- !query analysis
NoopCommand DROP FUNCTION, [foo1e3]


-- !query
DROP FUNCTION IF EXISTS foo1f1
-- !query analysis
NoopCommand DROP FUNCTION, [foo1f1]


-- !query
DROP FUNCTION IF EXISTS foo1f2
-- !query analysis
NoopCommand DROP FUNCTION, [foo1f2]


-- !query
DROP FUNCTION IF EXISTS foo1g1
-- !query analysis
NoopCommand DROP FUNCTION, [foo1g1]


-- !query
DROP FUNCTION IF EXISTS foo1g2
-- !query analysis
NoopCommand DROP FUNCTION, [foo1g2]


-- !query
DROP FUNCTION IF EXISTS foo2a0
-- !query analysis
NoopCommand DROP FUNCTION, [foo2a0]


-- !query
DROP FUNCTION IF EXISTS foo2a2
-- !query analysis
DropFunctionCommand spark_catalog.default.foo2a2, true, false


-- !query
DROP FUNCTION IF EXISTS foo2a4
-- !query analysis
DropFunctionCommand spark_catalog.default.foo2a4, true, false


-- !query
DROP FUNCTION IF EXISTS foo2b1
-- !query analysis
NoopCommand DROP FUNCTION, [foo2b1]


-- !query
DROP FUNCTION IF EXISTS foo2b2
-- !query analysis
NoopCommand DROP FUNCTION, [foo2b2]


-- !query
DROP FUNCTION IF EXISTS foo2c1
-- !query analysis
NoopCommand DROP FUNCTION, [foo2c1]


-- !query
DROP FUNCTION IF EXISTS foo31
-- !query analysis
NoopCommand DROP FUNCTION, [foo31]


-- !query
DROP FUNCTION IF EXISTS foo32
-- !query analysis
NoopCommand DROP FUNCTION, [foo32]


-- !query
DROP FUNCTION IF EXISTS foo33
-- !query analysis
NoopCommand DROP FUNCTION, [foo33]


-- !query
DROP FUNCTION IF EXISTS foo41
-- !query analysis
DropFunctionCommand spark_catalog.default.foo41, true, false


-- !query
DROP FUNCTION IF EXISTS foo42
-- !query analysis
NoopCommand DROP FUNCTION, [foo42]


-- !query
DROP FUNCTION IF EXISTS foo51
-- !query analysis
DropFunctionCommand spark_catalog.default.foo51, true, false


-- !query
DROP FUNCTION IF EXISTS foo52
-- !query analysis
DropFunctionCommand spark_catalog.default.foo52, true, false


-- !query
DROP FUNCTION IF EXISTS foo6c
-- !query analysis
DropFunctionCommand spark_catalog.default.foo6c, true, false


-- !query
DROP FUNCTION IF EXISTS foo6d
-- !query analysis
DropFunctionCommand spark_catalog.default.foo6d, true, false


-- !query
DROP FUNCTION IF EXISTS foo7a
-- !query analysis
DropFunctionCommand spark_catalog.default.foo7a, true, false


-- !query
DROP FUNCTION IF EXISTS foo7at
-- !query analysis
DropFunctionCommand spark_catalog.default.foo7at, true, false


-- !query
DROP FUNCTION IF EXISTS foo9a
-- !query analysis
DropFunctionCommand spark_catalog.default.foo9a, true, false


-- !query
DROP FUNCTION IF EXISTS foo9b
-- !query analysis
DropFunctionCommand spark_catalog.default.foo9b, true, false


-- !query
DROP FUNCTION IF EXISTS foo9c
-- !query analysis
DropFunctionCommand spark_catalog.default.foo9c, true, false


-- !query
DROP FUNCTION IF EXISTS foo9d
-- !query analysis
DropFunctionCommand spark_catalog.default.foo9d, true, false


-- !query
DROP FUNCTION IF EXISTS foo9e
-- !query analysis
DropFunctionCommand spark_catalog.default.foo9e, true, false


-- !query
DROP FUNCTION IF EXISTS foo9f
-- !query analysis
DropFunctionCommand spark_catalog.default.foo9f, true, false


-- !query
DROP FUNCTION IF EXISTS foo9g
-- !query analysis
DropFunctionCommand spark_catalog.default.foo9g, true, false


-- !query
DROP FUNCTION IF EXISTS foo9h
-- !query analysis
DropFunctionCommand spark_catalog.default.foo9h, true, false


-- !query
DROP FUNCTION IF EXISTS foo9i
-- !query analysis
NoopCommand DROP FUNCTION, [foo9i]


-- !query
DROP FUNCTION IF EXISTS foo9j
-- !query analysis
DropFunctionCommand spark_catalog.default.foo9j, true, false


-- !query
DROP FUNCTION IF EXISTS foo9l
-- !query analysis
DropFunctionCommand spark_catalog.default.foo9l, true, false


-- !query
DROP FUNCTION IF EXISTS foo9m
-- !query analysis
DropFunctionCommand spark_catalog.default.foo9m, true, false


-- !query
DROP FUNCTION IF EXISTS foo9n
-- !query analysis
DropFunctionCommand spark_catalog.default.foo9n, true, false


-- !query
DROP FUNCTION IF EXISTS foo9o
-- !query analysis
DropFunctionCommand spark_catalog.default.foo9o, true, false


-- !query
DROP FUNCTION IF EXISTS foo9p
-- !query analysis
DropFunctionCommand spark_catalog.default.foo9p, true, false


-- !query
DROP FUNCTION IF EXISTS foo9q
-- !query analysis
DropFunctionCommand spark_catalog.default.foo9q, true, false


-- !query
DROP FUNCTION IF EXISTS foo9r
-- !query analysis
DropFunctionCommand spark_catalog.default.foo9r, true, false


-- !query
DROP FUNCTION IF EXISTS foo1_10
-- !query analysis
DropFunctionCommand spark_catalog.default.foo1_10, true, false


-- !query
DROP FUNCTION IF EXISTS bar1_10
-- !query analysis
DropFunctionCommand spark_catalog.default.bar1_10, true, false


-- !query
DROP FUNCTION IF EXISTS foo1_11a
-- !query analysis
DropFunctionCommand spark_catalog.default.foo1_11a, true, false


-- !query
DROP FUNCTION IF EXISTS foo1_11b
-- !query analysis
DropFunctionCommand spark_catalog.default.foo1_11b, true, false


-- !query
DROP FUNCTION IF EXISTS foo1_11c
-- !query analysis
DropFunctionCommand spark_catalog.default.foo1_11c, true, false


-- !query
DROP FUNCTION IF EXISTS foo1_11d
-- !query analysis
DropFunctionCommand spark_catalog.default.foo1_11d, true, false


-- !query
DROP FUNCTION IF EXISTS foo1_11e
-- !query analysis
DropFunctionCommand spark_catalog.default.foo1_11e, true, false


-- !query
DROP FUNCTION IF EXISTS foo1_11f
-- !query analysis
DropFunctionCommand spark_catalog.default.foo1_11f, true, false


-- !query
DROP FUNCTION IF EXISTS foo1_11g
-- !query analysis
DropFunctionCommand spark_catalog.default.foo1_11g, true, false


-- !query
DROP FUNCTION IF EXISTS foo1_11h
-- !query analysis
DropFunctionCommand spark_catalog.default.foo1_11h, true, false


-- !query
DROP FUNCTION IF EXISTS foo1_11i
-- !query analysis
DropFunctionCommand spark_catalog.default.foo1_11i, true, false


-- !query
DROP FUNCTION IF EXISTS foo1_11j
-- !query analysis
DropFunctionCommand spark_catalog.default.foo1_11j, true, false


-- !query
DROP FUNCTION IF EXISTS foo1_11k
-- !query analysis
DropFunctionCommand spark_catalog.default.foo1_11k, true, false


-- !query
DROP FUNCTION IF EXISTS foo1_11l
-- !query analysis
DropFunctionCommand spark_catalog.default.foo1_11l, true, false


-- !query
DROP FUNCTION IF EXISTS foo1_11m
-- !query analysis
DropFunctionCommand spark_catalog.default.foo1_11m, true, false


-- !query
DROP FUNCTION IF EXISTS foo1_11n
-- !query analysis
DropFunctionCommand spark_catalog.default.foo1_11n, true, false


-- !query
DROP FUNCTION IF EXISTS foo1_11o
-- !query analysis
DropFunctionCommand spark_catalog.default.foo1_11o, true, false


-- !query
DROP FUNCTION IF EXISTS foo2_1a
-- !query analysis
DropFunctionCommand spark_catalog.default.foo2_1a, true, false


-- !query
DROP FUNCTION IF EXISTS foo2_1b
-- !query analysis
DropFunctionCommand spark_catalog.default.foo2_1b, true, false


-- !query
DROP FUNCTION IF EXISTS foo2_1c
-- !query analysis
DropFunctionCommand spark_catalog.default.foo2_1c, true, false


-- !query
DROP FUNCTION IF EXISTS foo2_1d
-- !query analysis
DropFunctionCommand spark_catalog.default.foo2_1d, true, false


-- !query
DROP FUNCTION IF EXISTS foo2_2a
-- !query analysis
DropFunctionCommand spark_catalog.default.foo2_2a, true, false


-- !query
DROP FUNCTION IF EXISTS foo2_2b
-- !query analysis
DropFunctionCommand spark_catalog.default.foo2_2b, true, false


-- !query
DROP FUNCTION IF EXISTS foo2_2c
-- !query analysis
NoopCommand DROP FUNCTION, [foo2_2c]


-- !query
DROP FUNCTION IF EXISTS foo2_2d
-- !query analysis
NoopCommand DROP FUNCTION, [foo2_2d]


-- !query
DROP FUNCTION IF EXISTS foo2_2e
-- !query analysis
DropFunctionCommand spark_catalog.default.foo2_2e, true, false


-- !query
DROP FUNCTION IF EXISTS foo2_2f
-- !query analysis
DropFunctionCommand spark_catalog.default.foo2_2f, true, false


-- !query
DROP FUNCTION IF EXISTS foo2_2g
-- !query analysis
DropFunctionCommand spark_catalog.default.foo2_2g, true, false


-- !query
DROP FUNCTION IF EXISTS foo2_3
-- !query analysis
DropFunctionCommand spark_catalog.default.foo2_3, true, false


-- !query
DROP FUNCTION IF EXISTS foo2_4a
-- !query analysis
DropFunctionCommand spark_catalog.default.foo2_4a, true, false


-- !query
DROP FUNCTION IF EXISTS foo2_4b
-- !query analysis
DropFunctionCommand spark_catalog.default.foo2_4b, true, false


-- !query
DROP FUNCTION IF EXISTS foo3_1a
-- !query analysis
DropFunctionCommand spark_catalog.default.foo3_1a, true, false


-- !query
DROP FUNCTION IF EXISTS foo3_1b
-- !query analysis
DropFunctionCommand spark_catalog.default.foo3_1b, true, false


-- !query
DROP FUNCTION IF EXISTS foo3_1c
-- !query analysis
DropFunctionCommand spark_catalog.default.foo3_1c, true, false


-- !query
DROP FUNCTION IF EXISTS foo3_1d
-- !query analysis
DropFunctionCommand spark_catalog.default.foo3_1d, true, false


-- !query
DROP FUNCTION IF EXISTS foo3_1e
-- !query analysis
DropFunctionCommand spark_catalog.default.foo3_1e, true, false


-- !query
DROP FUNCTION IF EXISTS foo3_1f
-- !query analysis
DropFunctionCommand spark_catalog.default.foo3_1f, true, false


-- !query
DROP FUNCTION IF EXISTS foo3_1g
-- !query analysis
DropFunctionCommand spark_catalog.default.foo3_1g, true, false


-- !query
DROP FUNCTION IF EXISTS foo3_1x
-- !query analysis
DropFunctionCommand spark_catalog.default.foo3_1x, true, false


-- !query
DROP FUNCTION IF EXISTS foo3_1y
-- !query analysis
DropFunctionCommand spark_catalog.default.foo3_1y, true, false


-- !query
DROP FUNCTION IF EXISTS foo3_2a
-- !query analysis
DropFunctionCommand spark_catalog.default.foo3_2a, true, false


-- !query
DROP FUNCTION IF EXISTS foo3_2b1
-- !query analysis
DropFunctionCommand spark_catalog.default.foo3_2b1, true, false


-- !query
DROP FUNCTION IF EXISTS foo3_2b2
-- !query analysis
DropFunctionCommand spark_catalog.default.foo3_2b2, true, false


-- !query
DROP FUNCTION IF EXISTS foo3_2b3
-- !query analysis
DropFunctionCommand spark_catalog.default.foo3_2b3, true, false


-- !query
DROP FUNCTION IF EXISTS foo3_2b4
-- !query analysis
DropFunctionCommand spark_catalog.default.foo3_2b4, true, false


-- !query
DROP FUNCTION IF EXISTS foo3_2b5
-- !query analysis
DropFunctionCommand spark_catalog.default.foo3_2b5, true, false


-- !query
DROP FUNCTION IF EXISTS foo3_2c1
-- !query analysis
DropFunctionCommand spark_catalog.default.foo3_2c1, true, false


-- !query
DROP FUNCTION IF EXISTS foo3_2c2
-- !query analysis
DropFunctionCommand spark_catalog.default.foo3_2c2, true, false


-- !query
DROP FUNCTION IF EXISTS foo3_2d1
-- !query analysis
DropFunctionCommand spark_catalog.default.foo3_2d1, true, false


-- !query
DROP FUNCTION IF EXISTS foo3_2d2
-- !query analysis
NoopCommand DROP FUNCTION, [foo3_2d2]


-- !query
DROP FUNCTION IF EXISTS foo3_2e1
-- !query analysis
DropFunctionCommand spark_catalog.default.foo3_2e1, true, false


-- !query
DROP FUNCTION IF EXISTS foo3_3a
-- !query analysis
DropFunctionCommand spark_catalog.default.foo3_3a, true, false


-- !query
DROP FUNCTION IF EXISTS foo3_3at
-- !query analysis
DropFunctionCommand spark_catalog.default.foo3_3at, true, false


-- !query
DROP FUNCTION IF EXISTS foo3_14a
-- !query analysis
DropFunctionCommand spark_catalog.default.foo3_14a, true, false


-- !query
DROP FUNCTION IF EXISTS foo3_14b
-- !query analysis
DropFunctionCommand spark_catalog.default.foo3_14b, true, false


-- !query
DROP FUNCTION IF EXISTS foo3_3c
-- !query analysis
DropFunctionCommand spark_catalog.default.foo3_3c, true, false


-- !query
DROP FUNCTION IF EXISTS foo3_3ct
-- !query analysis
DropFunctionCommand spark_catalog.default.foo3_3ct, true, false


-- !query
DROP FUNCTION IF EXISTS foo3_3d
-- !query analysis
DropFunctionCommand spark_catalog.default.foo3_3d, true, false


-- !query
DROP FUNCTION IF EXISTS foo3_3dt
-- !query analysis
DropFunctionCommand spark_catalog.default.foo3_3dt, true, false


-- !query
DROP FUNCTION IF EXISTS foo4_0
-- !query analysis
DropFunctionCommand spark_catalog.default.foo4_0, true, false


-- !query
DROP FUNCTION IF EXISTS foo4_1
-- !query analysis
DropFunctionCommand spark_catalog.default.foo4_1, true, false


-- !query
DROP FUNCTION IF EXISTS foo4_2
-- !query analysis
DropFunctionCommand spark_catalog.default.foo4_2, true, false


-- !query
DROP FUNCTION IF EXISTS foo4_3
-- !query analysis
DropFunctionCommand spark_catalog.default.foo4_3, true, false
