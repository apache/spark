-- Automatically generated by SQLQueryTestSuite
-- !query
create table t1(a int, b int, c int) using parquet
-- !query analysis
CreateDataSourceTableCommand `spark_catalog`.`default`.`t1`, false


-- !query
insert into t1 values(1,0,0)
-- !query analysis
InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/t1, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/t1], Append, `spark_catalog`.`default`.`t1`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/t1), [a, b, c]
+- Project [cast(col1#x as int) AS a#x, cast(col2#x as int) AS b#x, cast(col3#x as int) AS c#x]
   +- LocalRelation [col1#x, col2#x, col3#x]


-- !query
insert into t1 values(2,0,1)
-- !query analysis
InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/t1, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/t1], Append, `spark_catalog`.`default`.`t1`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/t1), [a, b, c]
+- Project [cast(col1#x as int) AS a#x, cast(col2#x as int) AS b#x, cast(col3#x as int) AS c#x]
   +- LocalRelation [col1#x, col2#x, col3#x]


-- !query
insert into t1 values(3,1,0)
-- !query analysis
InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/t1, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/t1], Append, `spark_catalog`.`default`.`t1`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/t1), [a, b, c]
+- Project [cast(col1#x as int) AS a#x, cast(col2#x as int) AS b#x, cast(col3#x as int) AS c#x]
   +- LocalRelation [col1#x, col2#x, col3#x]


-- !query
insert into t1 values(4,1,1)
-- !query analysis
InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/t1, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/t1], Append, `spark_catalog`.`default`.`t1`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/t1), [a, b, c]
+- Project [cast(col1#x as int) AS a#x, cast(col2#x as int) AS b#x, cast(col3#x as int) AS c#x]
   +- LocalRelation [col1#x, col2#x, col3#x]


-- !query
insert into t1 values(5,null,0)
-- !query analysis
InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/t1, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/t1], Append, `spark_catalog`.`default`.`t1`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/t1), [a, b, c]
+- Project [cast(col1#x as int) AS a#x, cast(col2#x as int) AS b#x, cast(col3#x as int) AS c#x]
   +- LocalRelation [col1#x, col2#x, col3#x]


-- !query
insert into t1 values(6,null,1)
-- !query analysis
InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/t1, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/t1], Append, `spark_catalog`.`default`.`t1`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/t1), [a, b, c]
+- Project [cast(col1#x as int) AS a#x, cast(col2#x as int) AS b#x, cast(col3#x as int) AS c#x]
   +- LocalRelation [col1#x, col2#x, col3#x]


-- !query
insert into t1 values(7,null,null)
-- !query analysis
InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/t1, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/t1], Append, `spark_catalog`.`default`.`t1`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/t1), [a, b, c]
+- Project [cast(col1#x as int) AS a#x, cast(col2#x as int) AS b#x, cast(col3#x as int) AS c#x]
   +- LocalRelation [col1#x, col2#x, col3#x]


-- !query
select a, b+c from t1
-- !query analysis
Project [a#x, (b#x + c#x) AS (b + c)#x]
+- SubqueryAlias spark_catalog.default.t1
   +- Relation spark_catalog.default.t1[a#x,b#x,c#x] parquet


-- !query
select b + 0 from t1 where a = 5
-- !query analysis
Project [(b#x + 0) AS (b + 0)#x]
+- Filter (a#x = 5)
   +- SubqueryAlias spark_catalog.default.t1
      +- Relation spark_catalog.default.t1[a#x,b#x,c#x] parquet


-- !query
select -100 + b + 100 from t1 where a = 5
-- !query analysis
Project [((-100 + b#x) + 100) AS ((-100 + b) + 100)#x]
+- Filter (a#x = 5)
   +- SubqueryAlias spark_catalog.default.t1
      +- Relation spark_catalog.default.t1[a#x,b#x,c#x] parquet


-- !query
select a+10, b*0 from t1
-- !query analysis
Project [(a#x + 10) AS (a + 10)#x, (b#x * 0) AS (b * 0)#x]
+- SubqueryAlias spark_catalog.default.t1
   +- Relation spark_catalog.default.t1[a#x,b#x,c#x] parquet


-- !query
select distinct b from t1
-- !query analysis
Distinct
+- Project [b#x]
   +- SubqueryAlias spark_catalog.default.t1
      +- Relation spark_catalog.default.t1[a#x,b#x,c#x] parquet


-- !query
select b from t1 union select b from t1
-- !query analysis
Distinct
+- Union false, false
   :- Project [b#x]
   :  +- SubqueryAlias spark_catalog.default.t1
   :     +- Relation spark_catalog.default.t1[a#x,b#x,c#x] parquet
   +- Project [b#x]
      +- SubqueryAlias spark_catalog.default.t1
         +- Relation spark_catalog.default.t1[a#x,b#x,c#x] parquet


-- !query
select a+20, case b when c then 1 else 0 end from t1
-- !query analysis
Project [(a#x + 20) AS (a + 20)#x, CASE WHEN (b#x = c#x) THEN 1 ELSE 0 END AS CASE WHEN (b = c) THEN 1 ELSE 0 END#x]
+- SubqueryAlias spark_catalog.default.t1
   +- Relation spark_catalog.default.t1[a#x,b#x,c#x] parquet


-- !query
select a+30, case c when b then 1 else 0 end from t1
-- !query analysis
Project [(a#x + 30) AS (a + 30)#x, CASE WHEN (c#x = b#x) THEN 1 ELSE 0 END AS CASE WHEN (c = b) THEN 1 ELSE 0 END#x]
+- SubqueryAlias spark_catalog.default.t1
   +- Relation spark_catalog.default.t1[a#x,b#x,c#x] parquet


-- !query
select a+40, case when b<>0 then 1 else 0 end from t1
-- !query analysis
Project [(a#x + 40) AS (a + 40)#x, CASE WHEN NOT (b#x = 0) THEN 1 ELSE 0 END AS CASE WHEN (NOT (b = 0)) THEN 1 ELSE 0 END#x]
+- SubqueryAlias spark_catalog.default.t1
   +- Relation spark_catalog.default.t1[a#x,b#x,c#x] parquet


-- !query
select a+50, case when not b<>0 then 1 else 0 end from t1
-- !query analysis
Project [(a#x + 50) AS (a + 50)#x, CASE WHEN NOT NOT (b#x = 0) THEN 1 ELSE 0 END AS CASE WHEN (NOT (NOT (b = 0))) THEN 1 ELSE 0 END#x]
+- SubqueryAlias spark_catalog.default.t1
   +- Relation spark_catalog.default.t1[a#x,b#x,c#x] parquet


-- !query
select a+60, case when b<>0 and c<>0 then 1 else 0 end from t1
-- !query analysis
Project [(a#x + 60) AS (a + 60)#x, CASE WHEN (NOT (b#x = 0) AND NOT (c#x = 0)) THEN 1 ELSE 0 END AS CASE WHEN ((NOT (b = 0)) AND (NOT (c = 0))) THEN 1 ELSE 0 END#x]
+- SubqueryAlias spark_catalog.default.t1
   +- Relation spark_catalog.default.t1[a#x,b#x,c#x] parquet


-- !query
select a+70, case when not (b<>0 and c<>0) then 1 else 0 end from t1
-- !query analysis
Project [(a#x + 70) AS (a + 70)#x, CASE WHEN NOT (NOT (b#x = 0) AND NOT (c#x = 0)) THEN 1 ELSE 0 END AS CASE WHEN (NOT ((NOT (b = 0)) AND (NOT (c = 0)))) THEN 1 ELSE 0 END#x]
+- SubqueryAlias spark_catalog.default.t1
   +- Relation spark_catalog.default.t1[a#x,b#x,c#x] parquet


-- !query
select a+80, case when b<>0 or c<>0 then 1 else 0 end from t1
-- !query analysis
Project [(a#x + 80) AS (a + 80)#x, CASE WHEN (NOT (b#x = 0) OR NOT (c#x = 0)) THEN 1 ELSE 0 END AS CASE WHEN ((NOT (b = 0)) OR (NOT (c = 0))) THEN 1 ELSE 0 END#x]
+- SubqueryAlias spark_catalog.default.t1
   +- Relation spark_catalog.default.t1[a#x,b#x,c#x] parquet


-- !query
select a+90, case when not (b<>0 or c<>0) then 1 else 0 end from t1
-- !query analysis
Project [(a#x + 90) AS (a + 90)#x, CASE WHEN NOT (NOT (b#x = 0) OR NOT (c#x = 0)) THEN 1 ELSE 0 END AS CASE WHEN (NOT ((NOT (b = 0)) OR (NOT (c = 0)))) THEN 1 ELSE 0 END#x]
+- SubqueryAlias spark_catalog.default.t1
   +- Relation spark_catalog.default.t1[a#x,b#x,c#x] parquet


-- !query
select count(*), count(b), sum(b), avg(b), min(b), max(b) from t1
-- !query analysis
Aggregate [count(1) AS count(1)#xL, count(b#x) AS count(b)#xL, sum(b#x) AS sum(b)#xL, avg(b#x) AS avg(b)#x, min(b#x) AS min(b)#x, max(b#x) AS max(b)#x]
+- SubqueryAlias spark_catalog.default.t1
   +- Relation spark_catalog.default.t1[a#x,b#x,c#x] parquet


-- !query
select a+100 from t1 where b<10
-- !query analysis
Project [(a#x + 100) AS (a + 100)#x]
+- Filter (b#x < 10)
   +- SubqueryAlias spark_catalog.default.t1
      +- Relation spark_catalog.default.t1[a#x,b#x,c#x] parquet


-- !query
select a+110 from t1 where not b>10
-- !query analysis
Project [(a#x + 110) AS (a + 110)#x]
+- Filter NOT (b#x > 10)
   +- SubqueryAlias spark_catalog.default.t1
      +- Relation spark_catalog.default.t1[a#x,b#x,c#x] parquet


-- !query
select a+120 from t1 where b<10 OR c=1
-- !query analysis
Project [(a#x + 120) AS (a + 120)#x]
+- Filter ((b#x < 10) OR (c#x = 1))
   +- SubqueryAlias spark_catalog.default.t1
      +- Relation spark_catalog.default.t1[a#x,b#x,c#x] parquet


-- !query
select a+130 from t1 where b<10 AND c=1
-- !query analysis
Project [(a#x + 130) AS (a + 130)#x]
+- Filter ((b#x < 10) AND (c#x = 1))
   +- SubqueryAlias spark_catalog.default.t1
      +- Relation spark_catalog.default.t1[a#x,b#x,c#x] parquet


-- !query
select a+140 from t1 where not (b<10 AND c=1)
-- !query analysis
Project [(a#x + 140) AS (a + 140)#x]
+- Filter NOT ((b#x < 10) AND (c#x = 1))
   +- SubqueryAlias spark_catalog.default.t1
      +- Relation spark_catalog.default.t1[a#x,b#x,c#x] parquet


-- !query
select a+150 from t1 where not (c=1 AND b<10)
-- !query analysis
Project [(a#x + 150) AS (a + 150)#x]
+- Filter NOT ((c#x = 1) AND (b#x < 10))
   +- SubqueryAlias spark_catalog.default.t1
      +- Relation spark_catalog.default.t1[a#x,b#x,c#x] parquet


-- !query
select b, c, equal_null(b, c), equal_null(c, b) from t1
-- !query analysis
Project [b#x, c#x, equal_null(b#x, c#x) AS equal_null(b, c)#x, equal_null(c#x, b#x) AS equal_null(c, b)#x]
+- SubqueryAlias spark_catalog.default.t1
   +- Relation spark_catalog.default.t1[a#x,b#x,c#x] parquet


-- !query
drop table t1
-- !query analysis
DropTable false, false
+- ResolvedIdentifier V2SessionCatalog(spark_catalog), default.t1
