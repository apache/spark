-- Automatically generated by SQLQueryTestSuite
-- !query
drop table if exists t
-- !query analysis
DropTable true, false
+- ResolvedIdentifier V2SessionCatalog(spark_catalog), default.t


-- !query
create table t(x int, y string) using csv
-- !query analysis
CreateDataSourceTableCommand `spark_catalog`.`default`.`t`, false


-- !query
insert into t values (0, 'abc'), (1, 'def')
-- !query analysis
InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/t, false, CSV, [path=file:[not included in comparison]/{warehouse_dir}/t], Append, `spark_catalog`.`default`.`t`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/t), [x, y]
+- Project [cast(col1#x as int) AS x#x, cast(col2#x as string) AS y#x]
   +- LocalRelation [col1#x, col2#x]


-- !query
drop table if exists other
-- !query analysis
DropTable true, false
+- ResolvedIdentifier V2SessionCatalog(spark_catalog), default.other


-- !query
create table other(a int, b int) using json
-- !query analysis
CreateDataSourceTableCommand `spark_catalog`.`default`.`other`, false


-- !query
insert into other values (1, 1), (1, 2), (2, 4)
-- !query analysis
InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/other, false, JSON, [path=file:[not included in comparison]/{warehouse_dir}/other], Append, `spark_catalog`.`default`.`other`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/other), [a, b]
+- Project [cast(col1#x as int) AS a#x, cast(col2#x as int) AS b#x]
   +- LocalRelation [col1#x, col2#x]


-- !query
drop table if exists st
-- !query analysis
DropTable true, false
+- ResolvedIdentifier V2SessionCatalog(spark_catalog), default.st


-- !query
create table st(x int, col struct<i1:int, i2:int>) using parquet
-- !query analysis
CreateDataSourceTableCommand `spark_catalog`.`default`.`st`, false


-- !query
insert into st values (1, (2, 3))
-- !query analysis
InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/st, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/st], Append, `spark_catalog`.`default`.`st`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/st), [x, col]
+- Project [cast(col1#x as int) AS x#x, named_struct(i1, cast(col2#x.col1 as int), i2, cast(col2#x.col2 as int)) AS col#x]
   +- LocalRelation [col1#x, col2#x]


-- !query
create temporary view courseSales as select * from values
  ("dotNET", 2012, 10000),
  ("Java", 2012, 20000),
  ("dotNET", 2012, 5000),
  ("dotNET", 2013, 48000),
  ("Java", 2013, 30000)
  as courseSales(course, year, earnings)
-- !query analysis
CreateViewCommand `courseSales`, select * from values
  ("dotNET", 2012, 10000),
  ("Java", 2012, 20000),
  ("dotNET", 2012, 5000),
  ("dotNET", 2013, 48000),
  ("Java", 2013, 30000)
  as courseSales(course, year, earnings), false, false, LocalTempView, UNSUPPORTED, true
   +- Project [course#x, year#x, earnings#x]
      +- SubqueryAlias courseSales
         +- LocalRelation [course#x, year#x, earnings#x]


-- !query
create temporary view courseEarnings as select * from values
  ("dotNET", 15000, 48000, 22500),
  ("Java", 20000, 30000, NULL)
  as courseEarnings(course, `2012`, `2013`, `2014`)
-- !query analysis
CreateViewCommand `courseEarnings`, select * from values
  ("dotNET", 15000, 48000, 22500),
  ("Java", 20000, 30000, NULL)
  as courseEarnings(course, `2012`, `2013`, `2014`), false, false, LocalTempView, UNSUPPORTED, true
   +- Project [course#x, 2012#x, 2013#x, 2014#x]
      +- SubqueryAlias courseEarnings
         +- LocalRelation [course#x, 2012#x, 2013#x, 2014#x]


-- !query
create temporary view courseEarningsAndSales as select * from values
  ("dotNET", 15000, NULL, 48000, 1, 22500, 1),
  ("Java", 20000, 1, 30000, 2, NULL, NULL)
  as courseEarningsAndSales(
    course, earnings2012, sales2012, earnings2013, sales2013, earnings2014, sales2014)
-- !query analysis
CreateViewCommand `courseEarningsAndSales`, select * from values
  ("dotNET", 15000, NULL, 48000, 1, 22500, 1),
  ("Java", 20000, 1, 30000, 2, NULL, NULL)
  as courseEarningsAndSales(
    course, earnings2012, sales2012, earnings2013, sales2013, earnings2014, sales2014), false, false, LocalTempView, UNSUPPORTED, true
   +- Project [course#x, earnings2012#x, sales2012#x, earnings2013#x, sales2013#x, earnings2014#x, sales2014#x]
      +- SubqueryAlias courseEarningsAndSales
         +- LocalRelation [course#x, earnings2012#x, sales2012#x, earnings2013#x, sales2013#x, earnings2014#x, sales2014#x]


-- !query
create temporary view yearsWithComplexTypes as select * from values
  (2012, array(1, 1), map('1', 1), struct(1, 'a')),
  (2013, array(2, 2), map('2', 2), struct(2, 'b'))
  as yearsWithComplexTypes(y, a, m, s)
-- !query analysis
CreateViewCommand `yearsWithComplexTypes`, select * from values
  (2012, array(1, 1), map('1', 1), struct(1, 'a')),
  (2013, array(2, 2), map('2', 2), struct(2, 'b'))
  as yearsWithComplexTypes(y, a, m, s), false, false, LocalTempView, UNSUPPORTED, true
   +- Project [y#x, a#x, m#x, s#x]
      +- SubqueryAlias yearsWithComplexTypes
         +- LocalRelation [y#x, a#x, m#x, s#x]


-- !query
create temporary view join_test_t1 as select * from values (1) as grouping(a)
-- !query analysis
CreateViewCommand `join_test_t1`, select * from values (1) as grouping(a), false, false, LocalTempView, UNSUPPORTED, true
   +- Project [a#x]
      +- SubqueryAlias grouping
         +- LocalRelation [a#x]


-- !query
create temporary view join_test_t2 as select * from values (1) as grouping(a)
-- !query analysis
CreateViewCommand `join_test_t2`, select * from values (1) as grouping(a), false, false, LocalTempView, UNSUPPORTED, true
   +- Project [a#x]
      +- SubqueryAlias grouping
         +- LocalRelation [a#x]


-- !query
create temporary view join_test_t3 as select * from values (1) as grouping(a)
-- !query analysis
CreateViewCommand `join_test_t3`, select * from values (1) as grouping(a), false, false, LocalTempView, UNSUPPORTED, true
   +- Project [a#x]
      +- SubqueryAlias grouping
         +- LocalRelation [a#x]


-- !query
create temporary view join_test_empty_table as select a from join_test_t2 where false
-- !query analysis
CreateViewCommand `join_test_empty_table`, select a from join_test_t2 where false, false, false, LocalTempView, UNSUPPORTED, true
   +- Project [a#x]
      +- Filter false
         +- SubqueryAlias join_test_t2
            +- View (`join_test_t2`, [a#x])
               +- Project [cast(a#x as int) AS a#x]
                  +- Project [a#x]
                     +- SubqueryAlias grouping
                        +- LocalRelation [a#x]


-- !query
create temporary view lateral_test_t1(c1, c2)
  as values (0, 1), (1, 2)
-- !query analysis
CreateViewCommand `lateral_test_t1`, [(c1,None), (c2,None)], values (0, 1), (1, 2), false, false, LocalTempView, UNSUPPORTED, true
   +- LocalRelation [col1#x, col2#x]


-- !query
create temporary view lateral_test_t2(c1, c2)
  as values (0, 2), (0, 3)
-- !query analysis
CreateViewCommand `lateral_test_t2`, [(c1,None), (c2,None)], values (0, 2), (0, 3), false, false, LocalTempView, UNSUPPORTED, true
   +- LocalRelation [col1#x, col2#x]


-- !query
create temporary view lateral_test_t3(c1, c2)
  as values (0, array(0, 1)), (1, array(2)), (2, array()), (null, array(4))
-- !query analysis
CreateViewCommand `lateral_test_t3`, [(c1,None), (c2,None)], values (0, array(0, 1)), (1, array(2)), (2, array()), (null, array(4)), false, false, LocalTempView, UNSUPPORTED, true
   +- LocalRelation [col1#x, col2#x]


-- !query
create temporary view lateral_test_t4(c1, c2)
  as values (0, 1), (0, 2), (1, 1), (1, 3)
-- !query analysis
CreateViewCommand `lateral_test_t4`, [(c1,None), (c2,None)], values (0, 1), (0, 2), (1, 1), (1, 3), false, false, LocalTempView, UNSUPPORTED, true
   +- LocalRelation [col1#x, col2#x]


-- !query
create temporary view natural_join_test_t1 as select * from values
  ("one", 1), ("two", 2), ("three", 3) as natural_join_test_t1(k, v1)
-- !query analysis
CreateViewCommand `natural_join_test_t1`, select * from values
  ("one", 1), ("two", 2), ("three", 3) as natural_join_test_t1(k, v1), false, false, LocalTempView, UNSUPPORTED, true
   +- Project [k#x, v1#x]
      +- SubqueryAlias natural_join_test_t1
         +- LocalRelation [k#x, v1#x]


-- !query
create temporary view natural_join_test_t2 as select * from values
  ("one", 1), ("two", 22), ("one", 5) as natural_join_test_t2(k, v2)
-- !query analysis
CreateViewCommand `natural_join_test_t2`, select * from values
  ("one", 1), ("two", 22), ("one", 5) as natural_join_test_t2(k, v2), false, false, LocalTempView, UNSUPPORTED, true
   +- Project [k#x, v2#x]
      +- SubqueryAlias natural_join_test_t2
         +- LocalRelation [k#x, v2#x]


-- !query
create temporary view natural_join_test_t3 as select * from values
  ("one", 4), ("two", 5), ("one", 6) as natural_join_test_t3(k, v3)
-- !query analysis
CreateViewCommand `natural_join_test_t3`, select * from values
  ("one", 4), ("two", 5), ("one", 6) as natural_join_test_t3(k, v3), false, false, LocalTempView, UNSUPPORTED, true
   +- Project [k#x, v3#x]
      +- SubqueryAlias natural_join_test_t3
         +- LocalRelation [k#x, v3#x]


-- !query
create temporary view windowTestData as select * from values
  (null, 1L, 1.0D, date("2017-08-01"), timestamp_seconds(1501545600), "a"),
  (1, 1L, 1.0D, date("2017-08-01"), timestamp_seconds(1501545600), "a"),
  (1, 2L, 2.5D, date("2017-08-02"), timestamp_seconds(1502000000), "a"),
  (2, 2147483650L, 100.001D, date("2020-12-31"), timestamp_seconds(1609372800), "a"),
  (1, null, 1.0D, date("2017-08-01"), timestamp_seconds(1501545600), "b"),
  (2, 3L, 3.3D, date("2017-08-03"), timestamp_seconds(1503000000), "b"),
  (3, 2147483650L, 100.001D, date("2020-12-31"), timestamp_seconds(1609372800), "b"),
  (null, null, null, null, null, null),
  (3, 1L, 1.0D, date("2017-08-01"), timestamp_seconds(1501545600), null)
  AS testData(val, val_long, val_double, val_date, val_timestamp, cate)
-- !query analysis
CreateViewCommand `windowTestData`, select * from values
  (null, 1L, 1.0D, date("2017-08-01"), timestamp_seconds(1501545600), "a"),
  (1, 1L, 1.0D, date("2017-08-01"), timestamp_seconds(1501545600), "a"),
  (1, 2L, 2.5D, date("2017-08-02"), timestamp_seconds(1502000000), "a"),
  (2, 2147483650L, 100.001D, date("2020-12-31"), timestamp_seconds(1609372800), "a"),
  (1, null, 1.0D, date("2017-08-01"), timestamp_seconds(1501545600), "b"),
  (2, 3L, 3.3D, date("2017-08-03"), timestamp_seconds(1503000000), "b"),
  (3, 2147483650L, 100.001D, date("2020-12-31"), timestamp_seconds(1609372800), "b"),
  (null, null, null, null, null, null),
  (3, 1L, 1.0D, date("2017-08-01"), timestamp_seconds(1501545600), null)
  AS testData(val, val_long, val_double, val_date, val_timestamp, cate), false, false, LocalTempView, UNSUPPORTED, true
   +- Project [val#x, val_long#xL, val_double#x, val_date#x, val_timestamp#x, cate#x]
      +- SubqueryAlias testData
         +- LocalRelation [val#x, val_long#xL, val_double#x, val_date#x, val_timestamp#x, cate#x]


-- !query
create table store_sales(
  `ss_sold_date_sk` int,
  `ss_sold_time_sk` int,
  `ss_item_sk` int,
  `ss_customer_sk` int,
  `ss_cdemo_sk` int,
  `ss_hdemo_sk` int,
  `ss_addr_sk` int,
  `ss_store_sk` int,
  `ss_promo_sk` int,
  `ss_ticket_number` int,
  `ss_quantity` int,
  `ss_wholesale_cost` decimal(7,2),
  `ss_list_price` decimal(7,2),
  `ss_sales_price` decimal(7,2),
  `ss_ext_discount_amt` decimal(7,2),
  `ss_ext_sales_price` decimal(7,2),
  `ss_ext_wholesale_cost` decimal(7,2),
  `ss_ext_list_price` decimal(7,2),
  `ss_ext_tax` decimal(7,2),
  `ss_coupon_amt` decimal(7,2),
  `ss_net_paid` decimal(7,2),
  `ss_net_paid_inc_tax` decimal(7,2),
  `ss_net_profit` decimal(7,2))
-- !query analysis
CreateDataSourceTableCommand `spark_catalog`.`default`.`store_sales`, false


-- !query
create table store_returns(
  `sr_returned_date_sk` int,
  `sr_return_time_sk` int,
  `sr_item_sk` int,
  `sr_customer_sk` int,
  `sr_cdemo_sk` int,
  `sr_hdemo_sk` int,
  `sr_addr_sk` int,
  `sr_store_sk` int,
  `sr_reason_sk` int,
  `sr_ticket_number` int,
  `sr_return_quantity` int,
  `sr_return_amt` decimal(7,2),
  `sr_return_tax` decimal(7,2),
  `sr_return_amt_inc_tax` decimal(7,2),
  `sr_fee` decimal(7,2),
  `sr_return_ship_cost` decimal(7,2),
  `sr_refunded_cash` decimal(7,2),
  `sr_reversed_charge` decimal(7,2),
  `sr_store_credit` decimal(7,2),
  `sr_net_loss` decimal(7,2))
-- !query analysis
CreateDataSourceTableCommand `spark_catalog`.`default`.`store_returns`, false


-- !query
create table catalog_sales(
  `cs_sold_date_sk` int,
  `cs_sold_time_sk` int,
  `cs_ship_date_sk` int,
  `cs_bill_customer_sk` int,
  `cs_bill_cdemo_sk` int,
  `cs_bill_hdemo_sk` int,
  `cs_bill_addr_sk` int,
  `cs_ship_customer_sk` int,
  `cs_ship_cdemo_sk` int,
  `cs_ship_hdemo_sk` int,
  `cs_ship_addr_sk` int,
  `cs_call_center_sk` int,
  `cs_catalog_page_sk` int,
  `cs_ship_mode_sk` int,
  `cs_warehouse_sk` int,
  `cs_item_sk` int,
  `cs_promo_sk` int,
  `cs_order_number` int,
  `cs_quantity` int,
  `cs_wholesale_cost` decimal(7,2),
  `cs_list_price` decimal(7,2),
  `cs_sales_price` decimal(7,2),
  `cs_ext_discount_amt` decimal(7,2),
  `cs_ext_sales_price` decimal(7,2),
  `cs_ext_wholesale_cost` decimal(7,2),
  `cs_ext_list_price` decimal(7,2),
  `cs_ext_tax` decimal(7,2),
  `cs_coupon_amt` decimal(7,2),
  `cs_ext_ship_cost` decimal(7,2),
  `cs_net_paid` decimal(7,2),
  `cs_net_paid_inc_tax` decimal(7,2),
  `cs_net_paid_inc_ship` decimal(7,2),
  `cs_net_paid_inc_ship_tax` decimal(7,2),
  `cs_net_profit` decimal(7,2))
-- !query analysis
CreateDataSourceTableCommand `spark_catalog`.`default`.`catalog_sales`, false


-- !query
create table catalog_returns(
  `cr_returned_date_sk` int,
  `cr_returned_time_sk` int,
  `cr_item_sk` int,
  `cr_refunded_customer_sk` int,
  `cr_refunded_cdemo_sk` int,
  `cr_refunded_hdemo_sk` int,
  `cr_refunded_addr_sk` int,
  `cr_returning_customer_sk` int,
  `cr_returning_cdemo_sk` int,
  `cr_returning_hdemo_sk` int,
  `cr_returning_addr_sk` int,
  `cr_call_center_sk` int,
  `cr_catalog_page_sk` int,
  `cr_ship_mode_sk` int,
  `cr_warehouse_sk` int,
  `cr_reason_sk` int,
  `cr_order_number` int,
  `cr_return_quantity` int,
  `cr_return_amount` decimal(7,2),
  `cr_return_tax` decimal(7,2),
  `cr_return_amt_inc_tax` decimal(7,2),
  `cr_fee` decimal(7,2),
  `cr_return_ship_cost` decimal(7,2),
  `cr_refunded_cash` decimal(7,2),
  `cr_reversed_charge` decimal(7,2),
  `cr_store_credit` decimal(7,2),
  `cr_net_loss` decimal(7,2))
-- !query analysis
CreateDataSourceTableCommand `spark_catalog`.`default`.`catalog_returns`, false


-- !query
create table web_sales(
  `ws_sold_date_sk` int,
  `ws_sold_time_sk` int,
  `ws_ship_date_sk` int,
  `ws_item_sk` int,
  `ws_bill_customer_sk` int,
  `ws_bill_cdemo_sk` int,
  `ws_bill_hdemo_sk` int,
  `ws_bill_addr_sk` int,
  `ws_ship_customer_sk` int,
  `ws_ship_cdemo_sk` int,
  `ws_ship_hdemo_sk` int,
  `ws_ship_addr_sk` int,
  `ws_web_page_sk` int,
  `ws_web_site_sk` int,
  `ws_ship_mode_sk` int,
  `ws_warehouse_sk` int,
  `ws_promo_sk` int,
  `ws_order_number` int,
  `ws_quantity` int,
  `ws_wholesale_cost` decimal(7,2),
  `ws_list_price` decimal(7,2),
  `ws_sales_price` decimal(7,2),
  `ws_ext_discount_amt` decimal(7,2),
  `ws_ext_sales_price` decimal(7,2),
  `ws_ext_wholesale_cost` decimal(7,2),
  `ws_ext_list_price` decimal(7,2),
  `ws_ext_tax` decimal(7,2),
  `ws_coupon_amt` decimal(7,2),
  `ws_ext_ship_cost` decimal(7,2),
  `ws_net_paid` decimal(7,2),
  `ws_net_paid_inc_tax` decimal(7,2),
  `ws_net_paid_inc_ship` decimal(7,2),
  `ws_net_paid_inc_ship_tax` decimal(7,2),
  `ws_net_profit` decimal(7,2))
-- !query analysis
CreateDataSourceTableCommand `spark_catalog`.`default`.`web_sales`, false


-- !query
create table web_returns(
  `wr_returned_date_sk` int,
  `wr_returned_time_sk` int,
  `wr_item_sk` int,
  `wr_refunded_customer_sk` int,
  `wr_refunded_cdemo_sk` int,
  `wr_refunded_hdemo_sk` int,
  `wr_refunded_addr_sk` int,
  `wr_returning_customer_sk` int,
  `wr_returning_cdemo_sk` int,
  `wr_returning_hdemo_sk` int,
  `wr_returning_addr_sk` int,
  `wr_web_page_sk` int,
  `wr_reason_sk` int,
  `wr_order_number` int,
  `wr_return_quantity` int,
  `wr_return_amt` decimal(7,2),
  `wr_return_tax` decimal(7,2),
  `wr_return_amt_inc_tax` decimal(7,2),
  `wr_fee` decimal(7,2),
  `wr_return_ship_cost` decimal(7,2),
  `wr_refunded_cash` decimal(7,2),
  `wr_reversed_charge` decimal(7,2),
  `wr_account_credit` decimal(7,2),
  `wr_net_loss` decimal(7,2))
-- !query analysis
CreateDataSourceTableCommand `spark_catalog`.`default`.`web_returns`, false


-- !query
create table inventory(
  `inv_date_sk` int,
  `inv_item_sk` int,
  `inv_warehouse_sk` int,
  `inv_quantity_on_hand` int)
-- !query analysis
CreateDataSourceTableCommand `spark_catalog`.`default`.`inventory`, false


-- !query
create table store(
  `s_store_sk` int,
  `s_store_id` char(16),
  `s_rec_start_date` date,
  `s_rec_end_date` date,
  `s_closed_date_sk` int,
  `s_store_name` varchar(50),
  `s_number_employees` int,
  `s_floor_space` int,
  `s_hours` char(20),
  `s_manager` varchar(40),
  `s_market_id` int,
  `s_geography_class` varchar(100),
  `s_market_desc` varchar(100),
  `s_market_manager` varchar(40),
  `s_division_id` int,
  `s_division_name` varchar(50),
  `s_company_id` int,
  `s_company_name` varchar(50),
  `s_street_number` varchar(10),
  `s_street_name` varchar(60),
  `s_street_type` char(15),
  `s_suite_number` char(10),
  `s_city` varchar(60),
  `s_county` varchar(30),
  `s_state` char(2),
  `s_zip` char(10),
  `s_country` varchar(20),
  `s_gmt_offset` decimal(5,2),
  `s_tax_percentage` decimal(5,2))
-- !query analysis
CreateDataSourceTableCommand `spark_catalog`.`default`.`store`, false


-- !query
create table call_center(
  `cc_call_center_sk` int,
  `cc_call_center_id` char(16),
  `cc_rec_start_date` date,
  `cc_rec_end_date` date,
  `cc_closed_date_sk` int,
  `cc_open_date_sk` int,
  `cc_name` varchar(50),
  `cc_class` varchar(50),
  `cc_employees` int,
  `cc_sq_ft` int,
  `cc_hours` char(20),
  `cc_manager` varchar(40),
  `cc_mkt_id` int,
  `cc_mkt_class` char(50),
  `cc_mkt_desc` varchar(100),
  `cc_market_manager` varchar(40),
  `cc_division` int,
  `cc_division_name` varchar(50),
  `cc_company` int,
  `cc_company_name` char(50),
  `cc_street_number` char(10),
  `cc_street_name` varchar(60),
  `cc_street_type` char(15),
  `cc_suite_number` char(10),
  `cc_city` varchar(60),
  `cc_county` varchar(30),
  `cc_state` char(2),
  `cc_zip` char(10),
  `cc_country` varchar(20),
  `cc_gmt_offset` decimal(5,2),
  `cc_tax_percentage` decimal(5,2))
-- !query analysis
CreateDataSourceTableCommand `spark_catalog`.`default`.`call_center`, false


-- !query
create table catalog_page(
  `cp_catalog_page_sk` int,
  `cp_catalog_page_id` char(16),
  `cp_start_date_sk` int,
  `cp_end_date_sk` int,
  `cp_department` varchar(50),
  `cp_catalog_number` int,
  `cp_catalog_page_number` int,
  `cp_description` varchar(100),
  `cp_type` varchar(100))
-- !query analysis
CreateDataSourceTableCommand `spark_catalog`.`default`.`catalog_page`, false


-- !query
create table web_site(
  `web_site_sk` int,
  `web_site_id` char(16),
  `web_rec_start_date` date,
  `web_rec_end_date` date,
  `web_name` varchar(50),
  `web_open_date_sk` int,
  `web_close_date_sk` int,
  `web_class` varchar(50),
  `web_manager` varchar(40),
  `web_mkt_id` int,
  `web_mkt_class` varchar(50),
  `web_mkt_desc` varchar(100),
  `web_market_manager` varchar(40),
  `web_company_id` int,
  `web_company_name` char(50),
  `web_street_number` char(10),
  `web_street_name` varchar(60),
  `web_street_type` char(15),
  `web_suite_number` char(10),
  `web_city` varchar(60),
  `web_county` varchar(30),
  `web_state` char(2),
  `web_zip` char(10),
  `web_country` varchar(20),
  `web_gmt_offset` decimal(5,2),
  `web_tax_percentage` decimal(5,2))
-- !query analysis
CreateDataSourceTableCommand `spark_catalog`.`default`.`web_site`, false


-- !query
create table web_page(
  `wp_web_page_sk` int,
  `wp_web_page_id` char(16),
  `wp_rec_start_date` date,
  `wp_rec_end_date` date,
  `wp_creation_date_sk` int,
  `wp_access_date_sk` int,
  `wp_autogen_flag` char(1),
  `wp_customer_sk` int,
  `wp_url` varchar(100),
  `wp_type` char(50),
  `wp_char_count` int,
  `wp_link_count` int,
  `wp_image_count` int,
  `wp_max_ad_count` int)
-- !query analysis
CreateDataSourceTableCommand `spark_catalog`.`default`.`web_page`, false


-- !query
create table warehouse(
  `w_warehouse_sk` int,
  `w_warehouse_id` char(16),
  `w_warehouse_name` varchar(20),
  `w_warehouse_sq_ft` int,
  `w_street_number` char(10),
  `w_street_name` varchar(20),
  `w_street_type` char(15),
  `w_suite_number` char(10),
  `w_city` varchar(60),
  `w_county` varchar(30),
  `w_state` char(2),
  `w_zip` char(10),
  `w_country` varchar(20),
  `w_gmt_offset` decimal(5,2))
-- !query analysis
CreateDataSourceTableCommand `spark_catalog`.`default`.`warehouse`, false


-- !query
create table customer(
  `c_customer_sk` int,
  `c_customer_id` char(16),
  `c_current_cdemo_sk` int,
  `c_current_hdemo_sk` int,
  `c_current_addr_sk` int,
  `c_first_shipto_date_sk` int,
  `c_first_sales_date_sk` int,
  `c_salutation` char(10),
  `c_first_name` char(20),
  `c_last_name` char(30),
  `c_preferred_cust_flag` char(1),
  `c_birth_day` int,
  `c_birth_month` int,
  `c_birth_year` int,
  `c_birth_country` varchar(20),
  `c_login` char(13),
  `c_email_address` char(50),
  `c_last_review_date` int)
-- !query analysis
CreateDataSourceTableCommand `spark_catalog`.`default`.`customer`, false


-- !query
create table customer_address(
  `ca_address_sk` int,
  `ca_address_id` char(16),
  `ca_street_number` char(10),
  `ca_street_name` varchar(60),
  `ca_street_type` char(15),
  `ca_suite_number` char(10),
  `ca_city` varchar(60),
  `ca_county` varchar(30),
  `ca_state` char(2),
  `ca_zip` char(10),
  `ca_country` varchar(20),
  `ca_gmt_offset` decimal(5,2),
  `ca_location_type` char(20))
-- !query analysis
CreateDataSourceTableCommand `spark_catalog`.`default`.`customer_address`, false


-- !query
create table customer_demographics(
  `cd_demo_sk` int,
  `cd_gender` char(1),
  `cd_marital_status` char(1),
  `cd_education_status` char(20),
  `cd_purchase_estimate` int,
  `cd_credit_rating` char(10),
  `cd_dep_count` int,
  `cd_dep_employed_count` int,
  `cd_dep_college_count` int)
-- !query analysis
CreateDataSourceTableCommand `spark_catalog`.`default`.`customer_demographics`, false


-- !query
create table date_dim(
  `d_date_sk` int,
  `d_date_id` char(16),
  `d_date` date,
  `d_month_seq` int,
  `d_week_seq` int,
  `d_quarter_seq` int,
  `d_year` int,
  `d_dow` int,
  `d_moy` int,
  `d_dom` int,
  `d_qoy` int,
  `d_fy_year` int,
  `d_fy_quarter_seq` int,
  `d_fy_week_seq` int,
  `d_day_name` char(9),
  `d_quarter_name` char(6),
  `d_holiday` char(1),
  `d_weekend` char(1),
  `d_following_holiday` char(1),
  `d_first_dom` int,
  `d_last_dom` int,
  `d_same_day_ly` int,
  `d_same_day_lq` int,
  `d_current_day` char(1),
  `d_current_week` char(1),
  `d_current_month` char(1),
  `d_current_quarter` char(1),
  `d_current_year` char(1))
-- !query analysis
CreateDataSourceTableCommand `spark_catalog`.`default`.`date_dim`, false


-- !query
create table household_demographics(
  `hd_demo_sk` int,
  `hd_income_band_sk` int,
  `hd_buy_potential` char(15),
  `hd_dep_count` int,
  `hd_vehicle_count` int)
-- !query analysis
CreateDataSourceTableCommand `spark_catalog`.`default`.`household_demographics`, false


-- !query
create table item(
  `i_item_sk` int,
  `i_item_id` char(16),
  `i_rec_start_date` date,
  `i_rec_end_date` date,
  `i_item_desc` varchar(200),
  `i_current_price` decimal(7,2),
  `i_wholesale_cost` decimal(7,2),
  `i_brand_id` int,
  `i_brand` char(50),
  `i_class_id` int,
  `i_class` char(50),
  `i_category_id` int,
  `i_category` char(50),
  `i_manufact_id` int,
  `i_manufact` char(50),
  `i_size` char(20),
  `i_formulation` char(20),
  `i_color` char(20),
  `i_units` char(10),
  `i_container` char(10),
  `i_manager_id` int,
  `i_product_name` char(50))
-- !query analysis
CreateDataSourceTableCommand `spark_catalog`.`default`.`item`, false


-- !query
create table income_band(
  `ib_income_band_sk` int,
  `ib_lower_bound` int,
  `ib_upper_bound` int)
-- !query analysis
CreateDataSourceTableCommand `spark_catalog`.`default`.`income_band`, false


-- !query
create table promotion(
  `p_promo_sk` int,
  `p_promo_id` char(16),
  `p_start_date_sk` int,
  `p_end_date_sk` int,
  `p_item_sk` int,
  `p_cost` decimal(15,2),
  `p_response_target` int,
  `p_promo_name` char(50),
  `p_channel_dmail` char(1),
  `p_channel_email` char(1),
  `p_channel_catalog` char(1),
  `p_channel_tv` char(1),
  `p_channel_radio` char(1),
  `p_channel_press` char(1),
  `p_channel_event` char(1),
  `p_channel_demo` char(1),
  `p_channel_details` varchar(100),
  `p_purpose` char(15),
  `p_discount_active` char(1))
-- !query analysis
CreateDataSourceTableCommand `spark_catalog`.`default`.`promotion`, false


-- !query
create table reason(
  `r_reason_sk` int,
  `r_reason_id` char(16),
  `r_reason_desc` char(100))
-- !query analysis
CreateDataSourceTableCommand `spark_catalog`.`default`.`reason`, false


-- !query
create table ship_mode(
  `sm_ship_mode_sk` int,
  `sm_ship_mode_id` char(16),
  `sm_type` char(30),
  `sm_code` char(10),
  `sm_carrier` char(20),
  `sm_contract` char(20))
-- !query analysis
CreateDataSourceTableCommand `spark_catalog`.`default`.`ship_mode`, false


-- !query
create table time_dim(
  `t_time_sk` int,
  `t_time_id` char(16),
  `t_time` int,
  `t_hour` int,
  `t_minute` int,
  `t_second` int,
  `t_am_pm` char(2),
  `t_shift` char(20),
  `t_sub_shift` char(20),
  `t_meal_time` char(20))
-- !query analysis
CreateDataSourceTableCommand `spark_catalog`.`default`.`time_dim`, false


-- !query
table t
|> select 1 as x
-- !query analysis
Project [pipeexpression(1, false, SELECT) AS x#x]
+- SubqueryAlias spark_catalog.default.t
   +- Relation spark_catalog.default.t[x#x,y#x] csv


-- !query
table t
|> select x, y
-- !query analysis
Project [x#x, y#x]
+- SubqueryAlias spark_catalog.default.t
   +- Relation spark_catalog.default.t[x#x,y#x] csv


-- !query
table t
|> select x, y
|> select x + length(y) as z
-- !query analysis
Project [pipeexpression((x#x + length(y#x)), false, SELECT) AS z#x]
+- Project [x#x, y#x]
   +- SubqueryAlias spark_catalog.default.t
      +- Relation spark_catalog.default.t[x#x,y#x] csv


-- !query
values (0), (1) tab(col)
|> select col * 2 as result
-- !query analysis
Project [pipeexpression((col#x * 2), false, SELECT) AS result#x]
+- SubqueryAlias tab
   +- LocalRelation [col#x]


-- !query
(select * from t union all select * from t)
|> select x + length(y) as result
-- !query analysis
Project [pipeexpression((x#x + length(y#x)), false, SELECT) AS result#x]
+- Union false, false
   :- Project [x#x, y#x]
   :  +- SubqueryAlias spark_catalog.default.t
   :     +- Relation spark_catalog.default.t[x#x,y#x] csv
   +- Project [x#x, y#x]
      +- SubqueryAlias spark_catalog.default.t
         +- Relation spark_catalog.default.t[x#x,y#x] csv


-- !query
(table t
 |> select x, y
 |> select x)
union all
select x from t where x < 1
-- !query analysis
Union false, false
:- Project [x#x]
:  +- Project [x#x, y#x]
:     +- SubqueryAlias spark_catalog.default.t
:        +- Relation spark_catalog.default.t[x#x,y#x] csv
+- Project [x#x]
   +- Filter (x#x < 1)
      +- SubqueryAlias spark_catalog.default.t
         +- Relation spark_catalog.default.t[x#x,y#x] csv


-- !query
(select col from st)
|> select col.i1
-- !query analysis
Project [col#x.i1 AS i1#x]
+- Project [col#x]
   +- SubqueryAlias spark_catalog.default.st
      +- Relation spark_catalog.default.st[x#x,col#x] parquet


-- !query
table st
|> select st.col.i1
-- !query analysis
Project [col#x.i1 AS i1#x]
+- SubqueryAlias spark_catalog.default.st
   +- Relation spark_catalog.default.st[x#x,col#x] parquet


-- !query
table t
|> select (select a from other where x = a limit 1) as result
-- !query analysis
Project [pipeexpression(scalar-subquery#x [x#x], false, SELECT) AS result#x]
:  +- GlobalLimit 1
:     +- LocalLimit 1
:        +- Project [a#x]
:           +- Filter (outer(x#x) = a#x)
:              +- SubqueryAlias spark_catalog.default.other
:                 +- Relation spark_catalog.default.other[a#x,b#x] json
+- SubqueryAlias spark_catalog.default.t
   +- Relation spark_catalog.default.t[x#x,y#x] csv


-- !query
select (values (0) tab(col) |> select col) as result
-- !query analysis
Project [scalar-subquery#x [] AS result#x]
:  +- Project [col#x]
:     +- SubqueryAlias tab
:        +- LocalRelation [col#x]
+- OneRowRelation


-- !query
table t
|> select (select any_value(a) from other where x = a limit 1) as result
-- !query analysis
Project [pipeexpression(scalar-subquery#x [x#x], false, SELECT) AS result#x]
:  +- GlobalLimit 1
:     +- LocalLimit 1
:        +- Aggregate [any_value(a#x, false) AS any_value(a)#x]
:           +- Filter (outer(x#x) = a#x)
:              +- SubqueryAlias spark_catalog.default.other
:                 +- Relation spark_catalog.default.other[a#x,b#x] json
+- SubqueryAlias spark_catalog.default.t
   +- Relation spark_catalog.default.t[x#x,y#x] csv


-- !query
table t
|> select x + length(x) as z, z + 1 as plus_one
-- !query analysis
Project [z#x, pipeexpression((z#x + 1), false, SELECT) AS plus_one#x]
+- Project [x#x, y#x, pipeexpression((x#x + length(cast(x#x as string))), false, SELECT) AS z#x]
   +- SubqueryAlias spark_catalog.default.t
      +- Relation spark_catalog.default.t[x#x,y#x] csv


-- !query
table t
|> select first_value(x) over (partition by y) as result
-- !query analysis
Project [result#x]
+- Project [x#x, y#x, _we0#x, pipeexpression(_we0#x, false, SELECT) AS result#x]
   +- Window [first_value(x#x, false) windowspecdefinition(y#x, specifiedwindowframe(RowFrame, unboundedpreceding$(), unboundedfollowing$())) AS _we0#x], [y#x]
      +- Project [x#x, y#x]
         +- SubqueryAlias spark_catalog.default.t
            +- Relation spark_catalog.default.t[x#x,y#x] csv


-- !query
select 1 x, 2 y, 3 z
|> select 1 + sum(x) over (),
     avg(y) over (),
     x,
     avg(x+1) over (partition by y order by z) AS a2
|> select a2
-- !query analysis
Project [a2#x]
+- Project [(1 + sum(x) OVER (ROWS BETWEEN UNBOUNDED PRECEDING AND UNBOUNDED FOLLOWING))#xL, avg(y) OVER (ROWS BETWEEN UNBOUNDED PRECEDING AND UNBOUNDED FOLLOWING)#x, x#x, a2#x]
   +- Project [x#x, y#x, _w1#x, z#x, _we0#xL, avg(y) OVER (ROWS BETWEEN UNBOUNDED PRECEDING AND UNBOUNDED FOLLOWING)#x, _we2#x, (cast(1 as bigint) + _we0#xL) AS (1 + sum(x) OVER (ROWS BETWEEN UNBOUNDED PRECEDING AND UNBOUNDED FOLLOWING))#xL, avg(y) OVER (ROWS BETWEEN UNBOUNDED PRECEDING AND UNBOUNDED FOLLOWING)#x, pipeexpression(_we2#x, false, SELECT) AS a2#x]
      +- Window [avg(_w1#x) windowspecdefinition(y#x, z#x ASC NULLS FIRST, specifiedwindowframe(RangeFrame, unboundedpreceding$(), currentrow$())) AS _we2#x], [y#x], [z#x ASC NULLS FIRST]
         +- Window [sum(x#x) windowspecdefinition(specifiedwindowframe(RowFrame, unboundedpreceding$(), unboundedfollowing$())) AS _we0#xL, avg(y#x) windowspecdefinition(specifiedwindowframe(RowFrame, unboundedpreceding$(), unboundedfollowing$())) AS avg(y) OVER (ROWS BETWEEN UNBOUNDED PRECEDING AND UNBOUNDED FOLLOWING)#x]
            +- Project [x#x, y#x, (x#x + 1) AS _w1#x, z#x]
               +- Project [1 AS x#x, 2 AS y#x, 3 AS z#x]
                  +- OneRowRelation


-- !query
table t
|> select x, count(*) over ()
|> select x
-- !query analysis
Project [x#x]
+- Project [x#x, count(1) OVER (ROWS BETWEEN UNBOUNDED PRECEDING AND UNBOUNDED FOLLOWING)#xL]
   +- Project [x#x, count(1) OVER (ROWS BETWEEN UNBOUNDED PRECEDING AND UNBOUNDED FOLLOWING)#xL, count(1) OVER (ROWS BETWEEN UNBOUNDED PRECEDING AND UNBOUNDED FOLLOWING)#xL]
      +- Window [count(1) windowspecdefinition(specifiedwindowframe(RowFrame, unboundedpreceding$(), unboundedfollowing$())) AS count(1) OVER (ROWS BETWEEN UNBOUNDED PRECEDING AND UNBOUNDED FOLLOWING)#xL]
         +- Project [x#x]
            +- SubqueryAlias spark_catalog.default.t
               +- Relation spark_catalog.default.t[x#x,y#x] csv


-- !query
table t
|> select distinct x, y
-- !query analysis
Distinct
+- Project [x#x, y#x]
   +- SubqueryAlias spark_catalog.default.t
      +- Relation spark_catalog.default.t[x#x,y#x] csv


-- !query
table t
|> select *
-- !query analysis
Project [x#x, y#x]
+- SubqueryAlias spark_catalog.default.t
   +- Relation spark_catalog.default.t[x#x,y#x] csv


-- !query
table t
|> select * except (y)
-- !query analysis
Project [x#x]
+- SubqueryAlias spark_catalog.default.t
   +- Relation spark_catalog.default.t[x#x,y#x] csv


-- !query
table t
|> select /*+ repartition(3) */ *
-- !query analysis
Repartition 3, true
+- Project [x#x, y#x]
   +- SubqueryAlias spark_catalog.default.t
      +- Relation spark_catalog.default.t[x#x,y#x] csv


-- !query
table t
|> select /*+ repartition(3) */ distinct x
-- !query analysis
Repartition 3, true
+- Distinct
   +- Project [x#x]
      +- SubqueryAlias spark_catalog.default.t
         +- Relation spark_catalog.default.t[x#x,y#x] csv


-- !query
table t
|> select /*+ repartition(3) */ all x
-- !query analysis
Repartition 3, true
+- Project [x#x]
   +- SubqueryAlias spark_catalog.default.t
      +- Relation spark_catalog.default.t[x#x,y#x] csv


-- !query
table t
|> select sum(x) as result
-- !query analysis
org.apache.spark.sql.AnalysisException
{
  "errorClass" : "PIPE_OPERATOR_CONTAINS_AGGREGATE_FUNCTION",
  "sqlState" : "0A000",
  "messageParameters" : {
    "clause" : "SELECT",
    "expr" : "sum(x#x)"
  },
  "queryContext" : [ {
    "objectType" : "",
    "objectName" : "",
    "startIndex" : 19,
    "stopIndex" : 24,
    "fragment" : "sum(x)"
  } ]
}


-- !query
table t
|> select y, length(y) + sum(x) as result
-- !query analysis
org.apache.spark.sql.AnalysisException
{
  "errorClass" : "PIPE_OPERATOR_CONTAINS_AGGREGATE_FUNCTION",
  "sqlState" : "0A000",
  "messageParameters" : {
    "clause" : "SELECT",
    "expr" : "sum(x#x)"
  },
  "queryContext" : [ {
    "objectType" : "",
    "objectName" : "",
    "startIndex" : 34,
    "stopIndex" : 39,
    "fragment" : "sum(x)"
  } ]
}


-- !query
table t
|> extend 1 as z
-- !query analysis
Project [x#x, y#x, pipeexpression(1, false, EXTEND) AS z#x]
+- SubqueryAlias spark_catalog.default.t
   +- Relation spark_catalog.default.t[x#x,y#x] csv


-- !query
table t
|> extend 1
-- !query analysis
Project [x#x, y#x, pipeexpression(1, false, EXTEND) AS pipeexpression(1)#x]
+- SubqueryAlias spark_catalog.default.t
   +- Relation spark_catalog.default.t[x#x,y#x] csv


-- !query
table t
|> extend x as z
-- !query analysis
Project [x#x, y#x, pipeexpression(x#x, false, EXTEND) AS z#x]
+- SubqueryAlias spark_catalog.default.t
   +- Relation spark_catalog.default.t[x#x,y#x] csv


-- !query
table t
|> extend x + length(y) as z
-- !query analysis
Project [x#x, y#x, pipeexpression((x#x + length(y#x)), false, EXTEND) AS z#x]
+- SubqueryAlias spark_catalog.default.t
   +- Relation spark_catalog.default.t[x#x,y#x] csv


-- !query
table t
|> extend x + length(y) as z, x + 1 as zz
-- !query analysis
Project [x#x, y#x, pipeexpression((x#x + length(y#x)), false, EXTEND) AS z#x, pipeexpression((x#x + 1), false, EXTEND) AS zz#x]
+- SubqueryAlias spark_catalog.default.t
   +- Relation spark_catalog.default.t[x#x,y#x] csv


-- !query
table t
|> extend x + length(y) as z
|> extend z + 1 as zz
-- !query analysis
Project [x#x, y#x, z#x, pipeexpression((z#x + 1), false, EXTEND) AS zz#x]
+- Project [x#x, y#x, pipeexpression((x#x + length(y#x)), false, EXTEND) AS z#x]
   +- SubqueryAlias spark_catalog.default.t
      +- Relation spark_catalog.default.t[x#x,y#x] csv


-- !query
select col from st
|> extend col.i1 as z
-- !query analysis
Project [col#x, pipeexpression(col#x.i1, false, EXTEND) AS z#x]
+- Project [col#x]
   +- SubqueryAlias spark_catalog.default.st
      +- Relation spark_catalog.default.st[x#x,col#x] parquet


-- !query
table t
|> extend (select a from other where x = a limit 1) as z
-- !query analysis
Project [x#x, y#x, pipeexpression(scalar-subquery#x [x#x], false, EXTEND) AS z#x]
:  +- GlobalLimit 1
:     +- LocalLimit 1
:        +- Project [a#x]
:           +- Filter (outer(x#x) = a#x)
:              +- SubqueryAlias spark_catalog.default.other
:                 +- Relation spark_catalog.default.other[a#x,b#x] json
+- SubqueryAlias spark_catalog.default.t
   +- Relation spark_catalog.default.t[x#x,y#x] csv


-- !query
table t
|> where exists (
    table other
    |> extend t.x
    |> select * except (a, b))
-- !query analysis
Filter exists#x [x#x]
:  +- Project [pipeexpression(outer(spark_catalog.default.t.x))#x]
:     +- Project [a#x, b#x, pipeexpression(outer(x#x), false, EXTEND) AS pipeexpression(outer(spark_catalog.default.t.x))#x]
:        +- SubqueryAlias spark_catalog.default.other
:           +- Relation spark_catalog.default.other[a#x,b#x] json
+- SubqueryAlias spark_catalog.default.t
   +- Relation spark_catalog.default.t[x#x,y#x] csv


-- !query
table t
|> extend 1 as x
-- !query analysis
Project [x#x, y#x, pipeexpression(1, false, EXTEND) AS x#x]
+- SubqueryAlias spark_catalog.default.t
   +- Relation spark_catalog.default.t[x#x,y#x] csv


-- !query
table t
|> extend first_value(x) over (partition by y) as result
-- !query analysis
Project [x#x, y#x, result#x]
+- Project [x#x, y#x, _we0#x, pipeexpression(_we0#x, false, EXTEND) AS result#x]
   +- Window [first_value(x#x, false) windowspecdefinition(y#x, specifiedwindowframe(RowFrame, unboundedpreceding$(), unboundedfollowing$())) AS _we0#x], [y#x]
      +- Project [x#x, y#x]
         +- SubqueryAlias spark_catalog.default.t
            +- Relation spark_catalog.default.t[x#x,y#x] csv


-- !query
table t
|> extend x + length(y) as z, z + 1 as plus_one
-- !query analysis
Project [x#x, y#x, z#x, pipeexpression((z#x + 1), false, EXTEND) AS plus_one#x]
+- Project [x#x, y#x, pipeexpression((x#x + length(y#x)), false, EXTEND) AS z#x]
   +- SubqueryAlias spark_catalog.default.t
      +- Relation spark_catalog.default.t[x#x,y#x] csv


-- !query
table t
|> extend sum(x) as z
-- !query analysis
org.apache.spark.sql.AnalysisException
{
  "errorClass" : "PIPE_OPERATOR_CONTAINS_AGGREGATE_FUNCTION",
  "sqlState" : "0A000",
  "messageParameters" : {
    "clause" : "EXTEND",
    "expr" : "sum(x#x)"
  },
  "queryContext" : [ {
    "objectType" : "",
    "objectName" : "",
    "startIndex" : 19,
    "stopIndex" : 24,
    "fragment" : "sum(x)"
  } ]
}


-- !query
table t
|> extend distinct x as z
-- !query analysis
org.apache.spark.sql.catalyst.parser.ParseException
{
  "errorClass" : "PARSE_SYNTAX_ERROR",
  "sqlState" : "42601",
  "messageParameters" : {
    "error" : "'as'",
    "hint" : ""
  }
}


-- !query
table t
|> extend *
-- !query analysis
org.apache.spark.sql.AnalysisException
{
  "errorClass" : "INVALID_USAGE_OF_STAR_OR_REGEX",
  "sqlState" : "42000",
  "messageParameters" : {
    "elem" : "'*'",
    "prettyName" : "expression `pipeexpression`"
  }
}


-- !query
table t
|> set x = 1
-- !query analysis
Project [pipeexpression(1, false, SET) AS x#x, y#x]
+- SubqueryAlias spark_catalog.default.t
   +- Relation spark_catalog.default.t[x#x,y#x] csv


-- !query
table t
|> set y = x
-- !query analysis
Project [x#x, pipeexpression(x#x, false, SET) AS y#x]
+- SubqueryAlias spark_catalog.default.t
   +- Relation spark_catalog.default.t[x#x,y#x] csv


-- !query
table t
|> extend 1 as z
|> set z = x + length(y)
-- !query analysis
Project [x#x, y#x, pipeexpression((x#x + length(y#x)), false, SET) AS z#x]
+- Project [x#x, y#x, pipeexpression(1, false, EXTEND) AS z#x]
   +- SubqueryAlias spark_catalog.default.t
      +- Relation spark_catalog.default.t[x#x,y#x] csv


-- !query
table t
|> extend 1 as z
|> extend 2 as zz
|> set z = x + length(y), zz = x + 1
-- !query analysis
Project [x#x, y#x, z#x, pipeexpression((x#x + 1), false, SET) AS zz#x]
+- Project [x#x, y#x, pipeexpression((x#x + length(y#x)), false, SET) AS z#x, zz#x]
   +- Project [x#x, y#x, z#x, pipeexpression(2, false, EXTEND) AS zz#x]
      +- Project [x#x, y#x, pipeexpression(1, false, EXTEND) AS z#x]
         +- SubqueryAlias spark_catalog.default.t
            +- Relation spark_catalog.default.t[x#x,y#x] csv


-- !query
table other
|> extend 3 as c
|> set a = b, b = c
-- !query analysis
Project [a#x, pipeexpression(c#x, false, SET) AS b#x, c#x]
+- Project [pipeexpression(b#x, false, SET) AS a#x, b#x, c#x]
   +- Project [a#x, b#x, pipeexpression(3, false, EXTEND) AS c#x]
      +- SubqueryAlias spark_catalog.default.other
         +- Relation spark_catalog.default.other[a#x,b#x] json


-- !query
table t
|> extend 1 as z
|> extend 2 as zz
|> set z = x + length(y), zz = z + 1
-- !query analysis
Project [x#x, y#x, z#x, pipeexpression((z#x + 1), false, SET) AS zz#x]
+- Project [x#x, y#x, pipeexpression((x#x + length(y#x)), false, SET) AS z#x, zz#x]
   +- Project [x#x, y#x, z#x, pipeexpression(2, false, EXTEND) AS zz#x]
      +- Project [x#x, y#x, pipeexpression(1, false, EXTEND) AS z#x]
         +- SubqueryAlias spark_catalog.default.t
            +- Relation spark_catalog.default.t[x#x,y#x] csv


-- !query
table t
|> extend 1 as z
|> set z = x + length(y)
|> set z = z + 1
-- !query analysis
Project [x#x, y#x, pipeexpression((z#x + 1), false, SET) AS z#x]
+- Project [x#x, y#x, pipeexpression((x#x + length(y#x)), false, SET) AS z#x]
   +- Project [x#x, y#x, pipeexpression(1, false, EXTEND) AS z#x]
      +- SubqueryAlias spark_catalog.default.t
         +- Relation spark_catalog.default.t[x#x,y#x] csv


-- !query
table t
|> extend 1 as z
|> set z = x + length(y), z = z + 1
-- !query analysis
Project [x#x, y#x, pipeexpression((z#x + 1), false, SET) AS z#x]
+- Project [x#x, y#x, pipeexpression((x#x + length(y#x)), false, SET) AS z#x]
   +- Project [x#x, y#x, pipeexpression(1, false, EXTEND) AS z#x]
      +- SubqueryAlias spark_catalog.default.t
         +- Relation spark_catalog.default.t[x#x,y#x] csv


-- !query
select col from st
|> extend 1 as z
|> set z = col.i1
-- !query analysis
Project [col#x, pipeexpression(col#x.i1, false, SET) AS z#x]
+- Project [col#x, pipeexpression(1, false, EXTEND) AS z#x]
   +- Project [col#x]
      +- SubqueryAlias spark_catalog.default.st
         +- Relation spark_catalog.default.st[x#x,col#x] parquet


-- !query
table t
|> set y = (select a from other where x = a limit 1)
-- !query analysis
Project [x#x, pipeexpression(scalar-subquery#x [x#x], false, SET) AS y#x]
:  +- GlobalLimit 1
:     +- LocalLimit 1
:        +- Project [a#x]
:           +- Filter (outer(x#x) = a#x)
:              +- SubqueryAlias spark_catalog.default.other
:                 +- Relation spark_catalog.default.other[a#x,b#x] json
+- SubqueryAlias spark_catalog.default.t
   +- Relation spark_catalog.default.t[x#x,y#x] csv


-- !query
table t
|> extend 1 as `x.y.z`
|> set `x.y.z` = x + length(y)
-- !query analysis
Project [x#x, y#x, pipeexpression((x#x + length(y#x)), false, SET) AS x.y.z#x]
+- Project [x#x, y#x, pipeexpression(1, false, EXTEND) AS x.y.z#x]
   +- SubqueryAlias spark_catalog.default.t
      +- Relation spark_catalog.default.t[x#x,y#x] csv


-- !query
table t
|> extend 1 as z
|> set z = first_value(x) over (partition by y)
-- !query analysis
Project [x#x, y#x, z#x]
+- Project [x#x, y#x, _we0#x, pipeexpression(_we0#x, false, SET) AS z#x]
   +- Window [first_value(x#x, false) windowspecdefinition(y#x, specifiedwindowframe(RowFrame, unboundedpreceding$(), unboundedfollowing$())) AS _we0#x], [y#x]
      +- Project [x#x, y#x]
         +- Project [x#x, y#x, pipeexpression(1, false, EXTEND) AS z#x]
            +- SubqueryAlias spark_catalog.default.t
               +- Relation spark_catalog.default.t[x#x,y#x] csv


-- !query
table t
|> set z = 1
-- !query analysis
org.apache.spark.sql.AnalysisException
{
  "errorClass" : "UNRESOLVED_COLUMN.WITH_SUGGESTION",
  "sqlState" : "42703",
  "messageParameters" : {
    "objectName" : "`z`",
    "proposal" : "`x`, `y`"
  },
  "queryContext" : [ {
    "objectType" : "",
    "objectName" : "",
    "startIndex" : 1,
    "stopIndex" : 20,
    "fragment" : "table t\n|> set z = 1"
  } ]
}


-- !query
table t
|> set x = 1 as z
-- !query analysis
org.apache.spark.sql.catalyst.parser.ParseException
{
  "errorClass" : "PARSE_SYNTAX_ERROR",
  "sqlState" : "42601",
  "messageParameters" : {
    "error" : "'as'",
    "hint" : ""
  }
}


-- !query
select col from st
|> set col.i1 = 42
-- !query analysis
org.apache.spark.sql.catalyst.parser.ParseException
{
  "errorClass" : "_LEGACY_ERROR_TEMP_0035",
  "messageParameters" : {
    "message" : "SQL pipe syntax |> SET operator with multi-part assignment key (only single-part keys are allowed)"
  },
  "queryContext" : [ {
    "objectType" : "",
    "objectName" : "",
    "startIndex" : 27,
    "stopIndex" : 37,
    "fragment" : "col.i1 = 42"
  } ]
}


-- !query
table t
|> drop y
-- !query analysis
Project [x#x]
+- SubqueryAlias spark_catalog.default.t
   +- Relation spark_catalog.default.t[x#x,y#x] csv


-- !query
select 1 as x, 2 as y, 3 as z
|> drop z, y
-- !query analysis
Project [x#x]
+- Project [1 AS x#x, 2 AS y#x, 3 AS z#x]
   +- OneRowRelation


-- !query
select 1 as x, 2 as y, 3 as z
|> drop z
|> drop y
-- !query analysis
Project [x#x]
+- Project [x#x, y#x]
   +- Project [1 AS x#x, 2 AS y#x, 3 AS z#x]
      +- OneRowRelation


-- !query
select x from t
|> drop x
-- !query analysis
Project
+- Project [x#x]
   +- SubqueryAlias spark_catalog.default.t
      +- Relation spark_catalog.default.t[x#x,y#x] csv


-- !query
table t
|> extend 1 as `x.y.z`
|> drop `x.y.z`
-- !query analysis
Project [x#x, y#x]
+- Project [x#x, y#x, pipeexpression(1, false, EXTEND) AS x.y.z#x]
   +- SubqueryAlias spark_catalog.default.t
      +- Relation spark_catalog.default.t[x#x,y#x] csv


-- !query
table t
|> drop z
-- !query analysis
org.apache.spark.sql.AnalysisException
{
  "errorClass" : "UNRESOLVED_COLUMN.WITH_SUGGESTION",
  "sqlState" : "42703",
  "messageParameters" : {
    "objectName" : "`z`",
    "proposal" : "`x`, `y`"
  },
  "queryContext" : [ {
    "objectType" : "",
    "objectName" : "",
    "startIndex" : 1,
    "stopIndex" : 17,
    "fragment" : "table t\n|> drop z"
  } ]
}


-- !query
table st
|> drop col.i1
-- !query analysis
org.apache.spark.sql.catalyst.parser.ParseException
{
  "errorClass" : "PARSE_SYNTAX_ERROR",
  "sqlState" : "42601",
  "messageParameters" : {
    "error" : "'.'",
    "hint" : ""
  }
}


-- !query
table st
|> drop `col.i1`
-- !query analysis
org.apache.spark.sql.AnalysisException
{
  "errorClass" : "UNRESOLVED_COLUMN.WITH_SUGGESTION",
  "sqlState" : "42703",
  "messageParameters" : {
    "objectName" : "`col.i1`",
    "proposal" : "`col`, `x`"
  },
  "queryContext" : [ {
    "objectType" : "",
    "objectName" : "",
    "startIndex" : 1,
    "stopIndex" : 25,
    "fragment" : "table st\n|> drop `col.i1`"
  } ]
}


-- !query
select 1 as x, 2 as y, 3 as z
|> drop z, y, z
-- !query analysis
org.apache.spark.sql.AnalysisException
{
  "errorClass" : "EXCEPT_OVERLAPPING_COLUMNS",
  "sqlState" : "42702",
  "messageParameters" : {
    "columns" : "z, y, z"
  },
  "queryContext" : [ {
    "objectType" : "",
    "objectName" : "",
    "startIndex" : 1,
    "stopIndex" : 45,
    "fragment" : "select 1 as x, 2 as y, 3 as z\n|> drop z, y, z"
  } ]
}


-- !query
table t
|> as u
|> select u.x, u.y
-- !query analysis
Project [x#x, y#x]
+- SubqueryAlias u
   +- SubqueryAlias spark_catalog.default.t
      +- Relation spark_catalog.default.t[x#x,y#x] csv


-- !query
select 1 as x, 2 as y
|> as u
|> select u.x, u.y
-- !query analysis
Project [x#x, y#x]
+- SubqueryAlias u
   +- Project [1 AS x#x, 2 AS y#x]
      +- OneRowRelation


-- !query
table t
|> as u
|> as v
|> select v.x, v.y
-- !query analysis
Project [x#x, y#x]
+- SubqueryAlias v
   +- SubqueryAlias spark_catalog.default.t
      +- Relation spark_catalog.default.t[x#x,y#x] csv


-- !query
table t
|> as u
|> where u.x = 1
-- !query analysis
Filter (x#x = 1)
+- SubqueryAlias u
   +- SubqueryAlias spark_catalog.default.t
      +- Relation spark_catalog.default.t[x#x,y#x] csv


-- !query
table t
|> as u, v
-- !query analysis
org.apache.spark.sql.catalyst.parser.ParseException
{
  "errorClass" : "PARSE_SYNTAX_ERROR",
  "sqlState" : "42601",
  "messageParameters" : {
    "error" : "','",
    "hint" : ""
  }
}


-- !query
table t
|> as 1 + 2
-- !query analysis
org.apache.spark.sql.catalyst.parser.ParseException
{
  "errorClass" : "PARSE_SYNTAX_ERROR",
  "sqlState" : "42601",
  "messageParameters" : {
    "error" : "'1'",
    "hint" : ""
  }
}


-- !query
table t
|> where true
-- !query analysis
Filter true
+- SubqueryAlias spark_catalog.default.t
   +- Relation spark_catalog.default.t[x#x,y#x] csv


-- !query
table t
|> where x + length(y) < 4
-- !query analysis
Filter ((x#x + length(y#x)) < 4)
+- SubqueryAlias spark_catalog.default.t
   +- Relation spark_catalog.default.t[x#x,y#x] csv


-- !query
table t
|> where x + length(y) < 4
|> where x + length(y) < 3
-- !query analysis
Filter ((x#x + length(y#x)) < 3)
+- Filter ((x#x + length(y#x)) < 4)
   +- SubqueryAlias spark_catalog.default.t
      +- Relation spark_catalog.default.t[x#x,y#x] csv


-- !query
(select x, sum(length(y)) as sum_len from t group by x)
|> where x = 1
-- !query analysis
Filter (x#x = 1)
+- SubqueryAlias __auto_generated_subquery_name
   +- Aggregate [x#x], [x#x, sum(length(y#x)) AS sum_len#xL]
      +- SubqueryAlias spark_catalog.default.t
         +- Relation spark_catalog.default.t[x#x,y#x] csv


-- !query
table t
|> where t.x = 1
-- !query analysis
Filter (x#x = 1)
+- SubqueryAlias spark_catalog.default.t
   +- Relation spark_catalog.default.t[x#x,y#x] csv


-- !query
table t
|> where spark_catalog.default.t.x = 1
-- !query analysis
Filter (x#x = 1)
+- SubqueryAlias spark_catalog.default.t
   +- Relation spark_catalog.default.t[x#x,y#x] csv


-- !query
(select col from st)
|> where col.i1 = 1
-- !query analysis
Filter (col#x.i1 = 1)
+- SubqueryAlias __auto_generated_subquery_name
   +- Project [col#x]
      +- SubqueryAlias spark_catalog.default.st
         +- Relation spark_catalog.default.st[x#x,col#x] parquet


-- !query
table st
|> where st.col.i1 = 2
-- !query analysis
Filter (col#x.i1 = 2)
+- SubqueryAlias spark_catalog.default.st
   +- Relation spark_catalog.default.st[x#x,col#x] parquet


-- !query
table t
|> where exists (select a from other where x = a limit 1)
-- !query analysis
Filter exists#x [x#x]
:  +- GlobalLimit 1
:     +- LocalLimit 1
:        +- Project [a#x]
:           +- Filter (outer(x#x) = a#x)
:              +- SubqueryAlias spark_catalog.default.other
:                 +- Relation spark_catalog.default.other[a#x,b#x] json
+- SubqueryAlias spark_catalog.default.t
   +- Relation spark_catalog.default.t[x#x,y#x] csv


-- !query
table t
|> where (select any_value(a) from other where x = a limit 1) = 1
-- !query analysis
Filter (scalar-subquery#x [x#x] = 1)
:  +- GlobalLimit 1
:     +- LocalLimit 1
:        +- Aggregate [any_value(a#x, false) AS any_value(a)#x]
:           +- Filter (outer(x#x) = a#x)
:              +- SubqueryAlias spark_catalog.default.other
:                 +- Relation spark_catalog.default.other[a#x,b#x] json
+- SubqueryAlias spark_catalog.default.t
   +- Relation spark_catalog.default.t[x#x,y#x] csv


-- !query
table t
|> where sum(x) = 1
-- !query analysis
org.apache.spark.sql.catalyst.ExtendedAnalysisException
{
  "errorClass" : "INVALID_WHERE_CONDITION",
  "sqlState" : "42903",
  "messageParameters" : {
    "condition" : "\"(sum(x) = 1)\"",
    "expressionList" : "sum(spark_catalog.default.t.x)"
  },
  "queryContext" : [ {
    "objectType" : "",
    "objectName" : "",
    "startIndex" : 1,
    "stopIndex" : 27,
    "fragment" : "table t\n|> where sum(x) = 1"
  } ]
}


-- !query
table t
|> where y = 'abc' or length(y) + sum(x) = 1
-- !query analysis
org.apache.spark.sql.catalyst.ExtendedAnalysisException
{
  "errorClass" : "INVALID_WHERE_CONDITION",
  "sqlState" : "42903",
  "messageParameters" : {
    "condition" : "\"((y = abc) OR ((length(y) + sum(x)) = 1))\"",
    "expressionList" : "sum(spark_catalog.default.t.x)"
  },
  "queryContext" : [ {
    "objectType" : "",
    "objectName" : "",
    "startIndex" : 1,
    "stopIndex" : 52,
    "fragment" : "table t\n|> where y = 'abc' or length(y) + sum(x) = 1"
  } ]
}


-- !query
table t
|> where sum(x) over (partition by y) = 1
-- !query analysis
org.apache.spark.sql.AnalysisException
{
  "errorClass" : "_LEGACY_ERROR_TEMP_1034",
  "messageParameters" : {
    "clauseName" : "WHERE"
  }
}


-- !query
table t
|> where sum(x) over w = 1
   window w as (partition by y)
-- !query analysis
org.apache.spark.sql.catalyst.parser.ParseException
{
  "errorClass" : "NOT_ALLOWED_IN_PIPE_OPERATOR_WHERE.WINDOW_CLAUSE",
  "sqlState" : "42601",
  "queryContext" : [ {
    "objectType" : "",
    "objectName" : "",
    "startIndex" : 1,
    "stopIndex" : 66,
    "fragment" : "table t\n|> where sum(x) over w = 1\n   window w as (partition by y)"
  } ]
}


-- !query
select * from t where sum(x) over (partition by y) = 1
-- !query analysis
org.apache.spark.sql.AnalysisException
{
  "errorClass" : "_LEGACY_ERROR_TEMP_1034",
  "messageParameters" : {
    "clauseName" : "WHERE"
  }
}


-- !query
table t
|> select x, length(y) as z
|> where x + length(y) < 4
-- !query analysis
org.apache.spark.sql.catalyst.ExtendedAnalysisException
{
  "errorClass" : "UNRESOLVED_COLUMN.WITH_SUGGESTION",
  "sqlState" : "42703",
  "messageParameters" : {
    "objectName" : "`y`",
    "proposal" : "`x`, `z`"
  },
  "queryContext" : [ {
    "objectType" : "",
    "objectName" : "",
    "startIndex" : 57,
    "stopIndex" : 57,
    "fragment" : "y"
  } ]
}


-- !query
(select x, sum(length(y)) as sum_len from t group by x)
|> where sum(length(y)) = 3
-- !query analysis
org.apache.spark.sql.catalyst.ExtendedAnalysisException
{
  "errorClass" : "UNRESOLVED_COLUMN.WITH_SUGGESTION",
  "sqlState" : "42703",
  "messageParameters" : {
    "objectName" : "`y`",
    "proposal" : "`x`, `sum_len`"
  },
  "queryContext" : [ {
    "objectType" : "",
    "objectName" : "",
    "startIndex" : 77,
    "stopIndex" : 77,
    "fragment" : "y"
  } ]
}


-- !query
table courseSales
|> select `year`, course, earnings
|> pivot (
     sum(earnings)
     for course in ('dotNET', 'Java')
  )
-- !query analysis
Project [year#x, __pivot_sum(coursesales.earnings) AS `sum(coursesales.earnings)`#x[0] AS dotNET#xL, __pivot_sum(coursesales.earnings) AS `sum(coursesales.earnings)`#x[1] AS Java#xL]
+- Aggregate [year#x], [year#x, pivotfirst(course#x, sum(coursesales.earnings)#xL, dotNET, Java, 0, 0) AS __pivot_sum(coursesales.earnings) AS `sum(coursesales.earnings)`#x]
   +- Aggregate [year#x, course#x], [year#x, course#x, sum(earnings#x) AS sum(coursesales.earnings)#xL]
      +- Project [year#x, course#x, earnings#x]
         +- SubqueryAlias coursesales
            +- View (`courseSales`, [course#x, year#x, earnings#x])
               +- Project [cast(course#x as string) AS course#x, cast(year#x as int) AS year#x, cast(earnings#x as int) AS earnings#x]
                  +- Project [course#x, year#x, earnings#x]
                     +- SubqueryAlias courseSales
                        +- LocalRelation [course#x, year#x, earnings#x]


-- !query
table courseSales
|> select `year` as y, course as c, earnings as e
|> pivot (
     sum(e) as s, avg(e) as a
     for y in (2012 as firstYear, 2013 as secondYear)
   )
-- !query analysis
Project [c#x, __pivot_sum(e) AS s AS `sum(e) AS s`#x[0] AS firstYear_s#xL, __pivot_avg(e) AS a AS `avg(e) AS a`#x[0] AS firstYear_a#x, __pivot_sum(e) AS s AS `sum(e) AS s`#x[1] AS secondYear_s#xL, __pivot_avg(e) AS a AS `avg(e) AS a`#x[1] AS secondYear_a#x]
+- Aggregate [c#x], [c#x, pivotfirst(y#x, sum(e) AS s#xL, 2012, 2013, 0, 0) AS __pivot_sum(e) AS s AS `sum(e) AS s`#x, pivotfirst(y#x, avg(e) AS a#x, 2012, 2013, 0, 0) AS __pivot_avg(e) AS a AS `avg(e) AS a`#x]
   +- Aggregate [c#x, y#x], [c#x, y#x, sum(e#x) AS sum(e) AS s#xL, avg(e#x) AS avg(e) AS a#x]
      +- Project [pipeexpression(year#x, false, SELECT) AS y#x, pipeexpression(course#x, false, SELECT) AS c#x, pipeexpression(earnings#x, false, SELECT) AS e#x]
         +- SubqueryAlias coursesales
            +- View (`courseSales`, [course#x, year#x, earnings#x])
               +- Project [cast(course#x as string) AS course#x, cast(year#x as int) AS year#x, cast(earnings#x as int) AS earnings#x]
                  +- Project [course#x, year#x, earnings#x]
                     +- SubqueryAlias courseSales
                        +- LocalRelation [course#x, year#x, earnings#x]


-- !query
select course, `year`, y, a
from courseSales
join yearsWithComplexTypes on `year` = y
|> pivot (
     max(a)
     for (y, course) in ((2012, 'dotNET'), (2013, 'Java'))
   )
-- !query analysis
Aggregate [year#x], [year#x, max(if ((named_struct(y, y#x, course, course#x) <=> cast(named_struct(col1, 2012, col2, dotNET) as struct<y:int,course:string>))) a#x else cast(null as array<int>)) AS {2012, dotNET}#x, max(if ((named_struct(y, y#x, course, course#x) <=> cast(named_struct(col1, 2013, col2, Java) as struct<y:int,course:string>))) a#x else cast(null as array<int>)) AS {2013, Java}#x]
+- Project [course#x, year#x, y#x, a#x]
   +- Join Inner, (year#x = y#x)
      :- SubqueryAlias coursesales
      :  +- View (`courseSales`, [course#x, year#x, earnings#x])
      :     +- Project [cast(course#x as string) AS course#x, cast(year#x as int) AS year#x, cast(earnings#x as int) AS earnings#x]
      :        +- Project [course#x, year#x, earnings#x]
      :           +- SubqueryAlias courseSales
      :              +- LocalRelation [course#x, year#x, earnings#x]
      +- SubqueryAlias yearswithcomplextypes
         +- View (`yearsWithComplexTypes`, [y#x, a#x, m#x, s#x])
            +- Project [cast(y#x as int) AS y#x, cast(a#x as array<int>) AS a#x, cast(m#x as map<string,int>) AS m#x, cast(s#x as struct<col1:int,col2:string>) AS s#x]
               +- Project [y#x, a#x, m#x, s#x]
                  +- SubqueryAlias yearsWithComplexTypes
                     +- LocalRelation [y#x, a#x, m#x, s#x]


-- !query
select earnings, `year`, s
from courseSales
join yearsWithComplexTypes on `year` = y
|> pivot (
     sum(earnings)
     for s in ((1, 'a'), (2, 'b'))
   )
-- !query analysis
Project [year#x, __pivot_sum(coursesales.earnings) AS `sum(coursesales.earnings)`#x[0] AS {1, a}#xL, __pivot_sum(coursesales.earnings) AS `sum(coursesales.earnings)`#x[1] AS {2, b}#xL]
+- Aggregate [year#x], [year#x, pivotfirst(s#x, sum(coursesales.earnings)#xL, [1,a], [2,b], 0, 0) AS __pivot_sum(coursesales.earnings) AS `sum(coursesales.earnings)`#x]
   +- Aggregate [year#x, s#x], [year#x, s#x, sum(earnings#x) AS sum(coursesales.earnings)#xL]
      +- Project [earnings#x, year#x, s#x]
         +- Join Inner, (year#x = y#x)
            :- SubqueryAlias coursesales
            :  +- View (`courseSales`, [course#x, year#x, earnings#x])
            :     +- Project [cast(course#x as string) AS course#x, cast(year#x as int) AS year#x, cast(earnings#x as int) AS earnings#x]
            :        +- Project [course#x, year#x, earnings#x]
            :           +- SubqueryAlias courseSales
            :              +- LocalRelation [course#x, year#x, earnings#x]
            +- SubqueryAlias yearswithcomplextypes
               +- View (`yearsWithComplexTypes`, [y#x, a#x, m#x, s#x])
                  +- Project [cast(y#x as int) AS y#x, cast(a#x as array<int>) AS a#x, cast(m#x as map<string,int>) AS m#x, cast(s#x as struct<col1:int,col2:string>) AS s#x]
                     +- Project [y#x, a#x, m#x, s#x]
                        +- SubqueryAlias yearsWithComplexTypes
                           +- LocalRelation [y#x, a#x, m#x, s#x]


-- !query
table courseEarnings
|> unpivot (
     earningsYear for `year` in (`2012`, `2013`, `2014`)
   )
-- !query analysis
Filter isnotnull(coalesce(earningsYear#x))
+- Expand [[course#x, 2012, 2012#x], [course#x, 2013, 2013#x], [course#x, 2014, 2014#x]], [course#x, year#x, earningsYear#x]
   +- SubqueryAlias courseearnings
      +- View (`courseEarnings`, [course#x, 2012#x, 2013#x, 2014#x])
         +- Project [cast(course#x as string) AS course#x, cast(2012#x as int) AS 2012#x, cast(2013#x as int) AS 2013#x, cast(2014#x as int) AS 2014#x]
            +- Project [course#x, 2012#x, 2013#x, 2014#x]
               +- SubqueryAlias courseEarnings
                  +- LocalRelation [course#x, 2012#x, 2013#x, 2014#x]


-- !query
table courseEarnings
|> unpivot include nulls (
     earningsYear for `year` in (`2012`, `2013`, `2014`)
   )
-- !query analysis
Expand [[course#x, 2012, 2012#x], [course#x, 2013, 2013#x], [course#x, 2014, 2014#x]], [course#x, year#x, earningsYear#x]
+- SubqueryAlias courseearnings
   +- View (`courseEarnings`, [course#x, 2012#x, 2013#x, 2014#x])
      +- Project [cast(course#x as string) AS course#x, cast(2012#x as int) AS 2012#x, cast(2013#x as int) AS 2013#x, cast(2014#x as int) AS 2014#x]
         +- Project [course#x, 2012#x, 2013#x, 2014#x]
            +- SubqueryAlias courseEarnings
               +- LocalRelation [course#x, 2012#x, 2013#x, 2014#x]


-- !query
table courseEarningsAndSales
|> unpivot include nulls (
     (earnings, sales) for `year` in (
       (earnings2012, sales2012) as `2012`,
       (earnings2013, sales2013) as `2013`,
       (earnings2014, sales2014) as `2014`)
   )
-- !query analysis
Expand [[course#x, 2012, earnings2012#x, sales2012#x], [course#x, 2013, earnings2013#x, sales2013#x], [course#x, 2014, earnings2014#x, sales2014#x]], [course#x, year#x, earnings#x, sales#x]
+- SubqueryAlias courseearningsandsales
   +- View (`courseEarningsAndSales`, [course#x, earnings2012#x, sales2012#x, earnings2013#x, sales2013#x, earnings2014#x, sales2014#x])
      +- Project [cast(course#x as string) AS course#x, cast(earnings2012#x as int) AS earnings2012#x, cast(sales2012#x as int) AS sales2012#x, cast(earnings2013#x as int) AS earnings2013#x, cast(sales2013#x as int) AS sales2013#x, cast(earnings2014#x as int) AS earnings2014#x, cast(sales2014#x as int) AS sales2014#x]
         +- Project [course#x, earnings2012#x, sales2012#x, earnings2013#x, sales2013#x, earnings2014#x, sales2014#x]
            +- SubqueryAlias courseEarningsAndSales
               +- LocalRelation [course#x, earnings2012#x, sales2012#x, earnings2013#x, sales2013#x, earnings2014#x, sales2014#x]


-- !query
table courseSales
|> select course, earnings
|> pivot (
     sum(earnings)
     for `year` in (2012, 2013)
   )
-- !query analysis
org.apache.spark.sql.catalyst.ExtendedAnalysisException
{
  "errorClass" : "UNRESOLVED_COLUMN.WITH_SUGGESTION",
  "sqlState" : "42703",
  "messageParameters" : {
    "objectName" : "`year`",
    "proposal" : "`course`, `earnings`"
  },
  "queryContext" : [ {
    "objectType" : "",
    "objectName" : "",
    "startIndex" : 49,
    "stopIndex" : 111,
    "fragment" : "pivot (\n     sum(earnings)\n     for `year` in (2012, 2013)\n   )"
  } ]
}


-- !query
table courseSales
|> pivot (
     sum(earnings)
     for `year` in (course, 2013)
   )
-- !query analysis
org.apache.spark.sql.AnalysisException
{
  "errorClass" : "NON_LITERAL_PIVOT_VALUES",
  "sqlState" : "42K08",
  "messageParameters" : {
    "expression" : "\"course\""
  }
}


-- !query
table courseSales
|> select course, earnings
|> pivot (
     sum(earnings)
     for `year` in (2012, 2013)
   )
   unpivot (
     earningsYear for `year` in (`2012`, `2013`, `2014`)
   )
-- !query analysis
org.apache.spark.sql.catalyst.parser.ParseException
{
  "errorClass" : "NOT_ALLOWED_IN_FROM.UNPIVOT_WITH_PIVOT",
  "sqlState" : "42601",
  "queryContext" : [ {
    "objectType" : "",
    "objectName" : "",
    "startIndex" : 1,
    "stopIndex" : 186,
    "fragment" : "table courseSales\n|> select course, earnings\n|> pivot (\n     sum(earnings)\n     for `year` in (2012, 2013)\n   )\n   unpivot (\n     earningsYear for `year` in (`2012`, `2013`, `2014`)\n   )"
  } ]
}


-- !query
table courseSales
|> select course, earnings
|> unpivot (
     earningsYear for `year` in (`2012`, `2013`, `2014`)
   )
   pivot (
     sum(earnings)
     for `year` in (2012, 2013)
   )
-- !query analysis
org.apache.spark.sql.catalyst.parser.ParseException
{
  "errorClass" : "NOT_ALLOWED_IN_FROM.UNPIVOT_WITH_PIVOT",
  "sqlState" : "42601",
  "queryContext" : [ {
    "objectType" : "",
    "objectName" : "",
    "startIndex" : 1,
    "stopIndex" : 186,
    "fragment" : "table courseSales\n|> select course, earnings\n|> unpivot (\n     earningsYear for `year` in (`2012`, `2013`, `2014`)\n   )\n   pivot (\n     sum(earnings)\n     for `year` in (2012, 2013)\n   )"
  } ]
}


-- !query
table courseSales
|> select course, earnings
|> pivot (
     sum(earnings)
     for `year` in (2012, 2013)
   )
   pivot (
     sum(earnings)
     for `year` in (2012, 2013)
   )
-- !query analysis
org.apache.spark.sql.catalyst.parser.ParseException
{
  "errorClass" : "PARSE_SYNTAX_ERROR",
  "sqlState" : "42601",
  "messageParameters" : {
    "error" : "'pivot'",
    "hint" : ""
  }
}


-- !query
table courseSales
|> select course, earnings
|> unpivot (
     earningsYear for `year` in (`2012`, `2013`, `2014`)
   )
   unpivot (
     earningsYear for `year` in (`2012`, `2013`, `2014`)
   )
   pivot (
     sum(earnings)
     for `year` in (2012, 2013)
   )
-- !query analysis
org.apache.spark.sql.catalyst.parser.ParseException
{
  "errorClass" : "PARSE_SYNTAX_ERROR",
  "sqlState" : "42601",
  "messageParameters" : {
    "error" : "'unpivot'",
    "hint" : ""
  }
}


-- !query
table t
|> tablesample (100 percent) repeatable (0)
-- !query analysis
Sample 0.0, 1.0, false, 0
+- SubqueryAlias spark_catalog.default.t
   +- Relation spark_catalog.default.t[x#x,y#x] csv


-- !query
table t
|> tablesample (2 rows) repeatable (0)
-- !query analysis
GlobalLimit 2
+- LocalLimit 2
   +- SubqueryAlias spark_catalog.default.t
      +- Relation spark_catalog.default.t[x#x,y#x] csv


-- !query
table t
|> tablesample (bucket 1 out of 1) repeatable (0)
-- !query analysis
Sample 0.0, 1.0, false, 0
+- SubqueryAlias spark_catalog.default.t
   +- Relation spark_catalog.default.t[x#x,y#x] csv


-- !query
table t
|> tablesample (100 percent) repeatable (0)
|> tablesample (5 rows) repeatable (0)
|> tablesample (bucket 1 out of 1) repeatable (0)
-- !query analysis
Sample 0.0, 1.0, false, 0
+- GlobalLimit 5
   +- LocalLimit 5
      +- Sample 0.0, 1.0, false, 0
         +- SubqueryAlias spark_catalog.default.t
            +- Relation spark_catalog.default.t[x#x,y#x] csv


-- !query
table t
|> tablesample ()
-- !query analysis
org.apache.spark.sql.catalyst.parser.ParseException
{
  "errorClass" : "_LEGACY_ERROR_TEMP_0014",
  "queryContext" : [ {
    "objectType" : "",
    "objectName" : "",
    "startIndex" : 12,
    "stopIndex" : 25,
    "fragment" : "tablesample ()"
  } ]
}


-- !query
table t
|> tablesample (-100 percent) repeatable (0)
-- !query analysis
org.apache.spark.sql.catalyst.parser.ParseException
{
  "errorClass" : "_LEGACY_ERROR_TEMP_0064",
  "messageParameters" : {
    "msg" : "Sampling fraction (-1.0) must be on interval [0, 1]"
  },
  "queryContext" : [ {
    "objectType" : "",
    "objectName" : "",
    "startIndex" : 12,
    "stopIndex" : 52,
    "fragment" : "tablesample (-100 percent) repeatable (0)"
  } ]
}


-- !query
table t
|> tablesample (-5 rows)
-- !query analysis
org.apache.spark.sql.catalyst.ExtendedAnalysisException
{
  "errorClass" : "INVALID_LIMIT_LIKE_EXPRESSION.IS_NEGATIVE",
  "sqlState" : "42K0E",
  "messageParameters" : {
    "expr" : "\"-5\"",
    "name" : "limit",
    "v" : "-5"
  },
  "queryContext" : [ {
    "objectType" : "",
    "objectName" : "",
    "startIndex" : 25,
    "stopIndex" : 26,
    "fragment" : "-5"
  } ]
}


-- !query
table t
|> tablesample (x rows)
-- !query analysis
org.apache.spark.sql.catalyst.ExtendedAnalysisException
{
  "errorClass" : "INVALID_LIMIT_LIKE_EXPRESSION.IS_UNFOLDABLE",
  "sqlState" : "42K0E",
  "messageParameters" : {
    "expr" : "\"x\"",
    "name" : "limit"
  },
  "queryContext" : [ {
    "objectType" : "",
    "objectName" : "",
    "startIndex" : 25,
    "stopIndex" : 25,
    "fragment" : "x"
  } ]
}


-- !query
table t
|> tablesample (bucket 2 out of 1)
-- !query analysis
org.apache.spark.sql.catalyst.parser.ParseException
{
  "errorClass" : "_LEGACY_ERROR_TEMP_0064",
  "messageParameters" : {
    "msg" : "Sampling fraction (2.0) must be on interval [0, 1]"
  },
  "queryContext" : [ {
    "objectType" : "",
    "objectName" : "",
    "startIndex" : 12,
    "stopIndex" : 42,
    "fragment" : "tablesample (bucket 2 out of 1)"
  } ]
}


-- !query
table t
|> tablesample (200b) repeatable (0)
-- !query analysis
org.apache.spark.sql.catalyst.parser.ParseException
{
  "errorClass" : "_LEGACY_ERROR_TEMP_0015",
  "messageParameters" : {
    "msg" : "byteLengthLiteral"
  },
  "queryContext" : [ {
    "objectType" : "",
    "objectName" : "",
    "startIndex" : 12,
    "stopIndex" : 44,
    "fragment" : "tablesample (200b) repeatable (0)"
  } ]
}


-- !query
table t
|> tablesample (200) repeatable (0)
-- !query analysis
org.apache.spark.sql.catalyst.parser.ParseException
{
  "errorClass" : "_LEGACY_ERROR_TEMP_0016",
  "messageParameters" : {
    "bytesStr" : "200"
  },
  "queryContext" : [ {
    "objectType" : "",
    "objectName" : "",
    "startIndex" : 12,
    "stopIndex" : 43,
    "fragment" : "tablesample (200) repeatable (0)"
  } ]
}


-- !query
table join_test_t1
|> inner join join_test_empty_table
-- !query analysis
Join Inner
:- SubqueryAlias join_test_t1
:  +- View (`join_test_t1`, [a#x])
:     +- Project [cast(a#x as int) AS a#x]
:        +- Project [a#x]
:           +- SubqueryAlias grouping
:              +- LocalRelation [a#x]
+- SubqueryAlias join_test_empty_table
   +- View (`join_test_empty_table`, [a#x])
      +- Project [cast(a#x as int) AS a#x]
         +- Project [a#x]
            +- Filter false
               +- SubqueryAlias join_test_t2
                  +- View (`join_test_t2`, [a#x])
                     +- Project [cast(a#x as int) AS a#x]
                        +- Project [a#x]
                           +- SubqueryAlias grouping
                              +- LocalRelation [a#x]


-- !query
table join_test_t1
|> cross join join_test_empty_table
-- !query analysis
Join Cross
:- SubqueryAlias join_test_t1
:  +- View (`join_test_t1`, [a#x])
:     +- Project [cast(a#x as int) AS a#x]
:        +- Project [a#x]
:           +- SubqueryAlias grouping
:              +- LocalRelation [a#x]
+- SubqueryAlias join_test_empty_table
   +- View (`join_test_empty_table`, [a#x])
      +- Project [cast(a#x as int) AS a#x]
         +- Project [a#x]
            +- Filter false
               +- SubqueryAlias join_test_t2
                  +- View (`join_test_t2`, [a#x])
                     +- Project [cast(a#x as int) AS a#x]
                        +- Project [a#x]
                           +- SubqueryAlias grouping
                              +- LocalRelation [a#x]


-- !query
table join_test_t1
|> left outer join join_test_empty_table
-- !query analysis
Join LeftOuter
:- SubqueryAlias join_test_t1
:  +- View (`join_test_t1`, [a#x])
:     +- Project [cast(a#x as int) AS a#x]
:        +- Project [a#x]
:           +- SubqueryAlias grouping
:              +- LocalRelation [a#x]
+- SubqueryAlias join_test_empty_table
   +- View (`join_test_empty_table`, [a#x])
      +- Project [cast(a#x as int) AS a#x]
         +- Project [a#x]
            +- Filter false
               +- SubqueryAlias join_test_t2
                  +- View (`join_test_t2`, [a#x])
                     +- Project [cast(a#x as int) AS a#x]
                        +- Project [a#x]
                           +- SubqueryAlias grouping
                              +- LocalRelation [a#x]


-- !query
table join_test_t1
|> right outer join join_test_empty_table
-- !query analysis
Join RightOuter
:- SubqueryAlias join_test_t1
:  +- View (`join_test_t1`, [a#x])
:     +- Project [cast(a#x as int) AS a#x]
:        +- Project [a#x]
:           +- SubqueryAlias grouping
:              +- LocalRelation [a#x]
+- SubqueryAlias join_test_empty_table
   +- View (`join_test_empty_table`, [a#x])
      +- Project [cast(a#x as int) AS a#x]
         +- Project [a#x]
            +- Filter false
               +- SubqueryAlias join_test_t2
                  +- View (`join_test_t2`, [a#x])
                     +- Project [cast(a#x as int) AS a#x]
                        +- Project [a#x]
                           +- SubqueryAlias grouping
                              +- LocalRelation [a#x]


-- !query
table join_test_t1
|> full outer join join_test_empty_table using (a)
-- !query analysis
Project [coalesce(a#x, a#x) AS a#x]
+- Join FullOuter, (a#x = a#x)
   :- SubqueryAlias join_test_t1
   :  +- View (`join_test_t1`, [a#x])
   :     +- Project [cast(a#x as int) AS a#x]
   :        +- Project [a#x]
   :           +- SubqueryAlias grouping
   :              +- LocalRelation [a#x]
   +- SubqueryAlias join_test_empty_table
      +- View (`join_test_empty_table`, [a#x])
         +- Project [cast(a#x as int) AS a#x]
            +- Project [a#x]
               +- Filter false
                  +- SubqueryAlias join_test_t2
                     +- View (`join_test_t2`, [a#x])
                        +- Project [cast(a#x as int) AS a#x]
                           +- Project [a#x]
                              +- SubqueryAlias grouping
                                 +- LocalRelation [a#x]


-- !query
table join_test_t1
|> full outer join join_test_empty_table on (join_test_t1.a = join_test_empty_table.a)
-- !query analysis
Join FullOuter, (a#x = a#x)
:- SubqueryAlias join_test_t1
:  +- View (`join_test_t1`, [a#x])
:     +- Project [cast(a#x as int) AS a#x]
:        +- Project [a#x]
:           +- SubqueryAlias grouping
:              +- LocalRelation [a#x]
+- SubqueryAlias join_test_empty_table
   +- View (`join_test_empty_table`, [a#x])
      +- Project [cast(a#x as int) AS a#x]
         +- Project [a#x]
            +- Filter false
               +- SubqueryAlias join_test_t2
                  +- View (`join_test_t2`, [a#x])
                     +- Project [cast(a#x as int) AS a#x]
                        +- Project [a#x]
                           +- SubqueryAlias grouping
                              +- LocalRelation [a#x]


-- !query
table join_test_t1
|> left semi join join_test_empty_table
-- !query analysis
Join LeftSemi
:- SubqueryAlias join_test_t1
:  +- View (`join_test_t1`, [a#x])
:     +- Project [cast(a#x as int) AS a#x]
:        +- Project [a#x]
:           +- SubqueryAlias grouping
:              +- LocalRelation [a#x]
+- SubqueryAlias join_test_empty_table
   +- View (`join_test_empty_table`, [a#x])
      +- Project [cast(a#x as int) AS a#x]
         +- Project [a#x]
            +- Filter false
               +- SubqueryAlias join_test_t2
                  +- View (`join_test_t2`, [a#x])
                     +- Project [cast(a#x as int) AS a#x]
                        +- Project [a#x]
                           +- SubqueryAlias grouping
                              +- LocalRelation [a#x]


-- !query
table join_test_t1
|> left anti join join_test_empty_table
-- !query analysis
Join LeftAnti
:- SubqueryAlias join_test_t1
:  +- View (`join_test_t1`, [a#x])
:     +- Project [cast(a#x as int) AS a#x]
:        +- Project [a#x]
:           +- SubqueryAlias grouping
:              +- LocalRelation [a#x]
+- SubqueryAlias join_test_empty_table
   +- View (`join_test_empty_table`, [a#x])
      +- Project [cast(a#x as int) AS a#x]
         +- Project [a#x]
            +- Filter false
               +- SubqueryAlias join_test_t2
                  +- View (`join_test_t2`, [a#x])
                     +- Project [cast(a#x as int) AS a#x]
                        +- Project [a#x]
                           +- SubqueryAlias grouping
                              +- LocalRelation [a#x]


-- !query
select * from join_test_t1 where true
|> inner join join_test_empty_table
-- !query analysis
Join Inner
:- Project [a#x]
:  +- Filter true
:     +- SubqueryAlias join_test_t1
:        +- View (`join_test_t1`, [a#x])
:           +- Project [cast(a#x as int) AS a#x]
:              +- Project [a#x]
:                 +- SubqueryAlias grouping
:                    +- LocalRelation [a#x]
+- SubqueryAlias join_test_empty_table
   +- View (`join_test_empty_table`, [a#x])
      +- Project [cast(a#x as int) AS a#x]
         +- Project [a#x]
            +- Filter false
               +- SubqueryAlias join_test_t2
                  +- View (`join_test_t2`, [a#x])
                     +- Project [cast(a#x as int) AS a#x]
                        +- Project [a#x]
                           +- SubqueryAlias grouping
                              +- LocalRelation [a#x]


-- !query
select 1 as x, 2 as y
|> inner join (select 1 as x, 4 as y) using (x)
-- !query analysis
Project [x#x, y#x, y#x]
+- Join Inner, (x#x = x#x)
   :- Project [1 AS x#x, 2 AS y#x]
   :  +- OneRowRelation
   +- SubqueryAlias __auto_generated_subquery_name
      +- Project [1 AS x#x, 4 AS y#x]
         +- OneRowRelation


-- !query
table join_test_t1
|> inner join (join_test_t2 jt2 inner join join_test_t3 jt3 using (a)) using (a)
|> select a, join_test_t1.a, jt2.a, jt3.a
-- !query analysis
Project [a#x, a#x, a#x, a#x]
+- Project [a#x, a#x, a#x]
   +- Join Inner, (a#x = a#x)
      :- SubqueryAlias join_test_t1
      :  +- View (`join_test_t1`, [a#x])
      :     +- Project [cast(a#x as int) AS a#x]
      :        +- Project [a#x]
      :           +- SubqueryAlias grouping
      :              +- LocalRelation [a#x]
      +- Project [a#x, a#x]
         +- Join Inner, (a#x = a#x)
            :- SubqueryAlias jt2
            :  +- SubqueryAlias join_test_t2
            :     +- View (`join_test_t2`, [a#x])
            :        +- Project [cast(a#x as int) AS a#x]
            :           +- Project [a#x]
            :              +- SubqueryAlias grouping
            :                 +- LocalRelation [a#x]
            +- SubqueryAlias jt3
               +- SubqueryAlias join_test_t3
                  +- View (`join_test_t3`, [a#x])
                     +- Project [cast(a#x as int) AS a#x]
                        +- Project [a#x]
                           +- SubqueryAlias grouping
                              +- LocalRelation [a#x]


-- !query
table join_test_t1
|> inner join join_test_t2 tablesample (100 percent) repeatable (0) jt2 using (a)
-- !query analysis
Project [a#x]
+- Join Inner, (a#x = a#x)
   :- SubqueryAlias join_test_t1
   :  +- View (`join_test_t1`, [a#x])
   :     +- Project [cast(a#x as int) AS a#x]
   :        +- Project [a#x]
   :           +- SubqueryAlias grouping
   :              +- LocalRelation [a#x]
   +- Sample 0.0, 1.0, false, 0
      +- SubqueryAlias jt2
         +- SubqueryAlias join_test_t2
            +- View (`join_test_t2`, [a#x])
               +- Project [cast(a#x as int) AS a#x]
                  +- Project [a#x]
                     +- SubqueryAlias grouping
                        +- LocalRelation [a#x]


-- !query
table join_test_t1
|> inner join (select 1 as a) tablesample (100 percent) repeatable (0) jt2 using (a)
-- !query analysis
Project [a#x]
+- Join Inner, (a#x = a#x)
   :- SubqueryAlias join_test_t1
   :  +- View (`join_test_t1`, [a#x])
   :     +- Project [cast(a#x as int) AS a#x]
   :        +- Project [a#x]
   :           +- SubqueryAlias grouping
   :              +- LocalRelation [a#x]
   +- SubqueryAlias jt2
      +- Sample 0.0, 1.0, false, 0
         +- Project [1 AS a#x]
            +- OneRowRelation


-- !query
table join_test_t1
|> join join_test_t1 using (a)
-- !query analysis
Project [a#x]
+- Join Inner, (a#x = a#x)
   :- SubqueryAlias join_test_t1
   :  +- View (`join_test_t1`, [a#x])
   :     +- Project [cast(a#x as int) AS a#x]
   :        +- Project [a#x]
   :           +- SubqueryAlias grouping
   :              +- LocalRelation [a#x]
   +- SubqueryAlias join_test_t1
      +- View (`join_test_t1`, [a#x])
         +- Project [cast(a#x as int) AS a#x]
            +- Project [a#x]
               +- SubqueryAlias grouping
                  +- LocalRelation [a#x]


-- !query
table lateral_test_t1
|> join lateral (select c1)
-- !query analysis
LateralJoin lateral-subquery#x [c1#x], Inner
:  +- SubqueryAlias __auto_generated_subquery_name
:     +- Project [outer(c1#x) AS c1#x]
:        +- OneRowRelation
+- SubqueryAlias lateral_test_t1
   +- View (`lateral_test_t1`, [c1#x, c2#x])
      +- Project [cast(col1#x as int) AS c1#x, cast(col2#x as int) AS c2#x]
         +- LocalRelation [col1#x, col2#x]


-- !query
table lateral_test_t1
|> join lateral (select c1 from lateral_test_t2)
-- !query analysis
LateralJoin lateral-subquery#x [], Inner
:  +- SubqueryAlias __auto_generated_subquery_name
:     +- Project [c1#x]
:        +- SubqueryAlias lateral_test_t2
:           +- View (`lateral_test_t2`, [c1#x, c2#x])
:              +- Project [cast(col1#x as int) AS c1#x, cast(col2#x as int) AS c2#x]
:                 +- LocalRelation [col1#x, col2#x]
+- SubqueryAlias lateral_test_t1
   +- View (`lateral_test_t1`, [c1#x, c2#x])
      +- Project [cast(col1#x as int) AS c1#x, cast(col2#x as int) AS c2#x]
         +- LocalRelation [col1#x, col2#x]


-- !query
table lateral_test_t1
|> join lateral (select lateral_test_t1.c1 from lateral_test_t2)
-- !query analysis
LateralJoin lateral-subquery#x [c1#x], Inner
:  +- SubqueryAlias __auto_generated_subquery_name
:     +- Project [outer(c1#x) AS c1#x]
:        +- SubqueryAlias lateral_test_t2
:           +- View (`lateral_test_t2`, [c1#x, c2#x])
:              +- Project [cast(col1#x as int) AS c1#x, cast(col2#x as int) AS c2#x]
:                 +- LocalRelation [col1#x, col2#x]
+- SubqueryAlias lateral_test_t1
   +- View (`lateral_test_t1`, [c1#x, c2#x])
      +- Project [cast(col1#x as int) AS c1#x, cast(col2#x as int) AS c2#x]
         +- LocalRelation [col1#x, col2#x]


-- !query
table lateral_test_t1
|> join lateral (select lateral_test_t1.c1 + t2.c1 from lateral_test_t2 t2)
-- !query analysis
LateralJoin lateral-subquery#x [c1#x], Inner
:  +- SubqueryAlias __auto_generated_subquery_name
:     +- Project [(outer(c1#x) + c1#x) AS (outer(lateral_test_t1.c1) + c1)#x]
:        +- SubqueryAlias t2
:           +- SubqueryAlias lateral_test_t2
:              +- View (`lateral_test_t2`, [c1#x, c2#x])
:                 +- Project [cast(col1#x as int) AS c1#x, cast(col2#x as int) AS c2#x]
:                    +- LocalRelation [col1#x, col2#x]
+- SubqueryAlias lateral_test_t1
   +- View (`lateral_test_t1`, [c1#x, c2#x])
      +- Project [cast(col1#x as int) AS c1#x, cast(col2#x as int) AS c2#x]
         +- LocalRelation [col1#x, col2#x]


-- !query
table lateral_test_t1
|> join lateral (select *)
-- !query analysis
LateralJoin lateral-subquery#x [], Inner
:  +- SubqueryAlias __auto_generated_subquery_name
:     +- Project
:        +- OneRowRelation
+- SubqueryAlias lateral_test_t1
   +- View (`lateral_test_t1`, [c1#x, c2#x])
      +- Project [cast(col1#x as int) AS c1#x, cast(col2#x as int) AS c2#x]
         +- LocalRelation [col1#x, col2#x]


-- !query
table lateral_test_t1
|> join lateral (select * from lateral_test_t2)
-- !query analysis
LateralJoin lateral-subquery#x [], Inner
:  +- SubqueryAlias __auto_generated_subquery_name
:     +- Project [c1#x, c2#x]
:        +- SubqueryAlias lateral_test_t2
:           +- View (`lateral_test_t2`, [c1#x, c2#x])
:              +- Project [cast(col1#x as int) AS c1#x, cast(col2#x as int) AS c2#x]
:                 +- LocalRelation [col1#x, col2#x]
+- SubqueryAlias lateral_test_t1
   +- View (`lateral_test_t1`, [c1#x, c2#x])
      +- Project [cast(col1#x as int) AS c1#x, cast(col2#x as int) AS c2#x]
         +- LocalRelation [col1#x, col2#x]


-- !query
table lateral_test_t1
|> join lateral (select lateral_test_t1.* from lateral_test_t2)
-- !query analysis
LateralJoin lateral-subquery#x [c1#x && c2#x], Inner
:  +- SubqueryAlias __auto_generated_subquery_name
:     +- Project [outer(c1#x) AS c1#x, outer(c2#x) AS c2#x]
:        +- SubqueryAlias lateral_test_t2
:           +- View (`lateral_test_t2`, [c1#x, c2#x])
:              +- Project [cast(col1#x as int) AS c1#x, cast(col2#x as int) AS c2#x]
:                 +- LocalRelation [col1#x, col2#x]
+- SubqueryAlias lateral_test_t1
   +- View (`lateral_test_t1`, [c1#x, c2#x])
      +- Project [cast(col1#x as int) AS c1#x, cast(col2#x as int) AS c2#x]
         +- LocalRelation [col1#x, col2#x]


-- !query
table lateral_test_t1
|> join lateral (select lateral_test_t1.*, t2.* from lateral_test_t2 t2)
-- !query analysis
LateralJoin lateral-subquery#x [c1#x && c2#x], Inner
:  +- SubqueryAlias __auto_generated_subquery_name
:     +- Project [outer(c1#x) AS c1#x, outer(c2#x) AS c2#x, c1#x, c2#x]
:        +- SubqueryAlias t2
:           +- SubqueryAlias lateral_test_t2
:              +- View (`lateral_test_t2`, [c1#x, c2#x])
:                 +- Project [cast(col1#x as int) AS c1#x, cast(col2#x as int) AS c2#x]
:                    +- LocalRelation [col1#x, col2#x]
+- SubqueryAlias lateral_test_t1
   +- View (`lateral_test_t1`, [c1#x, c2#x])
      +- Project [cast(col1#x as int) AS c1#x, cast(col2#x as int) AS c2#x]
         +- LocalRelation [col1#x, col2#x]


-- !query
table lateral_test_t1
|> join lateral_test_t2
|> join lateral (select lateral_test_t1.c2 + lateral_test_t2.c2)
-- !query analysis
LateralJoin lateral-subquery#x [c2#x && c2#x], Inner
:  +- SubqueryAlias __auto_generated_subquery_name
:     +- Project [(outer(c2#x) + outer(c2#x)) AS (outer(lateral_test_t1.c2) + outer(lateral_test_t2.c2))#x]
:        +- OneRowRelation
+- Join Inner
   :- SubqueryAlias lateral_test_t1
   :  +- View (`lateral_test_t1`, [c1#x, c2#x])
   :     +- Project [cast(col1#x as int) AS c1#x, cast(col2#x as int) AS c2#x]
   :        +- LocalRelation [col1#x, col2#x]
   +- SubqueryAlias lateral_test_t2
      +- View (`lateral_test_t2`, [c1#x, c2#x])
         +- Project [cast(col1#x as int) AS c1#x, cast(col2#x as int) AS c2#x]
            +- LocalRelation [col1#x, col2#x]


-- !query
table natural_join_test_t1
|> natural join natural_join_test_t2
|> where k = "one"
-- !query analysis
Filter (k#x = one)
+- Project [k#x, v1#x, v2#x]
   +- Join Inner, (k#x = k#x)
      :- SubqueryAlias natural_join_test_t1
      :  +- View (`natural_join_test_t1`, [k#x, v1#x])
      :     +- Project [cast(k#x as string) AS k#x, cast(v1#x as int) AS v1#x]
      :        +- Project [k#x, v1#x]
      :           +- SubqueryAlias natural_join_test_t1
      :              +- LocalRelation [k#x, v1#x]
      +- SubqueryAlias natural_join_test_t2
         +- View (`natural_join_test_t2`, [k#x, v2#x])
            +- Project [cast(k#x as string) AS k#x, cast(v2#x as int) AS v2#x]
               +- Project [k#x, v2#x]
                  +- SubqueryAlias natural_join_test_t2
                     +- LocalRelation [k#x, v2#x]


-- !query
table natural_join_test_t1
|> natural join natural_join_test_t2 nt2
|> select natural_join_test_t1.*
-- !query analysis
Project [k#x, v1#x]
+- Project [k#x, v1#x, v2#x]
   +- Join Inner, (k#x = k#x)
      :- SubqueryAlias natural_join_test_t1
      :  +- View (`natural_join_test_t1`, [k#x, v1#x])
      :     +- Project [cast(k#x as string) AS k#x, cast(v1#x as int) AS v1#x]
      :        +- Project [k#x, v1#x]
      :           +- SubqueryAlias natural_join_test_t1
      :              +- LocalRelation [k#x, v1#x]
      +- SubqueryAlias nt2
         +- SubqueryAlias natural_join_test_t2
            +- View (`natural_join_test_t2`, [k#x, v2#x])
               +- Project [cast(k#x as string) AS k#x, cast(v2#x as int) AS v2#x]
                  +- Project [k#x, v2#x]
                     +- SubqueryAlias natural_join_test_t2
                        +- LocalRelation [k#x, v2#x]


-- !query
table natural_join_test_t1
|> natural join natural_join_test_t2 nt2
|> natural join natural_join_test_t3 nt3
|> select natural_join_test_t1.*, nt2.*, nt3.*
-- !query analysis
Project [k#x, v1#x, k#x, v2#x, k#x, v3#x]
+- Project [k#x, v1#x, v2#x, v3#x, k#x, k#x]
   +- Join Inner, (k#x = k#x)
      :- Project [k#x, v1#x, v2#x, k#x]
      :  +- Join Inner, (k#x = k#x)
      :     :- SubqueryAlias natural_join_test_t1
      :     :  +- View (`natural_join_test_t1`, [k#x, v1#x])
      :     :     +- Project [cast(k#x as string) AS k#x, cast(v1#x as int) AS v1#x]
      :     :        +- Project [k#x, v1#x]
      :     :           +- SubqueryAlias natural_join_test_t1
      :     :              +- LocalRelation [k#x, v1#x]
      :     +- SubqueryAlias nt2
      :        +- SubqueryAlias natural_join_test_t2
      :           +- View (`natural_join_test_t2`, [k#x, v2#x])
      :              +- Project [cast(k#x as string) AS k#x, cast(v2#x as int) AS v2#x]
      :                 +- Project [k#x, v2#x]
      :                    +- SubqueryAlias natural_join_test_t2
      :                       +- LocalRelation [k#x, v2#x]
      +- SubqueryAlias nt3
         +- SubqueryAlias natural_join_test_t3
            +- View (`natural_join_test_t3`, [k#x, v3#x])
               +- Project [cast(k#x as string) AS k#x, cast(v3#x as int) AS v3#x]
                  +- Project [k#x, v3#x]
                     +- SubqueryAlias natural_join_test_t3
                        +- LocalRelation [k#x, v3#x]


-- !query
table join_test_t1
|> inner join join_test_empty_table
   inner join join_test_empty_table
-- !query analysis
org.apache.spark.sql.catalyst.parser.ParseException
{
  "errorClass" : "PARSE_SYNTAX_ERROR",
  "sqlState" : "42601",
  "messageParameters" : {
    "error" : "'inner'",
    "hint" : ""
  }
}


-- !query
table join_test_t1
|> select 1 + 2 as result
|> full outer join join_test_empty_table on (join_test_t1.a = join_test_empty_table.a)
-- !query analysis
org.apache.spark.sql.catalyst.ExtendedAnalysisException
{
  "errorClass" : "UNRESOLVED_COLUMN.WITH_SUGGESTION",
  "sqlState" : "42703",
  "messageParameters" : {
    "objectName" : "`join_test_t1`.`a`",
    "proposal" : "`result`, `join_test_empty_table`.`a`"
  },
  "queryContext" : [ {
    "objectType" : "",
    "objectName" : "",
    "startIndex" : 91,
    "stopIndex" : 104,
    "fragment" : "join_test_t1.a"
  } ]
}


-- !query
table join_test_t1 jt
|> cross join (select * from jt)
-- !query analysis
org.apache.spark.sql.catalyst.parser.ParseException
{
  "errorClass" : "PARSE_SYNTAX_ERROR",
  "sqlState" : "42601",
  "messageParameters" : {
    "error" : "'jt'",
    "hint" : ""
  }
}


-- !query
table t
|> union all table t
-- !query analysis
Union false, false
:- SubqueryAlias spark_catalog.default.t
:  +- Relation spark_catalog.default.t[x#x,y#x] csv
+- SubqueryAlias spark_catalog.default.t
   +- Relation spark_catalog.default.t[x#x,y#x] csv


-- !query
table t
|> union table t
-- !query analysis
Distinct
+- Union false, false
   :- SubqueryAlias spark_catalog.default.t
   :  +- Relation spark_catalog.default.t[x#x,y#x] csv
   +- SubqueryAlias spark_catalog.default.t
      +- Relation spark_catalog.default.t[x#x,y#x] csv


-- !query
(select * from t)
|> union all table t
-- !query analysis
Union false, false
:- Project [x#x, y#x]
:  +- SubqueryAlias spark_catalog.default.t
:     +- Relation spark_catalog.default.t[x#x,y#x] csv
+- SubqueryAlias spark_catalog.default.t
   +- Relation spark_catalog.default.t[x#x,y#x] csv


-- !query
(select * from t)
|> union table t
-- !query analysis
Distinct
+- Union false, false
   :- Project [x#x, y#x]
   :  +- SubqueryAlias spark_catalog.default.t
   :     +- Relation spark_catalog.default.t[x#x,y#x] csv
   +- SubqueryAlias spark_catalog.default.t
      +- Relation spark_catalog.default.t[x#x,y#x] csv


-- !query
values (0, 'abc') tab(x, y)
|> union all table t
-- !query analysis
Union false, false
:- SubqueryAlias tab
:  +- LocalRelation [x#x, y#x]
+- SubqueryAlias spark_catalog.default.t
   +- Relation spark_catalog.default.t[x#x,y#x] csv


-- !query
values (0, 1) tab(x, y)
|> union table t
|> where x = 0
-- !query analysis
Distinct
+- Union false, false
   :- Project [x#x, cast(y#x as bigint) AS y#xL]
   :  +- SubqueryAlias tab
   :     +- LocalRelation [x#x, y#x]
   +- Project [x#x, cast(y#x as bigint) AS y#xL]
      +- Filter (x#x = 0)
         +- SubqueryAlias spark_catalog.default.t
            +- Relation spark_catalog.default.t[x#x,y#x] csv


-- !query
(select * from t)
|> union all (select * from t)
-- !query analysis
Union false, false
:- Project [x#x, y#x]
:  +- SubqueryAlias spark_catalog.default.t
:     +- Relation spark_catalog.default.t[x#x,y#x] csv
+- Project [x#x, y#x]
   +- SubqueryAlias spark_catalog.default.t
      +- Relation spark_catalog.default.t[x#x,y#x] csv


-- !query
table t
|> except all table t
-- !query analysis
Except All true
:- SubqueryAlias spark_catalog.default.t
:  +- Relation spark_catalog.default.t[x#x,y#x] csv
+- SubqueryAlias spark_catalog.default.t
   +- Relation spark_catalog.default.t[x#x,y#x] csv


-- !query
table t
|> except table t
-- !query analysis
Except false
:- SubqueryAlias spark_catalog.default.t
:  +- Relation spark_catalog.default.t[x#x,y#x] csv
+- SubqueryAlias spark_catalog.default.t
   +- Relation spark_catalog.default.t[x#x,y#x] csv


-- !query
table t
|> intersect all table t
-- !query analysis
Intersect All true
:- SubqueryAlias spark_catalog.default.t
:  +- Relation spark_catalog.default.t[x#x,y#x] csv
+- SubqueryAlias spark_catalog.default.t
   +- Relation spark_catalog.default.t[x#x,y#x] csv


-- !query
table t
|> intersect table t
-- !query analysis
Intersect false
:- SubqueryAlias spark_catalog.default.t
:  +- Relation spark_catalog.default.t[x#x,y#x] csv
+- SubqueryAlias spark_catalog.default.t
   +- Relation spark_catalog.default.t[x#x,y#x] csv


-- !query
table t
|> minus all table t
-- !query analysis
Except All true
:- SubqueryAlias spark_catalog.default.t
:  +- Relation spark_catalog.default.t[x#x,y#x] csv
+- SubqueryAlias spark_catalog.default.t
   +- Relation spark_catalog.default.t[x#x,y#x] csv


-- !query
table t
|> minus table t
-- !query analysis
Except false
:- SubqueryAlias spark_catalog.default.t
:  +- Relation spark_catalog.default.t[x#x,y#x] csv
+- SubqueryAlias spark_catalog.default.t
   +- Relation spark_catalog.default.t[x#x,y#x] csv


-- !query
table t
|> select x
|> union all table t
-- !query analysis
org.apache.spark.sql.catalyst.ExtendedAnalysisException
{
  "errorClass" : "NUM_COLUMNS_MISMATCH",
  "sqlState" : "42826",
  "messageParameters" : {
    "firstNumColumns" : "1",
    "invalidNumColumns" : "2",
    "invalidOrdinalNum" : "second",
    "operator" : "UNION"
  },
  "queryContext" : [ {
    "objectType" : "",
    "objectName" : "",
    "startIndex" : 1,
    "stopIndex" : 40,
    "fragment" : "table t\n|> select x\n|> union all table t"
  } ]
}


-- !query
table t
|> union all table st
-- !query analysis
org.apache.spark.sql.catalyst.ExtendedAnalysisException
{
  "errorClass" : "INCOMPATIBLE_COLUMN_TYPE",
  "sqlState" : "42825",
  "messageParameters" : {
    "columnOrdinalNumber" : "second",
    "dataType1" : "\"STRUCT<i1: INT, i2: INT>\"",
    "dataType2" : "\"STRING\"",
    "hint" : "",
    "operator" : "UNION",
    "tableOrdinalNumber" : "second"
  },
  "queryContext" : [ {
    "objectType" : "",
    "objectName" : "",
    "startIndex" : 1,
    "stopIndex" : 29,
    "fragment" : "table t\n|> union all table st"
  } ]
}


-- !query
table t
|> order by x
-- !query analysis
Sort [x#x ASC NULLS FIRST], true
+- SubqueryAlias spark_catalog.default.t
   +- Relation spark_catalog.default.t[x#x,y#x] csv


-- !query
(select * from t)
|> order by x
-- !query analysis
Sort [x#x ASC NULLS FIRST], true
+- SubqueryAlias __auto_generated_subquery_name
   +- Project [x#x, y#x]
      +- SubqueryAlias spark_catalog.default.t
         +- Relation spark_catalog.default.t[x#x,y#x] csv


-- !query
values (0, 'abc') tab(x, y)
|> order by x
-- !query analysis
Sort [x#x ASC NULLS FIRST], true
+- SubqueryAlias tab
   +- LocalRelation [x#x, y#x]


-- !query
table t
|> order by x
|> limit 1
-- !query analysis
GlobalLimit 1
+- LocalLimit 1
   +- SubqueryAlias __auto_generated_subquery_name
      +- Sort [x#x ASC NULLS FIRST], true
         +- SubqueryAlias spark_catalog.default.t
            +- Relation spark_catalog.default.t[x#x,y#x] csv


-- !query
table t
|> where x = 1
|> select y
|> limit 2 offset 1
-- !query analysis
GlobalLimit 2
+- LocalLimit 2
   +- Offset 1
      +- SubqueryAlias __auto_generated_subquery_name
         +- Project [y#x]
            +- Filter (x#x = 1)
               +- SubqueryAlias spark_catalog.default.t
                  +- Relation spark_catalog.default.t[x#x,y#x] csv


-- !query
table t
|> where x = 1
|> select y
|> offset 1
-- !query analysis
Offset 1
+- SubqueryAlias __auto_generated_subquery_name
   +- Project [y#x]
      +- Filter (x#x = 1)
         +- SubqueryAlias spark_catalog.default.t
            +- Relation spark_catalog.default.t[x#x,y#x] csv


-- !query
table t
|> limit all offset 0
-- !query analysis
Offset 0
+- SubqueryAlias spark_catalog.default.t
   +- Relation spark_catalog.default.t[x#x,y#x] csv


-- !query
table t
|> distribute by x
-- !query analysis
RepartitionByExpression [x#x]
+- SubqueryAlias spark_catalog.default.t
   +- Relation spark_catalog.default.t[x#x,y#x] csv


-- !query
table t
|> cluster by x
-- !query analysis
Sort [x#x ASC NULLS FIRST], false
+- RepartitionByExpression [x#x]
   +- SubqueryAlias spark_catalog.default.t
      +- Relation spark_catalog.default.t[x#x,y#x] csv


-- !query
table t
|> sort by x distribute by x
-- !query analysis
RepartitionByExpression [x#x]
+- Sort [x#x ASC NULLS FIRST], false
   +- SubqueryAlias spark_catalog.default.t
      +- Relation spark_catalog.default.t[x#x,y#x] csv


-- !query
table t
|> order by x desc
order by y
-- !query analysis
Sort [y#x ASC NULLS FIRST], true
+- Sort [x#x DESC NULLS LAST], true
   +- SubqueryAlias spark_catalog.default.t
      +- Relation spark_catalog.default.t[x#x,y#x] csv


-- !query
table t
|> order by x desc order by x + y
order by y
-- !query analysis
org.apache.spark.sql.catalyst.parser.ParseException
{
  "errorClass" : "PARSE_SYNTAX_ERROR",
  "sqlState" : "42601",
  "messageParameters" : {
    "error" : "'order'",
    "hint" : ""
  }
}


-- !query
table t
|> select 1 + 2 as result
|> order by x
-- !query analysis
org.apache.spark.sql.catalyst.ExtendedAnalysisException
{
  "errorClass" : "UNRESOLVED_COLUMN.WITH_SUGGESTION",
  "sqlState" : "42703",
  "messageParameters" : {
    "objectName" : "`x`",
    "proposal" : "`result`"
  },
  "queryContext" : [ {
    "objectType" : "",
    "objectName" : "",
    "startIndex" : 47,
    "stopIndex" : 47,
    "fragment" : "x"
  } ]
}


-- !query
table t
|> select 1 + 2 as result
|> distribute by x
-- !query analysis
org.apache.spark.sql.catalyst.ExtendedAnalysisException
{
  "errorClass" : "UNRESOLVED_COLUMN.WITH_SUGGESTION",
  "sqlState" : "42703",
  "messageParameters" : {
    "objectName" : "`x`",
    "proposal" : "`result`"
  },
  "queryContext" : [ {
    "objectType" : "",
    "objectName" : "",
    "startIndex" : 52,
    "stopIndex" : 52,
    "fragment" : "x"
  } ]
}


-- !query
table t
|> order by x limit 1
-- !query analysis
org.apache.spark.sql.catalyst.parser.ParseException
{
  "errorClass" : "MULTIPLE_QUERY_RESULT_CLAUSES_WITH_PIPE_OPERATORS",
  "sqlState" : "42000",
  "messageParameters" : {
    "clause1" : "ORDER BY",
    "clause2" : "LIMIT"
  },
  "queryContext" : [ {
    "objectType" : "",
    "objectName" : "",
    "startIndex" : 12,
    "stopIndex" : 29,
    "fragment" : "order by x limit 1"
  } ]
}


-- !query
table t
|> order by x sort by x
-- !query analysis
org.apache.spark.sql.catalyst.parser.ParseException
{
  "errorClass" : "UNSUPPORTED_FEATURE.COMBINATION_QUERY_RESULT_CLAUSES",
  "sqlState" : "0A000",
  "queryContext" : [ {
    "objectType" : "",
    "objectName" : "",
    "startIndex" : 12,
    "stopIndex" : 31,
    "fragment" : "order by x sort by x"
  } ]
}


-- !query
table other
|> aggregate sum(b) as result group by a
-- !query analysis
Aggregate [a#x], [a#x, pipeexpression(sum(b#x), true, AGGREGATE) AS result#xL]
+- SubqueryAlias spark_catalog.default.other
   +- Relation spark_catalog.default.other[a#x,b#x] json


-- !query
table other
|> aggregate sum(b) as result group by a
|> select result
-- !query analysis
Project [result#xL]
+- Aggregate [a#x], [a#x, pipeexpression(sum(b#x), true, AGGREGATE) AS result#xL]
   +- SubqueryAlias spark_catalog.default.other
      +- Relation spark_catalog.default.other[a#x,b#x] json


-- !query
table other
|> aggregate sum(b) group by a + 1 as gkey
|> select gkey
-- !query analysis
Project [gkey#x]
+- Aggregate [(a#x + 1)], [(a#x + 1) AS gkey#x, pipeexpression(sum(b#x), true, AGGREGATE) AS pipeexpression(sum(b))#xL]
   +- SubqueryAlias spark_catalog.default.other
      +- Relation spark_catalog.default.other[a#x,b#x] json


-- !query
select 1 as x, 2 as y
|> aggregate group by x, y
-- !query analysis
Aggregate [x#x, y#x], [x#x, y#x]
+- Project [1 AS x#x, 2 AS y#x]
   +- OneRowRelation


-- !query
select 3 as x, 4 as y
|> aggregate group by 1, 2
-- !query analysis
Aggregate [1, 2], [1 AS 1#x, 2 AS 2#x]
+- Project [3 AS x#x, 4 AS y#x]
   +- OneRowRelation


-- !query
table t
|> aggregate sum(x)
-- !query analysis
Aggregate [pipeexpression(sum(x#x), true, AGGREGATE) AS pipeexpression(sum(x))#xL]
+- SubqueryAlias spark_catalog.default.t
   +- Relation spark_catalog.default.t[x#x,y#x] csv


-- !query
table t
|> aggregate sum(x) + 1 as result_plus_one
-- !query analysis
Aggregate [pipeexpression((sum(x#x) + cast(1 as bigint)), true, AGGREGATE) AS result_plus_one#xL]
+- SubqueryAlias spark_catalog.default.t
   +- Relation spark_catalog.default.t[x#x,y#x] csv


-- !query
table other
|> aggregate group by a
|> where a = 1
-- !query analysis
Filter (a#x = 1)
+- SubqueryAlias __auto_generated_subquery_name
   +- Aggregate [a#x], [a#x]
      +- SubqueryAlias spark_catalog.default.other
         +- Relation spark_catalog.default.other[a#x,b#x] json


-- !query
select 1 as x, 2 as y, 3 as z
|> aggregate group by x, y, x + y as z
-- !query analysis
Aggregate [x#x, y#x, (x#x + y#x)], [x#x, y#x, (x#x + y#x) AS z#x]
+- Project [1 AS x#x, 2 AS y#x, 3 AS z#x]
   +- OneRowRelation


-- !query
select 1 as x, 2 as y, 3 as z
|> aggregate group by x as z, x + y as z
-- !query analysis
Aggregate [x#x, (x#x + y#x)], [x#x AS z#x, (x#x + y#x) AS z#x]
+- Project [1 AS x#x, 2 AS y#x, 3 AS z#x]
   +- OneRowRelation


-- !query
select 1 as x, 2 as y, named_struct('z', 3) as st
|> aggregate group by x, y, x, x, st.z, (st).z, 1 + x, 2 + x
-- !query analysis
Aggregate [x#x, y#x, x#x, x#x, st#x.z, st#x.z, (1 + x#x), (2 + x#x)], [x#x, y#x, x#x, x#x, st#x.z AS z#x, st#x.z AS z#x, (1 + x#x) AS (1 + x)#x, (2 + x#x) AS (2 + x)#x]
+- Project [1 AS x#x, 2 AS y#x, named_struct(z, 3) AS st#x]
   +- OneRowRelation


-- !query
select 1 x, 2 y, 3 z
|> aggregate sum(z) z group by x, y
|> aggregate avg(z) z group by x
|> aggregate count(distinct z) c
-- !query analysis
Aggregate [pipeexpression(count(distinct z#x), true, AGGREGATE) AS c#xL]
+- Aggregate [x#x], [x#x, pipeexpression(avg(z#xL), true, AGGREGATE) AS z#x]
   +- Aggregate [x#x, y#x], [x#x, y#x, pipeexpression(sum(z#x), true, AGGREGATE) AS z#xL]
      +- Project [1 AS x#x, 2 AS y#x, 3 AS z#x]
         +- OneRowRelation


-- !query
select 1 x, 3 z
|> aggregate count(*) group by x, z, x
|> select x
-- !query analysis
Project [x#x]
+- Aggregate [x#x, z#x, x#x], [x#x, z#x, x#x, pipeexpression(count(1), true, AGGREGATE) AS pipeexpression(count(1))#xL]
   +- Project [1 AS x#x, 3 AS z#x]
      +- OneRowRelation


-- !query
table other
|> aggregate a + count(b) group by a
-- !query analysis
Aggregate [a#x], [a#x, pipeexpression((cast(a#x as bigint) + count(b#x)), true, AGGREGATE) AS pipeexpression((a + count(b)))#xL]
+- SubqueryAlias spark_catalog.default.other
   +- Relation spark_catalog.default.other[a#x,b#x] json


-- !query
table other
|> aggregate a group by a
-- !query analysis
org.apache.spark.sql.AnalysisException
{
  "errorClass" : "PIPE_OPERATOR_AGGREGATE_EXPRESSION_CONTAINS_NO_AGGREGATE_FUNCTION",
  "sqlState" : "0A000",
  "messageParameters" : {
    "expr" : "a#x"
  },
  "queryContext" : [ {
    "objectType" : "",
    "objectName" : "",
    "startIndex" : 37,
    "stopIndex" : 37,
    "fragment" : "a"
  } ]
}


-- !query
select 3 as x, 4 as y
|> aggregate group by all
-- !query analysis
org.apache.spark.sql.catalyst.parser.ParseException
{
  "errorClass" : "UNSUPPORTED_FEATURE.PIPE_OPERATOR_AGGREGATE_UNSUPPORTED_CASE",
  "sqlState" : "0A000",
  "messageParameters" : {
    "case" : "GROUP BY ALL"
  },
  "queryContext" : [ {
    "objectType" : "",
    "objectName" : "",
    "startIndex" : 1,
    "stopIndex" : 47,
    "fragment" : "select 3 as x, 4 as y\n|> aggregate group by all"
  } ]
}


-- !query
table courseSales
|> aggregate sum(earnings) group by rollup(course, `year`)
|> where course = 'dotNET' and `year` = '2013'
-- !query analysis
org.apache.spark.sql.catalyst.parser.ParseException
{
  "errorClass" : "UNSUPPORTED_FEATURE.PIPE_OPERATOR_AGGREGATE_UNSUPPORTED_CASE",
  "sqlState" : "0A000",
  "messageParameters" : {
    "case" : "GROUP BY ROLLUP"
  },
  "queryContext" : [ {
    "objectType" : "",
    "objectName" : "",
    "startIndex" : 1,
    "stopIndex" : 123,
    "fragment" : "table courseSales\n|> aggregate sum(earnings) group by rollup(course, `year`)\n|> where course = 'dotNET' and `year` = '2013'"
  } ]
}


-- !query
table courseSales
|> aggregate sum(earnings) group by cube(course, `year`)
|> where course = 'dotNET' and `year` = '2013'
-- !query analysis
org.apache.spark.sql.catalyst.parser.ParseException
{
  "errorClass" : "UNSUPPORTED_FEATURE.PIPE_OPERATOR_AGGREGATE_UNSUPPORTED_CASE",
  "sqlState" : "0A000",
  "messageParameters" : {
    "case" : "GROUP BY CUBE"
  },
  "queryContext" : [ {
    "objectType" : "",
    "objectName" : "",
    "startIndex" : 1,
    "stopIndex" : 121,
    "fragment" : "table courseSales\n|> aggregate sum(earnings) group by cube(course, `year`)\n|> where course = 'dotNET' and `year` = '2013'"
  } ]
}


-- !query
table courseSales
|> aggregate sum(earnings) group by course, `year` grouping sets(course, `year`)
|> where course = 'dotNET' and `year` = '2013'
-- !query analysis
org.apache.spark.sql.catalyst.parser.ParseException
{
  "errorClass" : "UNSUPPORTED_FEATURE.PIPE_OPERATOR_AGGREGATE_UNSUPPORTED_CASE",
  "sqlState" : "0A000",
  "messageParameters" : {
    "case" : "GROUPING SETS"
  },
  "queryContext" : [ {
    "objectType" : "",
    "objectName" : "",
    "startIndex" : 1,
    "stopIndex" : 145,
    "fragment" : "table courseSales\n|> aggregate sum(earnings) group by course, `year` grouping sets(course, `year`)\n|> where course = 'dotNET' and `year` = '2013'"
  } ]
}


-- !query
table courseSales
|> aggregate sum(earnings), grouping(course) + 1
   group by course
|> where course = 'dotNET' and `year` = '2013'
-- !query analysis
org.apache.spark.sql.catalyst.parser.ParseException
{
  "errorClass" : "UNSUPPORTED_FEATURE.PIPE_OPERATOR_AGGREGATE_UNSUPPORTED_CASE",
  "sqlState" : "0A000",
  "messageParameters" : {
    "case" : "GROUPING"
  },
  "queryContext" : [ {
    "objectType" : "",
    "objectName" : "",
    "startIndex" : 1,
    "stopIndex" : 132,
    "fragment" : "table courseSales\n|> aggregate sum(earnings), grouping(course) + 1\n   group by course\n|> where course = 'dotNET' and `year` = '2013'"
  } ]
}


-- !query
table courseSales
|> aggregate sum(earnings), grouping_id(course)
   group by course
|> where course = 'dotNET' and `year` = '2013'
-- !query analysis
org.apache.spark.sql.catalyst.parser.ParseException
{
  "errorClass" : "UNSUPPORTED_FEATURE.PIPE_OPERATOR_AGGREGATE_UNSUPPORTED_CASE",
  "sqlState" : "0A000",
  "messageParameters" : {
    "case" : "GROUPING_ID"
  },
  "queryContext" : [ {
    "objectType" : "",
    "objectName" : "",
    "startIndex" : 1,
    "stopIndex" : 131,
    "fragment" : "table courseSales\n|> aggregate sum(earnings), grouping_id(course)\n   group by course\n|> where course = 'dotNET' and `year` = '2013'"
  } ]
}


-- !query
select 1 as x, 2 as y
|> aggregate group by ()
-- !query analysis
org.apache.spark.sql.catalyst.parser.ParseException
{
  "errorClass" : "PARSE_SYNTAX_ERROR",
  "sqlState" : "42601",
  "messageParameters" : {
    "error" : "')'",
    "hint" : ""
  }
}


-- !query
table other
|> aggregate a
-- !query analysis
org.apache.spark.sql.AnalysisException
{
  "errorClass" : "PIPE_OPERATOR_AGGREGATE_EXPRESSION_CONTAINS_NO_AGGREGATE_FUNCTION",
  "sqlState" : "0A000",
  "messageParameters" : {
    "expr" : "a#x"
  },
  "queryContext" : [ {
    "objectType" : "",
    "objectName" : "",
    "startIndex" : 26,
    "stopIndex" : 26,
    "fragment" : "a"
  } ]
}


-- !query
table other
|> select sum(a) as result
-- !query analysis
org.apache.spark.sql.AnalysisException
{
  "errorClass" : "PIPE_OPERATOR_CONTAINS_AGGREGATE_FUNCTION",
  "sqlState" : "0A000",
  "messageParameters" : {
    "clause" : "SELECT",
    "expr" : "sum(a#x)"
  },
  "queryContext" : [ {
    "objectType" : "",
    "objectName" : "",
    "startIndex" : 23,
    "stopIndex" : 28,
    "fragment" : "sum(a)"
  } ]
}


-- !query
table other
|> aggregate
-- !query analysis
org.apache.spark.sql.catalyst.parser.ParseException
{
  "errorClass" : "_LEGACY_ERROR_TEMP_0035",
  "messageParameters" : {
    "message" : "The AGGREGATE clause requires a list of aggregate expressions or a list of grouping expressions, or both"
  },
  "queryContext" : [ {
    "objectType" : "",
    "objectName" : "",
    "startIndex" : 1,
    "stopIndex" : 24,
    "fragment" : "table other\n|> aggregate"
  } ]
}


-- !query
table other
|> aggregate group by
-- !query analysis
org.apache.spark.sql.catalyst.ExtendedAnalysisException
{
  "errorClass" : "UNRESOLVED_COLUMN.WITH_SUGGESTION",
  "sqlState" : "42703",
  "messageParameters" : {
    "objectName" : "`group`",
    "proposal" : "`a`, `b`"
  },
  "queryContext" : [ {
    "objectType" : "",
    "objectName" : "",
    "startIndex" : 26,
    "stopIndex" : 30,
    "fragment" : "group"
  } ]
}


-- !query
table other
|> group by a
-- !query analysis
org.apache.spark.sql.catalyst.parser.ParseException
{
  "errorClass" : "PARSE_SYNTAX_ERROR",
  "sqlState" : "42601",
  "messageParameters" : {
    "error" : "'group'",
    "hint" : ""
  }
}


-- !query
table other
|> aggregate sum(a) over () group by b
-- !query analysis
org.apache.spark.sql.catalyst.parser.ParseException
{
  "errorClass" : "UNSUPPORTED_FEATURE.PIPE_OPERATOR_AGGREGATE_UNSUPPORTED_CASE",
  "sqlState" : "0A000",
  "messageParameters" : {
    "case" : "window functions"
  },
  "queryContext" : [ {
    "objectType" : "",
    "objectName" : "",
    "startIndex" : 1,
    "stopIndex" : 50,
    "fragment" : "table other\n|> aggregate sum(a) over () group by b"
  } ]
}


-- !query
select 1 x, 2 y, 3 z
|> aggregate count(*) AS c, sum(x) AS x group by x
|> where c = 1
|> where x = 1
-- !query analysis
org.apache.spark.sql.AnalysisException
{
  "errorClass" : "AMBIGUOUS_REFERENCE",
  "sqlState" : "42704",
  "messageParameters" : {
    "name" : "`x`",
    "referenceNames" : "[`x`, `x`]"
  },
  "queryContext" : [ {
    "objectType" : "",
    "objectName" : "",
    "startIndex" : 97,
    "stopIndex" : 97,
    "fragment" : "x"
  } ]
}


-- !query
table windowTestData
|> select cate, sum(val) over w
   window w as (partition by cate order by val)
-- !query analysis
Project [cate#x, sum(val) OVER (PARTITION BY cate ORDER BY val ASC NULLS FIRST RANGE BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW)#xL]
+- Project [cate#x, val#x, sum(val) OVER (PARTITION BY cate ORDER BY val ASC NULLS FIRST RANGE BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW)#xL, sum(val) OVER (PARTITION BY cate ORDER BY val ASC NULLS FIRST RANGE BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW)#xL]
   +- Window [sum(val#x) windowspecdefinition(cate#x, val#x ASC NULLS FIRST, specifiedwindowframe(RangeFrame, unboundedpreceding$(), currentrow$())) AS sum(val) OVER (PARTITION BY cate ORDER BY val ASC NULLS FIRST RANGE BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW)#xL], [cate#x], [val#x ASC NULLS FIRST]
      +- Project [cate#x, val#x]
         +- SubqueryAlias windowtestdata
            +- View (`windowTestData`, [val#x, val_long#xL, val_double#x, val_date#x, val_timestamp#x, cate#x])
               +- Project [cast(val#x as int) AS val#x, cast(val_long#xL as bigint) AS val_long#xL, cast(val_double#x as double) AS val_double#x, cast(val_date#x as date) AS val_date#x, cast(val_timestamp#x as timestamp) AS val_timestamp#x, cast(cate#x as string) AS cate#x]
                  +- Project [val#x, val_long#xL, val_double#x, val_date#x, val_timestamp#x, cate#x]
                     +- SubqueryAlias testData
                        +- LocalRelation [val#x, val_long#xL, val_double#x, val_date#x, val_timestamp#x, cate#x]


-- !query
table windowTestData
|> select cate, sum(val) over w
   window w as (order by val_timestamp range between unbounded preceding and current row)
-- !query analysis
Project [cate#x, sum(val) OVER (ORDER BY val_timestamp ASC NULLS FIRST RANGE BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW)#xL]
+- Project [cate#x, val#x, val_timestamp#x, sum(val) OVER (ORDER BY val_timestamp ASC NULLS FIRST RANGE BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW)#xL, sum(val) OVER (ORDER BY val_timestamp ASC NULLS FIRST RANGE BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW)#xL]
   +- Window [sum(val#x) windowspecdefinition(val_timestamp#x ASC NULLS FIRST, specifiedwindowframe(RangeFrame, unboundedpreceding$(), currentrow$())) AS sum(val) OVER (ORDER BY val_timestamp ASC NULLS FIRST RANGE BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW)#xL], [val_timestamp#x ASC NULLS FIRST]
      +- Project [cate#x, val#x, val_timestamp#x]
         +- SubqueryAlias windowtestdata
            +- View (`windowTestData`, [val#x, val_long#xL, val_double#x, val_date#x, val_timestamp#x, cate#x])
               +- Project [cast(val#x as int) AS val#x, cast(val_long#xL as bigint) AS val_long#xL, cast(val_double#x as double) AS val_double#x, cast(val_date#x as date) AS val_date#x, cast(val_timestamp#x as timestamp) AS val_timestamp#x, cast(cate#x as string) AS cate#x]
                  +- Project [val#x, val_long#xL, val_double#x, val_date#x, val_timestamp#x, cate#x]
                     +- SubqueryAlias testData
                        +- LocalRelation [val#x, val_long#xL, val_double#x, val_date#x, val_timestamp#x, cate#x]


-- !query
table windowTestData
|> select cate, val
    window w as (partition by cate order by val)
-- !query analysis
Project [cate#x, val#x]
+- SubqueryAlias windowtestdata
   +- View (`windowTestData`, [val#x, val_long#xL, val_double#x, val_date#x, val_timestamp#x, cate#x])
      +- Project [cast(val#x as int) AS val#x, cast(val_long#xL as bigint) AS val_long#xL, cast(val_double#x as double) AS val_double#x, cast(val_date#x as date) AS val_date#x, cast(val_timestamp#x as timestamp) AS val_timestamp#x, cast(cate#x as string) AS cate#x]
         +- Project [val#x, val_long#xL, val_double#x, val_date#x, val_timestamp#x, cate#x]
            +- SubqueryAlias testData
               +- LocalRelation [val#x, val_long#xL, val_double#x, val_date#x, val_timestamp#x, cate#x]


-- !query
table windowTestData
|> select cate, val, sum(val) over w as sum_val
   window w as (partition by cate)
|> select cate, val, sum_val, first_value(cate) over w
   window w as (order by val)
-- !query analysis
Project [cate#x, val#x, sum_val#xL, first_value(cate) OVER (ORDER BY val ASC NULLS FIRST RANGE BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW)#x]
+- Project [cate#x, val#x, sum_val#xL, first_value(cate) OVER (ORDER BY val ASC NULLS FIRST RANGE BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW)#x, first_value(cate) OVER (ORDER BY val ASC NULLS FIRST RANGE BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW)#x]
   +- Window [first_value(cate#x, false) windowspecdefinition(val#x ASC NULLS FIRST, specifiedwindowframe(RangeFrame, unboundedpreceding$(), currentrow$())) AS first_value(cate) OVER (ORDER BY val ASC NULLS FIRST RANGE BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW)#x], [val#x ASC NULLS FIRST]
      +- Project [cate#x, val#x, sum_val#xL]
         +- Project [cate#x, val#x, sum_val#xL]
            +- Project [cate#x, val#x, _we0#xL, pipeexpression(_we0#xL, false, SELECT) AS sum_val#xL]
               +- Window [sum(val#x) windowspecdefinition(cate#x, specifiedwindowframe(RowFrame, unboundedpreceding$(), unboundedfollowing$())) AS _we0#xL], [cate#x]
                  +- Project [cate#x, val#x]
                     +- SubqueryAlias windowtestdata
                        +- View (`windowTestData`, [val#x, val_long#xL, val_double#x, val_date#x, val_timestamp#x, cate#x])
                           +- Project [cast(val#x as int) AS val#x, cast(val_long#xL as bigint) AS val_long#xL, cast(val_double#x as double) AS val_double#x, cast(val_date#x as date) AS val_date#x, cast(val_timestamp#x as timestamp) AS val_timestamp#x, cast(cate#x as string) AS cate#x]
                              +- Project [val#x, val_long#xL, val_double#x, val_date#x, val_timestamp#x, cate#x]
                                 +- SubqueryAlias testData
                                    +- LocalRelation [val#x, val_long#xL, val_double#x, val_date#x, val_timestamp#x, cate#x]


-- !query
table windowTestData
|> select cate, val, sum(val) over w1, first_value(cate) over w2
   window w1 as (partition by cate), w2 as (order by val)
-- !query analysis
Project [cate#x, val#x, sum(val) OVER (PARTITION BY cate ROWS BETWEEN UNBOUNDED PRECEDING AND UNBOUNDED FOLLOWING)#xL, first_value(cate) OVER (ORDER BY val ASC NULLS FIRST RANGE BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW)#x]
+- Project [cate#x, val#x, sum(val) OVER (PARTITION BY cate ROWS BETWEEN UNBOUNDED PRECEDING AND UNBOUNDED FOLLOWING)#xL, first_value(cate) OVER (ORDER BY val ASC NULLS FIRST RANGE BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW)#x, sum(val) OVER (PARTITION BY cate ROWS BETWEEN UNBOUNDED PRECEDING AND UNBOUNDED FOLLOWING)#xL, first_value(cate) OVER (ORDER BY val ASC NULLS FIRST RANGE BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW)#x]
   +- Window [first_value(cate#x, false) windowspecdefinition(val#x ASC NULLS FIRST, specifiedwindowframe(RangeFrame, unboundedpreceding$(), currentrow$())) AS first_value(cate) OVER (ORDER BY val ASC NULLS FIRST RANGE BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW)#x], [val#x ASC NULLS FIRST]
      +- Window [sum(val#x) windowspecdefinition(cate#x, specifiedwindowframe(RowFrame, unboundedpreceding$(), unboundedfollowing$())) AS sum(val) OVER (PARTITION BY cate ROWS BETWEEN UNBOUNDED PRECEDING AND UNBOUNDED FOLLOWING)#xL], [cate#x]
         +- Project [cate#x, val#x]
            +- SubqueryAlias windowtestdata
               +- View (`windowTestData`, [val#x, val_long#xL, val_double#x, val_date#x, val_timestamp#x, cate#x])
                  +- Project [cast(val#x as int) AS val#x, cast(val_long#xL as bigint) AS val_long#xL, cast(val_double#x as double) AS val_double#x, cast(val_date#x as date) AS val_date#x, cast(val_timestamp#x as timestamp) AS val_timestamp#x, cast(cate#x as string) AS cate#x]
                     +- Project [val#x, val_long#xL, val_double#x, val_date#x, val_timestamp#x, cate#x]
                        +- SubqueryAlias testData
                           +- LocalRelation [val#x, val_long#xL, val_double#x, val_date#x, val_timestamp#x, cate#x]


-- !query
table windowTestData
|> select cate, val, sum(val) over w, first_value(val) over w
   window w1 as (partition by cate order by val)
-- !query analysis
org.apache.spark.sql.AnalysisException
{
  "errorClass" : "MISSING_WINDOW_SPECIFICATION",
  "sqlState" : "42P20",
  "messageParameters" : {
    "docroot" : "https://spark.apache.org/docs/latest",
    "windowName" : "w"
  }
}


-- !query
(select col from st)
|> select col.i1, sum(col.i2) over w
   window w as (partition by col.i1 order by col.i2)
-- !query analysis
Project [i1#x, sum(col.i2) OVER (PARTITION BY col.i1 ORDER BY col.i2 ASC NULLS FIRST RANGE BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW)#xL]
+- Project [i1#x, _w0#x, _w1#x, sum(col.i2) OVER (PARTITION BY col.i1 ORDER BY col.i2 ASC NULLS FIRST RANGE BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW)#xL, sum(col.i2) OVER (PARTITION BY col.i1 ORDER BY col.i2 ASC NULLS FIRST RANGE BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW)#xL]
   +- Window [sum(_w0#x) windowspecdefinition(_w1#x, _w0#x ASC NULLS FIRST, specifiedwindowframe(RangeFrame, unboundedpreceding$(), currentrow$())) AS sum(col.i2) OVER (PARTITION BY col.i1 ORDER BY col.i2 ASC NULLS FIRST RANGE BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW)#xL], [_w1#x], [_w0#x ASC NULLS FIRST]
      +- Project [col#x.i1 AS i1#x, col#x.i2 AS _w0#x, col#x.i1 AS _w1#x]
         +- Project [col#x]
            +- SubqueryAlias spark_catalog.default.st
               +- Relation spark_catalog.default.st[x#x,col#x] parquet


-- !query
table st
|> select st.col.i1, sum(st.col.i2) over w
   window w as (partition by st.col.i1 order by st.col.i2)
-- !query analysis
Project [i1#x, sum(col.i2) OVER (PARTITION BY col.i1 ORDER BY col.i2 ASC NULLS FIRST RANGE BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW)#xL]
+- Project [i1#x, _w0#x, _w1#x, sum(col.i2) OVER (PARTITION BY col.i1 ORDER BY col.i2 ASC NULLS FIRST RANGE BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW)#xL, sum(col.i2) OVER (PARTITION BY col.i1 ORDER BY col.i2 ASC NULLS FIRST RANGE BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW)#xL]
   +- Window [sum(_w0#x) windowspecdefinition(_w1#x, _w0#x ASC NULLS FIRST, specifiedwindowframe(RangeFrame, unboundedpreceding$(), currentrow$())) AS sum(col.i2) OVER (PARTITION BY col.i1 ORDER BY col.i2 ASC NULLS FIRST RANGE BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW)#xL], [_w1#x], [_w0#x ASC NULLS FIRST]
      +- Project [col#x.i1 AS i1#x, col#x.i2 AS _w0#x, col#x.i1 AS _w1#x]
         +- SubqueryAlias spark_catalog.default.st
            +- Relation spark_catalog.default.st[x#x,col#x] parquet


-- !query
table st
|> select spark_catalog.default.st.col.i1, sum(spark_catalog.default.st.col.i2) over w
   window w as (partition by spark_catalog.default.st.col.i1 order by spark_catalog.default.st.col.i2)
-- !query analysis
Project [i1#x, sum(col.i2) OVER (PARTITION BY col.i1 ORDER BY col.i2 ASC NULLS FIRST RANGE BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW)#xL]
+- Project [i1#x, _w0#x, _w1#x, sum(col.i2) OVER (PARTITION BY col.i1 ORDER BY col.i2 ASC NULLS FIRST RANGE BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW)#xL, sum(col.i2) OVER (PARTITION BY col.i1 ORDER BY col.i2 ASC NULLS FIRST RANGE BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW)#xL]
   +- Window [sum(_w0#x) windowspecdefinition(_w1#x, _w0#x ASC NULLS FIRST, specifiedwindowframe(RangeFrame, unboundedpreceding$(), currentrow$())) AS sum(col.i2) OVER (PARTITION BY col.i1 ORDER BY col.i2 ASC NULLS FIRST RANGE BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW)#xL], [_w1#x], [_w0#x ASC NULLS FIRST]
      +- Project [col#x.i1 AS i1#x, col#x.i2 AS _w0#x, col#x.i1 AS _w1#x]
         +- SubqueryAlias spark_catalog.default.st
            +- Relation spark_catalog.default.st[x#x,col#x] parquet


-- !query
table windowTestData
|> select cate, sum(val) over val
   window val as (partition by cate order by val)
-- !query analysis
Project [cate#x, sum(val) OVER (PARTITION BY cate ORDER BY val ASC NULLS FIRST RANGE BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW)#xL]
+- Project [cate#x, val#x, sum(val) OVER (PARTITION BY cate ORDER BY val ASC NULLS FIRST RANGE BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW)#xL, sum(val) OVER (PARTITION BY cate ORDER BY val ASC NULLS FIRST RANGE BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW)#xL]
   +- Window [sum(val#x) windowspecdefinition(cate#x, val#x ASC NULLS FIRST, specifiedwindowframe(RangeFrame, unboundedpreceding$(), currentrow$())) AS sum(val) OVER (PARTITION BY cate ORDER BY val ASC NULLS FIRST RANGE BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW)#xL], [cate#x], [val#x ASC NULLS FIRST]
      +- Project [cate#x, val#x]
         +- SubqueryAlias windowtestdata
            +- View (`windowTestData`, [val#x, val_long#xL, val_double#x, val_date#x, val_timestamp#x, cate#x])
               +- Project [cast(val#x as int) AS val#x, cast(val_long#xL as bigint) AS val_long#xL, cast(val_double#x as double) AS val_double#x, cast(val_date#x as date) AS val_date#x, cast(val_timestamp#x as timestamp) AS val_timestamp#x, cast(cate#x as string) AS cate#x]
                  +- Project [val#x, val_long#xL, val_double#x, val_date#x, val_timestamp#x, cate#x]
                     +- SubqueryAlias testData
                        +- LocalRelation [val#x, val_long#xL, val_double#x, val_date#x, val_timestamp#x, cate#x]


-- !query
table windowTestData
|> select cate, sum(val) over w
-- !query analysis
org.apache.spark.sql.catalyst.ExtendedAnalysisException
{
  "errorClass" : "MISSING_WINDOW_SPECIFICATION",
  "sqlState" : "42P20",
  "messageParameters" : {
    "docroot" : "https://spark.apache.org/docs/latest",
    "windowName" : "w"
  }
}


-- !query
table windowTestData
|> select cate, val, sum(val) over w1, first_value(cate) over w2
   window w1 as (partition by cate)
   window w2 as (order by val)
-- !query analysis
org.apache.spark.sql.AnalysisException
{
  "errorClass" : "MISSING_WINDOW_SPECIFICATION",
  "sqlState" : "42P20",
  "messageParameters" : {
    "docroot" : "https://spark.apache.org/docs/latest",
    "windowName" : "w2"
  }
}


-- !query
table windowTestData
|> select cate, val, sum(val) over w as sum_val
   window w as (partition by cate order by val)
|> select cate, val, sum_val, first_value(cate) over w
-- !query analysis
org.apache.spark.sql.catalyst.ExtendedAnalysisException
{
  "errorClass" : "MISSING_WINDOW_SPECIFICATION",
  "sqlState" : "42P20",
  "messageParameters" : {
    "docroot" : "https://spark.apache.org/docs/latest",
    "windowName" : "w"
  }
}


-- !query
table windowTestData
|> select cate, val, first_value(cate) over w as first_val
|> select cate, val, sum(val) over w as sum_val
   window w as (order by val)
-- !query analysis
org.apache.spark.sql.catalyst.ExtendedAnalysisException
{
  "errorClass" : "MISSING_WINDOW_SPECIFICATION",
  "sqlState" : "42P20",
  "messageParameters" : {
    "docroot" : "https://spark.apache.org/docs/latest",
    "windowName" : "w"
  }
}


-- !query
with customer_total_return as
(select
    sr_customer_sk as ctr_customer_sk,
    sr_store_sk as ctr_store_sk,
    sum(sr_return_amt) as ctr_total_return
  from store_returns, date_dim
  where sr_returned_date_sk = d_date_sk and d_year = 2000
  group by sr_customer_sk, sr_store_sk)
select c_customer_id
from customer_total_return ctr1, store, customer
where ctr1.ctr_total_return >
  (select avg(ctr_total_return) * 1.2
  from customer_total_return ctr2
  where ctr1.ctr_store_sk = ctr2.ctr_store_sk)
  and s_store_sk = ctr1.ctr_store_sk
  and s_state = 'tn'
  and ctr1.ctr_customer_sk = c_customer_sk
order by c_customer_id
limit 100
-- !query analysis
WithCTE
:- CTERelationDef xxxx, false
:  +- SubqueryAlias customer_total_return
:     +- Aggregate [sr_customer_sk#x, sr_store_sk#x], [sr_customer_sk#x AS ctr_customer_sk#x, sr_store_sk#x AS ctr_store_sk#x, sum(sr_return_amt#x) AS ctr_total_return#x]
:        +- Filter ((sr_returned_date_sk#x = d_date_sk#x) AND (d_year#x = 2000))
:           +- Join Inner
:              :- SubqueryAlias spark_catalog.default.store_returns
:              :  +- Relation spark_catalog.default.store_returns[sr_returned_date_sk#x,sr_return_time_sk#x,sr_item_sk#x,sr_customer_sk#x,sr_cdemo_sk#x,sr_hdemo_sk#x,sr_addr_sk#x,sr_store_sk#x,sr_reason_sk#x,sr_ticket_number#x,sr_return_quantity#x,sr_return_amt#x,sr_return_tax#x,sr_return_amt_inc_tax#x,sr_fee#x,sr_return_ship_cost#x,sr_refunded_cash#x,sr_reversed_charge#x,sr_store_credit#x,sr_net_loss#x] parquet
:              +- SubqueryAlias spark_catalog.default.date_dim
:                 +- Project [d_date_sk#x, static_invoke(CharVarcharCodegenUtils.readSidePadding(d_date_id#x, 16)) AS d_date_id#x, d_date#x, d_month_seq#x, d_week_seq#x, d_quarter_seq#x, d_year#x, d_dow#x, d_moy#x, d_dom#x, d_qoy#x, d_fy_year#x, d_fy_quarter_seq#x, d_fy_week_seq#x, static_invoke(CharVarcharCodegenUtils.readSidePadding(d_day_name#x, 9)) AS d_day_name#x, static_invoke(CharVarcharCodegenUtils.readSidePadding(d_quarter_name#x, 6)) AS d_quarter_name#x, static_invoke(CharVarcharCodegenUtils.readSidePadding(d_holiday#x, 1)) AS d_holiday#x, static_invoke(CharVarcharCodegenUtils.readSidePadding(d_weekend#x, 1)) AS d_weekend#x, static_invoke(CharVarcharCodegenUtils.readSidePadding(d_following_holiday#x, 1)) AS d_following_holiday#x, d_first_dom#x, d_last_dom#x, d_same_day_ly#x, d_same_day_lq#x, static_invoke(CharVarcharCodegenUtils.readSidePadding(d_current_day#x, 1)) AS d_current_day#x, ... 4 more fields]
:                    +- Relation spark_catalog.default.date_dim[d_date_sk#x,d_date_id#x,d_date#x,d_month_seq#x,d_week_seq#x,d_quarter_seq#x,d_year#x,d_dow#x,d_moy#x,d_dom#x,d_qoy#x,d_fy_year#x,d_fy_quarter_seq#x,d_fy_week_seq#x,d_day_name#x,d_quarter_name#x,d_holiday#x,d_weekend#x,d_following_holiday#x,d_first_dom#x,d_last_dom#x,d_same_day_ly#x,d_same_day_lq#x,d_current_day#x,... 4 more fields] parquet
+- GlobalLimit 100
   +- LocalLimit 100
      +- Sort [c_customer_id#x ASC NULLS FIRST], true
         +- Project [c_customer_id#x]
            +- Filter (((cast(ctr_total_return#x as decimal(24,7)) > scalar-subquery#x [ctr_store_sk#x]) AND (s_store_sk#x = ctr_store_sk#x)) AND ((s_state#x = tn) AND (ctr_customer_sk#x = c_customer_sk#x)))
               :  +- Aggregate [(avg(ctr_total_return#x) * 1.2) AS (avg(ctr_total_return) * 1.2)#x]
               :     +- Filter (outer(ctr_store_sk#x) = ctr_store_sk#x)
               :        +- SubqueryAlias ctr2
               :           +- SubqueryAlias customer_total_return
               :              +- CTERelationRef xxxx, true, [ctr_customer_sk#x, ctr_store_sk#x, ctr_total_return#x], false
               +- Join Inner
                  :- Join Inner
                  :  :- SubqueryAlias ctr1
                  :  :  +- SubqueryAlias customer_total_return
                  :  :     +- CTERelationRef xxxx, true, [ctr_customer_sk#x, ctr_store_sk#x, ctr_total_return#x], false
                  :  +- SubqueryAlias spark_catalog.default.store
                  :     +- Project [s_store_sk#x, static_invoke(CharVarcharCodegenUtils.readSidePadding(s_store_id#x, 16)) AS s_store_id#x, s_rec_start_date#x, s_rec_end_date#x, s_closed_date_sk#x, s_store_name#x, s_number_employees#x, s_floor_space#x, static_invoke(CharVarcharCodegenUtils.readSidePadding(s_hours#x, 20)) AS s_hours#x, s_manager#x, s_market_id#x, s_geography_class#x, s_market_desc#x, s_market_manager#x, s_division_id#x, s_division_name#x, s_company_id#x, s_company_name#x, s_street_number#x, s_street_name#x, static_invoke(CharVarcharCodegenUtils.readSidePadding(s_street_type#x, 15)) AS s_street_type#x, static_invoke(CharVarcharCodegenUtils.readSidePadding(s_suite_number#x, 10)) AS s_suite_number#x, s_city#x, s_county#x, ... 5 more fields]
                  :        +- Relation spark_catalog.default.store[s_store_sk#x,s_store_id#x,s_rec_start_date#x,s_rec_end_date#x,s_closed_date_sk#x,s_store_name#x,s_number_employees#x,s_floor_space#x,s_hours#x,s_manager#x,s_market_id#x,s_geography_class#x,s_market_desc#x,s_market_manager#x,s_division_id#x,s_division_name#x,s_company_id#x,s_company_name#x,s_street_number#x,s_street_name#x,s_street_type#x,s_suite_number#x,s_city#x,s_county#x,... 5 more fields] parquet
                  +- SubqueryAlias spark_catalog.default.customer
                     +- Project [c_customer_sk#x, static_invoke(CharVarcharCodegenUtils.readSidePadding(c_customer_id#x, 16)) AS c_customer_id#x, c_current_cdemo_sk#x, c_current_hdemo_sk#x, c_current_addr_sk#x, c_first_shipto_date_sk#x, c_first_sales_date_sk#x, static_invoke(CharVarcharCodegenUtils.readSidePadding(c_salutation#x, 10)) AS c_salutation#x, static_invoke(CharVarcharCodegenUtils.readSidePadding(c_first_name#x, 20)) AS c_first_name#x, static_invoke(CharVarcharCodegenUtils.readSidePadding(c_last_name#x, 30)) AS c_last_name#x, static_invoke(CharVarcharCodegenUtils.readSidePadding(c_preferred_cust_flag#x, 1)) AS c_preferred_cust_flag#x, c_birth_day#x, c_birth_month#x, c_birth_year#x, c_birth_country#x, static_invoke(CharVarcharCodegenUtils.readSidePadding(c_login#x, 13)) AS c_login#x, static_invoke(CharVarcharCodegenUtils.readSidePadding(c_email_address#x, 50)) AS c_email_address#x, c_last_review_date#x]
                        +- Relation spark_catalog.default.customer[c_customer_sk#x,c_customer_id#x,c_current_cdemo_sk#x,c_current_hdemo_sk#x,c_current_addr_sk#x,c_first_shipto_date_sk#x,c_first_sales_date_sk#x,c_salutation#x,c_first_name#x,c_last_name#x,c_preferred_cust_flag#x,c_birth_day#x,c_birth_month#x,c_birth_year#x,c_birth_country#x,c_login#x,c_email_address#x,c_last_review_date#x] parquet


-- !query
with customer_total_return as
  (table store_returns
  |> join date_dim
  |> where sr_returned_date_sk = d_date_sk and d_year = 2000
  |> aggregate sum(sr_return_amt) as ctr_total_return
       group by sr_customer_sk as ctr_customer_sk, sr_store_sk as ctr_store_sk)
table customer_total_return
|> as ctr1
|> join store
|> join customer
|> where ctr1.ctr_total_return >
     (table customer_total_return
      |> as ctr2
      |> where ctr1.ctr_store_sk = ctr2.ctr_store_sk
      |> aggregate avg(ctr_total_return) * 1.2)
     and s_store_sk = ctr1.ctr_store_sk
     and s_state = 'tn'
     and ctr1.ctr_customer_sk = c_customer_sk
|> order by c_customer_id
|> limit 100
|> select c_customer_id
-- !query analysis
WithCTE
:- CTERelationDef xxxx, false
:  +- SubqueryAlias customer_total_return
:     +- Aggregate [sr_customer_sk#x, sr_store_sk#x], [sr_customer_sk#x AS ctr_customer_sk#x, sr_store_sk#x AS ctr_store_sk#x, pipeexpression(sum(sr_return_amt#x), true, AGGREGATE) AS ctr_total_return#x]
:        +- Filter ((sr_returned_date_sk#x = d_date_sk#x) AND (d_year#x = 2000))
:           +- Join Inner
:              :- SubqueryAlias spark_catalog.default.store_returns
:              :  +- Relation spark_catalog.default.store_returns[sr_returned_date_sk#x,sr_return_time_sk#x,sr_item_sk#x,sr_customer_sk#x,sr_cdemo_sk#x,sr_hdemo_sk#x,sr_addr_sk#x,sr_store_sk#x,sr_reason_sk#x,sr_ticket_number#x,sr_return_quantity#x,sr_return_amt#x,sr_return_tax#x,sr_return_amt_inc_tax#x,sr_fee#x,sr_return_ship_cost#x,sr_refunded_cash#x,sr_reversed_charge#x,sr_store_credit#x,sr_net_loss#x] parquet
:              +- SubqueryAlias spark_catalog.default.date_dim
:                 +- Project [d_date_sk#x, static_invoke(CharVarcharCodegenUtils.readSidePadding(d_date_id#x, 16)) AS d_date_id#x, d_date#x, d_month_seq#x, d_week_seq#x, d_quarter_seq#x, d_year#x, d_dow#x, d_moy#x, d_dom#x, d_qoy#x, d_fy_year#x, d_fy_quarter_seq#x, d_fy_week_seq#x, static_invoke(CharVarcharCodegenUtils.readSidePadding(d_day_name#x, 9)) AS d_day_name#x, static_invoke(CharVarcharCodegenUtils.readSidePadding(d_quarter_name#x, 6)) AS d_quarter_name#x, static_invoke(CharVarcharCodegenUtils.readSidePadding(d_holiday#x, 1)) AS d_holiday#x, static_invoke(CharVarcharCodegenUtils.readSidePadding(d_weekend#x, 1)) AS d_weekend#x, static_invoke(CharVarcharCodegenUtils.readSidePadding(d_following_holiday#x, 1)) AS d_following_holiday#x, d_first_dom#x, d_last_dom#x, d_same_day_ly#x, d_same_day_lq#x, static_invoke(CharVarcharCodegenUtils.readSidePadding(d_current_day#x, 1)) AS d_current_day#x, ... 4 more fields]
:                    +- Relation spark_catalog.default.date_dim[d_date_sk#x,d_date_id#x,d_date#x,d_month_seq#x,d_week_seq#x,d_quarter_seq#x,d_year#x,d_dow#x,d_moy#x,d_dom#x,d_qoy#x,d_fy_year#x,d_fy_quarter_seq#x,d_fy_week_seq#x,d_day_name#x,d_quarter_name#x,d_holiday#x,d_weekend#x,d_following_holiday#x,d_first_dom#x,d_last_dom#x,d_same_day_ly#x,d_same_day_lq#x,d_current_day#x,... 4 more fields] parquet
+- Project [c_customer_id#x]
   +- GlobalLimit 100
      +- LocalLimit 100
         +- SubqueryAlias __auto_generated_subquery_name
            +- Sort [c_customer_id#x ASC NULLS FIRST], true
               +- Filter (((cast(ctr_total_return#x as decimal(24,7)) > scalar-subquery#x [ctr_store_sk#x]) AND (s_store_sk#x = ctr_store_sk#x)) AND ((s_state#x = tn) AND (ctr_customer_sk#x = c_customer_sk#x)))
                  :  +- Aggregate [pipeexpression((avg(ctr_total_return#x) * 1.2), true, AGGREGATE) AS pipeexpression((avg(ctr_total_return) * 1.2))#x]
                  :     +- Filter (outer(ctr_store_sk#x) = ctr_store_sk#x)
                  :        +- SubqueryAlias ctr2
                  :           +- SubqueryAlias customer_total_return
                  :              +- CTERelationRef xxxx, true, [ctr_customer_sk#x, ctr_store_sk#x, ctr_total_return#x], false
                  +- Join Inner
                     :- Join Inner
                     :  :- SubqueryAlias ctr1
                     :  :  +- SubqueryAlias customer_total_return
                     :  :     +- CTERelationRef xxxx, true, [ctr_customer_sk#x, ctr_store_sk#x, ctr_total_return#x], false
                     :  +- SubqueryAlias spark_catalog.default.store
                     :     +- Project [s_store_sk#x, static_invoke(CharVarcharCodegenUtils.readSidePadding(s_store_id#x, 16)) AS s_store_id#x, s_rec_start_date#x, s_rec_end_date#x, s_closed_date_sk#x, s_store_name#x, s_number_employees#x, s_floor_space#x, static_invoke(CharVarcharCodegenUtils.readSidePadding(s_hours#x, 20)) AS s_hours#x, s_manager#x, s_market_id#x, s_geography_class#x, s_market_desc#x, s_market_manager#x, s_division_id#x, s_division_name#x, s_company_id#x, s_company_name#x, s_street_number#x, s_street_name#x, static_invoke(CharVarcharCodegenUtils.readSidePadding(s_street_type#x, 15)) AS s_street_type#x, static_invoke(CharVarcharCodegenUtils.readSidePadding(s_suite_number#x, 10)) AS s_suite_number#x, s_city#x, s_county#x, ... 5 more fields]
                     :        +- Relation spark_catalog.default.store[s_store_sk#x,s_store_id#x,s_rec_start_date#x,s_rec_end_date#x,s_closed_date_sk#x,s_store_name#x,s_number_employees#x,s_floor_space#x,s_hours#x,s_manager#x,s_market_id#x,s_geography_class#x,s_market_desc#x,s_market_manager#x,s_division_id#x,s_division_name#x,s_company_id#x,s_company_name#x,s_street_number#x,s_street_name#x,s_street_type#x,s_suite_number#x,s_city#x,s_county#x,... 5 more fields] parquet
                     +- SubqueryAlias spark_catalog.default.customer
                        +- Project [c_customer_sk#x, static_invoke(CharVarcharCodegenUtils.readSidePadding(c_customer_id#x, 16)) AS c_customer_id#x, c_current_cdemo_sk#x, c_current_hdemo_sk#x, c_current_addr_sk#x, c_first_shipto_date_sk#x, c_first_sales_date_sk#x, static_invoke(CharVarcharCodegenUtils.readSidePadding(c_salutation#x, 10)) AS c_salutation#x, static_invoke(CharVarcharCodegenUtils.readSidePadding(c_first_name#x, 20)) AS c_first_name#x, static_invoke(CharVarcharCodegenUtils.readSidePadding(c_last_name#x, 30)) AS c_last_name#x, static_invoke(CharVarcharCodegenUtils.readSidePadding(c_preferred_cust_flag#x, 1)) AS c_preferred_cust_flag#x, c_birth_day#x, c_birth_month#x, c_birth_year#x, c_birth_country#x, static_invoke(CharVarcharCodegenUtils.readSidePadding(c_login#x, 13)) AS c_login#x, static_invoke(CharVarcharCodegenUtils.readSidePadding(c_email_address#x, 50)) AS c_email_address#x, c_last_review_date#x]
                           +- Relation spark_catalog.default.customer[c_customer_sk#x,c_customer_id#x,c_current_cdemo_sk#x,c_current_hdemo_sk#x,c_current_addr_sk#x,c_first_shipto_date_sk#x,c_first_sales_date_sk#x,c_salutation#x,c_first_name#x,c_last_name#x,c_preferred_cust_flag#x,c_birth_day#x,c_birth_month#x,c_birth_year#x,c_birth_country#x,c_login#x,c_email_address#x,c_last_review_date#x] parquet


-- !query
with wscs as
( select
    sold_date_sk,
    sales_price
  from (select
    ws_sold_date_sk sold_date_sk,
    ws_ext_sales_price sales_price
  from web_sales) x
  union all
  (select
    cs_sold_date_sk sold_date_sk,
    cs_ext_sales_price sales_price
  from catalog_sales)),
    wswscs as
  ( select
    d_week_seq,
    sum(case when (d_day_name = 'sunday')
      then sales_price
        else null end)
    sun_sales,
    sum(case when (d_day_name = 'monday')
      then sales_price
        else null end)
    mon_sales,
    sum(case when (d_day_name = 'tuesday')
      then sales_price
        else null end)
    tue_sales,
    sum(case when (d_day_name = 'wednesday')
      then sales_price
        else null end)
    wed_sales,
    sum(case when (d_day_name = 'thursday')
      then sales_price
        else null end)
    thu_sales,
    sum(case when (d_day_name = 'friday')
      then sales_price
        else null end)
    fri_sales,
    sum(case when (d_day_name = 'saturday')
      then sales_price
        else null end)
    sat_sales
  from wscs, date_dim
  where d_date_sk = sold_date_sk
  group by d_week_seq)
select
  d_week_seq1,
  round(sun_sales1 / sun_sales2, 2),
  round(mon_sales1 / mon_sales2, 2),
  round(tue_sales1 / tue_sales2, 2),
  round(wed_sales1 / wed_sales2, 2),
  round(thu_sales1 / thu_sales2, 2),
  round(fri_sales1 / fri_sales2, 2),
  round(sat_sales1 / sat_sales2, 2)
from
  (select
    wswscs.d_week_seq d_week_seq1,
    sun_sales sun_sales1,
    mon_sales mon_sales1,
    tue_sales tue_sales1,
    wed_sales wed_sales1,
    thu_sales thu_sales1,
    fri_sales fri_sales1,
    sat_sales sat_sales1
  from wswscs, date_dim
  where date_dim.d_week_seq = wswscs.d_week_seq and d_year = 2001) y,
  (select
    wswscs.d_week_seq d_week_seq2,
    sun_sales sun_sales2,
    mon_sales mon_sales2,
    tue_sales tue_sales2,
    wed_sales wed_sales2,
    thu_sales thu_sales2,
    fri_sales fri_sales2,
    sat_sales sat_sales2
  from wswscs, date_dim
  where date_dim.d_week_seq = wswscs.d_week_seq and d_year = 2001 + 1) z
where d_week_seq1 = d_week_seq2 - 53
order by d_week_seq1
-- !query analysis
WithCTE
:- CTERelationDef xxxx, false
:  +- SubqueryAlias wscs
:     +- Union false, false
:        :- Project [sold_date_sk#x, sales_price#x]
:        :  +- SubqueryAlias x
:        :     +- Project [ws_sold_date_sk#x AS sold_date_sk#x, ws_ext_sales_price#x AS sales_price#x]
:        :        +- SubqueryAlias spark_catalog.default.web_sales
:        :           +- Relation spark_catalog.default.web_sales[ws_sold_date_sk#x,ws_sold_time_sk#x,ws_ship_date_sk#x,ws_item_sk#x,ws_bill_customer_sk#x,ws_bill_cdemo_sk#x,ws_bill_hdemo_sk#x,ws_bill_addr_sk#x,ws_ship_customer_sk#x,ws_ship_cdemo_sk#x,ws_ship_hdemo_sk#x,ws_ship_addr_sk#x,ws_web_page_sk#x,ws_web_site_sk#x,ws_ship_mode_sk#x,ws_warehouse_sk#x,ws_promo_sk#x,ws_order_number#x,ws_quantity#x,ws_wholesale_cost#x,ws_list_price#x,ws_sales_price#x,ws_ext_discount_amt#x,ws_ext_sales_price#x,... 10 more fields] parquet
:        +- Project [cs_sold_date_sk#x AS sold_date_sk#x, cs_ext_sales_price#x AS sales_price#x]
:           +- SubqueryAlias spark_catalog.default.catalog_sales
:              +- Relation spark_catalog.default.catalog_sales[cs_sold_date_sk#x,cs_sold_time_sk#x,cs_ship_date_sk#x,cs_bill_customer_sk#x,cs_bill_cdemo_sk#x,cs_bill_hdemo_sk#x,cs_bill_addr_sk#x,cs_ship_customer_sk#x,cs_ship_cdemo_sk#x,cs_ship_hdemo_sk#x,cs_ship_addr_sk#x,cs_call_center_sk#x,cs_catalog_page_sk#x,cs_ship_mode_sk#x,cs_warehouse_sk#x,cs_item_sk#x,cs_promo_sk#x,cs_order_number#x,cs_quantity#x,cs_wholesale_cost#x,cs_list_price#x,cs_sales_price#x,cs_ext_discount_amt#x,cs_ext_sales_price#x,... 10 more fields] parquet
:- CTERelationDef xxxx, false
:  +- SubqueryAlias wswscs
:     +- Aggregate [d_week_seq#x], [d_week_seq#x, sum(CASE WHEN (d_day_name#x = rpad(sunday, 9,  )) THEN sales_price#x ELSE cast(null as decimal(7,2)) END) AS sun_sales#x, sum(CASE WHEN (d_day_name#x = rpad(monday, 9,  )) THEN sales_price#x ELSE cast(null as decimal(7,2)) END) AS mon_sales#x, sum(CASE WHEN (d_day_name#x = rpad(tuesday, 9,  )) THEN sales_price#x ELSE cast(null as decimal(7,2)) END) AS tue_sales#x, sum(CASE WHEN (d_day_name#x = wednesday) THEN sales_price#x ELSE cast(null as decimal(7,2)) END) AS wed_sales#x, sum(CASE WHEN (d_day_name#x = rpad(thursday, 9,  )) THEN sales_price#x ELSE cast(null as decimal(7,2)) END) AS thu_sales#x, sum(CASE WHEN (d_day_name#x = rpad(friday, 9,  )) THEN sales_price#x ELSE cast(null as decimal(7,2)) END) AS fri_sales#x, sum(CASE WHEN (d_day_name#x = rpad(saturday, 9,  )) THEN sales_price#x ELSE cast(null as decimal(7,2)) END) AS sat_sales#x]
:        +- Filter (d_date_sk#x = sold_date_sk#x)
:           +- Join Inner
:              :- SubqueryAlias wscs
:              :  +- CTERelationRef xxxx, true, [sold_date_sk#x, sales_price#x], false
:              +- SubqueryAlias spark_catalog.default.date_dim
:                 +- Project [d_date_sk#x, static_invoke(CharVarcharCodegenUtils.readSidePadding(d_date_id#x, 16)) AS d_date_id#x, d_date#x, d_month_seq#x, d_week_seq#x, d_quarter_seq#x, d_year#x, d_dow#x, d_moy#x, d_dom#x, d_qoy#x, d_fy_year#x, d_fy_quarter_seq#x, d_fy_week_seq#x, static_invoke(CharVarcharCodegenUtils.readSidePadding(d_day_name#x, 9)) AS d_day_name#x, static_invoke(CharVarcharCodegenUtils.readSidePadding(d_quarter_name#x, 6)) AS d_quarter_name#x, static_invoke(CharVarcharCodegenUtils.readSidePadding(d_holiday#x, 1)) AS d_holiday#x, static_invoke(CharVarcharCodegenUtils.readSidePadding(d_weekend#x, 1)) AS d_weekend#x, static_invoke(CharVarcharCodegenUtils.readSidePadding(d_following_holiday#x, 1)) AS d_following_holiday#x, d_first_dom#x, d_last_dom#x, d_same_day_ly#x, d_same_day_lq#x, static_invoke(CharVarcharCodegenUtils.readSidePadding(d_current_day#x, 1)) AS d_current_day#x, ... 4 more fields]
:                    +- Relation spark_catalog.default.date_dim[d_date_sk#x,d_date_id#x,d_date#x,d_month_seq#x,d_week_seq#x,d_quarter_seq#x,d_year#x,d_dow#x,d_moy#x,d_dom#x,d_qoy#x,d_fy_year#x,d_fy_quarter_seq#x,d_fy_week_seq#x,d_day_name#x,d_quarter_name#x,d_holiday#x,d_weekend#x,d_following_holiday#x,d_first_dom#x,d_last_dom#x,d_same_day_ly#x,d_same_day_lq#x,d_current_day#x,... 4 more fields] parquet
+- Sort [d_week_seq1#x ASC NULLS FIRST], true
   +- Project [d_week_seq1#x, round((sun_sales1#x / sun_sales2#x), 2) AS round((sun_sales1 / sun_sales2), 2)#x, round((mon_sales1#x / mon_sales2#x), 2) AS round((mon_sales1 / mon_sales2), 2)#x, round((tue_sales1#x / tue_sales2#x), 2) AS round((tue_sales1 / tue_sales2), 2)#x, round((wed_sales1#x / wed_sales2#x), 2) AS round((wed_sales1 / wed_sales2), 2)#x, round((thu_sales1#x / thu_sales2#x), 2) AS round((thu_sales1 / thu_sales2), 2)#x, round((fri_sales1#x / fri_sales2#x), 2) AS round((fri_sales1 / fri_sales2), 2)#x, round((sat_sales1#x / sat_sales2#x), 2) AS round((sat_sales1 / sat_sales2), 2)#x]
      +- Filter (d_week_seq1#x = (d_week_seq2#x - 53))
         +- Join Inner
            :- SubqueryAlias y
            :  +- Project [d_week_seq#x AS d_week_seq1#x, sun_sales#x AS sun_sales1#x, mon_sales#x AS mon_sales1#x, tue_sales#x AS tue_sales1#x, wed_sales#x AS wed_sales1#x, thu_sales#x AS thu_sales1#x, fri_sales#x AS fri_sales1#x, sat_sales#x AS sat_sales1#x]
            :     +- Filter ((d_week_seq#x = d_week_seq#x) AND (d_year#x = 2001))
            :        +- Join Inner
            :           :- SubqueryAlias wswscs
            :           :  +- CTERelationRef xxxx, true, [d_week_seq#x, sun_sales#x, mon_sales#x, tue_sales#x, wed_sales#x, thu_sales#x, fri_sales#x, sat_sales#x], false
            :           +- SubqueryAlias spark_catalog.default.date_dim
            :              +- Project [d_date_sk#x, static_invoke(CharVarcharCodegenUtils.readSidePadding(d_date_id#x, 16)) AS d_date_id#x, d_date#x, d_month_seq#x, d_week_seq#x, d_quarter_seq#x, d_year#x, d_dow#x, d_moy#x, d_dom#x, d_qoy#x, d_fy_year#x, d_fy_quarter_seq#x, d_fy_week_seq#x, static_invoke(CharVarcharCodegenUtils.readSidePadding(d_day_name#x, 9)) AS d_day_name#x, static_invoke(CharVarcharCodegenUtils.readSidePadding(d_quarter_name#x, 6)) AS d_quarter_name#x, static_invoke(CharVarcharCodegenUtils.readSidePadding(d_holiday#x, 1)) AS d_holiday#x, static_invoke(CharVarcharCodegenUtils.readSidePadding(d_weekend#x, 1)) AS d_weekend#x, static_invoke(CharVarcharCodegenUtils.readSidePadding(d_following_holiday#x, 1)) AS d_following_holiday#x, d_first_dom#x, d_last_dom#x, d_same_day_ly#x, d_same_day_lq#x, static_invoke(CharVarcharCodegenUtils.readSidePadding(d_current_day#x, 1)) AS d_current_day#x, ... 4 more fields]
            :                 +- Relation spark_catalog.default.date_dim[d_date_sk#x,d_date_id#x,d_date#x,d_month_seq#x,d_week_seq#x,d_quarter_seq#x,d_year#x,d_dow#x,d_moy#x,d_dom#x,d_qoy#x,d_fy_year#x,d_fy_quarter_seq#x,d_fy_week_seq#x,d_day_name#x,d_quarter_name#x,d_holiday#x,d_weekend#x,d_following_holiday#x,d_first_dom#x,d_last_dom#x,d_same_day_ly#x,d_same_day_lq#x,d_current_day#x,... 4 more fields] parquet
            +- SubqueryAlias z
               +- Project [d_week_seq#x AS d_week_seq2#x, sun_sales#x AS sun_sales2#x, mon_sales#x AS mon_sales2#x, tue_sales#x AS tue_sales2#x, wed_sales#x AS wed_sales2#x, thu_sales#x AS thu_sales2#x, fri_sales#x AS fri_sales2#x, sat_sales#x AS sat_sales2#x]
                  +- Filter ((d_week_seq#x = d_week_seq#x) AND (d_year#x = (2001 + 1)))
                     +- Join Inner
                        :- SubqueryAlias wswscs
                        :  +- CTERelationRef xxxx, true, [d_week_seq#x, sun_sales#x, mon_sales#x, tue_sales#x, wed_sales#x, thu_sales#x, fri_sales#x, sat_sales#x], false
                        +- SubqueryAlias spark_catalog.default.date_dim
                           +- Project [d_date_sk#x, static_invoke(CharVarcharCodegenUtils.readSidePadding(d_date_id#x, 16)) AS d_date_id#x, d_date#x, d_month_seq#x, d_week_seq#x, d_quarter_seq#x, d_year#x, d_dow#x, d_moy#x, d_dom#x, d_qoy#x, d_fy_year#x, d_fy_quarter_seq#x, d_fy_week_seq#x, static_invoke(CharVarcharCodegenUtils.readSidePadding(d_day_name#x, 9)) AS d_day_name#x, static_invoke(CharVarcharCodegenUtils.readSidePadding(d_quarter_name#x, 6)) AS d_quarter_name#x, static_invoke(CharVarcharCodegenUtils.readSidePadding(d_holiday#x, 1)) AS d_holiday#x, static_invoke(CharVarcharCodegenUtils.readSidePadding(d_weekend#x, 1)) AS d_weekend#x, static_invoke(CharVarcharCodegenUtils.readSidePadding(d_following_holiday#x, 1)) AS d_following_holiday#x, d_first_dom#x, d_last_dom#x, d_same_day_ly#x, d_same_day_lq#x, static_invoke(CharVarcharCodegenUtils.readSidePadding(d_current_day#x, 1)) AS d_current_day#x, ... 4 more fields]
                              +- Relation spark_catalog.default.date_dim[d_date_sk#x,d_date_id#x,d_date#x,d_month_seq#x,d_week_seq#x,d_quarter_seq#x,d_year#x,d_dow#x,d_moy#x,d_dom#x,d_qoy#x,d_fy_year#x,d_fy_quarter_seq#x,d_fy_week_seq#x,d_day_name#x,d_quarter_name#x,d_holiday#x,d_weekend#x,d_following_holiday#x,d_first_dom#x,d_last_dom#x,d_same_day_ly#x,d_same_day_lq#x,d_current_day#x,... 4 more fields] parquet


-- !query
with wscs as
  (table web_sales
  |> select
       ws_sold_date_sk sold_date_sk,
       ws_ext_sales_price sales_price
  |> as x
  |> union all (
       table catalog_sales
       |> select
            cs_sold_date_sk sold_date_sk,
            cs_ext_sales_price sales_price)
  |> select
       sold_date_sk,
       sales_price),
wswscs as
  (table wscs
  |> join date_dim
  |> where d_date_sk = sold_date_sk
  |> aggregate
      sum(case when (d_day_name = 'sunday')
        then sales_price
          else null end)
      sun_sales,
      sum(case when (d_day_name = 'monday')
        then sales_price
          else null end)
      mon_sales,
      sum(case when (d_day_name = 'tuesday')
        then sales_price
          else null end)
      tue_sales,
      sum(case when (d_day_name = 'wednesday')
        then sales_price
          else null end)
      wed_sales,
      sum(case when (d_day_name = 'thursday')
        then sales_price
          else null end)
      thu_sales,
      sum(case when (d_day_name = 'friday')
        then sales_price
          else null end)
      fri_sales,
      sum(case when (d_day_name = 'saturday')
        then sales_price
          else null end)
      sat_sales
      group by d_week_seq)
table wswscs
|> join date_dim
|> where date_dim.d_week_seq = wswscs.d_week_seq AND d_year = 2001
|> select
     wswscs.d_week_seq d_week_seq1,
     sun_sales sun_sales1,
     mon_sales mon_sales1,
     tue_sales tue_sales1,
     wed_sales wed_sales1,
     thu_sales thu_sales1,
     fri_sales fri_sales1,
     sat_sales sat_sales1
|> as y
|> join (
     table wswscs
     |> join date_dim
     |> where date_dim.d_week_seq = wswscs.d_week_seq AND d_year = 2001 + 1
     |> select
          wswscs.d_week_seq d_week_seq2,
          sun_sales sun_sales2,
          mon_sales mon_sales2,
          tue_sales tue_sales2,
          wed_sales wed_sales2,
          thu_sales thu_sales2,
          fri_sales fri_sales2,
          sat_sales sat_sales2
     |> as z)
|> where d_week_seq1 = d_week_seq2 - 53
|> order by d_week_seq1
|> select
     d_week_seq1,
     round(sun_sales1 / sun_sales2, 2),
     round(mon_sales1 / mon_sales2, 2),
     round(tue_sales1 / tue_sales2, 2),
     round(wed_sales1 / wed_sales2, 2),
     round(thu_sales1 / thu_sales2, 2),
     round(fri_sales1 / fri_sales2, 2),
     round(sat_sales1 / sat_sales2, 2)
-- !query analysis
WithCTE
:- CTERelationDef xxxx, false
:  +- SubqueryAlias wscs
:     +- Union false, false
:        :- SubqueryAlias x
:        :  +- Project [pipeexpression(ws_sold_date_sk#x, false, SELECT) AS sold_date_sk#x, pipeexpression(ws_ext_sales_price#x, false, SELECT) AS sales_price#x]
:        :     +- SubqueryAlias spark_catalog.default.web_sales
:        :        +- Relation spark_catalog.default.web_sales[ws_sold_date_sk#x,ws_sold_time_sk#x,ws_ship_date_sk#x,ws_item_sk#x,ws_bill_customer_sk#x,ws_bill_cdemo_sk#x,ws_bill_hdemo_sk#x,ws_bill_addr_sk#x,ws_ship_customer_sk#x,ws_ship_cdemo_sk#x,ws_ship_hdemo_sk#x,ws_ship_addr_sk#x,ws_web_page_sk#x,ws_web_site_sk#x,ws_ship_mode_sk#x,ws_warehouse_sk#x,ws_promo_sk#x,ws_order_number#x,ws_quantity#x,ws_wholesale_cost#x,ws_list_price#x,ws_sales_price#x,ws_ext_discount_amt#x,ws_ext_sales_price#x,... 10 more fields] parquet
:        +- Project [sold_date_sk#x, sales_price#x]
:           +- Project [pipeexpression(cs_sold_date_sk#x, false, SELECT) AS sold_date_sk#x, pipeexpression(cs_ext_sales_price#x, false, SELECT) AS sales_price#x]
:              +- SubqueryAlias spark_catalog.default.catalog_sales
:                 +- Relation spark_catalog.default.catalog_sales[cs_sold_date_sk#x,cs_sold_time_sk#x,cs_ship_date_sk#x,cs_bill_customer_sk#x,cs_bill_cdemo_sk#x,cs_bill_hdemo_sk#x,cs_bill_addr_sk#x,cs_ship_customer_sk#x,cs_ship_cdemo_sk#x,cs_ship_hdemo_sk#x,cs_ship_addr_sk#x,cs_call_center_sk#x,cs_catalog_page_sk#x,cs_ship_mode_sk#x,cs_warehouse_sk#x,cs_item_sk#x,cs_promo_sk#x,cs_order_number#x,cs_quantity#x,cs_wholesale_cost#x,cs_list_price#x,cs_sales_price#x,cs_ext_discount_amt#x,cs_ext_sales_price#x,... 10 more fields] parquet
:- CTERelationDef xxxx, false
:  +- SubqueryAlias wswscs
:     +- Aggregate [d_week_seq#x], [d_week_seq#x, pipeexpression(sum(CASE WHEN (d_day_name#x = rpad(sunday, 9,  )) THEN sales_price#x ELSE cast(null as decimal(7,2)) END), true, AGGREGATE) AS sun_sales#x, pipeexpression(sum(CASE WHEN (d_day_name#x = rpad(monday, 9,  )) THEN sales_price#x ELSE cast(null as decimal(7,2)) END), true, AGGREGATE) AS mon_sales#x, pipeexpression(sum(CASE WHEN (d_day_name#x = rpad(tuesday, 9,  )) THEN sales_price#x ELSE cast(null as decimal(7,2)) END), true, AGGREGATE) AS tue_sales#x, pipeexpression(sum(CASE WHEN (d_day_name#x = wednesday) THEN sales_price#x ELSE cast(null as decimal(7,2)) END), true, AGGREGATE) AS wed_sales#x, pipeexpression(sum(CASE WHEN (d_day_name#x = rpad(thursday, 9,  )) THEN sales_price#x ELSE cast(null as decimal(7,2)) END), true, AGGREGATE) AS thu_sales#x, pipeexpression(sum(CASE WHEN (d_day_name#x = rpad(friday, 9,  )) THEN sales_price#x ELSE cast(null as decimal(7,2)) END), true, AGGREGATE) AS fri_sales#x, pipeexpression(sum(CASE WHEN (d_day_name#x = rpad(saturday, 9,  )) THEN sales_price#x ELSE cast(null as decimal(7,2)) END), true, AGGREGATE) AS sat_sales#x]
:        +- Filter (d_date_sk#x = sold_date_sk#x)
:           +- Join Inner
:              :- SubqueryAlias wscs
:              :  +- CTERelationRef xxxx, true, [sold_date_sk#x, sales_price#x], false
:              +- SubqueryAlias spark_catalog.default.date_dim
:                 +- Project [d_date_sk#x, static_invoke(CharVarcharCodegenUtils.readSidePadding(d_date_id#x, 16)) AS d_date_id#x, d_date#x, d_month_seq#x, d_week_seq#x, d_quarter_seq#x, d_year#x, d_dow#x, d_moy#x, d_dom#x, d_qoy#x, d_fy_year#x, d_fy_quarter_seq#x, d_fy_week_seq#x, static_invoke(CharVarcharCodegenUtils.readSidePadding(d_day_name#x, 9)) AS d_day_name#x, static_invoke(CharVarcharCodegenUtils.readSidePadding(d_quarter_name#x, 6)) AS d_quarter_name#x, static_invoke(CharVarcharCodegenUtils.readSidePadding(d_holiday#x, 1)) AS d_holiday#x, static_invoke(CharVarcharCodegenUtils.readSidePadding(d_weekend#x, 1)) AS d_weekend#x, static_invoke(CharVarcharCodegenUtils.readSidePadding(d_following_holiday#x, 1)) AS d_following_holiday#x, d_first_dom#x, d_last_dom#x, d_same_day_ly#x, d_same_day_lq#x, static_invoke(CharVarcharCodegenUtils.readSidePadding(d_current_day#x, 1)) AS d_current_day#x, ... 4 more fields]
:                    +- Relation spark_catalog.default.date_dim[d_date_sk#x,d_date_id#x,d_date#x,d_month_seq#x,d_week_seq#x,d_quarter_seq#x,d_year#x,d_dow#x,d_moy#x,d_dom#x,d_qoy#x,d_fy_year#x,d_fy_quarter_seq#x,d_fy_week_seq#x,d_day_name#x,d_quarter_name#x,d_holiday#x,d_weekend#x,d_following_holiday#x,d_first_dom#x,d_last_dom#x,d_same_day_ly#x,d_same_day_lq#x,d_current_day#x,... 4 more fields] parquet
+- Project [d_week_seq1#x, round((sun_sales1#x / sun_sales2#x), 2) AS round((sun_sales1 / sun_sales2), 2)#x, round((mon_sales1#x / mon_sales2#x), 2) AS round((mon_sales1 / mon_sales2), 2)#x, round((tue_sales1#x / tue_sales2#x), 2) AS round((tue_sales1 / tue_sales2), 2)#x, round((wed_sales1#x / wed_sales2#x), 2) AS round((wed_sales1 / wed_sales2), 2)#x, round((thu_sales1#x / thu_sales2#x), 2) AS round((thu_sales1 / thu_sales2), 2)#x, round((fri_sales1#x / fri_sales2#x), 2) AS round((fri_sales1 / fri_sales2), 2)#x, round((sat_sales1#x / sat_sales2#x), 2) AS round((sat_sales1 / sat_sales2), 2)#x]
   +- Sort [d_week_seq1#x ASC NULLS FIRST], true
      +- Filter (d_week_seq1#x = (d_week_seq2#x - 53))
         +- Join Inner
            :- SubqueryAlias y
            :  +- Project [pipeexpression(d_week_seq#x, false, SELECT) AS d_week_seq1#x, pipeexpression(sun_sales#x, false, SELECT) AS sun_sales1#x, pipeexpression(mon_sales#x, false, SELECT) AS mon_sales1#x, pipeexpression(tue_sales#x, false, SELECT) AS tue_sales1#x, pipeexpression(wed_sales#x, false, SELECT) AS wed_sales1#x, pipeexpression(thu_sales#x, false, SELECT) AS thu_sales1#x, pipeexpression(fri_sales#x, false, SELECT) AS fri_sales1#x, pipeexpression(sat_sales#x, false, SELECT) AS sat_sales1#x]
            :     +- Filter ((d_week_seq#x = d_week_seq#x) AND (d_year#x = 2001))
            :        +- Join Inner
            :           :- SubqueryAlias wswscs
            :           :  +- CTERelationRef xxxx, true, [d_week_seq#x, sun_sales#x, mon_sales#x, tue_sales#x, wed_sales#x, thu_sales#x, fri_sales#x, sat_sales#x], false
            :           +- SubqueryAlias spark_catalog.default.date_dim
            :              +- Project [d_date_sk#x, static_invoke(CharVarcharCodegenUtils.readSidePadding(d_date_id#x, 16)) AS d_date_id#x, d_date#x, d_month_seq#x, d_week_seq#x, d_quarter_seq#x, d_year#x, d_dow#x, d_moy#x, d_dom#x, d_qoy#x, d_fy_year#x, d_fy_quarter_seq#x, d_fy_week_seq#x, static_invoke(CharVarcharCodegenUtils.readSidePadding(d_day_name#x, 9)) AS d_day_name#x, static_invoke(CharVarcharCodegenUtils.readSidePadding(d_quarter_name#x, 6)) AS d_quarter_name#x, static_invoke(CharVarcharCodegenUtils.readSidePadding(d_holiday#x, 1)) AS d_holiday#x, static_invoke(CharVarcharCodegenUtils.readSidePadding(d_weekend#x, 1)) AS d_weekend#x, static_invoke(CharVarcharCodegenUtils.readSidePadding(d_following_holiday#x, 1)) AS d_following_holiday#x, d_first_dom#x, d_last_dom#x, d_same_day_ly#x, d_same_day_lq#x, static_invoke(CharVarcharCodegenUtils.readSidePadding(d_current_day#x, 1)) AS d_current_day#x, ... 4 more fields]
            :                 +- Relation spark_catalog.default.date_dim[d_date_sk#x,d_date_id#x,d_date#x,d_month_seq#x,d_week_seq#x,d_quarter_seq#x,d_year#x,d_dow#x,d_moy#x,d_dom#x,d_qoy#x,d_fy_year#x,d_fy_quarter_seq#x,d_fy_week_seq#x,d_day_name#x,d_quarter_name#x,d_holiday#x,d_weekend#x,d_following_holiday#x,d_first_dom#x,d_last_dom#x,d_same_day_ly#x,d_same_day_lq#x,d_current_day#x,... 4 more fields] parquet
            +- SubqueryAlias __auto_generated_subquery_name
               +- SubqueryAlias z
                  +- Project [pipeexpression(d_week_seq#x, false, SELECT) AS d_week_seq2#x, pipeexpression(sun_sales#x, false, SELECT) AS sun_sales2#x, pipeexpression(mon_sales#x, false, SELECT) AS mon_sales2#x, pipeexpression(tue_sales#x, false, SELECT) AS tue_sales2#x, pipeexpression(wed_sales#x, false, SELECT) AS wed_sales2#x, pipeexpression(thu_sales#x, false, SELECT) AS thu_sales2#x, pipeexpression(fri_sales#x, false, SELECT) AS fri_sales2#x, pipeexpression(sat_sales#x, false, SELECT) AS sat_sales2#x]
                     +- Filter ((d_week_seq#x = d_week_seq#x) AND (d_year#x = (2001 + 1)))
                        +- Join Inner
                           :- SubqueryAlias wswscs
                           :  +- CTERelationRef xxxx, true, [d_week_seq#x, sun_sales#x, mon_sales#x, tue_sales#x, wed_sales#x, thu_sales#x, fri_sales#x, sat_sales#x], false
                           +- SubqueryAlias spark_catalog.default.date_dim
                              +- Project [d_date_sk#x, static_invoke(CharVarcharCodegenUtils.readSidePadding(d_date_id#x, 16)) AS d_date_id#x, d_date#x, d_month_seq#x, d_week_seq#x, d_quarter_seq#x, d_year#x, d_dow#x, d_moy#x, d_dom#x, d_qoy#x, d_fy_year#x, d_fy_quarter_seq#x, d_fy_week_seq#x, static_invoke(CharVarcharCodegenUtils.readSidePadding(d_day_name#x, 9)) AS d_day_name#x, static_invoke(CharVarcharCodegenUtils.readSidePadding(d_quarter_name#x, 6)) AS d_quarter_name#x, static_invoke(CharVarcharCodegenUtils.readSidePadding(d_holiday#x, 1)) AS d_holiday#x, static_invoke(CharVarcharCodegenUtils.readSidePadding(d_weekend#x, 1)) AS d_weekend#x, static_invoke(CharVarcharCodegenUtils.readSidePadding(d_following_holiday#x, 1)) AS d_following_holiday#x, d_first_dom#x, d_last_dom#x, d_same_day_ly#x, d_same_day_lq#x, static_invoke(CharVarcharCodegenUtils.readSidePadding(d_current_day#x, 1)) AS d_current_day#x, ... 4 more fields]
                                 +- Relation spark_catalog.default.date_dim[d_date_sk#x,d_date_id#x,d_date#x,d_month_seq#x,d_week_seq#x,d_quarter_seq#x,d_year#x,d_dow#x,d_moy#x,d_dom#x,d_qoy#x,d_fy_year#x,d_fy_quarter_seq#x,d_fy_week_seq#x,d_day_name#x,d_quarter_name#x,d_holiday#x,d_weekend#x,d_following_holiday#x,d_first_dom#x,d_last_dom#x,d_same_day_ly#x,d_same_day_lq#x,d_current_day#x,... 4 more fields] parquet


-- !query
select
  dt.d_year,
  item.i_brand_id brand_id,
  item.i_brand brand,
  sum(ss_ext_sales_price) sum_agg
from date_dim dt, store_sales, item
where dt.d_date_sk = store_sales.ss_sold_date_sk
  and store_sales.ss_item_sk = item.i_item_sk
  and item.i_manufact_id = 128
  and dt.d_moy = 11
group by dt.d_year, item.i_brand, item.i_brand_id
order by dt.d_year, sum_agg desc, brand_id
limit 100
-- !query analysis
GlobalLimit 100
+- LocalLimit 100
   +- Sort [d_year#x ASC NULLS FIRST, sum_agg#x DESC NULLS LAST, brand_id#x ASC NULLS FIRST], true
      +- Aggregate [d_year#x, i_brand#x, i_brand_id#x], [d_year#x, i_brand_id#x AS brand_id#x, i_brand#x AS brand#x, sum(ss_ext_sales_price#x) AS sum_agg#x]
         +- Filter (((d_date_sk#x = ss_sold_date_sk#x) AND (ss_item_sk#x = i_item_sk#x)) AND ((i_manufact_id#x = 128) AND (d_moy#x = 11)))
            +- Join Inner
               :- Join Inner
               :  :- SubqueryAlias dt
               :  :  +- SubqueryAlias spark_catalog.default.date_dim
               :  :     +- Project [d_date_sk#x, static_invoke(CharVarcharCodegenUtils.readSidePadding(d_date_id#x, 16)) AS d_date_id#x, d_date#x, d_month_seq#x, d_week_seq#x, d_quarter_seq#x, d_year#x, d_dow#x, d_moy#x, d_dom#x, d_qoy#x, d_fy_year#x, d_fy_quarter_seq#x, d_fy_week_seq#x, static_invoke(CharVarcharCodegenUtils.readSidePadding(d_day_name#x, 9)) AS d_day_name#x, static_invoke(CharVarcharCodegenUtils.readSidePadding(d_quarter_name#x, 6)) AS d_quarter_name#x, static_invoke(CharVarcharCodegenUtils.readSidePadding(d_holiday#x, 1)) AS d_holiday#x, static_invoke(CharVarcharCodegenUtils.readSidePadding(d_weekend#x, 1)) AS d_weekend#x, static_invoke(CharVarcharCodegenUtils.readSidePadding(d_following_holiday#x, 1)) AS d_following_holiday#x, d_first_dom#x, d_last_dom#x, d_same_day_ly#x, d_same_day_lq#x, static_invoke(CharVarcharCodegenUtils.readSidePadding(d_current_day#x, 1)) AS d_current_day#x, ... 4 more fields]
               :  :        +- Relation spark_catalog.default.date_dim[d_date_sk#x,d_date_id#x,d_date#x,d_month_seq#x,d_week_seq#x,d_quarter_seq#x,d_year#x,d_dow#x,d_moy#x,d_dom#x,d_qoy#x,d_fy_year#x,d_fy_quarter_seq#x,d_fy_week_seq#x,d_day_name#x,d_quarter_name#x,d_holiday#x,d_weekend#x,d_following_holiday#x,d_first_dom#x,d_last_dom#x,d_same_day_ly#x,d_same_day_lq#x,d_current_day#x,... 4 more fields] parquet
               :  +- SubqueryAlias spark_catalog.default.store_sales
               :     +- Relation spark_catalog.default.store_sales[ss_sold_date_sk#x,ss_sold_time_sk#x,ss_item_sk#x,ss_customer_sk#x,ss_cdemo_sk#x,ss_hdemo_sk#x,ss_addr_sk#x,ss_store_sk#x,ss_promo_sk#x,ss_ticket_number#x,ss_quantity#x,ss_wholesale_cost#x,ss_list_price#x,ss_sales_price#x,ss_ext_discount_amt#x,ss_ext_sales_price#x,ss_ext_wholesale_cost#x,ss_ext_list_price#x,ss_ext_tax#x,ss_coupon_amt#x,ss_net_paid#x,ss_net_paid_inc_tax#x,ss_net_profit#x] parquet
               +- SubqueryAlias spark_catalog.default.item
                  +- Project [i_item_sk#x, static_invoke(CharVarcharCodegenUtils.readSidePadding(i_item_id#x, 16)) AS i_item_id#x, i_rec_start_date#x, i_rec_end_date#x, i_item_desc#x, i_current_price#x, i_wholesale_cost#x, i_brand_id#x, static_invoke(CharVarcharCodegenUtils.readSidePadding(i_brand#x, 50)) AS i_brand#x, i_class_id#x, static_invoke(CharVarcharCodegenUtils.readSidePadding(i_class#x, 50)) AS i_class#x, i_category_id#x, static_invoke(CharVarcharCodegenUtils.readSidePadding(i_category#x, 50)) AS i_category#x, i_manufact_id#x, static_invoke(CharVarcharCodegenUtils.readSidePadding(i_manufact#x, 50)) AS i_manufact#x, static_invoke(CharVarcharCodegenUtils.readSidePadding(i_size#x, 20)) AS i_size#x, static_invoke(CharVarcharCodegenUtils.readSidePadding(i_formulation#x, 20)) AS i_formulation#x, static_invoke(CharVarcharCodegenUtils.readSidePadding(i_color#x, 20)) AS i_color#x, static_invoke(CharVarcharCodegenUtils.readSidePadding(i_units#x, 10)) AS i_units#x, static_invoke(CharVarcharCodegenUtils.readSidePadding(i_container#x, 10)) AS i_container#x, i_manager_id#x, static_invoke(CharVarcharCodegenUtils.readSidePadding(i_product_name#x, 50)) AS i_product_name#x]
                     +- Relation spark_catalog.default.item[i_item_sk#x,i_item_id#x,i_rec_start_date#x,i_rec_end_date#x,i_item_desc#x,i_current_price#x,i_wholesale_cost#x,i_brand_id#x,i_brand#x,i_class_id#x,i_class#x,i_category_id#x,i_category#x,i_manufact_id#x,i_manufact#x,i_size#x,i_formulation#x,i_color#x,i_units#x,i_container#x,i_manager_id#x,i_product_name#x] parquet


-- !query
table date_dim
|> as dt
|> join store_sales
|> join item
|> where dt.d_date_sk = store_sales.ss_sold_date_sk
     and store_sales.ss_item_sk = item.i_item_sk
     and item.i_manufact_id = 128
     and dt.d_moy = 11
|> aggregate sum(ss_ext_sales_price) sum_agg
     group by dt.d_year d_year, item.i_brand_id brand_id, item.i_brand brand
|> order by d_year, sum_agg desc, brand_id
|> limit 100
-- !query analysis
GlobalLimit 100
+- LocalLimit 100
   +- SubqueryAlias __auto_generated_subquery_name
      +- Sort [d_year#x ASC NULLS FIRST, sum_agg#x DESC NULLS LAST, brand_id#x ASC NULLS FIRST], true
         +- SubqueryAlias __auto_generated_subquery_name
            +- Aggregate [d_year#x, i_brand_id#x, i_brand#x], [d_year#x AS d_year#x, i_brand_id#x AS brand_id#x, i_brand#x AS brand#x, pipeexpression(sum(ss_ext_sales_price#x), true, AGGREGATE) AS sum_agg#x]
               +- Filter (((d_date_sk#x = ss_sold_date_sk#x) AND (ss_item_sk#x = i_item_sk#x)) AND ((i_manufact_id#x = 128) AND (d_moy#x = 11)))
                  +- Join Inner
                     :- Join Inner
                     :  :- SubqueryAlias dt
                     :  :  +- SubqueryAlias spark_catalog.default.date_dim
                     :  :     +- Project [d_date_sk#x, static_invoke(CharVarcharCodegenUtils.readSidePadding(d_date_id#x, 16)) AS d_date_id#x, d_date#x, d_month_seq#x, d_week_seq#x, d_quarter_seq#x, d_year#x, d_dow#x, d_moy#x, d_dom#x, d_qoy#x, d_fy_year#x, d_fy_quarter_seq#x, d_fy_week_seq#x, static_invoke(CharVarcharCodegenUtils.readSidePadding(d_day_name#x, 9)) AS d_day_name#x, static_invoke(CharVarcharCodegenUtils.readSidePadding(d_quarter_name#x, 6)) AS d_quarter_name#x, static_invoke(CharVarcharCodegenUtils.readSidePadding(d_holiday#x, 1)) AS d_holiday#x, static_invoke(CharVarcharCodegenUtils.readSidePadding(d_weekend#x, 1)) AS d_weekend#x, static_invoke(CharVarcharCodegenUtils.readSidePadding(d_following_holiday#x, 1)) AS d_following_holiday#x, d_first_dom#x, d_last_dom#x, d_same_day_ly#x, d_same_day_lq#x, static_invoke(CharVarcharCodegenUtils.readSidePadding(d_current_day#x, 1)) AS d_current_day#x, ... 4 more fields]
                     :  :        +- Relation spark_catalog.default.date_dim[d_date_sk#x,d_date_id#x,d_date#x,d_month_seq#x,d_week_seq#x,d_quarter_seq#x,d_year#x,d_dow#x,d_moy#x,d_dom#x,d_qoy#x,d_fy_year#x,d_fy_quarter_seq#x,d_fy_week_seq#x,d_day_name#x,d_quarter_name#x,d_holiday#x,d_weekend#x,d_following_holiday#x,d_first_dom#x,d_last_dom#x,d_same_day_ly#x,d_same_day_lq#x,d_current_day#x,... 4 more fields] parquet
                     :  +- SubqueryAlias spark_catalog.default.store_sales
                     :     +- Relation spark_catalog.default.store_sales[ss_sold_date_sk#x,ss_sold_time_sk#x,ss_item_sk#x,ss_customer_sk#x,ss_cdemo_sk#x,ss_hdemo_sk#x,ss_addr_sk#x,ss_store_sk#x,ss_promo_sk#x,ss_ticket_number#x,ss_quantity#x,ss_wholesale_cost#x,ss_list_price#x,ss_sales_price#x,ss_ext_discount_amt#x,ss_ext_sales_price#x,ss_ext_wholesale_cost#x,ss_ext_list_price#x,ss_ext_tax#x,ss_coupon_amt#x,ss_net_paid#x,ss_net_paid_inc_tax#x,ss_net_profit#x] parquet
                     +- SubqueryAlias spark_catalog.default.item
                        +- Project [i_item_sk#x, static_invoke(CharVarcharCodegenUtils.readSidePadding(i_item_id#x, 16)) AS i_item_id#x, i_rec_start_date#x, i_rec_end_date#x, i_item_desc#x, i_current_price#x, i_wholesale_cost#x, i_brand_id#x, static_invoke(CharVarcharCodegenUtils.readSidePadding(i_brand#x, 50)) AS i_brand#x, i_class_id#x, static_invoke(CharVarcharCodegenUtils.readSidePadding(i_class#x, 50)) AS i_class#x, i_category_id#x, static_invoke(CharVarcharCodegenUtils.readSidePadding(i_category#x, 50)) AS i_category#x, i_manufact_id#x, static_invoke(CharVarcharCodegenUtils.readSidePadding(i_manufact#x, 50)) AS i_manufact#x, static_invoke(CharVarcharCodegenUtils.readSidePadding(i_size#x, 20)) AS i_size#x, static_invoke(CharVarcharCodegenUtils.readSidePadding(i_formulation#x, 20)) AS i_formulation#x, static_invoke(CharVarcharCodegenUtils.readSidePadding(i_color#x, 20)) AS i_color#x, static_invoke(CharVarcharCodegenUtils.readSidePadding(i_units#x, 10)) AS i_units#x, static_invoke(CharVarcharCodegenUtils.readSidePadding(i_container#x, 10)) AS i_container#x, i_manager_id#x, static_invoke(CharVarcharCodegenUtils.readSidePadding(i_product_name#x, 50)) AS i_product_name#x]
                           +- Relation spark_catalog.default.item[i_item_sk#x,i_item_id#x,i_rec_start_date#x,i_rec_end_date#x,i_item_desc#x,i_current_price#x,i_wholesale_cost#x,i_brand_id#x,i_brand#x,i_class_id#x,i_class#x,i_category_id#x,i_category#x,i_manufact_id#x,i_manufact#x,i_size#x,i_formulation#x,i_color#x,i_units#x,i_container#x,i_manager_id#x,i_product_name#x] parquet


-- !query
drop table t
-- !query analysis
DropTable false, false
+- ResolvedIdentifier V2SessionCatalog(spark_catalog), default.t


-- !query
drop table other
-- !query analysis
DropTable false, false
+- ResolvedIdentifier V2SessionCatalog(spark_catalog), default.other


-- !query
drop table st
-- !query analysis
DropTable false, false
+- ResolvedIdentifier V2SessionCatalog(spark_catalog), default.st


-- !query
drop table call_center
-- !query analysis
DropTable false, false
+- ResolvedIdentifier V2SessionCatalog(spark_catalog), default.call_center


-- !query
drop table catalog_page
-- !query analysis
DropTable false, false
+- ResolvedIdentifier V2SessionCatalog(spark_catalog), default.catalog_page


-- !query
drop table catalog_returns
-- !query analysis
DropTable false, false
+- ResolvedIdentifier V2SessionCatalog(spark_catalog), default.catalog_returns


-- !query
drop table catalog_sales
-- !query analysis
DropTable false, false
+- ResolvedIdentifier V2SessionCatalog(spark_catalog), default.catalog_sales


-- !query
drop table customer
-- !query analysis
DropTable false, false
+- ResolvedIdentifier V2SessionCatalog(spark_catalog), default.customer


-- !query
drop table customer_address
-- !query analysis
DropTable false, false
+- ResolvedIdentifier V2SessionCatalog(spark_catalog), default.customer_address


-- !query
drop table customer_demographics
-- !query analysis
DropTable false, false
+- ResolvedIdentifier V2SessionCatalog(spark_catalog), default.customer_demographics


-- !query
drop table date_dim
-- !query analysis
DropTable false, false
+- ResolvedIdentifier V2SessionCatalog(spark_catalog), default.date_dim


-- !query
drop table household_demographics
-- !query analysis
DropTable false, false
+- ResolvedIdentifier V2SessionCatalog(spark_catalog), default.household_demographics


-- !query
drop table income_band
-- !query analysis
DropTable false, false
+- ResolvedIdentifier V2SessionCatalog(spark_catalog), default.income_band


-- !query
drop table inventory
-- !query analysis
DropTable false, false
+- ResolvedIdentifier V2SessionCatalog(spark_catalog), default.inventory


-- !query
drop table item
-- !query analysis
DropTable false, false
+- ResolvedIdentifier V2SessionCatalog(spark_catalog), default.item


-- !query
drop table promotion
-- !query analysis
DropTable false, false
+- ResolvedIdentifier V2SessionCatalog(spark_catalog), default.promotion


-- !query
drop table reason
-- !query analysis
DropTable false, false
+- ResolvedIdentifier V2SessionCatalog(spark_catalog), default.reason


-- !query
drop table ship_mode
-- !query analysis
DropTable false, false
+- ResolvedIdentifier V2SessionCatalog(spark_catalog), default.ship_mode


-- !query
drop table store
-- !query analysis
DropTable false, false
+- ResolvedIdentifier V2SessionCatalog(spark_catalog), default.store


-- !query
drop table store_returns
-- !query analysis
DropTable false, false
+- ResolvedIdentifier V2SessionCatalog(spark_catalog), default.store_returns


-- !query
drop table store_sales
-- !query analysis
DropTable false, false
+- ResolvedIdentifier V2SessionCatalog(spark_catalog), default.store_sales


-- !query
drop table time_dim
-- !query analysis
DropTable false, false
+- ResolvedIdentifier V2SessionCatalog(spark_catalog), default.time_dim


-- !query
drop table warehouse
-- !query analysis
DropTable false, false
+- ResolvedIdentifier V2SessionCatalog(spark_catalog), default.warehouse


-- !query
drop table web_page
-- !query analysis
DropTable false, false
+- ResolvedIdentifier V2SessionCatalog(spark_catalog), default.web_page


-- !query
drop table web_returns
-- !query analysis
DropTable false, false
+- ResolvedIdentifier V2SessionCatalog(spark_catalog), default.web_returns


-- !query
drop table web_sales
-- !query analysis
DropTable false, false
+- ResolvedIdentifier V2SessionCatalog(spark_catalog), default.web_sales


-- !query
drop table web_site
-- !query analysis
DropTable false, false
+- ResolvedIdentifier V2SessionCatalog(spark_catalog), default.web_site
