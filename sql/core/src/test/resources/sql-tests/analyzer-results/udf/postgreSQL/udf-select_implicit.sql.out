-- Automatically generated by SQLQueryTestSuite
-- !query
CREATE TABLE test_missing_target (a int, b int, c string, d string) using parquet
-- !query analysis
CreateDataSourceTableCommand `spark_catalog`.`default`.`test_missing_target`, false


-- !query
INSERT INTO test_missing_target VALUES (0, 1, 'XXXX', 'A')
-- !query analysis
InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/test_missing_target, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/test_missing_target], Append, `spark_catalog`.`default`.`test_missing_target`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/test_missing_target), [a, b, c, d]
+- Project [cast(col1#x as int) AS a#x, cast(col2#x as int) AS b#x, cast(col3#x as string) AS c#x, cast(col4#x as string) AS d#x]
   +- LocalRelation [col1#x, col2#x, col3#x, col4#x]


-- !query
INSERT INTO test_missing_target VALUES (1, 2, 'ABAB', 'b')
-- !query analysis
InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/test_missing_target, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/test_missing_target], Append, `spark_catalog`.`default`.`test_missing_target`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/test_missing_target), [a, b, c, d]
+- Project [cast(col1#x as int) AS a#x, cast(col2#x as int) AS b#x, cast(col3#x as string) AS c#x, cast(col4#x as string) AS d#x]
   +- LocalRelation [col1#x, col2#x, col3#x, col4#x]


-- !query
INSERT INTO test_missing_target VALUES (2, 2, 'ABAB', 'c')
-- !query analysis
InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/test_missing_target, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/test_missing_target], Append, `spark_catalog`.`default`.`test_missing_target`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/test_missing_target), [a, b, c, d]
+- Project [cast(col1#x as int) AS a#x, cast(col2#x as int) AS b#x, cast(col3#x as string) AS c#x, cast(col4#x as string) AS d#x]
   +- LocalRelation [col1#x, col2#x, col3#x, col4#x]


-- !query
INSERT INTO test_missing_target VALUES (3, 3, 'BBBB', 'D')
-- !query analysis
InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/test_missing_target, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/test_missing_target], Append, `spark_catalog`.`default`.`test_missing_target`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/test_missing_target), [a, b, c, d]
+- Project [cast(col1#x as int) AS a#x, cast(col2#x as int) AS b#x, cast(col3#x as string) AS c#x, cast(col4#x as string) AS d#x]
   +- LocalRelation [col1#x, col2#x, col3#x, col4#x]


-- !query
INSERT INTO test_missing_target VALUES (4, 3, 'BBBB', 'e')
-- !query analysis
InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/test_missing_target, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/test_missing_target], Append, `spark_catalog`.`default`.`test_missing_target`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/test_missing_target), [a, b, c, d]
+- Project [cast(col1#x as int) AS a#x, cast(col2#x as int) AS b#x, cast(col3#x as string) AS c#x, cast(col4#x as string) AS d#x]
   +- LocalRelation [col1#x, col2#x, col3#x, col4#x]


-- !query
INSERT INTO test_missing_target VALUES (5, 3, 'bbbb', 'F')
-- !query analysis
InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/test_missing_target, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/test_missing_target], Append, `spark_catalog`.`default`.`test_missing_target`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/test_missing_target), [a, b, c, d]
+- Project [cast(col1#x as int) AS a#x, cast(col2#x as int) AS b#x, cast(col3#x as string) AS c#x, cast(col4#x as string) AS d#x]
   +- LocalRelation [col1#x, col2#x, col3#x, col4#x]


-- !query
INSERT INTO test_missing_target VALUES (6, 4, 'cccc', 'g')
-- !query analysis
InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/test_missing_target, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/test_missing_target], Append, `spark_catalog`.`default`.`test_missing_target`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/test_missing_target), [a, b, c, d]
+- Project [cast(col1#x as int) AS a#x, cast(col2#x as int) AS b#x, cast(col3#x as string) AS c#x, cast(col4#x as string) AS d#x]
   +- LocalRelation [col1#x, col2#x, col3#x, col4#x]


-- !query
INSERT INTO test_missing_target VALUES (7, 4, 'cccc', 'h')
-- !query analysis
InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/test_missing_target, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/test_missing_target], Append, `spark_catalog`.`default`.`test_missing_target`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/test_missing_target), [a, b, c, d]
+- Project [cast(col1#x as int) AS a#x, cast(col2#x as int) AS b#x, cast(col3#x as string) AS c#x, cast(col4#x as string) AS d#x]
   +- LocalRelation [col1#x, col2#x, col3#x, col4#x]


-- !query
INSERT INTO test_missing_target VALUES (8, 4, 'CCCC', 'I')
-- !query analysis
InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/test_missing_target, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/test_missing_target], Append, `spark_catalog`.`default`.`test_missing_target`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/test_missing_target), [a, b, c, d]
+- Project [cast(col1#x as int) AS a#x, cast(col2#x as int) AS b#x, cast(col3#x as string) AS c#x, cast(col4#x as string) AS d#x]
   +- LocalRelation [col1#x, col2#x, col3#x, col4#x]


-- !query
INSERT INTO test_missing_target VALUES (9, 4, 'CCCC', 'j')
-- !query analysis
InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/test_missing_target, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/test_missing_target], Append, `spark_catalog`.`default`.`test_missing_target`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/test_missing_target), [a, b, c, d]
+- Project [cast(col1#x as int) AS a#x, cast(col2#x as int) AS b#x, cast(col3#x as string) AS c#x, cast(col4#x as string) AS d#x]
   +- LocalRelation [col1#x, col2#x, col3#x, col4#x]


-- !query
SELECT udf(c), udf(count(*)) FROM test_missing_target GROUP BY
udf(test_missing_target.c)
ORDER BY udf(c)
-- !query analysis
Sort [udf(c)#x ASC NULLS FIRST], true
+- Aggregate [cast(udf(cast(c#x as string)) as string)], [cast(udf(cast(c#x as string)) as string) AS udf(c)#x, cast(udf(cast(count(1) as string)) as bigint) AS udf(count(1))#xL]
   +- SubqueryAlias spark_catalog.default.test_missing_target
      +- Relation spark_catalog.default.test_missing_target[a#x,b#x,c#x,d#x] parquet


-- !query
SELECT udf(count(*)) FROM test_missing_target GROUP BY udf(test_missing_target.c)
ORDER BY udf(c)
-- !query analysis
Project [udf(count(1))#xL]
+- Sort [cast(udf(cast(c#x as string)) as string)#x ASC NULLS FIRST], true
   +- Aggregate [cast(udf(cast(c#x as string)) as string)], [cast(udf(cast(count(1) as string)) as bigint) AS udf(count(1))#xL, cast(udf(cast(c#x as string)) as string) AS cast(udf(cast(c#x as string)) as string)#x]
      +- SubqueryAlias spark_catalog.default.test_missing_target
         +- Relation spark_catalog.default.test_missing_target[a#x,b#x,c#x,d#x] parquet


-- !query
SELECT udf(count(*)) FROM test_missing_target GROUP BY udf(a) ORDER BY udf(b)
-- !query analysis
org.apache.spark.sql.catalyst.ExtendedAnalysisException
{
  "condition" : "UNRESOLVED_COLUMN.WITH_SUGGESTION",
  "sqlState" : "42703",
  "messageParameters" : {
    "objectName" : "`b`",
    "proposal" : "`udf(count(1))`"
  },
  "queryContext" : [ {
    "objectType" : "",
    "objectName" : "",
    "startIndex" : 76,
    "stopIndex" : 76,
    "fragment" : "b"
  } ]
}


-- !query
SELECT udf(count(*)) FROM test_missing_target GROUP BY udf(b) ORDER BY udf(b)
-- !query analysis
Project [udf(count(1))#xL]
+- Sort [cast(udf(cast(b#x as string)) as int)#x ASC NULLS FIRST], true
   +- Aggregate [cast(udf(cast(b#x as string)) as int)], [cast(udf(cast(count(1) as string)) as bigint) AS udf(count(1))#xL, cast(udf(cast(b#x as string)) as int) AS cast(udf(cast(b#x as string)) as int)#x]
      +- SubqueryAlias spark_catalog.default.test_missing_target
         +- Relation spark_catalog.default.test_missing_target[a#x,b#x,c#x,d#x] parquet


-- !query
SELECT udf(test_missing_target.b), udf(count(*))
  FROM test_missing_target GROUP BY udf(b) ORDER BY udf(b)
-- !query analysis
Sort [udf(b)#x ASC NULLS FIRST], true
+- Aggregate [cast(udf(cast(b#x as string)) as int)], [cast(udf(cast(b#x as string)) as int) AS udf(b)#x, cast(udf(cast(count(1) as string)) as bigint) AS udf(count(1))#xL]
   +- SubqueryAlias spark_catalog.default.test_missing_target
      +- Relation spark_catalog.default.test_missing_target[a#x,b#x,c#x,d#x] parquet


-- !query
SELECT udf(c) FROM test_missing_target ORDER BY udf(a)
-- !query analysis
Project [udf(c)#x]
+- Sort [cast(udf(cast(a#x as string)) as int) ASC NULLS FIRST], true
   +- Project [cast(udf(cast(c#x as string)) as string) AS udf(c)#x, a#x]
      +- SubqueryAlias spark_catalog.default.test_missing_target
         +- Relation spark_catalog.default.test_missing_target[a#x,b#x,c#x,d#x] parquet


-- !query
SELECT udf(count(*)) FROM test_missing_target GROUP BY udf(b) ORDER BY udf(b) desc
-- !query analysis
Project [udf(count(1))#xL]
+- Sort [cast(udf(cast(b#x as string)) as int)#x DESC NULLS LAST], true
   +- Aggregate [cast(udf(cast(b#x as string)) as int)], [cast(udf(cast(count(1) as string)) as bigint) AS udf(count(1))#xL, cast(udf(cast(b#x as string)) as int) AS cast(udf(cast(b#x as string)) as int)#x]
      +- SubqueryAlias spark_catalog.default.test_missing_target
         +- Relation spark_catalog.default.test_missing_target[a#x,b#x,c#x,d#x] parquet


-- !query
SELECT udf(count(*)) FROM test_missing_target ORDER BY udf(1) desc
-- !query analysis
Sort [cast(udf(cast(1 as string)) as int) DESC NULLS LAST], true
+- Aggregate [cast(udf(cast(count(1) as string)) as bigint) AS udf(count(1))#xL]
   +- SubqueryAlias spark_catalog.default.test_missing_target
      +- Relation spark_catalog.default.test_missing_target[a#x,b#x,c#x,d#x] parquet


-- !query
SELECT udf(c), udf(count(*)) FROM test_missing_target GROUP BY 1 ORDER BY 1
-- !query analysis
Sort [udf(c)#x ASC NULLS FIRST], true
+- Aggregate [cast(udf(cast(c#x as string)) as string)], [cast(udf(cast(c#x as string)) as string) AS udf(c)#x, cast(udf(cast(count(1) as string)) as bigint) AS udf(count(1))#xL]
   +- SubqueryAlias spark_catalog.default.test_missing_target
      +- Relation spark_catalog.default.test_missing_target[a#x,b#x,c#x,d#x] parquet


-- !query
SELECT udf(c), udf(count(*)) FROM test_missing_target GROUP BY 3
-- !query analysis
org.apache.spark.sql.AnalysisException
{
  "condition" : "GROUP_BY_POS_OUT_OF_RANGE",
  "sqlState" : "42805",
  "messageParameters" : {
    "index" : "3",
    "size" : "2"
  },
  "queryContext" : [ {
    "objectType" : "",
    "objectName" : "",
    "startIndex" : 64,
    "stopIndex" : 64,
    "fragment" : "3"
  } ]
}


-- !query
SELECT udf(count(*)) FROM test_missing_target x, test_missing_target y
	WHERE udf(x.a) = udf(y.a)
	GROUP BY udf(b) ORDER BY udf(b)
-- !query analysis
org.apache.spark.sql.AnalysisException
{
  "condition" : "AMBIGUOUS_REFERENCE",
  "sqlState" : "42704",
  "messageParameters" : {
    "name" : "`b`",
    "referenceNames" : "[`x`.`b`, `y`.`b`]"
  },
  "queryContext" : [ {
    "objectType" : "",
    "objectName" : "",
    "startIndex" : 113,
    "stopIndex" : 113,
    "fragment" : "b"
  } ]
}


-- !query
SELECT udf(a), udf(a) FROM test_missing_target
	ORDER BY udf(a)
-- !query analysis
Project [udf(a)#x, udf(a)#x]
+- Sort [cast(udf(cast(a#x as string)) as int) ASC NULLS FIRST], true
   +- Project [cast(udf(cast(a#x as string)) as int) AS udf(a)#x, cast(udf(cast(a#x as string)) as int) AS udf(a)#x, a#x]
      +- SubqueryAlias spark_catalog.default.test_missing_target
         +- Relation spark_catalog.default.test_missing_target[a#x,b#x,c#x,d#x] parquet


-- !query
SELECT udf(udf(a)/2), udf(udf(a)/2) FROM test_missing_target
	ORDER BY udf(udf(a)/2)
-- !query analysis
Project [udf((udf(a) / 2))#x, udf((udf(a) / 2))#x]
+- Sort [cast(udf(cast((cast(cast(udf(cast(a#x as string)) as int) as double) / cast(2 as double)) as string)) as double) ASC NULLS FIRST], true
   +- Project [cast(udf(cast((cast(cast(udf(cast(a#x as string)) as int) as double) / cast(2 as double)) as string)) as double) AS udf((udf(a) / 2))#x, cast(udf(cast((cast(cast(udf(cast(a#x as string)) as int) as double) / cast(2 as double)) as string)) as double) AS udf((udf(a) / 2))#x, a#x]
      +- SubqueryAlias spark_catalog.default.test_missing_target
         +- Relation spark_catalog.default.test_missing_target[a#x,b#x,c#x,d#x] parquet


-- !query
SELECT udf(a/2), udf(a/2) FROM test_missing_target
	GROUP BY udf(a/2) ORDER BY udf(a/2)
-- !query analysis
Sort [udf((a / 2))#x ASC NULLS FIRST], true
+- Aggregate [cast(udf(cast((cast(a#x as double) / cast(2 as double)) as string)) as double)], [cast(udf(cast((cast(a#x as double) / cast(2 as double)) as string)) as double) AS udf((a / 2))#x, cast(udf(cast((cast(a#x as double) / cast(2 as double)) as string)) as double) AS udf((a / 2))#x]
   +- SubqueryAlias spark_catalog.default.test_missing_target
      +- Relation spark_catalog.default.test_missing_target[a#x,b#x,c#x,d#x] parquet


-- !query
SELECT udf(x.b), udf(count(*)) FROM test_missing_target x, test_missing_target y
	WHERE udf(x.a) = udf(y.a)
	GROUP BY udf(x.b) ORDER BY udf(x.b)
-- !query analysis
Sort [udf(b)#x ASC NULLS FIRST], true
+- Aggregate [cast(udf(cast(b#x as string)) as int)], [cast(udf(cast(b#x as string)) as int) AS udf(b)#x, cast(udf(cast(count(1) as string)) as bigint) AS udf(count(1))#xL]
   +- Filter (cast(udf(cast(a#x as string)) as int) = cast(udf(cast(a#x as string)) as int))
      +- Join Inner
         :- SubqueryAlias x
         :  +- SubqueryAlias spark_catalog.default.test_missing_target
         :     +- Relation spark_catalog.default.test_missing_target[a#x,b#x,c#x,d#x] parquet
         +- SubqueryAlias y
            +- SubqueryAlias spark_catalog.default.test_missing_target
               +- Relation spark_catalog.default.test_missing_target[a#x,b#x,c#x,d#x] parquet


-- !query
SELECT udf(count(*)) FROM test_missing_target x, test_missing_target y
	WHERE udf(x.a) = udf(y.a)
	GROUP BY udf(x.b) ORDER BY udf(x.b)
-- !query analysis
Project [udf(count(1))#xL]
+- Sort [cast(udf(cast(b#x as string)) as int)#x ASC NULLS FIRST], true
   +- Aggregate [cast(udf(cast(b#x as string)) as int)], [cast(udf(cast(count(1) as string)) as bigint) AS udf(count(1))#xL, cast(udf(cast(b#x as string)) as int) AS cast(udf(cast(b#x as string)) as int)#x]
      +- Filter (cast(udf(cast(a#x as string)) as int) = cast(udf(cast(a#x as string)) as int))
         +- Join Inner
            :- SubqueryAlias x
            :  +- SubqueryAlias spark_catalog.default.test_missing_target
            :     +- Relation spark_catalog.default.test_missing_target[a#x,b#x,c#x,d#x] parquet
            +- SubqueryAlias y
               +- SubqueryAlias spark_catalog.default.test_missing_target
                  +- Relation spark_catalog.default.test_missing_target[a#x,b#x,c#x,d#x] parquet


-- !query
SELECT udf(a%2), udf(count(udf(b))) FROM test_missing_target
GROUP BY udf(test_missing_target.a%2)
ORDER BY udf(test_missing_target.a%2)
-- !query analysis
Sort [udf((a % 2))#x ASC NULLS FIRST], true
+- Aggregate [cast(udf(cast((a#x % 2) as string)) as int)], [cast(udf(cast((a#x % 2) as string)) as int) AS udf((a % 2))#x, cast(udf(cast(count(cast(udf(cast(b#x as string)) as int)) as string)) as bigint) AS udf(count(udf(b)))#xL]
   +- SubqueryAlias spark_catalog.default.test_missing_target
      +- Relation spark_catalog.default.test_missing_target[a#x,b#x,c#x,d#x] parquet


-- !query
SELECT udf(count(c)) FROM test_missing_target
GROUP BY udf(lower(test_missing_target.c))
ORDER BY udf(lower(test_missing_target.c))
-- !query analysis
Project [udf(count(c))#xL]
+- Sort [cast(udf(cast(lower(c#x) as string)) as string)#x ASC NULLS FIRST], true
   +- Aggregate [cast(udf(cast(lower(c#x) as string)) as string)], [cast(udf(cast(count(c#x) as string)) as bigint) AS udf(count(c))#xL, cast(udf(cast(lower(c#x) as string)) as string) AS cast(udf(cast(lower(c#x) as string)) as string)#x]
      +- SubqueryAlias spark_catalog.default.test_missing_target
         +- Relation spark_catalog.default.test_missing_target[a#x,b#x,c#x,d#x] parquet


-- !query
SELECT udf(count(udf(a))) FROM test_missing_target GROUP BY udf(a) ORDER BY udf(b)
-- !query analysis
org.apache.spark.sql.catalyst.ExtendedAnalysisException
{
  "condition" : "UNRESOLVED_COLUMN.WITH_SUGGESTION",
  "sqlState" : "42703",
  "messageParameters" : {
    "objectName" : "`b`",
    "proposal" : "`udf(count(udf(a)))`"
  },
  "queryContext" : [ {
    "objectType" : "",
    "objectName" : "",
    "startIndex" : 81,
    "stopIndex" : 81,
    "fragment" : "b"
  } ]
}


-- !query
SELECT udf(count(b)) FROM test_missing_target GROUP BY udf(b/2) ORDER BY udf(b/2)
-- !query analysis
Project [udf(count(b))#xL]
+- Sort [cast(udf(cast((cast(b#x as double) / cast(2 as double)) as string)) as double)#x ASC NULLS FIRST], true
   +- Aggregate [cast(udf(cast((cast(b#x as double) / cast(2 as double)) as string)) as double)], [cast(udf(cast(count(b#x) as string)) as bigint) AS udf(count(b))#xL, cast(udf(cast((cast(b#x as double) / cast(2 as double)) as string)) as double) AS cast(udf(cast((cast(b#x as double) / cast(2 as double)) as string)) as double)#x]
      +- SubqueryAlias spark_catalog.default.test_missing_target
         +- Relation spark_catalog.default.test_missing_target[a#x,b#x,c#x,d#x] parquet


-- !query
SELECT udf(lower(test_missing_target.c)), udf(count(udf(c)))
  FROM test_missing_target GROUP BY udf(lower(c)) ORDER BY udf(lower(c))
-- !query analysis
Sort [udf(lower(c))#x ASC NULLS FIRST], true
+- Aggregate [cast(udf(cast(lower(c#x) as string)) as string)], [cast(udf(cast(lower(c#x) as string)) as string) AS udf(lower(c))#x, cast(udf(cast(count(cast(udf(cast(c#x as string)) as string)) as string)) as bigint) AS udf(count(udf(c)))#xL]
   +- SubqueryAlias spark_catalog.default.test_missing_target
      +- Relation spark_catalog.default.test_missing_target[a#x,b#x,c#x,d#x] parquet


-- !query
SELECT udf(a) FROM test_missing_target ORDER BY udf(upper(udf(d)))
-- !query analysis
Project [udf(a)#x]
+- Sort [cast(udf(cast(upper(cast(udf(cast(d#x as string)) as string)) as string)) as string) ASC NULLS FIRST], true
   +- Project [cast(udf(cast(a#x as string)) as int) AS udf(a)#x, d#x]
      +- SubqueryAlias spark_catalog.default.test_missing_target
         +- Relation spark_catalog.default.test_missing_target[a#x,b#x,c#x,d#x] parquet


-- !query
SELECT udf(count(b)) FROM test_missing_target
	GROUP BY udf((b + 1) / 2) ORDER BY udf((b + 1) / 2) desc
-- !query analysis
Project [udf(count(b))#xL]
+- Sort [cast(udf(cast((cast((b#x + 1) as double) / cast(2 as double)) as string)) as double)#x DESC NULLS LAST], true
   +- Aggregate [cast(udf(cast((cast((b#x + 1) as double) / cast(2 as double)) as string)) as double)], [cast(udf(cast(count(b#x) as string)) as bigint) AS udf(count(b))#xL, cast(udf(cast((cast((b#x + 1) as double) / cast(2 as double)) as string)) as double) AS cast(udf(cast((cast((b#x + 1) as double) / cast(2 as double)) as string)) as double)#x]
      +- SubqueryAlias spark_catalog.default.test_missing_target
         +- Relation spark_catalog.default.test_missing_target[a#x,b#x,c#x,d#x] parquet


-- !query
SELECT udf(count(udf(x.a))) FROM test_missing_target x, test_missing_target y
	WHERE udf(x.a) = udf(y.a)
	GROUP BY udf(b/2) ORDER BY udf(b/2)
-- !query analysis
org.apache.spark.sql.AnalysisException
{
  "condition" : "AMBIGUOUS_REFERENCE",
  "sqlState" : "42704",
  "messageParameters" : {
    "name" : "`b`",
    "referenceNames" : "[`x`.`b`, `y`.`b`]"
  },
  "queryContext" : [ {
    "objectType" : "",
    "objectName" : "",
    "startIndex" : 120,
    "stopIndex" : 120,
    "fragment" : "b"
  } ]
}


-- !query
SELECT udf(x.b/2), udf(count(udf(x.b))) FROM test_missing_target x,
test_missing_target y
	WHERE udf(x.a) = udf(y.a)
	GROUP BY udf(x.b/2) ORDER BY udf(x.b/2)
-- !query analysis
Sort [udf((b / 2))#x ASC NULLS FIRST], true
+- Aggregate [cast(udf(cast((cast(b#x as double) / cast(2 as double)) as string)) as double)], [cast(udf(cast((cast(b#x as double) / cast(2 as double)) as string)) as double) AS udf((b / 2))#x, cast(udf(cast(count(cast(udf(cast(b#x as string)) as int)) as string)) as bigint) AS udf(count(udf(b)))#xL]
   +- Filter (cast(udf(cast(a#x as string)) as int) = cast(udf(cast(a#x as string)) as int))
      +- Join Inner
         :- SubqueryAlias x
         :  +- SubqueryAlias spark_catalog.default.test_missing_target
         :     +- Relation spark_catalog.default.test_missing_target[a#x,b#x,c#x,d#x] parquet
         +- SubqueryAlias y
            +- SubqueryAlias spark_catalog.default.test_missing_target
               +- Relation spark_catalog.default.test_missing_target[a#x,b#x,c#x,d#x] parquet


-- !query
SELECT udf(count(udf(b))) FROM test_missing_target x, test_missing_target y
	WHERE udf(x.a) = udf(y.a)
	GROUP BY udf(x.b/2)
-- !query analysis
org.apache.spark.sql.AnalysisException
{
  "condition" : "AMBIGUOUS_REFERENCE",
  "sqlState" : "42704",
  "messageParameters" : {
    "name" : "`b`",
    "referenceNames" : "[`x`.`b`, `y`.`b`]"
  },
  "queryContext" : [ {
    "objectType" : "",
    "objectName" : "",
    "startIndex" : 22,
    "stopIndex" : 22,
    "fragment" : "b"
  } ]
}


-- !query
DROP TABLE test_missing_target
-- !query analysis
DropTable false, false
+- ResolvedIdentifier V2SessionCatalog(spark_catalog), default.test_missing_target
