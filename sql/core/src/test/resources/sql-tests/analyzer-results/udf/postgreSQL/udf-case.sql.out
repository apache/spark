-- Automatically generated by SQLQueryTestSuite
-- !query
CREATE TABLE CASE_TBL (
  i integer,
  f double
) USING parquet
-- !query analysis
CreateDataSourceTableCommand `spark_catalog`.`default`.`CASE_TBL`, false


-- !query
CREATE TABLE CASE2_TBL (
  i integer,
  j integer
) USING parquet
-- !query analysis
CreateDataSourceTableCommand `spark_catalog`.`default`.`CASE2_TBL`, false


-- !query
INSERT INTO CASE_TBL VALUES (1, 10.1)
-- !query analysis
InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/case_tbl, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/case_tbl], Append, `spark_catalog`.`default`.`case_tbl`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/case_tbl), [i, f]
+- Project [cast(col1#x as int) AS i#x, cast(col2#x as double) AS f#x]
   +- LocalRelation [col1#x, col2#x]


-- !query
INSERT INTO CASE_TBL VALUES (2, 20.2)
-- !query analysis
InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/case_tbl, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/case_tbl], Append, `spark_catalog`.`default`.`case_tbl`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/case_tbl), [i, f]
+- Project [cast(col1#x as int) AS i#x, cast(col2#x as double) AS f#x]
   +- LocalRelation [col1#x, col2#x]


-- !query
INSERT INTO CASE_TBL VALUES (3, -30.3)
-- !query analysis
InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/case_tbl, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/case_tbl], Append, `spark_catalog`.`default`.`case_tbl`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/case_tbl), [i, f]
+- Project [cast(col1#x as int) AS i#x, cast(col2#x as double) AS f#x]
   +- LocalRelation [col1#x, col2#x]


-- !query
INSERT INTO CASE_TBL VALUES (4, NULL)
-- !query analysis
InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/case_tbl, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/case_tbl], Append, `spark_catalog`.`default`.`case_tbl`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/case_tbl), [i, f]
+- Project [cast(col1#x as int) AS i#x, cast(col2#x as double) AS f#x]
   +- LocalRelation [col1#x, col2#x]


-- !query
INSERT INTO CASE2_TBL VALUES (1, -1)
-- !query analysis
InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/case2_tbl, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/case2_tbl], Append, `spark_catalog`.`default`.`case2_tbl`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/case2_tbl), [i, j]
+- Project [cast(col1#x as int) AS i#x, cast(col2#x as int) AS j#x]
   +- LocalRelation [col1#x, col2#x]


-- !query
INSERT INTO CASE2_TBL VALUES (2, -2)
-- !query analysis
InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/case2_tbl, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/case2_tbl], Append, `spark_catalog`.`default`.`case2_tbl`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/case2_tbl), [i, j]
+- Project [cast(col1#x as int) AS i#x, cast(col2#x as int) AS j#x]
   +- LocalRelation [col1#x, col2#x]


-- !query
INSERT INTO CASE2_TBL VALUES (3, -3)
-- !query analysis
InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/case2_tbl, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/case2_tbl], Append, `spark_catalog`.`default`.`case2_tbl`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/case2_tbl), [i, j]
+- Project [cast(col1#x as int) AS i#x, cast(col2#x as int) AS j#x]
   +- LocalRelation [col1#x, col2#x]


-- !query
INSERT INTO CASE2_TBL VALUES (2, -4)
-- !query analysis
InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/case2_tbl, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/case2_tbl], Append, `spark_catalog`.`default`.`case2_tbl`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/case2_tbl), [i, j]
+- Project [cast(col1#x as int) AS i#x, cast(col2#x as int) AS j#x]
   +- LocalRelation [col1#x, col2#x]


-- !query
INSERT INTO CASE2_TBL VALUES (1, NULL)
-- !query analysis
InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/case2_tbl, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/case2_tbl], Append, `spark_catalog`.`default`.`case2_tbl`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/case2_tbl), [i, j]
+- Project [cast(col1#x as int) AS i#x, cast(col2#x as int) AS j#x]
   +- LocalRelation [col1#x, col2#x]


-- !query
INSERT INTO CASE2_TBL VALUES (NULL, -6)
-- !query analysis
InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/case2_tbl, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/case2_tbl], Append, `spark_catalog`.`default`.`case2_tbl`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/case2_tbl), [i, j]
+- Project [cast(col1#x as int) AS i#x, cast(col2#x as int) AS j#x]
   +- LocalRelation [col1#x, col2#x]


-- !query
SELECT '3' AS `One`,
  CASE
    WHEN udf(1 < 2) THEN 3
  END AS `Simple WHEN`
-- !query analysis
Project [3 AS One#x, CASE WHEN cast(udf(cast((1 < 2) as string)) as boolean) THEN 3 END AS Simple WHEN#x]
+- OneRowRelation


-- !query
SELECT '<NULL>' AS `One`,
  CASE
    WHEN 1 > 2 THEN udf(3)
  END AS `Simple default`
-- !query analysis
Project [<NULL> AS One#x, CASE WHEN (1 > 2) THEN cast(udf(cast(3 as string)) as int) END AS Simple default#x]
+- OneRowRelation


-- !query
SELECT '3' AS `One`,
  CASE
    WHEN udf(1) < 2 THEN udf(3)
    ELSE udf(4)
  END AS `Simple ELSE`
-- !query analysis
Project [3 AS One#x, CASE WHEN (cast(udf(cast(1 as string)) as int) < 2) THEN cast(udf(cast(3 as string)) as int) ELSE cast(udf(cast(4 as string)) as int) END AS Simple ELSE#x]
+- OneRowRelation


-- !query
SELECT udf('4') AS `One`,
  CASE
    WHEN 1 > 2 THEN 3
    ELSE 4
  END AS `ELSE default`
-- !query analysis
Project [cast(udf(cast(4 as string)) as string) AS One#x, CASE WHEN (1 > 2) THEN 3 ELSE 4 END AS ELSE default#x]
+- OneRowRelation


-- !query
SELECT udf('6') AS `One`,
  CASE
    WHEN udf(1 > 2) THEN 3
    WHEN udf(4) < 5 THEN 6
    ELSE 7
  END AS `Two WHEN with default`
-- !query analysis
Project [cast(udf(cast(6 as string)) as string) AS One#x, CASE WHEN cast(udf(cast((1 > 2) as string)) as boolean) THEN 3 WHEN (cast(udf(cast(4 as string)) as int) < 5) THEN 6 ELSE 7 END AS Two WHEN with default#x]
+- OneRowRelation


-- !query
SELECT '7' AS `None`,
  CASE WHEN rand() < udf(0) THEN 1
  END AS `NULL on no matches`
-- !query analysis
[Analyzer test output redacted due to nondeterminism]


-- !query
SELECT CASE WHEN udf(1=0) THEN 1/0 WHEN 1=1 THEN 1 ELSE 2/0 END
-- !query analysis
Project [CASE WHEN cast(udf(cast((1 = 0) as string)) as boolean) THEN (cast(1 as double) / cast(0 as double)) WHEN (1 = 1) THEN cast(1 as double) ELSE (cast(2 as double) / cast(0 as double)) END AS CASE WHEN udf((1 = 0)) THEN (1 / 0) WHEN (1 = 1) THEN 1 ELSE (2 / 0) END#x]
+- OneRowRelation


-- !query
SELECT CASE 1 WHEN 0 THEN 1/udf(0) WHEN 1 THEN 1 ELSE 2/0 END
-- !query analysis
Project [CASE WHEN (1 = 0) THEN (cast(1 as double) / cast(cast(udf(cast(0 as string)) as int) as double)) WHEN (1 = 1) THEN cast(1 as double) ELSE (cast(2 as double) / cast(0 as double)) END AS CASE WHEN (1 = 0) THEN (1 / udf(0)) WHEN (1 = 1) THEN 1 ELSE (2 / 0) END#x]
+- OneRowRelation


-- !query
SELECT CASE 'a' WHEN 'a' THEN udf(1) ELSE udf(2) END
-- !query analysis
Project [CASE WHEN (a = a) THEN cast(udf(cast(1 as string)) as int) ELSE cast(udf(cast(2 as string)) as int) END AS CASE WHEN (a = a) THEN udf(1) ELSE udf(2) END#x]
+- OneRowRelation


-- !query
SELECT '' AS `Five`,
  CASE
    WHEN i >= 3 THEN i
  END AS `>= 3 or Null`
  FROM CASE_TBL
-- !query analysis
Project [ AS Five#x, CASE WHEN (i#x >= 3) THEN i#x END AS >= 3 or Null#x]
+- SubqueryAlias spark_catalog.default.case_tbl
   +- Relation spark_catalog.default.case_tbl[i#x,f#x] parquet


-- !query
SELECT '' AS `Five`,
  CASE WHEN i >= 3 THEN (i + i)
       ELSE i
  END AS `Simplest Math`
  FROM CASE_TBL
-- !query analysis
Project [ AS Five#x, CASE WHEN (i#x >= 3) THEN (i#x + i#x) ELSE i#x END AS Simplest Math#x]
+- SubqueryAlias spark_catalog.default.case_tbl
   +- Relation spark_catalog.default.case_tbl[i#x,f#x] parquet


-- !query
SELECT '' AS `Five`, i AS `Value`,
  CASE WHEN (i < 0) THEN 'small'
       WHEN (i = 0) THEN 'zero'
       WHEN (i = 1) THEN 'one'
       WHEN (i = 2) THEN 'two'
       ELSE 'big'
  END AS `Category`
  FROM CASE_TBL
-- !query analysis
Project [ AS Five#x, i#x AS Value#x, CASE WHEN (i#x < 0) THEN small WHEN (i#x = 0) THEN zero WHEN (i#x = 1) THEN one WHEN (i#x = 2) THEN two ELSE big END AS Category#x]
+- SubqueryAlias spark_catalog.default.case_tbl
   +- Relation spark_catalog.default.case_tbl[i#x,f#x] parquet


-- !query
SELECT '' AS `Five`,
  CASE WHEN ((i < 0) or (i < 0)) THEN 'small'
       WHEN ((i = 0) or (i = 0)) THEN 'zero'
       WHEN ((i = 1) or (i = 1)) THEN 'one'
       WHEN ((i = 2) or (i = 2)) THEN 'two'
       ELSE 'big'
  END AS `Category`
  FROM CASE_TBL
-- !query analysis
Project [ AS Five#x, CASE WHEN ((i#x < 0) OR (i#x < 0)) THEN small WHEN ((i#x = 0) OR (i#x = 0)) THEN zero WHEN ((i#x = 1) OR (i#x = 1)) THEN one WHEN ((i#x = 2) OR (i#x = 2)) THEN two ELSE big END AS Category#x]
+- SubqueryAlias spark_catalog.default.case_tbl
   +- Relation spark_catalog.default.case_tbl[i#x,f#x] parquet


-- !query
SELECT * FROM CASE_TBL WHERE udf(COALESCE(f,i)) = 4
-- !query analysis
Project [i#x, f#x]
+- Filter (cast(udf(cast(coalesce(f#x, cast(i#x as double)) as string)) as double) = cast(4 as double))
   +- SubqueryAlias spark_catalog.default.case_tbl
      +- Relation spark_catalog.default.case_tbl[i#x,f#x] parquet


-- !query
SELECT * FROM CASE_TBL WHERE udf(NULLIF(f,i)) = 2
-- !query analysis
Project [i#x, f#x]
+- Filter (cast(udf(cast(nullif(f#x, i#x) as string)) as double) = cast(2 as double))
   +- SubqueryAlias spark_catalog.default.case_tbl
      +- Relation spark_catalog.default.case_tbl[i#x,f#x] parquet


-- !query
SELECT udf(COALESCE(a.f, b.i, b.j))
  FROM CASE_TBL a, CASE2_TBL b
-- !query analysis
Project [cast(udf(cast(coalesce(f#x, cast(i#x as double), cast(j#x as double)) as string)) as double) AS udf(coalesce(f, i, j))#x]
+- Join Inner
   :- SubqueryAlias a
   :  +- SubqueryAlias spark_catalog.default.case_tbl
   :     +- Relation spark_catalog.default.case_tbl[i#x,f#x] parquet
   +- SubqueryAlias b
      +- SubqueryAlias spark_catalog.default.case2_tbl
         +- Relation spark_catalog.default.case2_tbl[i#x,j#x] parquet


-- !query
SELECT *
   FROM CASE_TBL a, CASE2_TBL b
   WHERE udf(COALESCE(a.f, b.i, b.j)) = 2
-- !query analysis
Project [i#x, f#x, i#x, j#x]
+- Filter (cast(udf(cast(coalesce(f#x, cast(i#x as double), cast(j#x as double)) as string)) as double) = cast(2 as double))
   +- Join Inner
      :- SubqueryAlias a
      :  +- SubqueryAlias spark_catalog.default.case_tbl
      :     +- Relation spark_catalog.default.case_tbl[i#x,f#x] parquet
      +- SubqueryAlias b
         +- SubqueryAlias spark_catalog.default.case2_tbl
            +- Relation spark_catalog.default.case2_tbl[i#x,j#x] parquet


-- !query
SELECT udf('') AS Five, NULLIF(a.i,b.i) AS `NULLIF(a.i,b.i)`,
  NULLIF(b.i, 4) AS `NULLIF(b.i,4)`
  FROM CASE_TBL a, CASE2_TBL b
-- !query analysis
Project [cast(udf(cast( as string)) as string) AS Five#x, nullif(i#x, i#x) AS NULLIF(a.i,b.i)#x, nullif(i#x, 4) AS NULLIF(b.i,4)#x]
+- Join Inner
   :- SubqueryAlias a
   :  +- SubqueryAlias spark_catalog.default.case_tbl
   :     +- Relation spark_catalog.default.case_tbl[i#x,f#x] parquet
   +- SubqueryAlias b
      +- SubqueryAlias spark_catalog.default.case2_tbl
         +- Relation spark_catalog.default.case2_tbl[i#x,j#x] parquet


-- !query
SELECT '' AS `Two`, *
  FROM CASE_TBL a, CASE2_TBL b
  WHERE udf(COALESCE(f,b.i) = 2)
-- !query analysis
Project [ AS Two#x, i#x, f#x, i#x, j#x]
+- Filter cast(udf(cast((coalesce(f#x, cast(i#x as double)) = cast(2 as double)) as string)) as boolean)
   +- Join Inner
      :- SubqueryAlias a
      :  +- SubqueryAlias spark_catalog.default.case_tbl
      :     +- Relation spark_catalog.default.case_tbl[i#x,f#x] parquet
      +- SubqueryAlias b
         +- SubqueryAlias spark_catalog.default.case2_tbl
            +- Relation spark_catalog.default.case2_tbl[i#x,j#x] parquet


-- !query
SELECT CASE
  (CASE vol('bar')
    WHEN udf('foo') THEN 'it was foo!'
    WHEN udf(vol(null)) THEN 'null input'
    WHEN 'bar' THEN 'it was bar!' END
  )
  WHEN udf('it was foo!') THEN 'foo recognized'
  WHEN 'it was bar!' THEN udf('bar recognized')
  ELSE 'unrecognized' END AS col
-- !query analysis
Project [CASE WHEN (CASE WHEN (vol(bar) = cast(udf(cast(foo as string)) as string)) THEN it was foo! WHEN (vol(bar) = cast(udf(cast(vol(cast(null as string)) as string)) as string)) THEN null input WHEN (vol(bar) = bar) THEN it was bar! END = cast(udf(cast(it was foo! as string)) as string)) THEN foo recognized WHEN (CASE WHEN (vol(bar) = cast(udf(cast(foo as string)) as string)) THEN it was foo! WHEN (vol(bar) = cast(udf(cast(vol(cast(null as string)) as string)) as string)) THEN null input WHEN (vol(bar) = bar) THEN it was bar! END = it was bar!) THEN cast(udf(cast(bar recognized as string)) as string) ELSE unrecognized END AS col#x]
+- OneRowRelation


-- !query
DROP TABLE CASE_TBL
-- !query analysis
DropTable false, false
+- ResolvedIdentifier V2SessionCatalog(spark_catalog), default.CASE_TBL


-- !query
DROP TABLE CASE2_TBL
-- !query analysis
DropTable false, false
+- ResolvedIdentifier V2SessionCatalog(spark_catalog), default.CASE2_TBL
