-- Automatically generated by SQLQueryTestSuite
-- !query
CREATE TABLE test_having (a int, b int, c string, d string) USING parquet
-- !query analysis
CreateDataSourceTableCommand `spark_catalog`.`default`.`test_having`, false


-- !query
INSERT INTO test_having VALUES (0, 1, 'XXXX', 'A')
-- !query analysis
InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/test_having, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/test_having], Append, `spark_catalog`.`default`.`test_having`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/test_having), [a, b, c, d]
+- Project [cast(col1#x as int) AS a#x, cast(col2#x as int) AS b#x, cast(col3#x as string) AS c#x, cast(col4#x as string) AS d#x]
   +- LocalRelation [col1#x, col2#x, col3#x, col4#x]


-- !query
INSERT INTO test_having VALUES (1, 2, 'AAAA', 'b')
-- !query analysis
InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/test_having, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/test_having], Append, `spark_catalog`.`default`.`test_having`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/test_having), [a, b, c, d]
+- Project [cast(col1#x as int) AS a#x, cast(col2#x as int) AS b#x, cast(col3#x as string) AS c#x, cast(col4#x as string) AS d#x]
   +- LocalRelation [col1#x, col2#x, col3#x, col4#x]


-- !query
INSERT INTO test_having VALUES (2, 2, 'AAAA', 'c')
-- !query analysis
InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/test_having, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/test_having], Append, `spark_catalog`.`default`.`test_having`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/test_having), [a, b, c, d]
+- Project [cast(col1#x as int) AS a#x, cast(col2#x as int) AS b#x, cast(col3#x as string) AS c#x, cast(col4#x as string) AS d#x]
   +- LocalRelation [col1#x, col2#x, col3#x, col4#x]


-- !query
INSERT INTO test_having VALUES (3, 3, 'BBBB', 'D')
-- !query analysis
InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/test_having, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/test_having], Append, `spark_catalog`.`default`.`test_having`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/test_having), [a, b, c, d]
+- Project [cast(col1#x as int) AS a#x, cast(col2#x as int) AS b#x, cast(col3#x as string) AS c#x, cast(col4#x as string) AS d#x]
   +- LocalRelation [col1#x, col2#x, col3#x, col4#x]


-- !query
INSERT INTO test_having VALUES (4, 3, 'BBBB', 'e')
-- !query analysis
InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/test_having, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/test_having], Append, `spark_catalog`.`default`.`test_having`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/test_having), [a, b, c, d]
+- Project [cast(col1#x as int) AS a#x, cast(col2#x as int) AS b#x, cast(col3#x as string) AS c#x, cast(col4#x as string) AS d#x]
   +- LocalRelation [col1#x, col2#x, col3#x, col4#x]


-- !query
INSERT INTO test_having VALUES (5, 3, 'bbbb', 'F')
-- !query analysis
InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/test_having, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/test_having], Append, `spark_catalog`.`default`.`test_having`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/test_having), [a, b, c, d]
+- Project [cast(col1#x as int) AS a#x, cast(col2#x as int) AS b#x, cast(col3#x as string) AS c#x, cast(col4#x as string) AS d#x]
   +- LocalRelation [col1#x, col2#x, col3#x, col4#x]


-- !query
INSERT INTO test_having VALUES (6, 4, 'cccc', 'g')
-- !query analysis
InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/test_having, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/test_having], Append, `spark_catalog`.`default`.`test_having`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/test_having), [a, b, c, d]
+- Project [cast(col1#x as int) AS a#x, cast(col2#x as int) AS b#x, cast(col3#x as string) AS c#x, cast(col4#x as string) AS d#x]
   +- LocalRelation [col1#x, col2#x, col3#x, col4#x]


-- !query
INSERT INTO test_having VALUES (7, 4, 'cccc', 'h')
-- !query analysis
InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/test_having, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/test_having], Append, `spark_catalog`.`default`.`test_having`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/test_having), [a, b, c, d]
+- Project [cast(col1#x as int) AS a#x, cast(col2#x as int) AS b#x, cast(col3#x as string) AS c#x, cast(col4#x as string) AS d#x]
   +- LocalRelation [col1#x, col2#x, col3#x, col4#x]


-- !query
INSERT INTO test_having VALUES (8, 4, 'CCCC', 'I')
-- !query analysis
InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/test_having, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/test_having], Append, `spark_catalog`.`default`.`test_having`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/test_having), [a, b, c, d]
+- Project [cast(col1#x as int) AS a#x, cast(col2#x as int) AS b#x, cast(col3#x as string) AS c#x, cast(col4#x as string) AS d#x]
   +- LocalRelation [col1#x, col2#x, col3#x, col4#x]


-- !query
INSERT INTO test_having VALUES (9, 4, 'CCCC', 'j')
-- !query analysis
InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/test_having, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/test_having], Append, `spark_catalog`.`default`.`test_having`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/test_having), [a, b, c, d]
+- Project [cast(col1#x as int) AS a#x, cast(col2#x as int) AS b#x, cast(col3#x as string) AS c#x, cast(col4#x as string) AS d#x]
   +- LocalRelation [col1#x, col2#x, col3#x, col4#x]


-- !query
SELECT udf(b), udf(c) FROM test_having
	GROUP BY b, c HAVING udf(count(*)) = 1 ORDER BY udf(b), udf(c)
-- !query analysis
Project [udf(b)#x, udf(c)#x]
+- Sort [cast(udf(cast(b#x as string)) as int) ASC NULLS FIRST, cast(udf(cast(c#x as string)) as string) ASC NULLS FIRST], true
   +- Project [udf(b)#x, udf(c)#x, b#x, c#x]
      +- Filter (cast(udf(cast(count(1)#xL as string)) as bigint) = cast(1 as bigint))
         +- Aggregate [b#x, c#x], [cast(udf(cast(b#x as string)) as int) AS udf(b)#x, cast(udf(cast(c#x as string)) as string) AS udf(c)#x, count(1) AS count(1)#xL, b#x, c#x]
            +- SubqueryAlias spark_catalog.default.test_having
               +- Relation spark_catalog.default.test_having[a#x,b#x,c#x,d#x] parquet


-- !query
SELECT udf(b), udf(c) FROM test_having
	GROUP BY b, c HAVING udf(b) = 3 ORDER BY udf(b), udf(c)
-- !query analysis
Sort [udf(b)#x ASC NULLS FIRST, udf(c)#x ASC NULLS FIRST], true
+- Filter (udf(b)#x = 3)
   +- Aggregate [b#x, c#x], [cast(udf(cast(b#x as string)) as int) AS udf(b)#x, cast(udf(cast(c#x as string)) as string) AS udf(c)#x]
      +- SubqueryAlias spark_catalog.default.test_having
         +- Relation spark_catalog.default.test_having[a#x,b#x,c#x,d#x] parquet


-- !query
SELECT udf(c), max(udf(a)) FROM test_having
	GROUP BY c HAVING udf(count(*)) > 2 OR udf(min(a)) = udf(max(a))
	ORDER BY c
-- !query analysis
Project [udf(c)#x, max(udf(a))#x]
+- Sort [c#x ASC NULLS FIRST], true
   +- Project [udf(c)#x, max(udf(a))#x, c#x]
      +- Filter ((cast(udf(cast(count(1)#xL as string)) as bigint) > cast(2 as bigint)) OR (cast(udf(cast(min(a)#x as string)) as int) = cast(udf(cast(max(a)#x as string)) as int)))
         +- Aggregate [c#x], [cast(udf(cast(c#x as string)) as string) AS udf(c)#x, max(cast(udf(cast(a#x as string)) as int)) AS max(udf(a))#x, count(1) AS count(1)#xL, min(a#x) AS min(a)#x, max(a#x) AS max(a)#x, c#x]
            +- SubqueryAlias spark_catalog.default.test_having
               +- Relation spark_catalog.default.test_having[a#x,b#x,c#x,d#x] parquet


-- !query
SELECT udf(udf(min(udf(a)))), udf(udf(max(udf(a)))) FROM test_having HAVING udf(udf(min(udf(a)))) = udf(udf(max(udf(a))))
-- !query analysis
Filter (udf(udf(min(udf(a))))#x = udf(udf(max(udf(a))))#x)
+- Aggregate [cast(udf(cast(cast(udf(cast(min(cast(udf(cast(a#x as string)) as int)) as string)) as int) as string)) as int) AS udf(udf(min(udf(a))))#x, cast(udf(cast(cast(udf(cast(max(cast(udf(cast(a#x as string)) as int)) as string)) as int) as string)) as int) AS udf(udf(max(udf(a))))#x]
   +- SubqueryAlias spark_catalog.default.test_having
      +- Relation spark_catalog.default.test_having[a#x,b#x,c#x,d#x] parquet


-- !query
SELECT udf(min(udf(a))), udf(udf(max(a))) FROM test_having HAVING udf(min(a)) < udf(max(udf(a)))
-- !query analysis
Project [udf(min(udf(a)))#x, udf(udf(max(a)))#x]
+- Filter (cast(udf(cast(min(a)#x as string)) as int) < cast(udf(cast(max(udf(a))#x as string)) as int))
   +- Aggregate [cast(udf(cast(min(cast(udf(cast(a#x as string)) as int)) as string)) as int) AS udf(min(udf(a)))#x, cast(udf(cast(cast(udf(cast(max(a#x) as string)) as int) as string)) as int) AS udf(udf(max(a)))#x, min(a#x) AS min(a)#x, max(cast(udf(cast(a#x as string)) as int)) AS max(udf(a))#x]
      +- SubqueryAlias spark_catalog.default.test_having
         +- Relation spark_catalog.default.test_having[a#x,b#x,c#x,d#x] parquet


-- !query
SELECT udf(a) FROM test_having HAVING udf(min(a)) < udf(max(a))
-- !query analysis
org.apache.spark.sql.catalyst.ExtendedAnalysisException
{
  "errorClass" : "MISSING_GROUP_BY",
  "sqlState" : "42803",
  "queryContext" : [ {
    "objectType" : "",
    "objectName" : "",
    "startIndex" : 1,
    "stopIndex" : 63,
    "fragment" : "SELECT udf(a) FROM test_having HAVING udf(min(a)) < udf(max(a))"
  } ]
}


-- !query
SELECT 1 AS one FROM test_having HAVING udf(a) > 1
-- !query analysis
org.apache.spark.sql.catalyst.ExtendedAnalysisException
{
  "errorClass" : "UNRESOLVED_COLUMN.WITH_SUGGESTION",
  "sqlState" : "42703",
  "messageParameters" : {
    "objectName" : "`a`",
    "proposal" : "`one`"
  },
  "queryContext" : [ {
    "objectType" : "",
    "objectName" : "",
    "startIndex" : 45,
    "stopIndex" : 45,
    "fragment" : "a"
  } ]
}


-- !query
SELECT 1 AS one FROM test_having HAVING udf(udf(1) > udf(2))
-- !query analysis
Filter cast(cast(udf(cast((cast(udf(cast(one#x as string)) as int) > cast(udf(cast(2 as string)) as int)) as string)) as boolean) as boolean)
+- Aggregate [1 AS one#x]
   +- SubqueryAlias spark_catalog.default.test_having
      +- Relation spark_catalog.default.test_having[a#x,b#x,c#x,d#x] parquet


-- !query
SELECT 1 AS one FROM test_having HAVING udf(udf(1) < udf(2))
-- !query analysis
Filter cast(cast(udf(cast((cast(udf(cast(one#x as string)) as int) < cast(udf(cast(2 as string)) as int)) as string)) as boolean) as boolean)
+- Aggregate [1 AS one#x]
   +- SubqueryAlias spark_catalog.default.test_having
      +- Relation spark_catalog.default.test_having[a#x,b#x,c#x,d#x] parquet


-- !query
SELECT 1 AS one FROM test_having WHERE 1/udf(a) = 1 HAVING 1 < 2
-- !query analysis
Filter (one#x < 2)
+- Aggregate [1 AS one#x]
   +- Filter ((cast(1 as double) / cast(cast(udf(cast(a#x as string)) as int) as double)) = cast(1 as double))
      +- SubqueryAlias spark_catalog.default.test_having
         +- Relation spark_catalog.default.test_having[a#x,b#x,c#x,d#x] parquet


-- !query
DROP TABLE test_having
-- !query analysis
DropTable false, false
+- ResolvedIdentifier V2SessionCatalog(spark_catalog), default.test_having
