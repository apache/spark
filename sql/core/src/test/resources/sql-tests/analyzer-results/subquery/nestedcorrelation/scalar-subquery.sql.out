-- Automatically generated by SQLQueryTestSuite
-- !query
set spark.sql.optimizer.supportNestedCorrelatedSubqueries.enabled=true
-- !query analysis
SetCommand (spark.sql.optimizer.supportNestedCorrelatedSubqueries.enabled,Some(true))


-- !query
set spark.sql.optimizer.supportNestedCorrelatedSubqueriesForScalarSubqueries.enabled=true
-- !query analysis
SetCommand (spark.sql.optimizer.supportNestedCorrelatedSubqueriesForScalarSubqueries.enabled,Some(true))


-- !query
DROP TABLE IF EXISTS myt1
-- !query analysis
DropTable true, false
+- ResolvedIdentifier V2SessionCatalog(spark_catalog), default.myt1


-- !query
DROP TABLE IF EXISTS myt2
-- !query analysis
DropTable true, false
+- ResolvedIdentifier V2SessionCatalog(spark_catalog), default.myt2


-- !query
DROP TABLE IF EXISTS myt3
-- !query analysis
DropTable true, false
+- ResolvedIdentifier V2SessionCatalog(spark_catalog), default.myt3


-- !query
CREATE TABLE myt1(a INT, b INT, c INT)
-- !query analysis
CreateDataSourceTableCommand `spark_catalog`.`default`.`myt1`, false


-- !query
CREATE TABLE myt2(a INT, b INT, c INT)
-- !query analysis
CreateDataSourceTableCommand `spark_catalog`.`default`.`myt2`, false


-- !query
CREATE TABLE myt3(a INT, b INT, c INT)
-- !query analysis
CreateDataSourceTableCommand `spark_catalog`.`default`.`myt3`, false


-- !query
INSERT INTO myt1 VALUES (0, 0, 0), (1, 1, 1), (2, 2, 2), (3, 3, 3), (NULL, NULL, NULL)
-- !query analysis
InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/myt1, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/myt1], Append, `spark_catalog`.`default`.`myt1`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/myt1), [a, b, c]
+- Project [cast(col1#x as int) AS a#x, cast(col2#x as int) AS b#x, cast(col3#x as int) AS c#x]
   +- LocalRelation [col1#x, col2#x, col3#x]


-- !query
INSERT INTO myt2 VALUES (0, 0, 0), (1, 1, 1), (2, 2, 2), (3, 3, 3), (NULL, NULL, NULL)
-- !query analysis
InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/myt2, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/myt2], Append, `spark_catalog`.`default`.`myt2`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/myt2), [a, b, c]
+- Project [cast(col1#x as int) AS a#x, cast(col2#x as int) AS b#x, cast(col3#x as int) AS c#x]
   +- LocalRelation [col1#x, col2#x, col3#x]


-- !query
INSERT INTO myt3 VALUES (0, 0, 0), (1, 1, 1), (2, 2, 2), (3, 3, 3), (NULL, NULL, NULL)
-- !query analysis
InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/myt3, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/myt3], Append, `spark_catalog`.`default`.`myt3`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/myt3), [a, b, c]
+- Project [cast(col1#x as int) AS a#x, cast(col2#x as int) AS b#x, cast(col3#x as int) AS c#x]
   +- LocalRelation [col1#x, col2#x, col3#x]


-- !query
SELECT *
FROM myt1
WHERE myt1.a = (
  SELECT MAX(myt2.a)
  FROM myt2
  WHERE myt2.a = (
   SELECT MAX(myt3.a)
   FROM myt3
   WHERE myt3.b > myt2.b AND myt3.c > myt1.c
  ) AND myt2.b > myt1.b
)
-- !query analysis
Project [a#x, b#x, c#x]
+- Filter (a#x = scalar-subquery#x [b#x && c#x])
   :  +- Aggregate [max(a#x) AS max(a)#x]
   :     +- Filter ((a#x = scalar-subquery#x [b#x && c#x]) AND (b#x > outer(b#x)))
   :        :  +- Aggregate [max(a#x) AS max(a)#x]
   :        :     +- Filter ((b#x > outer(b#x)) AND (c#x > outer(c#x)))
   :        :        +- SubqueryAlias spark_catalog.default.myt3
   :        :           +- Relation spark_catalog.default.myt3[a#x,b#x,c#x] parquet
   :        +- SubqueryAlias spark_catalog.default.myt2
   :           +- Relation spark_catalog.default.myt2[a#x,b#x,c#x] parquet
   +- SubqueryAlias spark_catalog.default.myt1
      +- Relation spark_catalog.default.myt1[a#x,b#x,c#x] parquet


-- !query
SELECT *
FROM myt1
WHERE myt1.a = (
  SELECT MAX(myt2.a)
  FROM myt2
  WHERE myt2.a = (
   SELECT MAX(myt3.a)
   FROM myt3
   WHERE myt3.b = myt2.b AND myt3.c = myt1.c
  ) AND myt2.b = myt1.b
)
-- !query analysis
Project [a#x, b#x, c#x]
+- Filter (a#x = scalar-subquery#x [b#x && c#x])
   :  +- Aggregate [max(a#x) AS max(a)#x]
   :     +- Filter ((a#x = scalar-subquery#x [b#x && c#x]) AND (b#x = outer(b#x)))
   :        :  +- Aggregate [max(a#x) AS max(a)#x]
   :        :     +- Filter ((b#x = outer(b#x)) AND (c#x = outer(c#x)))
   :        :        +- SubqueryAlias spark_catalog.default.myt3
   :        :           +- Relation spark_catalog.default.myt3[a#x,b#x,c#x] parquet
   :        +- SubqueryAlias spark_catalog.default.myt2
   :           +- Relation spark_catalog.default.myt2[a#x,b#x,c#x] parquet
   +- SubqueryAlias spark_catalog.default.myt1
      +- Relation spark_catalog.default.myt1[a#x,b#x,c#x] parquet


-- !query
SELECT *
FROM myt1
WHERE myt1.a = (
  SELECT COUNT(myt2.a)
  FROM myt2
  WHERE myt2.a = (
   SELECT COUNT(myt3.a)
   FROM myt3
   WHERE myt3.b > myt2.b AND myt3.c > myt1.c
  ) AND myt2.b > myt1.b
)
-- !query analysis
Project [a#x, b#x, c#x]
+- Filter (cast(a#x as bigint) = scalar-subquery#x [b#x && c#x])
   :  +- Aggregate [count(a#x) AS count(a)#xL]
   :     +- Filter ((cast(a#x as bigint) = scalar-subquery#x [b#x && c#x]) AND (b#x > outer(b#x)))
   :        :  +- Aggregate [count(a#x) AS count(a)#xL]
   :        :     +- Filter ((b#x > outer(b#x)) AND (c#x > outer(c#x)))
   :        :        +- SubqueryAlias spark_catalog.default.myt3
   :        :           +- Relation spark_catalog.default.myt3[a#x,b#x,c#x] parquet
   :        +- SubqueryAlias spark_catalog.default.myt2
   :           +- Relation spark_catalog.default.myt2[a#x,b#x,c#x] parquet
   +- SubqueryAlias spark_catalog.default.myt1
      +- Relation spark_catalog.default.myt1[a#x,b#x,c#x] parquet


-- !query
SELECT *
FROM myt1
WHERE myt1.a = (
  SELECT COUNT(myt2.a)
  FROM myt2
  WHERE myt2.a = (
   SELECT COUNT(myt3.a)
   FROM myt3
   WHERE myt3.b = myt2.b AND myt3.c = myt1.c
  ) AND myt2.b = myt1.b
)
-- !query analysis
Project [a#x, b#x, c#x]
+- Filter (cast(a#x as bigint) = scalar-subquery#x [b#x && c#x])
   :  +- Aggregate [count(a#x) AS count(a)#xL]
   :     +- Filter ((cast(a#x as bigint) = scalar-subquery#x [b#x && c#x]) AND (b#x = outer(b#x)))
   :        :  +- Aggregate [count(a#x) AS count(a)#xL]
   :        :     +- Filter ((b#x = outer(b#x)) AND (c#x = outer(c#x)))
   :        :        +- SubqueryAlias spark_catalog.default.myt3
   :        :           +- Relation spark_catalog.default.myt3[a#x,b#x,c#x] parquet
   :        +- SubqueryAlias spark_catalog.default.myt2
   :           +- Relation spark_catalog.default.myt2[a#x,b#x,c#x] parquet
   +- SubqueryAlias spark_catalog.default.myt1
      +- Relation spark_catalog.default.myt1[a#x,b#x,c#x] parquet


-- !query
SELECT myt1.a, (
    SELECT (
            SELECT MAX(myt3.a)
            FROM myt3
            WHERE myt3.b > myt2.b AND myt3.c > myt1.c
        )
    FROM myt2
)
FROM myt1
-- !query analysis
Project [a#x, scalar-subquery#x [c#x] AS scalarsubquery(c)#x]
:  +- Project [scalar-subquery#x [b#x && c#x] AS scalarsubquery(b, c)#x]
:     :  +- Aggregate [max(a#x) AS max(a)#x]
:     :     +- Filter ((b#x > outer(b#x)) AND (c#x > outer(c#x)))
:     :        +- SubqueryAlias spark_catalog.default.myt3
:     :           +- Relation spark_catalog.default.myt3[a#x,b#x,c#x] parquet
:     +- SubqueryAlias spark_catalog.default.myt2
:        +- Relation spark_catalog.default.myt2[a#x,b#x,c#x] parquet
+- SubqueryAlias spark_catalog.default.myt1
   +- Relation spark_catalog.default.myt1[a#x,b#x,c#x] parquet


-- !query
SELECT myt1.a, (
    SELECT (
            SELECT MAX(myt3.a)
            FROM myt3
            WHERE myt3.b = myt2.b AND myt3.c = myt1.c
        )
    FROM myt2
)
FROM myt1
-- !query analysis
Project [a#x, scalar-subquery#x [c#x] AS scalarsubquery(c)#x]
:  +- Project [scalar-subquery#x [b#x && c#x] AS scalarsubquery(b, c)#x]
:     :  +- Aggregate [max(a#x) AS max(a)#x]
:     :     +- Filter ((b#x = outer(b#x)) AND (c#x = outer(c#x)))
:     :        +- SubqueryAlias spark_catalog.default.myt3
:     :           +- Relation spark_catalog.default.myt3[a#x,b#x,c#x] parquet
:     +- SubqueryAlias spark_catalog.default.myt2
:        +- Relation spark_catalog.default.myt2[a#x,b#x,c#x] parquet
+- SubqueryAlias spark_catalog.default.myt1
   +- Relation spark_catalog.default.myt1[a#x,b#x,c#x] parquet


-- !query
SELECT myt1.a, (
    SELECT (
            SELECT COUNT(myt3.a)
            FROM myt3
            WHERE myt3.b > myt2.b AND myt3.c > myt1.c
        )
    FROM myt2
)
FROM myt1
-- !query analysis
Project [a#x, scalar-subquery#x [c#x] AS scalarsubquery(c)#xL]
:  +- Project [scalar-subquery#x [b#x && c#x] AS scalarsubquery(b, c)#xL]
:     :  +- Aggregate [count(a#x) AS count(a)#xL]
:     :     +- Filter ((b#x > outer(b#x)) AND (c#x > outer(c#x)))
:     :        +- SubqueryAlias spark_catalog.default.myt3
:     :           +- Relation spark_catalog.default.myt3[a#x,b#x,c#x] parquet
:     +- SubqueryAlias spark_catalog.default.myt2
:        +- Relation spark_catalog.default.myt2[a#x,b#x,c#x] parquet
+- SubqueryAlias spark_catalog.default.myt1
   +- Relation spark_catalog.default.myt1[a#x,b#x,c#x] parquet


-- !query
SELECT myt1.a, (
    SELECT (
            SELECT COUNT(myt3.a)
            FROM myt3
            WHERE myt3.b = myt2.b AND myt3.c = myt1.c
        )
    FROM myt2
)
FROM myt1
-- !query analysis
Project [a#x, scalar-subquery#x [c#x] AS scalarsubquery(c)#xL]
:  +- Project [scalar-subquery#x [b#x && c#x] AS scalarsubquery(b, c)#xL]
:     :  +- Aggregate [count(a#x) AS count(a)#xL]
:     :     +- Filter ((b#x = outer(b#x)) AND (c#x = outer(c#x)))
:     :        +- SubqueryAlias spark_catalog.default.myt3
:     :           +- Relation spark_catalog.default.myt3[a#x,b#x,c#x] parquet
:     +- SubqueryAlias spark_catalog.default.myt2
:        +- Relation spark_catalog.default.myt2[a#x,b#x,c#x] parquet
+- SubqueryAlias spark_catalog.default.myt1
   +- Relation spark_catalog.default.myt1[a#x,b#x,c#x] parquet


-- !query
SELECT MIN(
        SELECT MAX(
            SELECT MAX(myt3.a)
            FROM myt3
            WHERE myt3.b > myt2.b AND myt3.c > myt1.c
        )
        FROM myt2
    )
FROM myt1
-- !query analysis
org.apache.spark.sql.catalyst.parser.ParseException
{
  "errorClass" : "PARSE_SYNTAX_ERROR",
  "sqlState" : "42601",
  "messageParameters" : {
    "error" : "'MAX'",
    "hint" : ""
  }
}


-- !query
SELECT MIN(
        SELECT MAX(
            SELECT MAX(myt3.a)
            FROM myt3
            WHERE myt3.b = myt2.b AND myt3.c = myt1.c
        )
        FROM myt2
    )
FROM myt1
-- !query analysis
org.apache.spark.sql.catalyst.parser.ParseException
{
  "errorClass" : "PARSE_SYNTAX_ERROR",
  "sqlState" : "42601",
  "messageParameters" : {
    "error" : "'MAX'",
    "hint" : ""
  }
}


-- !query
SELECT COUNT(
        SELECT COUNT(
            SELECT COUNT(myt3.a)
            FROM myt3
            WHERE myt3.b > myt2.b AND myt3.c > myt1.c
        )
        FROM myt2
    )
FROM myt1
-- !query analysis
org.apache.spark.sql.catalyst.parser.ParseException
{
  "errorClass" : "PARSE_SYNTAX_ERROR",
  "sqlState" : "42601",
  "messageParameters" : {
    "error" : "'COUNT'",
    "hint" : ""
  }
}


-- !query
SELECT COUNT(
        SELECT COUNT(
            SELECT COUNT(myt3.a)
            FROM myt3
            WHERE myt3.b = myt2.b AND myt3.c = myt1.c
        )
        FROM myt2
    )
FROM myt1
-- !query analysis
org.apache.spark.sql.catalyst.parser.ParseException
{
  "errorClass" : "PARSE_SYNTAX_ERROR",
  "sqlState" : "42601",
  "messageParameters" : {
    "error" : "'COUNT'",
    "hint" : ""
  }
}


-- !query
SELECT b, MAX(myt1.a)
FROM myt1
GROUP BY b
HAVING (
    SELECT MAX(myt2.a)
    FROM myt2
    WHERE myt2.a = (
        SELECT MAX(myt3.a)
        FROM myt3
        WHERE myt3.a > MAX(myt1.a)
    ) AND myt2.b > myt1.b
)
-- !query analysis
Filter cast(scalar-subquery#x [b#x && max(a)#x] as boolean)
:  +- Aggregate [max(a#x) AS max(a)#x]
:     +- Filter ((a#x = scalar-subquery#x [max(a)#x]) AND (b#x > outer(b#x)))
:        :  +- Aggregate [max(a#x) AS max(a)#x]
:        :     +- Filter (a#x > outer(max(a)#x))
:        :        +- SubqueryAlias spark_catalog.default.myt3
:        :           +- Relation spark_catalog.default.myt3[a#x,b#x,c#x] parquet
:        +- SubqueryAlias spark_catalog.default.myt2
:           +- Relation spark_catalog.default.myt2[a#x,b#x,c#x] parquet
+- Aggregate [b#x], [b#x, max(a#x) AS max(a)#x]
   +- SubqueryAlias spark_catalog.default.myt1
      +- Relation spark_catalog.default.myt1[a#x,b#x,c#x] parquet


-- !query
SELECT b, MAX(myt1.a)
FROM myt1
GROUP BY b
HAVING (
    SELECT MAX(myt2.a)
    FROM myt2
    WHERE myt2.a = (
        SELECT MAX(myt3.a)
        FROM myt3
        WHERE myt3.a = MAX(myt1.a)
    ) AND myt2.b = myt1.b
)
-- !query analysis
Filter cast(scalar-subquery#x [b#x && max(a)#x] as boolean)
:  +- Aggregate [max(a#x) AS max(a)#x]
:     +- Filter ((a#x = scalar-subquery#x [max(a)#x]) AND (b#x = outer(b#x)))
:        :  +- Aggregate [max(a#x) AS max(a)#x]
:        :     +- Filter (a#x = outer(max(a)#x))
:        :        +- SubqueryAlias spark_catalog.default.myt3
:        :           +- Relation spark_catalog.default.myt3[a#x,b#x,c#x] parquet
:        +- SubqueryAlias spark_catalog.default.myt2
:           +- Relation spark_catalog.default.myt2[a#x,b#x,c#x] parquet
+- Aggregate [b#x], [b#x, max(a#x) AS max(a)#x]
   +- SubqueryAlias spark_catalog.default.myt1
      +- Relation spark_catalog.default.myt1[a#x,b#x,c#x] parquet


-- !query
SELECT b, MAX(myt1.a)
FROM myt1
GROUP BY b
HAVING (
    SELECT COUNT(myt2.a)
    FROM myt2
    WHERE myt2.a = (
        SELECT COUNT(myt3.a)
        FROM myt3
        WHERE myt3.a > MAX(myt1.a)
    ) AND myt2.b > myt1.b
)
-- !query analysis
Filter cast(scalar-subquery#x [b#x && max(a)#x] as boolean)
:  +- Aggregate [count(a#x) AS count(a)#xL]
:     +- Filter ((cast(a#x as bigint) = scalar-subquery#x [max(a)#x]) AND (b#x > outer(b#x)))
:        :  +- Aggregate [count(a#x) AS count(a)#xL]
:        :     +- Filter (a#x > outer(max(a)#x))
:        :        +- SubqueryAlias spark_catalog.default.myt3
:        :           +- Relation spark_catalog.default.myt3[a#x,b#x,c#x] parquet
:        +- SubqueryAlias spark_catalog.default.myt2
:           +- Relation spark_catalog.default.myt2[a#x,b#x,c#x] parquet
+- Aggregate [b#x], [b#x, max(a#x) AS max(a)#x]
   +- SubqueryAlias spark_catalog.default.myt1
      +- Relation spark_catalog.default.myt1[a#x,b#x,c#x] parquet


-- !query
SELECT b, MAX(myt1.a)
FROM myt1
GROUP BY b
HAVING (
    SELECT COUNT(myt2.a)
    FROM myt2
    WHERE myt2.a = (
        SELECT COUNT(myt3.a)
        FROM myt3
        WHERE myt3.a = MAX(myt1.a)
    ) AND myt2.b = myt1.b
)
-- !query analysis
Filter cast(scalar-subquery#x [b#x && max(a)#x] as boolean)
:  +- Aggregate [count(a#x) AS count(a)#xL]
:     +- Filter ((cast(a#x as bigint) = scalar-subquery#x [max(a)#x]) AND (b#x = outer(b#x)))
:        :  +- Aggregate [count(a#x) AS count(a)#xL]
:        :     +- Filter (a#x = outer(max(a)#x))
:        :        +- SubqueryAlias spark_catalog.default.myt3
:        :           +- Relation spark_catalog.default.myt3[a#x,b#x,c#x] parquet
:        +- SubqueryAlias spark_catalog.default.myt2
:           +- Relation spark_catalog.default.myt2[a#x,b#x,c#x] parquet
+- Aggregate [b#x], [b#x, max(a#x) AS max(a)#x]
   +- SubqueryAlias spark_catalog.default.myt1
      +- Relation spark_catalog.default.myt1[a#x,b#x,c#x] parquet


-- !query
SELECT myt1.a
FROM myt1
WHERE EXISTS (
  SELECT 1
  FROM myt2
  WHERE myt2.a = (
    SELECT MAX(myt3.a)
    FROM myt3
    WHERE myt3.b > myt2.b AND myt3.c > myt1.c
  ) AND myt2.b > myt1.b
)
-- !query analysis
Project [a#x]
+- Filter exists#x [b#x && c#x]
   :  +- Project [1 AS 1#x]
   :     +- Filter ((a#x = scalar-subquery#x [b#x && c#x]) AND (b#x > outer(b#x)))
   :        :  +- Aggregate [max(a#x) AS max(a)#x]
   :        :     +- Filter ((b#x > outer(b#x)) AND (c#x > outer(c#x)))
   :        :        +- SubqueryAlias spark_catalog.default.myt3
   :        :           +- Relation spark_catalog.default.myt3[a#x,b#x,c#x] parquet
   :        +- SubqueryAlias spark_catalog.default.myt2
   :           +- Relation spark_catalog.default.myt2[a#x,b#x,c#x] parquet
   +- SubqueryAlias spark_catalog.default.myt1
      +- Relation spark_catalog.default.myt1[a#x,b#x,c#x] parquet


-- !query
SELECT myt1.a
FROM myt1
WHERE myt1.b = (
  SELECT myt2.b
  FROM myt2
  WHERE EXISTS (
    SELECT 1
    FROM myt3
    WHERE myt3.b > myt2.b AND myt3.c > myt1.c
  ) AND myt2.b > myt1.b
)
-- !query analysis
org.apache.spark.sql.catalyst.ExtendedAnalysisException
{
  "errorClass" : "NESTED_REFERENCES_IN_SUBQUERY_NOT_SUPPORTED",
  "sqlState" : "0A000",
  "messageParameters" : {
    "expression" : "spark_catalog.default.myt1.c"
  }
}


-- !query
SELECT 1 FROM (SELECT 1) t0(c0) WHERE (SELECT (SELECT c0)) = 1
-- !query analysis
Project [1 AS 1#x]
+- Filter (scalar-subquery#x [c0#x] = 1)
   :  +- Project [scalar-subquery#x [c0#x] AS scalarsubquery(c0)#x]
   :     :  +- Project [outer(c0#x)]
   :     :     +- OneRowRelation
   :     +- OneRowRelation
   +- SubqueryAlias t0
      +- Project [1#x AS c0#x]
         +- Project [1 AS 1#x]
            +- OneRowRelation


-- !query
DROP TABLE IF EXISTS table_integers
-- !query analysis
DropTable true, false
+- ResolvedIdentifier V2SessionCatalog(spark_catalog), default.table_integers


-- !query
CREATE TABLE table_integers(i INTEGER)
-- !query analysis
CreateDataSourceTableCommand `spark_catalog`.`default`.`table_integers`, false


-- !query
INSERT INTO table_integers VALUES (1), (2), (3), (NULL)
-- !query analysis
InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/table_integers, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/table_integers], Append, `spark_catalog`.`default`.`table_integers`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/table_integers), [i]
+- Project [cast(col1#x as int) AS i#x]
   +- LocalRelation [col1#x]


-- !query
SELECT i, (SELECT (SELECT 42+i1.i)+42+i1.i) AS j FROM table_integers i1 ORDER BY i
-- !query analysis
Sort [i#x ASC NULLS FIRST], true
+- Project [i#x, scalar-subquery#x [i#x && i#x] AS j#x]
   :  +- Project [((scalar-subquery#x [i#x] + 42) + outer(i#x)) AS ((scalarsubquery(i) + 42) + outer(i1.i))#x]
   :     :  +- Project [(42 + outer(i#x)) AS (42 + outer(i1.i))#x]
   :     :     +- OneRowRelation
   :     +- OneRowRelation
   +- SubqueryAlias i1
      +- SubqueryAlias spark_catalog.default.table_integers
         +- Relation spark_catalog.default.table_integers[i#x] parquet


-- !query
SELECT i, (SELECT (SELECT (SELECT (SELECT 42+i1.i)++i1.i)+42+i1.i)+42+i1.i) AS j FROM table_integers i1 ORDER BY i
-- !query analysis
Sort [i#x ASC NULLS FIRST], true
+- Project [i#x, scalar-subquery#x [i#x && i#x && i#x && i#x] AS j#x]
   :  +- Project [((scalar-subquery#x [i#x && i#x && i#x] + 42) + outer(i#x)) AS ((scalarsubquery(i, i, i) + 42) + outer(i1.i))#x]
   :     :  +- Project [((scalar-subquery#x [i#x && i#x] + 42) + outer(i#x)) AS ((scalarsubquery(i, i) + 42) + outer(i1.i))#x]
   :     :     :  +- Project [(scalar-subquery#x [i#x] + positive(outer(i#x))) AS (scalarsubquery(i) + (+ outer(i1.i)))#x]
   :     :     :     :  +- Project [(42 + outer(i#x)) AS (42 + outer(i1.i))#x]
   :     :     :     :     +- OneRowRelation
   :     :     :     +- OneRowRelation
   :     :     +- OneRowRelation
   :     +- OneRowRelation
   +- SubqueryAlias i1
      +- SubqueryAlias spark_catalog.default.table_integers
         +- Relation spark_catalog.default.table_integers[i#x] parquet


-- !query
SELECT i, (SELECT (SELECT (SELECT (SELECT i1.i+i1.i+i1.i+i1.i+i1.i)))) AS j FROM table_integers i1 ORDER BY i
-- !query analysis
Sort [i#x ASC NULLS FIRST], true
+- Project [i#x, scalar-subquery#x [i#x && i#x && i#x && i#x && i#x] AS j#x]
   :  +- Project [scalar-subquery#x [i#x && i#x && i#x && i#x && i#x] AS scalarsubquery(i, i, i, i, i)#x]
   :     :  +- Project [scalar-subquery#x [i#x && i#x && i#x && i#x && i#x] AS scalarsubquery(i, i, i, i, i)#x]
   :     :     :  +- Project [scalar-subquery#x [i#x && i#x && i#x && i#x && i#x] AS scalarsubquery(i, i, i, i, i)#x]
   :     :     :     :  +- Project [((((outer(i#x) + outer(i#x)) + outer(i#x)) + outer(i#x)) + outer(i#x)) AS ((((outer(i1.i) + outer(i1.i)) + outer(i1.i)) + outer(i1.i)) + outer(i1.i))#x]
   :     :     :     :     +- OneRowRelation
   :     :     :     +- OneRowRelation
   :     :     +- OneRowRelation
   :     +- OneRowRelation
   +- SubqueryAlias i1
      +- SubqueryAlias spark_catalog.default.table_integers
         +- Relation spark_catalog.default.table_integers[i#x] parquet


-- !query
SELECT i, (SELECT (SELECT (SELECT (SELECT i1.i+i1.i+i1.i+i1.i+i1.i+i2.i) FROM table_integers i2 WHERE i2.i=i1.i))) AS j FROM table_integers i1 ORDER BY i
-- !query analysis
Sort [i#x ASC NULLS FIRST], true
+- Project [i#x, scalar-subquery#x [i#x] AS j#x]
   :  +- Project [scalar-subquery#x [i#x] AS scalarsubquery(i)#x]
   :     :  +- Project [scalar-subquery#x [i#x] AS scalarsubquery(i)#x]
   :     :     :  +- Project [scalar-subquery#x [i#x && i#x && i#x && i#x && i#x && i#x] AS scalarsubquery(i, i, i, i, i, i)#x]
   :     :     :     :  +- Project [(((((outer(i#x) + outer(i#x)) + outer(i#x)) + outer(i#x)) + outer(i#x)) + outer(i#x)) AS (((((outer(i1.i) + outer(i1.i)) + outer(i1.i)) + outer(i1.i)) + outer(i1.i)) + outer(i2.i))#x]
   :     :     :     :     +- OneRowRelation
   :     :     :     +- Filter (i#x = outer(i#x))
   :     :     :        +- SubqueryAlias i2
   :     :     :           +- SubqueryAlias spark_catalog.default.table_integers
   :     :     :              +- Relation spark_catalog.default.table_integers[i#x] parquet
   :     :     +- OneRowRelation
   :     +- OneRowRelation
   +- SubqueryAlias i1
      +- SubqueryAlias spark_catalog.default.table_integers
         +- Relation spark_catalog.default.table_integers[i#x] parquet


-- !query
SELECT i, (SELECT SUM(s1.i) FROM (SELECT i FROM table_integers WHERE i=i1.i) s1 LEFT OUTER JOIN table_integers s2 ON s1.i=s2.i) AS j FROM table_integers i1 ORDER BY i
-- !query analysis
Sort [i#x ASC NULLS FIRST], true
+- Project [i#x, scalar-subquery#x [i#x] AS j#xL]
   :  +- Aggregate [sum(i#x) AS sum(i)#xL]
   :     +- Join LeftOuter, (i#x = i#x)
   :        :- SubqueryAlias s1
   :        :  +- Project [i#x]
   :        :     +- Filter (i#x = outer(i#x))
   :        :        +- SubqueryAlias spark_catalog.default.table_integers
   :        :           +- Relation spark_catalog.default.table_integers[i#x] parquet
   :        +- SubqueryAlias s2
   :           +- SubqueryAlias spark_catalog.default.table_integers
   :              +- Relation spark_catalog.default.table_integers[i#x] parquet
   +- SubqueryAlias i1
      +- SubqueryAlias spark_catalog.default.table_integers
         +- Relation spark_catalog.default.table_integers[i#x] parquet


-- !query
SELECT i, (SELECT SUM(s1.i) FROM (SELECT i FROM table_integers WHERE i<>i1.i) s1 LEFT OUTER JOIN table_integers s2 ON s1.i=s2.i) AS j FROM table_integers i1 ORDER BY i
-- !query analysis
Sort [i#x ASC NULLS FIRST], true
+- Project [i#x, scalar-subquery#x [i#x] AS j#xL]
   :  +- Aggregate [sum(i#x) AS sum(i)#xL]
   :     +- Join LeftOuter, (i#x = i#x)
   :        :- SubqueryAlias s1
   :        :  +- Project [i#x]
   :        :     +- Filter NOT (i#x = outer(i#x))
   :        :        +- SubqueryAlias spark_catalog.default.table_integers
   :        :           +- Relation spark_catalog.default.table_integers[i#x] parquet
   :        +- SubqueryAlias s2
   :           +- SubqueryAlias spark_catalog.default.table_integers
   :              +- Relation spark_catalog.default.table_integers[i#x] parquet
   +- SubqueryAlias i1
      +- SubqueryAlias spark_catalog.default.table_integers
         +- Relation spark_catalog.default.table_integers[i#x] parquet


-- !query
SELECT i, (SELECT SUM(ss2.i) FROM (SELECT i FROM table_integers s1 WHERE i=i1.i) ss2) AS j FROM table_integers i1 ORDER BY i
-- !query analysis
Sort [i#x ASC NULLS FIRST], true
+- Project [i#x, scalar-subquery#x [i#x] AS j#xL]
   :  +- Aggregate [sum(i#x) AS sum(i)#xL]
   :     +- SubqueryAlias ss2
   :        +- Project [i#x]
   :           +- Filter (i#x = outer(i#x))
   :              +- SubqueryAlias s1
   :                 +- SubqueryAlias spark_catalog.default.table_integers
   :                    +- Relation spark_catalog.default.table_integers[i#x] parquet
   +- SubqueryAlias i1
      +- SubqueryAlias spark_catalog.default.table_integers
         +- Relation spark_catalog.default.table_integers[i#x] parquet


-- !query
SELECT i, (SELECT * FROM (SELECT (SELECT 42+i1.i)) s1) AS j FROM table_integers i1 ORDER BY i
-- !query analysis
Sort [i#x ASC NULLS FIRST], true
+- Project [i#x, scalar-subquery#x [i#x] AS j#x]
   :  +- Project [scalarsubquery(i)#x]
   :     +- SubqueryAlias s1
   :        +- Project [scalar-subquery#x [i#x] AS scalarsubquery(i)#x]
   :           :  +- Project [(42 + outer(i#x)) AS (42 + outer(i1.i))#x]
   :           :     +- OneRowRelation
   :           +- OneRowRelation
   +- SubqueryAlias i1
      +- SubqueryAlias spark_catalog.default.table_integers
         +- Relation spark_catalog.default.table_integers[i#x] parquet


-- !query
SELECT i, (SELECT s1.k+s2.k FROM (SELECT (SELECT 42+i1.i) AS k) s1, (SELECT (SELECT 42+i1.i) AS k) s2) AS j FROM table_integers i1 ORDER BY i
-- !query analysis
Sort [i#x ASC NULLS FIRST], true
+- Project [i#x, scalar-subquery#x [i#x && i#x] AS j#x]
   :  +- Project [(k#x + k#x) AS (k + k)#x]
   :     +- Join Inner
   :        :- SubqueryAlias s1
   :        :  +- Project [scalar-subquery#x [i#x] AS k#x]
   :        :     :  +- Project [(42 + outer(i#x)) AS (42 + outer(i1.i))#x]
   :        :     :     +- OneRowRelation
   :        :     +- OneRowRelation
   :        +- SubqueryAlias s2
   :           +- Project [scalar-subquery#x [i#x] AS k#x]
   :              :  +- Project [(42 + outer(i#x)) AS (42 + outer(i1.i))#x]
   :              :     +- OneRowRelation
   :              +- OneRowRelation
   +- SubqueryAlias i1
      +- SubqueryAlias spark_catalog.default.table_integers
         +- Relation spark_catalog.default.table_integers[i#x] parquet


-- !query
SELECT i, (SELECT s1.k+s2.k FROM (SELECT (SELECT 42+i1.i) AS k) s1 LEFT OUTER JOIN (SELECT (SELECT 42+i1.i) AS k) s2 ON s1.k=s2.k) AS j FROM table_integers i1 ORDER BY i
-- !query analysis
Sort [i#x ASC NULLS FIRST], true
+- Project [i#x, scalar-subquery#x [i#x && i#x] AS j#x]
   :  +- Project [(k#x + k#x) AS (k + k)#x]
   :     +- Join LeftOuter, (k#x = k#x)
   :        :- SubqueryAlias s1
   :        :  +- Project [scalar-subquery#x [i#x] AS k#x]
   :        :     :  +- Project [(42 + outer(i#x)) AS (42 + outer(i1.i))#x]
   :        :     :     +- OneRowRelation
   :        :     +- OneRowRelation
   :        +- SubqueryAlias s2
   :           +- Project [scalar-subquery#x [i#x] AS k#x]
   :              :  +- Project [(42 + outer(i#x)) AS (42 + outer(i1.i))#x]
   :              :     +- OneRowRelation
   :              +- OneRowRelation
   +- SubqueryAlias i1
      +- SubqueryAlias spark_catalog.default.table_integers
         +- Relation spark_catalog.default.table_integers[i#x] parquet


-- !query
SELECT i, (SELECT i1.i IN (1, 2, 3, 4, 5, 6, 7, 8)) AS j FROM table_integers i1 ORDER BY i
-- !query analysis
Sort [i#x ASC NULLS FIRST], true
+- Project [i#x, scalar-subquery#x [i#x] AS j#x]
   :  +- Project [outer(i#x) IN (1,2,3,4,5,6,7,8) AS (outer(i1.i) IN (1, 2, 3, 4, 5, 6, 7, 8))#x]
   :     +- OneRowRelation
   +- SubqueryAlias i1
      +- SubqueryAlias spark_catalog.default.table_integers
         +- Relation spark_catalog.default.table_integers[i#x] parquet
