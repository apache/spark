-- Automatically generated by SQLQueryTestSuite
-- !query
drop table if exists x
-- !query analysis
DropTable true, false
+- ResolvedIdentifier V2SessionCatalog(spark_catalog), default.x


-- !query
drop table if exists y
-- !query analysis
DropTable true, false
+- ResolvedIdentifier V2SessionCatalog(spark_catalog), default.y


-- !query
create table x(xm map<int, int>, x2 int) using parquet
-- !query analysis
CreateDataSourceTableCommand `spark_catalog`.`default`.`x`, false


-- !query
insert into x values (map(1, 2), 3), (map(1, 4), 5), (map(2, 3), 4), (map(5, 6), 7)
-- !query analysis
InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/x, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/x], Append, `spark_catalog`.`default`.`x`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/x), [xm, x2]
+- Project [map_from_arrays(transform(map_keys(col1#x), lambdafunction(lambda key#x, lambda key#x, false)), transform(map_values(col1#x), lambdafunction(lambda value#x, lambda value#x, false))) AS xm#x, cast(col2#x as int) AS x2#x]
   +- LocalRelation [col1#x, col2#x]


-- !query
create table y(ym map<int, int>, y2 int) using parquet
-- !query analysis
CreateDataSourceTableCommand `spark_catalog`.`default`.`y`, false


-- !query
insert into y values (map(1, 2), 10), (map(1, 3), 20), (map(2, 3), 20), (map(8, 3), 20)
-- !query analysis
InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/y, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/y], Append, `spark_catalog`.`default`.`y`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/y), [ym, y2]
+- Project [map_from_arrays(transform(map_keys(col1#x), lambdafunction(lambda key#x, lambda key#x, false)), transform(map_values(col1#x), lambdafunction(lambda value#x, lambda value#x, false))) AS ym#x, cast(col2#x as int) AS y2#x]
   +- LocalRelation [col1#x, col2#x]


-- !query
select * from x where (select sum(y2) from y where xm[1] = ym[1]) > 2
-- !query analysis
Project [xm#x, x2#x]
+- Filter (scalar-subquery#x [xm#x] > cast(2 as bigint))
   :  +- Aggregate [sum(y2#x) AS sum(y2)#xL]
   :     +- Filter (outer(xm#x)[1] = ym#x[1])
   :        +- SubqueryAlias spark_catalog.default.y
   :           +- Relation spark_catalog.default.y[ym#x,y2#x] parquet
   +- SubqueryAlias spark_catalog.default.x
      +- Relation spark_catalog.default.x[xm#x,x2#x] parquet


-- !query
select * from x where exists (select * from y where xm[1] = ym[1])
-- !query analysis
Project [xm#x, x2#x]
+- Filter exists#x [xm#x]
   :  +- Project [ym#x, y2#x]
   :     +- Filter (outer(xm#x)[1] = ym#x[1])
   :        +- SubqueryAlias spark_catalog.default.y
   :           +- Relation spark_catalog.default.y[ym#x,y2#x] parquet
   +- SubqueryAlias spark_catalog.default.x
      +- Relation spark_catalog.default.x[xm#x,x2#x] parquet


-- !query
select * from x where exists (select * from y where xm[1] = ym[1] limit 1)
-- !query analysis
Project [xm#x, x2#x]
+- Filter exists#x [xm#x]
   :  +- GlobalLimit 1
   :     +- LocalLimit 1
   :        +- Project [ym#x, y2#x]
   :           +- Filter (outer(xm#x)[1] = ym#x[1])
   :              +- SubqueryAlias spark_catalog.default.y
   :                 +- Relation spark_catalog.default.y[ym#x,y2#x] parquet
   +- SubqueryAlias spark_catalog.default.x
      +- Relation spark_catalog.default.x[xm#x,x2#x] parquet


-- !query
select * from x join lateral (select * from y where xm[1] = ym[1])
-- !query analysis
Project [xm#x, x2#x, ym#x, y2#x]
+- LateralJoin lateral-subquery#x [xm#x], Inner
   :  +- SubqueryAlias __auto_generated_subquery_name
   :     +- Project [ym#x, y2#x]
   :        +- Filter (outer(xm#x)[1] = ym#x[1])
   :           +- SubqueryAlias spark_catalog.default.y
   :              +- Relation spark_catalog.default.y[ym#x,y2#x] parquet
   +- SubqueryAlias spark_catalog.default.x
      +- Relation spark_catalog.default.x[xm#x,x2#x] parquet


-- !query
select * from x join lateral (select * from y where xm[1] = ym[1] union all select * from y where xm[1] = ym[1] + 1)
-- !query analysis
Project [xm#x, x2#x, ym#x, y2#x]
+- LateralJoin lateral-subquery#x [xm#x && xm#x], Inner
   :  +- SubqueryAlias __auto_generated_subquery_name
   :     +- Union false, false
   :        :- Project [ym#x, y2#x]
   :        :  +- Filter (outer(xm#x)[1] = ym#x[1])
   :        :     +- SubqueryAlias spark_catalog.default.y
   :        :        +- Relation spark_catalog.default.y[ym#x,y2#x] parquet
   :        +- Project [ym#x, y2#x]
   :           +- Filter (outer(xm#x)[1] = (ym#x[1] + 1))
   :              +- SubqueryAlias spark_catalog.default.y
   :                 +- Relation spark_catalog.default.y[ym#x,y2#x] parquet
   +- SubqueryAlias spark_catalog.default.x
      +- Relation spark_catalog.default.x[xm#x,x2#x] parquet


-- !query
select * from x join lateral (select * from y where xm[1] = ym[1] limit 1)
-- !query analysis
Project [xm#x, x2#x, ym#x, y2#x]
+- LateralJoin lateral-subquery#x [xm#x], Inner
   :  +- SubqueryAlias __auto_generated_subquery_name
   :     +- GlobalLimit 1
   :        +- LocalLimit 1
   :           +- Project [ym#x, y2#x]
   :              +- Filter (outer(xm#x)[1] = ym#x[1])
   :                 +- SubqueryAlias spark_catalog.default.y
   :                    +- Relation spark_catalog.default.y[ym#x,y2#x] parquet
   +- SubqueryAlias spark_catalog.default.x
      +- Relation spark_catalog.default.x[xm#x,x2#x] parquet


-- !query
select * from x join lateral (select count(*) from y where xm[1] = ym[1] group by y2)
-- !query analysis
Project [xm#x, x2#x, count(1)#xL]
+- LateralJoin lateral-subquery#x [xm#x], Inner
   :  +- SubqueryAlias __auto_generated_subquery_name
   :     +- Aggregate [y2#x], [count(1) AS count(1)#xL]
   :        +- Filter (outer(xm#x)[1] = ym#x[1])
   :           +- SubqueryAlias spark_catalog.default.y
   :              +- Relation spark_catalog.default.y[ym#x,y2#x] parquet
   +- SubqueryAlias spark_catalog.default.x
      +- Relation spark_catalog.default.x[xm#x,x2#x] parquet


-- !query
select * from x where xm[1] in (select ym[1] from y)
-- !query analysis
Project [xm#x, x2#x]
+- Filter xm#x[1] IN (list#x [])
   :  +- Project [ym#x[1] AS ym[1]#x]
   :     +- SubqueryAlias spark_catalog.default.y
   :        +- Relation spark_catalog.default.y[ym#x,y2#x] parquet
   +- SubqueryAlias spark_catalog.default.x
      +- Relation spark_catalog.default.x[xm#x,x2#x] parquet


-- !query
select * from x where xm[1] in (select sum(ym[1]) from y group by y2)
-- !query analysis
Project [xm#x, x2#x]
+- Filter cast(xm#x[1] as bigint) IN (list#x [])
   :  +- Project [sum(ym[1])#xL]
   :     +- Aggregate [y2#x], [sum(ym#x[1]) AS sum(ym[1])#xL]
   :        +- SubqueryAlias spark_catalog.default.y
   :           +- Relation spark_catalog.default.y[ym#x,y2#x] parquet
   +- SubqueryAlias spark_catalog.default.x
      +- Relation spark_catalog.default.x[xm#x,x2#x] parquet


-- !query
select * from x where (select sum(y2) from y where xm[1] = ym[1] and xm[1] >= 1) > 2
-- !query analysis
Project [xm#x, x2#x]
+- Filter (scalar-subquery#x [xm#x && xm#x] > cast(2 as bigint))
   :  +- Aggregate [sum(y2#x) AS sum(y2)#xL]
   :     +- Filter ((outer(xm#x)[1] = ym#x[1]) AND (outer(xm#x)[1] >= 1))
   :        +- SubqueryAlias spark_catalog.default.y
   :           +- Relation spark_catalog.default.y[ym#x,y2#x] parquet
   +- SubqueryAlias spark_catalog.default.x
      +- Relation spark_catalog.default.x[xm#x,x2#x] parquet


-- !query
select * from x where (select sum(y2) from y where xm[1] = ym[1] and xm[2] >= ym[2]) > 2
-- !query analysis
Project [xm#x, x2#x]
+- Filter (scalar-subquery#x [xm#x && xm#x] > cast(2 as bigint))
   :  +- Aggregate [sum(y2#x) AS sum(y2)#xL]
   :     +- Filter ((outer(xm#x)[1] = ym#x[1]) AND (outer(xm#x)[2] >= ym#x[2]))
   :        +- SubqueryAlias spark_catalog.default.y
   :           +- Relation spark_catalog.default.y[ym#x,y2#x] parquet
   +- SubqueryAlias spark_catalog.default.x
      +- Relation spark_catalog.default.x[xm#x,x2#x] parquet


-- !query
select * from x where (select sum(y2) from y where xm[1] = ym[1]) > 2 and (select count(y2) from y where xm[1] = ym[1]) < 3
-- !query analysis
Project [xm#x, x2#x]
+- Filter ((scalar-subquery#x [xm#x] > cast(2 as bigint)) AND (scalar-subquery#x [xm#x] < cast(3 as bigint)))
   :  :- Aggregate [sum(y2#x) AS sum(y2)#xL]
   :  :  +- Filter (outer(xm#x)[1] = ym#x[1])
   :  :     +- SubqueryAlias spark_catalog.default.y
   :  :        +- Relation spark_catalog.default.y[ym#x,y2#x] parquet
   :  +- Aggregate [count(y2#x) AS count(y2)#xL]
   :     +- Filter (outer(xm#x)[1] = ym#x[1])
   :        +- SubqueryAlias spark_catalog.default.y
   :           +- Relation spark_catalog.default.y[ym#x,y2#x] parquet
   +- SubqueryAlias spark_catalog.default.x
      +- Relation spark_catalog.default.x[xm#x,x2#x] parquet


-- !query
select * from x join lateral (select xm[1] - ym[1] from y)
-- !query analysis
Project [xm#x, x2#x, (outer(spark_catalog.default.x.xm)[1] - ym[1])#x]
+- LateralJoin lateral-subquery#x [xm#x], Inner
   :  +- SubqueryAlias __auto_generated_subquery_name
   :     +- Project [(outer(xm#x)[1] - ym#x[1]) AS (outer(spark_catalog.default.x.xm)[1] - ym[1])#x]
   :        +- SubqueryAlias spark_catalog.default.y
   :           +- Relation spark_catalog.default.y[ym#x,y2#x] parquet
   +- SubqueryAlias spark_catalog.default.x
      +- Relation spark_catalog.default.x[xm#x,x2#x] parquet


-- !query
select * from x join lateral (select xm[1], xm[1] as s1, xm[1] - ym[1] as s2 from y)
-- !query analysis
Project [xm#x, x2#x, outer(spark_catalog.default.x.xm)[1]#x, s1#x, s2#x]
+- LateralJoin lateral-subquery#x [xm#x && xm#x && xm#x], Inner
   :  +- SubqueryAlias __auto_generated_subquery_name
   :     +- Project [outer(xm#x)[1] AS outer(spark_catalog.default.x.xm)[1]#x, outer(xm#x)[1] AS s1#x, (outer(xm#x)[1] - ym#x[1]) AS s2#x]
   :        +- SubqueryAlias spark_catalog.default.y
   :           +- Relation spark_catalog.default.y[ym#x,y2#x] parquet
   +- SubqueryAlias spark_catalog.default.x
      +- Relation spark_catalog.default.x[xm#x,x2#x] parquet


-- !query
select * from x join lateral (select xm[1], sum(ym[1]), xm[1] - sum(ym[1]) from y group by xm[1])
-- !query analysis
org.apache.spark.sql.catalyst.ExtendedAnalysisException
{
  "errorClass" : "UNSUPPORTED_SUBQUERY_EXPRESSION_CATEGORY.CORRELATED_REFERENCE",
  "sqlState" : "0A000",
  "messageParameters" : {
    "sqlExprs" : "\"xm[1]\",\"xm[1] AS `outer(spark_catalog.default.x.xm)[1]`\",\"(xm[1] - sum(ym[1])) AS `(outer(spark_catalog.default.x.xm)[1] - sum(ym[1]))`\""
  },
  "queryContext" : [ {
    "objectType" : "",
    "objectName" : "",
    "startIndex" : 83,
    "stopIndex" : 96,
    "fragment" : "group by xm[1]"
  } ]
}


-- !query
select * from x where (select sum(y2) from y where xm[x2] = ym[1]) > 2
-- !query analysis
Project [xm#x, x2#x]
+- Filter (scalar-subquery#x [xm#x && x2#x] > cast(2 as bigint))
   :  +- Aggregate [sum(y2#x) AS sum(y2)#xL]
   :     +- Filter (outer(xm#x)[outer(x2#x)] = ym#x[1])
   :        +- SubqueryAlias spark_catalog.default.y
   :           +- Relation spark_catalog.default.y[ym#x,y2#x] parquet
   +- SubqueryAlias spark_catalog.default.x
      +- Relation spark_catalog.default.x[xm#x,x2#x] parquet


-- !query
select * from x where (select sum(y2) from y where xm[x2+1] = ym[1]) > 2
-- !query analysis
Project [xm#x, x2#x]
+- Filter (scalar-subquery#x [xm#x && x2#x] > cast(2 as bigint))
   :  +- Aggregate [sum(y2#x) AS sum(y2)#xL]
   :     +- Filter (outer(xm#x)[(outer(x2#x) + 1)] = ym#x[1])
   :        +- SubqueryAlias spark_catalog.default.y
   :           +- Relation spark_catalog.default.y[ym#x,y2#x] parquet
   +- SubqueryAlias spark_catalog.default.x
      +- Relation spark_catalog.default.x[xm#x,x2#x] parquet


-- !query
select * from x where (select sum(y2) from y where xm[x2+1] = ym[1] and xm[1+x2] = ym[2]) > 2
-- !query analysis
Project [xm#x, x2#x]
+- Filter (scalar-subquery#x [xm#x && x2#x && xm#x && x2#x] > cast(2 as bigint))
   :  +- Aggregate [sum(y2#x) AS sum(y2)#xL]
   :     +- Filter ((outer(xm#x)[(outer(x2#x) + 1)] = ym#x[1]) AND (outer(xm#x)[(1 + outer(x2#x))] = ym#x[2]))
   :        +- SubqueryAlias spark_catalog.default.y
   :           +- Relation spark_catalog.default.y[ym#x,y2#x] parquet
   +- SubqueryAlias spark_catalog.default.x
      +- Relation spark_catalog.default.x[xm#x,x2#x] parquet


-- !query
select * from x where (select sum(y2) from y where xm[y2] = ym[1]) > 2
-- !query analysis
Project [xm#x, x2#x]
+- Filter (scalar-subquery#x [xm#x] > cast(2 as bigint))
   :  +- Aggregate [sum(y2#x) AS sum(y2)#xL]
   :     +- Filter (outer(xm#x)[y2#x] = ym#x[1])
   :        +- SubqueryAlias spark_catalog.default.y
   :           +- Relation spark_catalog.default.y[ym#x,y2#x] parquet
   +- SubqueryAlias spark_catalog.default.x
      +- Relation spark_catalog.default.x[xm#x,x2#x] parquet


-- !query
set spark.sql.optimizer.pullOutNestedDataOuterRefExpressions.enabled = false
-- !query analysis
SetCommand (spark.sql.optimizer.pullOutNestedDataOuterRefExpressions.enabled,Some(false))


-- !query
select * from x where (select sum(y2) from y where xm[1] = ym[1]) > 2
-- !query analysis
Project [xm#x, x2#x]
+- Filter (scalar-subquery#x [xm#x] > cast(2 as bigint))
   :  +- Aggregate [sum(y2#x) AS sum(y2)#xL]
   :     +- Filter (outer(xm#x)[1] = ym#x[1])
   :        +- SubqueryAlias spark_catalog.default.y
   :           +- Relation spark_catalog.default.y[ym#x,y2#x] parquet
   +- SubqueryAlias spark_catalog.default.x
      +- Relation spark_catalog.default.x[xm#x,x2#x] parquet


-- !query
reset spark.sql.optimizer.rewriteNestedDataCorrelation.enabled
-- !query analysis
ResetCommand spark.sql.optimizer.rewriteNestedDataCorrelation.enabled


-- !query
drop table x
-- !query analysis
DropTable false, false
+- ResolvedIdentifier V2SessionCatalog(spark_catalog), default.x


-- !query
drop table y
-- !query analysis
DropTable false, false
+- ResolvedIdentifier V2SessionCatalog(spark_catalog), default.y
