-- Automatically generated by SQLQueryTestSuite
-- !query
create table t1(s string, utf8_binary string collate utf8_binary, utf8_lcase string collate utf8_lcase) using parquet
-- !query analysis
CreateDataSourceTableCommand `spark_catalog`.`default`.`t1`, false


-- !query
insert into t1 values ('Spark', 'Spark', 'SQL')
-- !query analysis
InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/t1, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/t1], Append, `spark_catalog`.`default`.`t1`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/t1), [s, utf8_binary, utf8_lcase]
+- Project [col1#x AS s#x, col2#x AS utf8_binary#x, cast(col3#x as string collate UTF8_LCASE) AS utf8_lcase#x]
   +- LocalRelation [col1#x, col2#x, col3#x]


-- !query
insert into t1 values ('aaAaAAaA', 'aaAaAAaA', 'aaAaAAaA')
-- !query analysis
InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/t1, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/t1], Append, `spark_catalog`.`default`.`t1`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/t1), [s, utf8_binary, utf8_lcase]
+- Project [col1#x AS s#x, col2#x AS utf8_binary#x, cast(col3#x as string collate UTF8_LCASE) AS utf8_lcase#x]
   +- LocalRelation [col1#x, col2#x, col3#x]


-- !query
insert into t1 values ('aaAaAAaA', 'aaAaAAaA', 'aaAaaAaA')
-- !query analysis
InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/t1, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/t1], Append, `spark_catalog`.`default`.`t1`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/t1), [s, utf8_binary, utf8_lcase]
+- Project [col1#x AS s#x, col2#x AS utf8_binary#x, cast(col3#x as string collate UTF8_LCASE) AS utf8_lcase#x]
   +- LocalRelation [col1#x, col2#x, col3#x]


-- !query
insert into t1 values ('aaAaAAaA', 'aaAaAAaA', 'aaAaaAaAaaAaaAaAaaAaaAaA')
-- !query analysis
InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/t1, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/t1], Append, `spark_catalog`.`default`.`t1`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/t1), [s, utf8_binary, utf8_lcase]
+- Project [col1#x AS s#x, col2#x AS utf8_binary#x, cast(col3#x as string collate UTF8_LCASE) AS utf8_lcase#x]
   +- LocalRelation [col1#x, col2#x, col3#x]


-- !query
insert into t1 values ('bbAbaAbA', 'bbAbAAbA', 'a')
-- !query analysis
InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/t1, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/t1], Append, `spark_catalog`.`default`.`t1`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/t1), [s, utf8_binary, utf8_lcase]
+- Project [col1#x AS s#x, col2#x AS utf8_binary#x, cast(col3#x as string collate UTF8_LCASE) AS utf8_lcase#x]
   +- LocalRelation [col1#x, col2#x, col3#x]


-- !query
insert into t1 values ('İo', 'İo', 'İo')
-- !query analysis
InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/t1, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/t1], Append, `spark_catalog`.`default`.`t1`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/t1), [s, utf8_binary, utf8_lcase]
+- Project [col1#x AS s#x, col2#x AS utf8_binary#x, cast(col3#x as string collate UTF8_LCASE) AS utf8_lcase#x]
   +- LocalRelation [col1#x, col2#x, col3#x]


-- !query
insert into t1 values ('İo', 'İo', 'İo ')
-- !query analysis
InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/t1, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/t1], Append, `spark_catalog`.`default`.`t1`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/t1), [s, utf8_binary, utf8_lcase]
+- Project [col1#x AS s#x, col2#x AS utf8_binary#x, cast(col3#x as string collate UTF8_LCASE) AS utf8_lcase#x]
   +- LocalRelation [col1#x, col2#x, col3#x]


-- !query
insert into t1 values ('İo', 'İo ', 'İo')
-- !query analysis
InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/t1, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/t1], Append, `spark_catalog`.`default`.`t1`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/t1), [s, utf8_binary, utf8_lcase]
+- Project [col1#x AS s#x, col2#x AS utf8_binary#x, cast(col3#x as string collate UTF8_LCASE) AS utf8_lcase#x]
   +- LocalRelation [col1#x, col2#x, col3#x]


-- !query
insert into t1 values ('İo', 'İo', 'i̇o')
-- !query analysis
InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/t1, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/t1], Append, `spark_catalog`.`default`.`t1`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/t1), [s, utf8_binary, utf8_lcase]
+- Project [col1#x AS s#x, col2#x AS utf8_binary#x, cast(col3#x as string collate UTF8_LCASE) AS utf8_lcase#x]
   +- LocalRelation [col1#x, col2#x, col3#x]


-- !query
insert into t1 values ('efd2', 'efd2', 'efd2')
-- !query analysis
InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/t1, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/t1], Append, `spark_catalog`.`default`.`t1`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/t1), [s, utf8_binary, utf8_lcase]
+- Project [col1#x AS s#x, col2#x AS utf8_binary#x, cast(col3#x as string collate UTF8_LCASE) AS utf8_lcase#x]
   +- LocalRelation [col1#x, col2#x, col3#x]


-- !query
insert into t1 values ('Hello, world! Nice day.', 'Hello, world! Nice day.', 'Hello, world! Nice day.')
-- !query analysis
InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/t1, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/t1], Append, `spark_catalog`.`default`.`t1`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/t1), [s, utf8_binary, utf8_lcase]
+- Project [col1#x AS s#x, col2#x AS utf8_binary#x, cast(col3#x as string collate UTF8_LCASE) AS utf8_lcase#x]
   +- LocalRelation [col1#x, col2#x, col3#x]


-- !query
insert into t1 values ('Something else. Nothing here.', 'Something else. Nothing here.', 'Something else. Nothing here.')
-- !query analysis
InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/t1, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/t1], Append, `spark_catalog`.`default`.`t1`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/t1), [s, utf8_binary, utf8_lcase]
+- Project [col1#x AS s#x, col2#x AS utf8_binary#x, cast(col3#x as string collate UTF8_LCASE) AS utf8_lcase#x]
   +- LocalRelation [col1#x, col2#x, col3#x]


-- !query
insert into t1 values ('kitten', 'kitten', 'sitTing')
-- !query analysis
InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/t1, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/t1], Append, `spark_catalog`.`default`.`t1`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/t1), [s, utf8_binary, utf8_lcase]
+- Project [col1#x AS s#x, col2#x AS utf8_binary#x, cast(col3#x as string collate UTF8_LCASE) AS utf8_lcase#x]
   +- LocalRelation [col1#x, col2#x, col3#x]


-- !query
insert into t1 values ('abc', 'abc', 'abc')
-- !query analysis
InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/t1, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/t1], Append, `spark_catalog`.`default`.`t1`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/t1), [s, utf8_binary, utf8_lcase]
+- Project [col1#x AS s#x, col2#x AS utf8_binary#x, cast(col3#x as string collate UTF8_LCASE) AS utf8_lcase#x]
   +- LocalRelation [col1#x, col2#x, col3#x]


-- !query
insert into t1 values ('abcdcba', 'abcdcba', 'aBcDCbA')
-- !query analysis
InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/t1, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/t1], Append, `spark_catalog`.`default`.`t1`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/t1), [s, utf8_binary, utf8_lcase]
+- Project [col1#x AS s#x, col2#x AS utf8_binary#x, cast(col3#x as string collate UTF8_LCASE) AS utf8_lcase#x]
   +- LocalRelation [col1#x, col2#x, col3#x]


-- !query
create table t2(ascii long) using parquet
-- !query analysis
CreateDataSourceTableCommand `spark_catalog`.`default`.`t2`, false


-- !query
insert into t2 values (97)
-- !query analysis
InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/t2, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/t2], Append, `spark_catalog`.`default`.`t2`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/t2), [ascii]
+- Project [cast(col1#x as bigint) AS ascii#xL]
   +- LocalRelation [col1#x]


-- !query
insert into t2 values (66)
-- !query analysis
InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/t2, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/t2], Append, `spark_catalog`.`default`.`t2`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/t2), [ascii]
+- Project [cast(col1#x as bigint) AS ascii#xL]
   +- LocalRelation [col1#x]


-- !query
create table t3(format string collate utf8_binary, utf8_binary string collate utf8_binary, utf8_lcase string collate utf8_lcase) using parquet
-- !query analysis
CreateDataSourceTableCommand `spark_catalog`.`default`.`t3`, false


-- !query
insert into t3 values ('%s%s', 'abCdE', 'abCdE')
-- !query analysis
InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/t3, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/t3], Append, `spark_catalog`.`default`.`t3`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/t3), [format, utf8_binary, utf8_lcase]
+- Project [col1#x AS format#x, col2#x AS utf8_binary#x, cast(col3#x as string collate UTF8_LCASE) AS utf8_lcase#x]
   +- LocalRelation [col1#x, col2#x, col3#x]


-- !query
select concat_ws(' ', utf8_lcase, utf8_lcase) from t1
-- !query analysis
Project [concat_ws( , utf8_lcase#x, utf8_lcase#x) AS concat_ws(' ' collate UTF8_LCASE, utf8_lcase, utf8_lcase)#x]
+- SubqueryAlias spark_catalog.default.t1
   +- Relation spark_catalog.default.t1[s#x,utf8_binary#x,utf8_lcase#x] parquet


-- !query
select concat_ws(' ', utf8_binary, utf8_lcase) from t1
-- !query analysis
Project [concat_ws( , cast(utf8_binary#x as string collate null), cast(utf8_lcase#x as string collate null)) AS concat_ws(' ' collate null, utf8_binary, utf8_lcase)#x]
+- SubqueryAlias spark_catalog.default.t1
   +- Relation spark_catalog.default.t1[s#x,utf8_binary#x,utf8_lcase#x] parquet


-- !query
select concat_ws(' ' collate utf8_binary, utf8_binary, 'SQL' collate utf8_lcase) from t1
-- !query analysis
org.apache.spark.sql.AnalysisException
{
  "errorClass" : "COLLATION_MISMATCH.EXPLICIT",
  "sqlState" : "42P21",
  "messageParameters" : {
    "explicitTypes" : "\"STRING\", \"STRING COLLATE UTF8_LCASE\""
  }
}


-- !query
select concat_ws(' ' collate utf8_lcase, utf8_binary, 'SQL' collate utf8_lcase) from t1
-- !query analysis
Project [concat_ws(collate( , utf8_lcase), cast(utf8_binary#x as string collate UTF8_LCASE), collate(SQL, utf8_lcase)) AS concat_ws(collate( , utf8_lcase), utf8_binary, collate(SQL, utf8_lcase))#x]
+- SubqueryAlias spark_catalog.default.t1
   +- Relation spark_catalog.default.t1[s#x,utf8_binary#x,utf8_lcase#x] parquet


-- !query
select concat_ws(',', utf8_lcase, 'word'), concat_ws(',', utf8_binary, 'word') from t1
-- !query analysis
Project [concat_ws(,, utf8_lcase#x, word) AS concat_ws(',' collate UTF8_LCASE, utf8_lcase, 'word' collate UTF8_LCASE)#x, concat_ws(,, utf8_binary#x, word) AS concat_ws(,, utf8_binary, word)#x]
+- SubqueryAlias spark_catalog.default.t1
   +- Relation spark_catalog.default.t1[s#x,utf8_binary#x,utf8_lcase#x] parquet


-- !query
select concat_ws(',', utf8_lcase, 'word' collate utf8_binary), concat_ws(',', utf8_binary, 'word' collate utf8_lcase) from t1
-- !query analysis
Project [concat_ws(,, cast(utf8_lcase#x as string), collate(word, utf8_binary)) AS concat_ws(,, utf8_lcase, collate(word, utf8_binary))#x, concat_ws(,, cast(utf8_binary#x as string collate UTF8_LCASE), collate(word, utf8_lcase)) AS concat_ws(',' collate UTF8_LCASE, utf8_binary, collate(word, utf8_lcase))#x]
+- SubqueryAlias spark_catalog.default.t1
   +- Relation spark_catalog.default.t1[s#x,utf8_binary#x,utf8_lcase#x] parquet


-- !query
select split_part(utf8_binary, utf8_lcase, 3) from t1
-- !query analysis
Project [split_part(cast(utf8_binary#x as string collate null), cast(utf8_lcase#x as string collate null), 3) AS split_part(utf8_binary, utf8_lcase, 3)#x]
+- SubqueryAlias spark_catalog.default.t1
   +- Relation spark_catalog.default.t1[s#x,utf8_binary#x,utf8_lcase#x] parquet


-- !query
select split_part(s, utf8_binary, 1) from t1
-- !query analysis
Project [split_part(s#x, utf8_binary#x, 1) AS split_part(s, utf8_binary, 1)#x]
+- SubqueryAlias spark_catalog.default.t1
   +- Relation spark_catalog.default.t1[s#x,utf8_binary#x,utf8_lcase#x] parquet


-- !query
select split_part(utf8_binary collate utf8_binary, s collate utf8_lcase, 1) from t1
-- !query analysis
org.apache.spark.sql.AnalysisException
{
  "errorClass" : "COLLATION_MISMATCH.EXPLICIT",
  "sqlState" : "42P21",
  "messageParameters" : {
    "explicitTypes" : "\"STRING\", \"STRING COLLATE UTF8_LCASE\""
  }
}


-- !query
select split_part(utf8_binary, utf8_lcase collate utf8_binary, 2) from t1
-- !query analysis
Project [split_part(utf8_binary#x, collate(utf8_lcase#x, utf8_binary), 2) AS split_part(utf8_binary, collate(utf8_lcase, utf8_binary), 2)#x]
+- SubqueryAlias spark_catalog.default.t1
   +- Relation spark_catalog.default.t1[s#x,utf8_binary#x,utf8_lcase#x] parquet


-- !query
select split_part(utf8_binary collate utf8_lcase, utf8_lcase collate utf8_lcase, 2) from t1
-- !query analysis
Project [split_part(collate(utf8_binary#x, utf8_lcase), collate(utf8_lcase#x, utf8_lcase), 2) AS split_part(collate(utf8_binary, utf8_lcase), collate(utf8_lcase, utf8_lcase), 2)#x]
+- SubqueryAlias spark_catalog.default.t1
   +- Relation spark_catalog.default.t1[s#x,utf8_binary#x,utf8_lcase#x] parquet


-- !query
select split_part(utf8_binary collate unicode_ai, utf8_lcase collate unicode_ai, 2) from t1
-- !query analysis
org.apache.spark.sql.catalyst.ExtendedAnalysisException
{
  "errorClass" : "DATATYPE_MISMATCH.UNEXPECTED_INPUT_TYPE",
  "sqlState" : "42K09",
  "messageParameters" : {
    "inputSql" : "\"collate(utf8_binary, unicode_ai)\"",
    "inputType" : "\"STRING COLLATE UNICODE_AI\"",
    "paramIndex" : "first",
    "requiredType" : "\"STRING\"",
    "sqlExpr" : "\"split_part(collate(utf8_binary, unicode_ai), collate(utf8_lcase, unicode_ai), 2)\""
  },
  "queryContext" : [ {
    "objectType" : "",
    "objectName" : "",
    "startIndex" : 8,
    "stopIndex" : 83,
    "fragment" : "split_part(utf8_binary collate unicode_ai, utf8_lcase collate unicode_ai, 2)"
  } ]
}


-- !query
select split_part(utf8_binary, 'a', 3), split_part(utf8_lcase, 'a', 3) from t1
-- !query analysis
Project [split_part(utf8_binary#x, a, 3) AS split_part(utf8_binary, a, 3)#x, split_part(utf8_lcase#x, a, 3) AS split_part(utf8_lcase, 'a' collate UTF8_LCASE, 3)#x]
+- SubqueryAlias spark_catalog.default.t1
   +- Relation spark_catalog.default.t1[s#x,utf8_binary#x,utf8_lcase#x] parquet


-- !query
select split_part(utf8_binary, 'a' collate utf8_lcase, 3), split_part(utf8_lcase, 'a' collate utf8_binary, 3) from t1
-- !query analysis
Project [split_part(cast(utf8_binary#x as string collate UTF8_LCASE), collate(a, utf8_lcase), 3) AS split_part(utf8_binary, collate(a, utf8_lcase), 3)#x, split_part(cast(utf8_lcase#x as string), collate(a, utf8_binary), 3) AS split_part(utf8_lcase, collate(a, utf8_binary), 3)#x]
+- SubqueryAlias spark_catalog.default.t1
   +- Relation spark_catalog.default.t1[s#x,utf8_binary#x,utf8_lcase#x] parquet


-- !query
select split_part(utf8_binary, 'a ' collate utf8_lcase_rtrim, 3), split_part(utf8_lcase, 'a' collate utf8_binary, 3) from t1
-- !query analysis
Project [split_part(cast(utf8_binary#x as string collate UTF8_LCASE_RTRIM), collate(a , utf8_lcase_rtrim), 3) AS split_part(utf8_binary, collate(a , utf8_lcase_rtrim), 3)#x, split_part(cast(utf8_lcase#x as string), collate(a, utf8_binary), 3) AS split_part(utf8_lcase, collate(a, utf8_binary), 3)#x]
+- SubqueryAlias spark_catalog.default.t1
   +- Relation spark_catalog.default.t1[s#x,utf8_binary#x,utf8_lcase#x] parquet


-- !query
select contains(utf8_binary, utf8_lcase) from t1
-- !query analysis
org.apache.spark.sql.AnalysisException
{
  "errorClass" : "INDETERMINATE_COLLATION_IN_EXPRESSION",
  "sqlState" : "42P22",
  "messageParameters" : {
    "expr" : "\"contains(utf8_binary, utf8_lcase)\""
  },
  "queryContext" : [ {
    "objectType" : "",
    "objectName" : "",
    "startIndex" : 8,
    "stopIndex" : 40,
    "fragment" : "contains(utf8_binary, utf8_lcase)"
  } ]
}


-- !query
select contains(s, utf8_binary) from t1
-- !query analysis
Project [Contains(s#x, utf8_binary#x) AS contains(s, utf8_binary)#x]
+- SubqueryAlias spark_catalog.default.t1
   +- Relation spark_catalog.default.t1[s#x,utf8_binary#x,utf8_lcase#x] parquet


-- !query
select contains(utf8_binary collate utf8_binary, s collate utf8_lcase) from t1
-- !query analysis
org.apache.spark.sql.AnalysisException
{
  "errorClass" : "COLLATION_MISMATCH.EXPLICIT",
  "sqlState" : "42P21",
  "messageParameters" : {
    "explicitTypes" : "\"STRING\", \"STRING COLLATE UTF8_LCASE\""
  }
}


-- !query
select contains(utf8_binary, utf8_lcase collate utf8_binary) from t1
-- !query analysis
Project [Contains(utf8_binary#x, collate(utf8_lcase#x, utf8_binary)) AS contains(utf8_binary, collate(utf8_lcase, utf8_binary))#x]
+- SubqueryAlias spark_catalog.default.t1
   +- Relation spark_catalog.default.t1[s#x,utf8_binary#x,utf8_lcase#x] parquet


-- !query
select contains(utf8_binary collate utf8_lcase, utf8_lcase collate utf8_lcase) from t1
-- !query analysis
Project [Contains(collate(utf8_binary#x, utf8_lcase), collate(utf8_lcase#x, utf8_lcase)) AS contains(collate(utf8_binary, utf8_lcase), collate(utf8_lcase, utf8_lcase))#x]
+- SubqueryAlias spark_catalog.default.t1
   +- Relation spark_catalog.default.t1[s#x,utf8_binary#x,utf8_lcase#x] parquet


-- !query
select contains(utf8_binary collate unicode_ai, utf8_lcase collate unicode_ai) from t1
-- !query analysis
org.apache.spark.sql.catalyst.ExtendedAnalysisException
{
  "errorClass" : "DATATYPE_MISMATCH.UNEXPECTED_INPUT_TYPE",
  "sqlState" : "42K09",
  "messageParameters" : {
    "inputSql" : "\"collate(utf8_binary, unicode_ai)\"",
    "inputType" : "\"STRING COLLATE UNICODE_AI\"",
    "paramIndex" : "first",
    "requiredType" : "\"STRING\"",
    "sqlExpr" : "\"contains(collate(utf8_binary, unicode_ai), collate(utf8_lcase, unicode_ai))\""
  },
  "queryContext" : [ {
    "objectType" : "",
    "objectName" : "",
    "startIndex" : 8,
    "stopIndex" : 78,
    "fragment" : "contains(utf8_binary collate unicode_ai, utf8_lcase collate unicode_ai)"
  } ]
}


-- !query
select contains(utf8_binary, 'a'), contains(utf8_lcase, 'a') from t1
-- !query analysis
Project [Contains(utf8_binary#x, a) AS contains(utf8_binary, a)#x, Contains(utf8_lcase#x, a) AS contains(utf8_lcase, 'a' collate UTF8_LCASE)#x]
+- SubqueryAlias spark_catalog.default.t1
   +- Relation spark_catalog.default.t1[s#x,utf8_binary#x,utf8_lcase#x] parquet


-- !query
select contains(utf8_binary, 'AaAA' collate utf8_lcase), contains(utf8_lcase, 'AAa' collate utf8_binary) from t1
-- !query analysis
Project [Contains(cast(utf8_binary#x as string collate UTF8_LCASE), collate(AaAA, utf8_lcase)) AS contains(utf8_binary, collate(AaAA, utf8_lcase))#x, Contains(cast(utf8_lcase#x as string), collate(AAa, utf8_binary)) AS contains(utf8_lcase, collate(AAa, utf8_binary))#x]
+- SubqueryAlias spark_catalog.default.t1
   +- Relation spark_catalog.default.t1[s#x,utf8_binary#x,utf8_lcase#x] parquet


-- !query
select contains(utf8_binary, 'AaAA ' collate utf8_lcase_rtrim), contains(utf8_lcase, 'AAa ' collate utf8_binary_rtrim) from t1
-- !query analysis
Project [Contains(cast(utf8_binary#x as string collate UTF8_LCASE_RTRIM), collate(AaAA , utf8_lcase_rtrim)) AS contains(utf8_binary, collate(AaAA , utf8_lcase_rtrim))#x, Contains(cast(utf8_lcase#x as string collate UTF8_BINARY_RTRIM), collate(AAa , utf8_binary_rtrim)) AS contains(utf8_lcase, collate(AAa , utf8_binary_rtrim))#x]
+- SubqueryAlias spark_catalog.default.t1
   +- Relation spark_catalog.default.t1[s#x,utf8_binary#x,utf8_lcase#x] parquet


-- !query
select substring_index(utf8_binary, utf8_lcase, 2) from t1
-- !query analysis
org.apache.spark.sql.AnalysisException
{
  "errorClass" : "INDETERMINATE_COLLATION_IN_EXPRESSION",
  "sqlState" : "42P22",
  "messageParameters" : {
    "expr" : "\"substring_index(utf8_binary, utf8_lcase, 2)\""
  },
  "queryContext" : [ {
    "objectType" : "",
    "objectName" : "",
    "startIndex" : 8,
    "stopIndex" : 50,
    "fragment" : "substring_index(utf8_binary, utf8_lcase, 2)"
  } ]
}


-- !query
select substring_index(s, utf8_binary,1) from t1
-- !query analysis
Project [substring_index(s#x, utf8_binary#x, 1) AS substring_index(s, utf8_binary, 1)#x]
+- SubqueryAlias spark_catalog.default.t1
   +- Relation spark_catalog.default.t1[s#x,utf8_binary#x,utf8_lcase#x] parquet


-- !query
select substring_index(utf8_binary collate utf8_binary, s collate utf8_lcase, 3) from t1
-- !query analysis
org.apache.spark.sql.AnalysisException
{
  "errorClass" : "COLLATION_MISMATCH.EXPLICIT",
  "sqlState" : "42P21",
  "messageParameters" : {
    "explicitTypes" : "\"STRING\", \"STRING COLLATE UTF8_LCASE\""
  }
}


-- !query
select substring_index(utf8_binary, utf8_lcase collate utf8_binary, 2) from t1
-- !query analysis
Project [substring_index(utf8_binary#x, collate(utf8_lcase#x, utf8_binary), 2) AS substring_index(utf8_binary, collate(utf8_lcase, utf8_binary), 2)#x]
+- SubqueryAlias spark_catalog.default.t1
   +- Relation spark_catalog.default.t1[s#x,utf8_binary#x,utf8_lcase#x] parquet


-- !query
select substring_index(utf8_binary collate utf8_lcase, utf8_lcase collate utf8_lcase, 2) from t1
-- !query analysis
Project [substring_index(collate(utf8_binary#x, utf8_lcase), collate(utf8_lcase#x, utf8_lcase), 2) AS substring_index(collate(utf8_binary, utf8_lcase), collate(utf8_lcase, utf8_lcase), 2)#x]
+- SubqueryAlias spark_catalog.default.t1
   +- Relation spark_catalog.default.t1[s#x,utf8_binary#x,utf8_lcase#x] parquet


-- !query
select substring_index(utf8_binary collate unicode_ai, utf8_lcase collate unicode_ai, 2) from t1
-- !query analysis
org.apache.spark.sql.catalyst.ExtendedAnalysisException
{
  "errorClass" : "DATATYPE_MISMATCH.UNEXPECTED_INPUT_TYPE",
  "sqlState" : "42K09",
  "messageParameters" : {
    "inputSql" : "\"collate(utf8_binary, unicode_ai)\"",
    "inputType" : "\"STRING COLLATE UNICODE_AI\"",
    "paramIndex" : "first",
    "requiredType" : "\"STRING\"",
    "sqlExpr" : "\"substring_index(collate(utf8_binary, unicode_ai), collate(utf8_lcase, unicode_ai), 2)\""
  },
  "queryContext" : [ {
    "objectType" : "",
    "objectName" : "",
    "startIndex" : 8,
    "stopIndex" : 88,
    "fragment" : "substring_index(utf8_binary collate unicode_ai, utf8_lcase collate unicode_ai, 2)"
  } ]
}


-- !query
select substring_index(utf8_binary, 'a', 2), substring_index(utf8_lcase, 'a', 2) from t1
-- !query analysis
Project [substring_index(utf8_binary#x, a, 2) AS substring_index(utf8_binary, a, 2)#x, substring_index(utf8_lcase#x, a, 2) AS substring_index(utf8_lcase, 'a' collate UTF8_LCASE, 2)#x]
+- SubqueryAlias spark_catalog.default.t1
   +- Relation spark_catalog.default.t1[s#x,utf8_binary#x,utf8_lcase#x] parquet


-- !query
select substring_index(utf8_binary, 'AaAA' collate utf8_lcase, 2), substring_index(utf8_lcase, 'AAa' collate utf8_binary, 2) from t1
-- !query analysis
Project [substring_index(cast(utf8_binary#x as string collate UTF8_LCASE), collate(AaAA, utf8_lcase), 2) AS substring_index(utf8_binary, collate(AaAA, utf8_lcase), 2)#x, substring_index(cast(utf8_lcase#x as string), collate(AAa, utf8_binary), 2) AS substring_index(utf8_lcase, collate(AAa, utf8_binary), 2)#x]
+- SubqueryAlias spark_catalog.default.t1
   +- Relation spark_catalog.default.t1[s#x,utf8_binary#x,utf8_lcase#x] parquet


-- !query
select substring_index(utf8_binary, 'AaAA ' collate utf8_lcase_rtrim, 2), substring_index(utf8_lcase, 'AAa' collate utf8_binary, 2) from t1
-- !query analysis
Project [substring_index(cast(utf8_binary#x as string collate UTF8_LCASE_RTRIM), collate(AaAA , utf8_lcase_rtrim), 2) AS substring_index(utf8_binary, collate(AaAA , utf8_lcase_rtrim), 2)#x, substring_index(cast(utf8_lcase#x as string), collate(AAa, utf8_binary), 2) AS substring_index(utf8_lcase, collate(AAa, utf8_binary), 2)#x]
+- SubqueryAlias spark_catalog.default.t1
   +- Relation spark_catalog.default.t1[s#x,utf8_binary#x,utf8_lcase#x] parquet


-- !query
select instr(utf8_binary, utf8_lcase) from t1
-- !query analysis
org.apache.spark.sql.AnalysisException
{
  "errorClass" : "INDETERMINATE_COLLATION_IN_EXPRESSION",
  "sqlState" : "42P22",
  "messageParameters" : {
    "expr" : "\"instr(utf8_binary, utf8_lcase)\""
  },
  "queryContext" : [ {
    "objectType" : "",
    "objectName" : "",
    "startIndex" : 8,
    "stopIndex" : 37,
    "fragment" : "instr(utf8_binary, utf8_lcase)"
  } ]
}


-- !query
select instr(s, utf8_binary) from t1
-- !query analysis
Project [instr(s#x, utf8_binary#x) AS instr(s, utf8_binary)#x]
+- SubqueryAlias spark_catalog.default.t1
   +- Relation spark_catalog.default.t1[s#x,utf8_binary#x,utf8_lcase#x] parquet


-- !query
select instr(utf8_binary collate utf8_binary, s collate utf8_lcase) from t1
-- !query analysis
org.apache.spark.sql.AnalysisException
{
  "errorClass" : "COLLATION_MISMATCH.EXPLICIT",
  "sqlState" : "42P21",
  "messageParameters" : {
    "explicitTypes" : "\"STRING\", \"STRING COLLATE UTF8_LCASE\""
  }
}


-- !query
select instr(utf8_binary, utf8_lcase collate utf8_binary) from t1
-- !query analysis
Project [instr(utf8_binary#x, collate(utf8_lcase#x, utf8_binary)) AS instr(utf8_binary, collate(utf8_lcase, utf8_binary))#x]
+- SubqueryAlias spark_catalog.default.t1
   +- Relation spark_catalog.default.t1[s#x,utf8_binary#x,utf8_lcase#x] parquet


-- !query
select instr(utf8_binary collate utf8_lcase, utf8_lcase collate utf8_lcase) from t1
-- !query analysis
Project [instr(collate(utf8_binary#x, utf8_lcase), collate(utf8_lcase#x, utf8_lcase)) AS instr(collate(utf8_binary, utf8_lcase), collate(utf8_lcase, utf8_lcase))#x]
+- SubqueryAlias spark_catalog.default.t1
   +- Relation spark_catalog.default.t1[s#x,utf8_binary#x,utf8_lcase#x] parquet


-- !query
select instr(utf8_binary collate unicode_ai, utf8_lcase collate unicode_ai) from t1
-- !query analysis
org.apache.spark.sql.catalyst.ExtendedAnalysisException
{
  "errorClass" : "DATATYPE_MISMATCH.UNEXPECTED_INPUT_TYPE",
  "sqlState" : "42K09",
  "messageParameters" : {
    "inputSql" : "\"collate(utf8_binary, unicode_ai)\"",
    "inputType" : "\"STRING COLLATE UNICODE_AI\"",
    "paramIndex" : "first",
    "requiredType" : "\"STRING\"",
    "sqlExpr" : "\"instr(collate(utf8_binary, unicode_ai), collate(utf8_lcase, unicode_ai))\""
  },
  "queryContext" : [ {
    "objectType" : "",
    "objectName" : "",
    "startIndex" : 8,
    "stopIndex" : 75,
    "fragment" : "instr(utf8_binary collate unicode_ai, utf8_lcase collate unicode_ai)"
  } ]
}


-- !query
select instr(utf8_binary, 'a'), instr(utf8_lcase, 'a') from t1
-- !query analysis
Project [instr(utf8_binary#x, a) AS instr(utf8_binary, a)#x, instr(utf8_lcase#x, a) AS instr(utf8_lcase, 'a' collate UTF8_LCASE)#x]
+- SubqueryAlias spark_catalog.default.t1
   +- Relation spark_catalog.default.t1[s#x,utf8_binary#x,utf8_lcase#x] parquet


-- !query
select instr(utf8_binary, 'AaAA' collate utf8_lcase), instr(utf8_lcase, 'AAa' collate utf8_binary) from t1
-- !query analysis
Project [instr(cast(utf8_binary#x as string collate UTF8_LCASE), collate(AaAA, utf8_lcase)) AS instr(utf8_binary, collate(AaAA, utf8_lcase))#x, instr(cast(utf8_lcase#x as string), collate(AAa, utf8_binary)) AS instr(utf8_lcase, collate(AAa, utf8_binary))#x]
+- SubqueryAlias spark_catalog.default.t1
   +- Relation spark_catalog.default.t1[s#x,utf8_binary#x,utf8_lcase#x] parquet


-- !query
select find_in_set(utf8_binary, utf8_lcase) from t1
-- !query analysis
org.apache.spark.sql.AnalysisException
{
  "errorClass" : "INDETERMINATE_COLLATION_IN_EXPRESSION",
  "sqlState" : "42P22",
  "messageParameters" : {
    "expr" : "\"find_in_set(utf8_binary, utf8_lcase)\""
  },
  "queryContext" : [ {
    "objectType" : "",
    "objectName" : "",
    "startIndex" : 8,
    "stopIndex" : 43,
    "fragment" : "find_in_set(utf8_binary, utf8_lcase)"
  } ]
}


-- !query
select find_in_set(s, utf8_binary) from t1
-- !query analysis
Project [find_in_set(s#x, utf8_binary#x) AS find_in_set(s, utf8_binary)#x]
+- SubqueryAlias spark_catalog.default.t1
   +- Relation spark_catalog.default.t1[s#x,utf8_binary#x,utf8_lcase#x] parquet


-- !query
select find_in_set(utf8_binary collate utf8_binary, s collate utf8_lcase) from t1
-- !query analysis
org.apache.spark.sql.AnalysisException
{
  "errorClass" : "COLLATION_MISMATCH.EXPLICIT",
  "sqlState" : "42P21",
  "messageParameters" : {
    "explicitTypes" : "\"STRING\", \"STRING COLLATE UTF8_LCASE\""
  }
}


-- !query
select find_in_set(utf8_binary, utf8_lcase collate utf8_binary) from t1
-- !query analysis
Project [find_in_set(utf8_binary#x, collate(utf8_lcase#x, utf8_binary)) AS find_in_set(utf8_binary, collate(utf8_lcase, utf8_binary))#x]
+- SubqueryAlias spark_catalog.default.t1
   +- Relation spark_catalog.default.t1[s#x,utf8_binary#x,utf8_lcase#x] parquet


-- !query
select find_in_set(utf8_binary collate utf8_lcase, utf8_lcase collate utf8_lcase) from t1
-- !query analysis
Project [find_in_set(collate(utf8_binary#x, utf8_lcase), collate(utf8_lcase#x, utf8_lcase)) AS find_in_set(collate(utf8_binary, utf8_lcase), collate(utf8_lcase, utf8_lcase))#x]
+- SubqueryAlias spark_catalog.default.t1
   +- Relation spark_catalog.default.t1[s#x,utf8_binary#x,utf8_lcase#x] parquet


-- !query
select find_in_set(utf8_binary, 'aaAaaAaA,i̇o'), find_in_set(utf8_lcase, 'aaAaaAaA,i̇o') from t1
-- !query analysis
Project [find_in_set(utf8_binary#x, aaAaaAaA,i̇o) AS find_in_set(utf8_binary, aaAaaAaA,i̇o)#x, find_in_set(utf8_lcase#x, aaAaaAaA,i̇o) AS find_in_set(utf8_lcase, 'aaAaaAaA,i̇o' collate UTF8_LCASE)#x]
+- SubqueryAlias spark_catalog.default.t1
   +- Relation spark_catalog.default.t1[s#x,utf8_binary#x,utf8_lcase#x] parquet


-- !query
select find_in_set(utf8_binary, 'aaAaaAaA,i̇o' collate utf8_lcase), find_in_set(utf8_lcase, 'aaAaaAaA,i̇o' collate utf8_binary) from t1
-- !query analysis
Project [find_in_set(cast(utf8_binary#x as string collate UTF8_LCASE), collate(aaAaaAaA,i̇o, utf8_lcase)) AS find_in_set(utf8_binary, collate(aaAaaAaA,i̇o, utf8_lcase))#x, find_in_set(cast(utf8_lcase#x as string), collate(aaAaaAaA,i̇o, utf8_binary)) AS find_in_set(utf8_lcase, collate(aaAaaAaA,i̇o, utf8_binary))#x]
+- SubqueryAlias spark_catalog.default.t1
   +- Relation spark_catalog.default.t1[s#x,utf8_binary#x,utf8_lcase#x] parquet


-- !query
select find_in_set(utf8_binary, 'aaAaaAaA,i̇o ' collate utf8_lcase_rtrim), find_in_set(utf8_lcase, 'aaAaaAaA,i̇o' collate utf8_binary) from t1
-- !query analysis
Project [find_in_set(cast(utf8_binary#x as string collate UTF8_LCASE_RTRIM), collate(aaAaaAaA,i̇o , utf8_lcase_rtrim)) AS find_in_set(utf8_binary, collate(aaAaaAaA,i̇o , utf8_lcase_rtrim))#x, find_in_set(cast(utf8_lcase#x as string), collate(aaAaaAaA,i̇o, utf8_binary)) AS find_in_set(utf8_lcase, collate(aaAaaAaA,i̇o, utf8_binary))#x]
+- SubqueryAlias spark_catalog.default.t1
   +- Relation spark_catalog.default.t1[s#x,utf8_binary#x,utf8_lcase#x] parquet


-- !query
select startswith(utf8_binary, utf8_lcase) from t1
-- !query analysis
org.apache.spark.sql.AnalysisException
{
  "errorClass" : "INDETERMINATE_COLLATION_IN_EXPRESSION",
  "sqlState" : "42P22",
  "messageParameters" : {
    "expr" : "\"startswith(utf8_binary, utf8_lcase)\""
  },
  "queryContext" : [ {
    "objectType" : "",
    "objectName" : "",
    "startIndex" : 8,
    "stopIndex" : 42,
    "fragment" : "startswith(utf8_binary, utf8_lcase)"
  } ]
}


-- !query
select startswith(s, utf8_binary) from t1
-- !query analysis
Project [StartsWith(s#x, utf8_binary#x) AS startswith(s, utf8_binary)#x]
+- SubqueryAlias spark_catalog.default.t1
   +- Relation spark_catalog.default.t1[s#x,utf8_binary#x,utf8_lcase#x] parquet


-- !query
select startswith(utf8_binary collate utf8_binary, s collate utf8_lcase) from t1
-- !query analysis
org.apache.spark.sql.AnalysisException
{
  "errorClass" : "COLLATION_MISMATCH.EXPLICIT",
  "sqlState" : "42P21",
  "messageParameters" : {
    "explicitTypes" : "\"STRING\", \"STRING COLLATE UTF8_LCASE\""
  }
}


-- !query
select startswith(utf8_binary, utf8_lcase collate utf8_binary) from t1
-- !query analysis
Project [StartsWith(utf8_binary#x, collate(utf8_lcase#x, utf8_binary)) AS startswith(utf8_binary, collate(utf8_lcase, utf8_binary))#x]
+- SubqueryAlias spark_catalog.default.t1
   +- Relation spark_catalog.default.t1[s#x,utf8_binary#x,utf8_lcase#x] parquet


-- !query
select startswith(utf8_binary collate utf8_lcase, utf8_lcase collate utf8_lcase) from t1
-- !query analysis
Project [StartsWith(collate(utf8_binary#x, utf8_lcase), collate(utf8_lcase#x, utf8_lcase)) AS startswith(collate(utf8_binary, utf8_lcase), collate(utf8_lcase, utf8_lcase))#x]
+- SubqueryAlias spark_catalog.default.t1
   +- Relation spark_catalog.default.t1[s#x,utf8_binary#x,utf8_lcase#x] parquet


-- !query
select startswith(utf8_binary collate unicode_ai, utf8_lcase collate unicode_ai) from t1
-- !query analysis
org.apache.spark.sql.catalyst.ExtendedAnalysisException
{
  "errorClass" : "DATATYPE_MISMATCH.UNEXPECTED_INPUT_TYPE",
  "sqlState" : "42K09",
  "messageParameters" : {
    "inputSql" : "\"collate(utf8_binary, unicode_ai)\"",
    "inputType" : "\"STRING COLLATE UNICODE_AI\"",
    "paramIndex" : "first",
    "requiredType" : "\"STRING\"",
    "sqlExpr" : "\"startswith(collate(utf8_binary, unicode_ai), collate(utf8_lcase, unicode_ai))\""
  },
  "queryContext" : [ {
    "objectType" : "",
    "objectName" : "",
    "startIndex" : 8,
    "stopIndex" : 80,
    "fragment" : "startswith(utf8_binary collate unicode_ai, utf8_lcase collate unicode_ai)"
  } ]
}


-- !query
select startswith(utf8_binary, 'aaAaaAaA'), startswith(utf8_lcase, 'aaAaaAaA') from t1
-- !query analysis
Project [StartsWith(utf8_binary#x, aaAaaAaA) AS startswith(utf8_binary, aaAaaAaA)#x, StartsWith(utf8_lcase#x, aaAaaAaA) AS startswith(utf8_lcase, 'aaAaaAaA' collate UTF8_LCASE)#x]
+- SubqueryAlias spark_catalog.default.t1
   +- Relation spark_catalog.default.t1[s#x,utf8_binary#x,utf8_lcase#x] parquet


-- !query
select startswith(utf8_binary, 'aaAaaAaA' collate utf8_lcase), startswith(utf8_lcase, 'aaAaaAaA' collate utf8_binary) from t1
-- !query analysis
Project [StartsWith(cast(utf8_binary#x as string collate UTF8_LCASE), collate(aaAaaAaA, utf8_lcase)) AS startswith(utf8_binary, collate(aaAaaAaA, utf8_lcase))#x, StartsWith(cast(utf8_lcase#x as string), collate(aaAaaAaA, utf8_binary)) AS startswith(utf8_lcase, collate(aaAaaAaA, utf8_binary))#x]
+- SubqueryAlias spark_catalog.default.t1
   +- Relation spark_catalog.default.t1[s#x,utf8_binary#x,utf8_lcase#x] parquet


-- !query
select startswith(utf8_binary, 'aaAaaAaA ' collate utf8_lcase_rtrim), startswith(utf8_lcase, 'aaAaaAaA' collate utf8_binary) from t1
-- !query analysis
Project [StartsWith(cast(utf8_binary#x as string collate UTF8_LCASE_RTRIM), collate(aaAaaAaA , utf8_lcase_rtrim)) AS startswith(utf8_binary, collate(aaAaaAaA , utf8_lcase_rtrim))#x, StartsWith(cast(utf8_lcase#x as string), collate(aaAaaAaA, utf8_binary)) AS startswith(utf8_lcase, collate(aaAaaAaA, utf8_binary))#x]
+- SubqueryAlias spark_catalog.default.t1
   +- Relation spark_catalog.default.t1[s#x,utf8_binary#x,utf8_lcase#x] parquet


-- !query
select translate(utf8_lcase, utf8_lcase, '12345') from t1
-- !query analysis
Project [translate(utf8_lcase#x, utf8_lcase#x, 12345) AS translate(utf8_lcase, utf8_lcase, '12345' collate UTF8_LCASE)#x]
+- SubqueryAlias spark_catalog.default.t1
   +- Relation spark_catalog.default.t1[s#x,utf8_binary#x,utf8_lcase#x] parquet


-- !query
select translate(utf8_binary, utf8_lcase, '12345') from t1
-- !query analysis
org.apache.spark.sql.AnalysisException
{
  "errorClass" : "INDETERMINATE_COLLATION_IN_EXPRESSION",
  "sqlState" : "42P22",
  "messageParameters" : {
    "expr" : "\"translate(utf8_binary, utf8_lcase, 12345)\""
  },
  "queryContext" : [ {
    "objectType" : "",
    "objectName" : "",
    "startIndex" : 8,
    "stopIndex" : 50,
    "fragment" : "translate(utf8_binary, utf8_lcase, '12345')"
  } ]
}


-- !query
select translate(utf8_binary, 'aBc' collate utf8_lcase, '12345' collate utf8_binary) from t1
-- !query analysis
org.apache.spark.sql.AnalysisException
{
  "errorClass" : "COLLATION_MISMATCH.EXPLICIT",
  "sqlState" : "42P21",
  "messageParameters" : {
    "explicitTypes" : "\"STRING COLLATE UTF8_LCASE\", \"STRING\""
  }
}


-- !query
select translate(utf8_binary, 'SQL' collate utf8_lcase, '12345' collate utf8_lcase) from t1
-- !query analysis
Project [translate(cast(utf8_binary#x as string collate UTF8_LCASE), collate(SQL, utf8_lcase), collate(12345, utf8_lcase)) AS translate(utf8_binary, collate(SQL, utf8_lcase), collate(12345, utf8_lcase))#x]
+- SubqueryAlias spark_catalog.default.t1
   +- Relation spark_catalog.default.t1[s#x,utf8_binary#x,utf8_lcase#x] parquet


-- !query
select translate(utf8_binary, 'SQL' collate unicode_ai, '12345' collate unicode_ai) from t1
-- !query analysis
org.apache.spark.sql.catalyst.ExtendedAnalysisException
{
  "errorClass" : "DATATYPE_MISMATCH.UNEXPECTED_INPUT_TYPE",
  "sqlState" : "42K09",
  "messageParameters" : {
    "inputSql" : "\"utf8_binary\"",
    "inputType" : "\"STRING COLLATE UNICODE_AI\"",
    "paramIndex" : "first",
    "requiredType" : "\"STRING\"",
    "sqlExpr" : "\"translate(utf8_binary, collate(SQL, unicode_ai), collate(12345, unicode_ai))\""
  },
  "queryContext" : [ {
    "objectType" : "",
    "objectName" : "",
    "startIndex" : 8,
    "stopIndex" : 83,
    "fragment" : "translate(utf8_binary, 'SQL' collate unicode_ai, '12345' collate unicode_ai)"
  } ]
}


-- !query
select translate(utf8_lcase, 'aaAaaAaA', '12345'), translate(utf8_binary, 'aaAaaAaA', '12345') from t1
-- !query analysis
Project [translate(utf8_lcase#x, aaAaaAaA, 12345) AS translate(utf8_lcase, 'aaAaaAaA' collate UTF8_LCASE, '12345' collate UTF8_LCASE)#x, translate(utf8_binary#x, aaAaaAaA, 12345) AS translate(utf8_binary, aaAaaAaA, 12345)#x]
+- SubqueryAlias spark_catalog.default.t1
   +- Relation spark_catalog.default.t1[s#x,utf8_binary#x,utf8_lcase#x] parquet


-- !query
select translate(utf8_lcase, 'aBc' collate utf8_binary, '12345'), translate(utf8_binary, 'aBc' collate utf8_lcase, '12345') from t1
-- !query analysis
Project [translate(cast(utf8_lcase#x as string), collate(aBc, utf8_binary), 12345) AS translate(utf8_lcase, collate(aBc, utf8_binary), 12345)#x, translate(cast(utf8_binary#x as string collate UTF8_LCASE), collate(aBc, utf8_lcase), 12345) AS translate(utf8_binary, collate(aBc, utf8_lcase), '12345' collate UTF8_LCASE)#x]
+- SubqueryAlias spark_catalog.default.t1
   +- Relation spark_catalog.default.t1[s#x,utf8_binary#x,utf8_lcase#x] parquet


-- !query
select translate(utf8_lcase, 'aBc ' collate utf8_binary_rtrim, '12345'), translate(utf8_binary, 'aBc' collate utf8_lcase, '12345') from t1
-- !query analysis
Project [translate(cast(utf8_lcase#x as string collate UTF8_BINARY_RTRIM), collate(aBc , utf8_binary_rtrim), 12345) AS translate(utf8_lcase, collate(aBc , utf8_binary_rtrim), '12345' collate UTF8_BINARY_RTRIM)#x, translate(cast(utf8_binary#x as string collate UTF8_LCASE), collate(aBc, utf8_lcase), 12345) AS translate(utf8_binary, collate(aBc, utf8_lcase), '12345' collate UTF8_LCASE)#x]
+- SubqueryAlias spark_catalog.default.t1
   +- Relation spark_catalog.default.t1[s#x,utf8_binary#x,utf8_lcase#x] parquet


-- !query
select replace(utf8_binary, utf8_lcase, 'abc') from t1
-- !query analysis
org.apache.spark.sql.AnalysisException
{
  "errorClass" : "INDETERMINATE_COLLATION_IN_EXPRESSION",
  "sqlState" : "42P22",
  "messageParameters" : {
    "expr" : "\"replace(utf8_binary, utf8_lcase, abc)\""
  },
  "queryContext" : [ {
    "objectType" : "",
    "objectName" : "",
    "startIndex" : 8,
    "stopIndex" : 46,
    "fragment" : "replace(utf8_binary, utf8_lcase, 'abc')"
  } ]
}


-- !query
select replace(s, utf8_binary, 'abc') from t1
-- !query analysis
Project [replace(s#x, utf8_binary#x, abc) AS replace(s, utf8_binary, abc)#x]
+- SubqueryAlias spark_catalog.default.t1
   +- Relation spark_catalog.default.t1[s#x,utf8_binary#x,utf8_lcase#x] parquet


-- !query
select replace(utf8_binary collate utf8_binary, s collate utf8_lcase, 'abc') from t1
-- !query analysis
org.apache.spark.sql.AnalysisException
{
  "errorClass" : "COLLATION_MISMATCH.EXPLICIT",
  "sqlState" : "42P21",
  "messageParameters" : {
    "explicitTypes" : "\"STRING\", \"STRING COLLATE UTF8_LCASE\""
  }
}


-- !query
select replace(utf8_binary, utf8_lcase collate utf8_binary, 'abc') from t1
-- !query analysis
Project [replace(utf8_binary#x, collate(utf8_lcase#x, utf8_binary), abc) AS replace(utf8_binary, collate(utf8_lcase, utf8_binary), abc)#x]
+- SubqueryAlias spark_catalog.default.t1
   +- Relation spark_catalog.default.t1[s#x,utf8_binary#x,utf8_lcase#x] parquet


-- !query
select replace(utf8_binary collate utf8_lcase, utf8_lcase collate utf8_lcase, 'abc') from t1
-- !query analysis
Project [replace(collate(utf8_binary#x, utf8_lcase), collate(utf8_lcase#x, utf8_lcase), abc) AS replace(collate(utf8_binary, utf8_lcase), collate(utf8_lcase, utf8_lcase), 'abc' collate UTF8_LCASE)#x]
+- SubqueryAlias spark_catalog.default.t1
   +- Relation spark_catalog.default.t1[s#x,utf8_binary#x,utf8_lcase#x] parquet


-- !query
select replace(utf8_binary collate unicode_ai, utf8_lcase collate unicode_ai, 'abc') from t1
-- !query analysis
org.apache.spark.sql.catalyst.ExtendedAnalysisException
{
  "errorClass" : "DATATYPE_MISMATCH.UNEXPECTED_INPUT_TYPE",
  "sqlState" : "42K09",
  "messageParameters" : {
    "inputSql" : "\"collate(utf8_binary, unicode_ai)\"",
    "inputType" : "\"STRING COLLATE UNICODE_AI\"",
    "paramIndex" : "first",
    "requiredType" : "\"STRING\"",
    "sqlExpr" : "\"replace(collate(utf8_binary, unicode_ai), collate(utf8_lcase, unicode_ai), 'abc' collate UNICODE_AI)\""
  },
  "queryContext" : [ {
    "objectType" : "",
    "objectName" : "",
    "startIndex" : 8,
    "stopIndex" : 84,
    "fragment" : "replace(utf8_binary collate unicode_ai, utf8_lcase collate unicode_ai, 'abc')"
  } ]
}


-- !query
select replace(utf8_binary, 'aaAaaAaA', 'abc'), replace(utf8_lcase, 'aaAaaAaA', 'abc') from t1
-- !query analysis
Project [replace(utf8_binary#x, aaAaaAaA, abc) AS replace(utf8_binary, aaAaaAaA, abc)#x, replace(utf8_lcase#x, aaAaaAaA, abc) AS replace(utf8_lcase, 'aaAaaAaA' collate UTF8_LCASE, 'abc' collate UTF8_LCASE)#x]
+- SubqueryAlias spark_catalog.default.t1
   +- Relation spark_catalog.default.t1[s#x,utf8_binary#x,utf8_lcase#x] parquet


-- !query
select replace(utf8_binary, 'aaAaaAaA' collate utf8_lcase, 'abc'), replace(utf8_lcase, 'aaAaaAaA' collate utf8_binary, 'abc') from t1
-- !query analysis
Project [replace(cast(utf8_binary#x as string collate UTF8_LCASE), collate(aaAaaAaA, utf8_lcase), abc) AS replace(utf8_binary, collate(aaAaaAaA, utf8_lcase), 'abc' collate UTF8_LCASE)#x, replace(cast(utf8_lcase#x as string), collate(aaAaaAaA, utf8_binary), abc) AS replace(utf8_lcase, collate(aaAaaAaA, utf8_binary), abc)#x]
+- SubqueryAlias spark_catalog.default.t1
   +- Relation spark_catalog.default.t1[s#x,utf8_binary#x,utf8_lcase#x] parquet


-- !query
select replace(utf8_binary, 'aaAaaAaA ' collate utf8_lcase_rtrim, 'abc'), replace(utf8_lcase, 'aaAaaAaA' collate utf8_binary, 'abc') from t1
-- !query analysis
Project [replace(cast(utf8_binary#x as string collate UTF8_LCASE_RTRIM), collate(aaAaaAaA , utf8_lcase_rtrim), abc) AS replace(utf8_binary, collate(aaAaaAaA , utf8_lcase_rtrim), 'abc' collate UTF8_LCASE_RTRIM)#x, replace(cast(utf8_lcase#x as string), collate(aaAaaAaA, utf8_binary), abc) AS replace(utf8_lcase, collate(aaAaaAaA, utf8_binary), abc)#x]
+- SubqueryAlias spark_catalog.default.t1
   +- Relation spark_catalog.default.t1[s#x,utf8_binary#x,utf8_lcase#x] parquet


-- !query
select endswith(utf8_binary, utf8_lcase) from t1
-- !query analysis
org.apache.spark.sql.AnalysisException
{
  "errorClass" : "INDETERMINATE_COLLATION_IN_EXPRESSION",
  "sqlState" : "42P22",
  "messageParameters" : {
    "expr" : "\"endswith(utf8_binary, utf8_lcase)\""
  },
  "queryContext" : [ {
    "objectType" : "",
    "objectName" : "",
    "startIndex" : 8,
    "stopIndex" : 40,
    "fragment" : "endswith(utf8_binary, utf8_lcase)"
  } ]
}


-- !query
select endswith(s, utf8_binary) from t1
-- !query analysis
Project [EndsWith(s#x, utf8_binary#x) AS endswith(s, utf8_binary)#x]
+- SubqueryAlias spark_catalog.default.t1
   +- Relation spark_catalog.default.t1[s#x,utf8_binary#x,utf8_lcase#x] parquet


-- !query
select endswith(utf8_binary collate utf8_binary, s collate utf8_lcase) from t1
-- !query analysis
org.apache.spark.sql.AnalysisException
{
  "errorClass" : "COLLATION_MISMATCH.EXPLICIT",
  "sqlState" : "42P21",
  "messageParameters" : {
    "explicitTypes" : "\"STRING\", \"STRING COLLATE UTF8_LCASE\""
  }
}


-- !query
select endswith(utf8_binary, utf8_lcase collate utf8_binary) from t1
-- !query analysis
Project [EndsWith(utf8_binary#x, collate(utf8_lcase#x, utf8_binary)) AS endswith(utf8_binary, collate(utf8_lcase, utf8_binary))#x]
+- SubqueryAlias spark_catalog.default.t1
   +- Relation spark_catalog.default.t1[s#x,utf8_binary#x,utf8_lcase#x] parquet


-- !query
select endswith(utf8_binary collate utf8_lcase, utf8_lcase collate utf8_lcase) from t1
-- !query analysis
Project [EndsWith(collate(utf8_binary#x, utf8_lcase), collate(utf8_lcase#x, utf8_lcase)) AS endswith(collate(utf8_binary, utf8_lcase), collate(utf8_lcase, utf8_lcase))#x]
+- SubqueryAlias spark_catalog.default.t1
   +- Relation spark_catalog.default.t1[s#x,utf8_binary#x,utf8_lcase#x] parquet


-- !query
select endswith(utf8_binary collate unicode_ai, utf8_lcase collate unicode_ai) from t1
-- !query analysis
org.apache.spark.sql.catalyst.ExtendedAnalysisException
{
  "errorClass" : "DATATYPE_MISMATCH.UNEXPECTED_INPUT_TYPE",
  "sqlState" : "42K09",
  "messageParameters" : {
    "inputSql" : "\"collate(utf8_binary, unicode_ai)\"",
    "inputType" : "\"STRING COLLATE UNICODE_AI\"",
    "paramIndex" : "first",
    "requiredType" : "\"STRING\"",
    "sqlExpr" : "\"endswith(collate(utf8_binary, unicode_ai), collate(utf8_lcase, unicode_ai))\""
  },
  "queryContext" : [ {
    "objectType" : "",
    "objectName" : "",
    "startIndex" : 8,
    "stopIndex" : 78,
    "fragment" : "endswith(utf8_binary collate unicode_ai, utf8_lcase collate unicode_ai)"
  } ]
}


-- !query
select endswith(utf8_binary, 'aaAaaAaA'), endswith(utf8_lcase, 'aaAaaAaA') from t1
-- !query analysis
Project [EndsWith(utf8_binary#x, aaAaaAaA) AS endswith(utf8_binary, aaAaaAaA)#x, EndsWith(utf8_lcase#x, aaAaaAaA) AS endswith(utf8_lcase, 'aaAaaAaA' collate UTF8_LCASE)#x]
+- SubqueryAlias spark_catalog.default.t1
   +- Relation spark_catalog.default.t1[s#x,utf8_binary#x,utf8_lcase#x] parquet


-- !query
select endswith(utf8_binary, 'aaAaaAaA' collate utf8_lcase), endswith(utf8_lcase, 'aaAaaAaA' collate utf8_binary) from t1
-- !query analysis
Project [EndsWith(cast(utf8_binary#x as string collate UTF8_LCASE), collate(aaAaaAaA, utf8_lcase)) AS endswith(utf8_binary, collate(aaAaaAaA, utf8_lcase))#x, EndsWith(cast(utf8_lcase#x as string), collate(aaAaaAaA, utf8_binary)) AS endswith(utf8_lcase, collate(aaAaaAaA, utf8_binary))#x]
+- SubqueryAlias spark_catalog.default.t1
   +- Relation spark_catalog.default.t1[s#x,utf8_binary#x,utf8_lcase#x] parquet


-- !query
select endswith(utf8_binary, 'aaAaaAaA ' collate utf8_lcase_rtrim), endswith(utf8_lcase, 'aaAaaAaA' collate utf8_binary) from t1
-- !query analysis
Project [EndsWith(cast(utf8_binary#x as string collate UTF8_LCASE_RTRIM), collate(aaAaaAaA , utf8_lcase_rtrim)) AS endswith(utf8_binary, collate(aaAaaAaA , utf8_lcase_rtrim))#x, EndsWith(cast(utf8_lcase#x as string), collate(aaAaaAaA, utf8_binary)) AS endswith(utf8_lcase, collate(aaAaaAaA, utf8_binary))#x]
+- SubqueryAlias spark_catalog.default.t1
   +- Relation spark_catalog.default.t1[s#x,utf8_binary#x,utf8_lcase#x] parquet


-- !query
select repeat(utf8_binary, 3), repeat(utf8_lcase, 2) from t1
-- !query analysis
Project [repeat(utf8_binary#x, 3) AS repeat(utf8_binary, 3)#x, repeat(utf8_lcase#x, 2) AS repeat(utf8_lcase, 2)#x]
+- SubqueryAlias spark_catalog.default.t1
   +- Relation spark_catalog.default.t1[s#x,utf8_binary#x,utf8_lcase#x] parquet


-- !query
select repeat(utf8_binary collate utf8_lcase, 3), repeat(utf8_lcase collate utf8_binary, 2) from t1
-- !query analysis
Project [repeat(collate(utf8_binary#x, utf8_lcase), 3) AS repeat(collate(utf8_binary, utf8_lcase), 3)#x, repeat(collate(utf8_lcase#x, utf8_binary), 2) AS repeat(collate(utf8_lcase, utf8_binary), 2)#x]
+- SubqueryAlias spark_catalog.default.t1
   +- Relation spark_catalog.default.t1[s#x,utf8_binary#x,utf8_lcase#x] parquet


-- !query
select chr(ascii) from t2
-- !query analysis
Project [chr(ascii#xL) AS chr(ascii)#x]
+- SubqueryAlias spark_catalog.default.t2
   +- Relation spark_catalog.default.t2[ascii#xL] parquet


-- !query
select sentences(utf8_binary), sentences(utf8_lcase) from t1
-- !query analysis
Project [sentences(utf8_binary#x, , ) AS sentences(utf8_binary, , )#x, sentences(utf8_lcase#x, , ) AS sentences(utf8_lcase, , )#x]
+- SubqueryAlias spark_catalog.default.t1
   +- Relation spark_catalog.default.t1[s#x,utf8_binary#x,utf8_lcase#x] parquet


-- !query
select sentences(utf8_binary collate utf8_lcase), sentences(utf8_lcase collate utf8_binary) from t1
-- !query analysis
Project [sentences(collate(utf8_binary#x, utf8_lcase), , ) AS sentences(collate(utf8_binary, utf8_lcase), , )#x, sentences(collate(utf8_lcase#x, utf8_binary), , ) AS sentences(collate(utf8_lcase, utf8_binary), , )#x]
+- SubqueryAlias spark_catalog.default.t1
   +- Relation spark_catalog.default.t1[s#x,utf8_binary#x,utf8_lcase#x] parquet


-- !query
select upper(utf8_binary), upper(utf8_lcase) from t1
-- !query analysis
Project [upper(utf8_binary#x) AS upper(utf8_binary)#x, upper(utf8_lcase#x) AS upper(utf8_lcase)#x]
+- SubqueryAlias spark_catalog.default.t1
   +- Relation spark_catalog.default.t1[s#x,utf8_binary#x,utf8_lcase#x] parquet


-- !query
select upper(utf8_binary collate utf8_lcase), upper(utf8_lcase collate utf8_binary) from t1
-- !query analysis
Project [upper(collate(utf8_binary#x, utf8_lcase)) AS upper(collate(utf8_binary, utf8_lcase))#x, upper(collate(utf8_lcase#x, utf8_binary)) AS upper(collate(utf8_lcase, utf8_binary))#x]
+- SubqueryAlias spark_catalog.default.t1
   +- Relation spark_catalog.default.t1[s#x,utf8_binary#x,utf8_lcase#x] parquet


-- !query
select lower(utf8_binary), lower(utf8_lcase) from t1
-- !query analysis
Project [lower(utf8_binary#x) AS lower(utf8_binary)#x, lower(utf8_lcase#x) AS lower(utf8_lcase)#x]
+- SubqueryAlias spark_catalog.default.t1
   +- Relation spark_catalog.default.t1[s#x,utf8_binary#x,utf8_lcase#x] parquet


-- !query
select lower(utf8_binary collate utf8_lcase), lower(utf8_lcase collate utf8_binary) from t1
-- !query analysis
Project [lower(collate(utf8_binary#x, utf8_lcase)) AS lower(collate(utf8_binary, utf8_lcase))#x, lower(collate(utf8_lcase#x, utf8_binary)) AS lower(collate(utf8_lcase, utf8_binary))#x]
+- SubqueryAlias spark_catalog.default.t1
   +- Relation spark_catalog.default.t1[s#x,utf8_binary#x,utf8_lcase#x] parquet


-- !query
select initcap(utf8_binary), initcap(utf8_lcase) from t1
-- !query analysis
Project [initcap(utf8_binary#x) AS initcap(utf8_binary)#x, initcap(utf8_lcase#x) AS initcap(utf8_lcase)#x]
+- SubqueryAlias spark_catalog.default.t1
   +- Relation spark_catalog.default.t1[s#x,utf8_binary#x,utf8_lcase#x] parquet


-- !query
select initcap(utf8_binary collate utf8_lcase), initcap(utf8_lcase collate utf8_binary) from t1
-- !query analysis
Project [initcap(collate(utf8_binary#x, utf8_lcase)) AS initcap(collate(utf8_binary, utf8_lcase))#x, initcap(collate(utf8_lcase#x, utf8_binary)) AS initcap(collate(utf8_lcase, utf8_binary))#x]
+- SubqueryAlias spark_catalog.default.t1
   +- Relation spark_catalog.default.t1[s#x,utf8_binary#x,utf8_lcase#x] parquet


-- !query
select overlay(utf8_binary, utf8_lcase, 2) from t1
-- !query analysis
Project [overlay(cast(utf8_binary#x as string collate null), cast(utf8_lcase#x as string collate null), 2, -1) AS overlay(utf8_binary, utf8_lcase, 2, -1)#x]
+- SubqueryAlias spark_catalog.default.t1
   +- Relation spark_catalog.default.t1[s#x,utf8_binary#x,utf8_lcase#x] parquet


-- !query
select overlay(s, utf8_binary,1) from t1
-- !query analysis
Project [overlay(s#x, utf8_binary#x, 1, -1) AS overlay(s, utf8_binary, 1, -1)#x]
+- SubqueryAlias spark_catalog.default.t1
   +- Relation spark_catalog.default.t1[s#x,utf8_binary#x,utf8_lcase#x] parquet


-- !query
select overlay(utf8_binary collate utf8_binary, s collate utf8_lcase, 3) from t1
-- !query analysis
org.apache.spark.sql.AnalysisException
{
  "errorClass" : "COLLATION_MISMATCH.EXPLICIT",
  "sqlState" : "42P21",
  "messageParameters" : {
    "explicitTypes" : "\"STRING\", \"STRING COLLATE UTF8_LCASE\""
  }
}


-- !query
select overlay(utf8_binary, utf8_lcase collate utf8_binary, 2) from t1
-- !query analysis
Project [overlay(utf8_binary#x, collate(utf8_lcase#x, utf8_binary), 2, -1) AS overlay(utf8_binary, collate(utf8_lcase, utf8_binary), 2, -1)#x]
+- SubqueryAlias spark_catalog.default.t1
   +- Relation spark_catalog.default.t1[s#x,utf8_binary#x,utf8_lcase#x] parquet


-- !query
select overlay(utf8_binary collate utf8_lcase, utf8_lcase collate utf8_lcase, 2) from t1
-- !query analysis
Project [overlay(collate(utf8_binary#x, utf8_lcase), collate(utf8_lcase#x, utf8_lcase), 2, -1) AS overlay(collate(utf8_binary, utf8_lcase), collate(utf8_lcase, utf8_lcase), 2, -1)#x]
+- SubqueryAlias spark_catalog.default.t1
   +- Relation spark_catalog.default.t1[s#x,utf8_binary#x,utf8_lcase#x] parquet


-- !query
select overlay(utf8_binary, 'a', 2), overlay(utf8_lcase, 'a', 2) from t1
-- !query analysis
Project [overlay(utf8_binary#x, a, 2, -1) AS overlay(utf8_binary, a, 2, -1)#x, overlay(utf8_lcase#x, a, 2, -1) AS overlay(utf8_lcase, 'a' collate UTF8_LCASE, 2, -1)#x]
+- SubqueryAlias spark_catalog.default.t1
   +- Relation spark_catalog.default.t1[s#x,utf8_binary#x,utf8_lcase#x] parquet


-- !query
select overlay(utf8_binary, 'AaAA' collate utf8_lcase, 2), overlay(utf8_lcase, 'AAa' collate utf8_binary, 2) from t1
-- !query analysis
Project [overlay(cast(utf8_binary#x as string collate UTF8_LCASE), collate(AaAA, utf8_lcase), 2, -1) AS overlay(utf8_binary, collate(AaAA, utf8_lcase), 2, -1)#x, overlay(cast(utf8_lcase#x as string), collate(AAa, utf8_binary), 2, -1) AS overlay(utf8_lcase, collate(AAa, utf8_binary), 2, -1)#x]
+- SubqueryAlias spark_catalog.default.t1
   +- Relation spark_catalog.default.t1[s#x,utf8_binary#x,utf8_lcase#x] parquet


-- !query
select format_string(format, utf8_binary, utf8_lcase) from t3
-- !query analysis
Project [format_string(format#x, utf8_binary#x, utf8_lcase#x) AS format_string(format, utf8_binary, utf8_lcase)#x]
+- SubqueryAlias spark_catalog.default.t3
   +- Relation spark_catalog.default.t3[format#x,utf8_binary#x,utf8_lcase#x] parquet


-- !query
select format_string(format collate utf8_lcase, utf8_lcase, utf8_binary collate utf8_lcase, 3), format_string(format, utf8_lcase collate utf8_binary, utf8_binary) from t3
-- !query analysis
Project [format_string(collate(format#x, utf8_lcase), utf8_lcase#x, collate(utf8_binary#x, utf8_lcase), 3) AS format_string(collate(format, utf8_lcase), utf8_lcase, collate(utf8_binary, utf8_lcase), 3)#x, format_string(format#x, collate(utf8_lcase#x, utf8_binary), utf8_binary#x) AS format_string(format, collate(utf8_lcase, utf8_binary), utf8_binary)#x]
+- SubqueryAlias spark_catalog.default.t3
   +- Relation spark_catalog.default.t3[format#x,utf8_binary#x,utf8_lcase#x] parquet


-- !query
select format_string(format, utf8_binary, utf8_lcase) from t3
-- !query analysis
Project [format_string(format#x, utf8_binary#x, utf8_lcase#x) AS format_string(format, utf8_binary, utf8_lcase)#x]
+- SubqueryAlias spark_catalog.default.t3
   +- Relation spark_catalog.default.t3[format#x,utf8_binary#x,utf8_lcase#x] parquet


-- !query
select length(utf8_binary), length(utf8_lcase) from t1
-- !query analysis
Project [length(utf8_binary#x) AS length(utf8_binary)#x, length(utf8_lcase#x) AS length(utf8_lcase)#x]
+- SubqueryAlias spark_catalog.default.t1
   +- Relation spark_catalog.default.t1[s#x,utf8_binary#x,utf8_lcase#x] parquet


-- !query
select length(utf8_binary collate utf8_lcase), length(utf8_lcase collate utf8_binary) from t1
-- !query analysis
Project [length(collate(utf8_binary#x, utf8_lcase)) AS length(collate(utf8_binary, utf8_lcase))#x, length(collate(utf8_lcase#x, utf8_binary)) AS length(collate(utf8_lcase, utf8_binary))#x]
+- SubqueryAlias spark_catalog.default.t1
   +- Relation spark_catalog.default.t1[s#x,utf8_binary#x,utf8_lcase#x] parquet


-- !query
select bit_length(utf8_binary), bit_length(utf8_lcase) from t1
-- !query analysis
Project [bit_length(utf8_binary#x) AS bit_length(utf8_binary)#x, bit_length(utf8_lcase#x) AS bit_length(utf8_lcase)#x]
+- SubqueryAlias spark_catalog.default.t1
   +- Relation spark_catalog.default.t1[s#x,utf8_binary#x,utf8_lcase#x] parquet


-- !query
select bit_length(utf8_binary collate utf8_lcase), bit_length(utf8_lcase collate utf8_binary) from t1
-- !query analysis
Project [bit_length(collate(utf8_binary#x, utf8_lcase)) AS bit_length(collate(utf8_binary, utf8_lcase))#x, bit_length(collate(utf8_lcase#x, utf8_binary)) AS bit_length(collate(utf8_lcase, utf8_binary))#x]
+- SubqueryAlias spark_catalog.default.t1
   +- Relation spark_catalog.default.t1[s#x,utf8_binary#x,utf8_lcase#x] parquet


-- !query
select octet_length(utf8_binary), octet_length(utf8_lcase) from t1
-- !query analysis
Project [octet_length(utf8_binary#x) AS octet_length(utf8_binary)#x, octet_length(utf8_lcase#x) AS octet_length(utf8_lcase)#x]
+- SubqueryAlias spark_catalog.default.t1
   +- Relation spark_catalog.default.t1[s#x,utf8_binary#x,utf8_lcase#x] parquet


-- !query
select octet_length(utf8_binary collate utf8_lcase), octet_length(utf8_lcase collate utf8_binary) from t1
-- !query analysis
Project [octet_length(collate(utf8_binary#x, utf8_lcase)) AS octet_length(collate(utf8_binary, utf8_lcase))#x, octet_length(collate(utf8_lcase#x, utf8_binary)) AS octet_length(collate(utf8_lcase, utf8_binary))#x]
+- SubqueryAlias spark_catalog.default.t1
   +- Relation spark_catalog.default.t1[s#x,utf8_binary#x,utf8_lcase#x] parquet


-- !query
select octet_length(utf8_binary collate utf8_lcase_rtrim), octet_length(utf8_lcase collate utf8_binary_rtrim) from t1
-- !query analysis
Project [octet_length(collate(utf8_binary#x, utf8_lcase_rtrim)) AS octet_length(collate(utf8_binary, utf8_lcase_rtrim))#x, octet_length(collate(utf8_lcase#x, utf8_binary_rtrim)) AS octet_length(collate(utf8_lcase, utf8_binary_rtrim))#x]
+- SubqueryAlias spark_catalog.default.t1
   +- Relation spark_catalog.default.t1[s#x,utf8_binary#x,utf8_lcase#x] parquet


-- !query
select substr(utf8_binary, 2, 2), substr(utf8_lcase, 2, 2) from t1
-- !query analysis
Project [substr(utf8_binary#x, 2, 2) AS substr(utf8_binary, 2, 2)#x, substr(utf8_lcase#x, 2, 2) AS substr(utf8_lcase, 2, 2)#x]
+- SubqueryAlias spark_catalog.default.t1
   +- Relation spark_catalog.default.t1[s#x,utf8_binary#x,utf8_lcase#x] parquet


-- !query
select substr(utf8_binary collate utf8_lcase, 2, 2), substr(utf8_lcase collate utf8_binary, 2, 2) from t1
-- !query analysis
Project [substr(collate(utf8_binary#x, utf8_lcase), 2, 2) AS substr(collate(utf8_binary, utf8_lcase), 2, 2)#x, substr(collate(utf8_lcase#x, utf8_binary), 2, 2) AS substr(collate(utf8_lcase, utf8_binary), 2, 2)#x]
+- SubqueryAlias spark_catalog.default.t1
   +- Relation spark_catalog.default.t1[s#x,utf8_binary#x,utf8_lcase#x] parquet


-- !query
select right(utf8_binary, 2), right(utf8_lcase, 2) from t1
-- !query analysis
Project [right(utf8_binary#x, 2) AS right(utf8_binary, 2)#x, right(utf8_lcase#x, 2) AS right(utf8_lcase, 2)#x]
+- SubqueryAlias spark_catalog.default.t1
   +- Relation spark_catalog.default.t1[s#x,utf8_binary#x,utf8_lcase#x] parquet


-- !query
select right(utf8_binary collate utf8_lcase, 2), right(utf8_lcase collate utf8_binary, 2) from t1
-- !query analysis
Project [right(collate(utf8_binary#x, utf8_lcase), 2) AS right(collate(utf8_binary, utf8_lcase), 2)#x, right(collate(utf8_lcase#x, utf8_binary), 2) AS right(collate(utf8_lcase, utf8_binary), 2)#x]
+- SubqueryAlias spark_catalog.default.t1
   +- Relation spark_catalog.default.t1[s#x,utf8_binary#x,utf8_lcase#x] parquet


-- !query
select left(utf8_binary, '2' collate utf8_lcase), left(utf8_lcase, 2) from t1
-- !query analysis
Project [left(utf8_binary#x, cast(collate(2, utf8_lcase) as int)) AS left(utf8_binary, collate(2, utf8_lcase))#x, left(utf8_lcase#x, 2) AS left(utf8_lcase, 2)#x]
+- SubqueryAlias spark_catalog.default.t1
   +- Relation spark_catalog.default.t1[s#x,utf8_binary#x,utf8_lcase#x] parquet


-- !query
select left(utf8_binary collate utf8_lcase, 2), left(utf8_lcase collate utf8_binary, 2) from t1
-- !query analysis
Project [left(collate(utf8_binary#x, utf8_lcase), 2) AS left(collate(utf8_binary, utf8_lcase), 2)#x, left(collate(utf8_lcase#x, utf8_binary), 2) AS left(collate(utf8_lcase, utf8_binary), 2)#x]
+- SubqueryAlias spark_catalog.default.t1
   +- Relation spark_catalog.default.t1[s#x,utf8_binary#x,utf8_lcase#x] parquet


-- !query
select locate(utf8_binary, utf8_lcase) from t1
-- !query analysis
org.apache.spark.sql.AnalysisException
{
  "errorClass" : "INDETERMINATE_COLLATION_IN_EXPRESSION",
  "sqlState" : "42P22",
  "messageParameters" : {
    "expr" : "\"locate(utf8_binary, utf8_lcase, 1)\""
  },
  "queryContext" : [ {
    "objectType" : "",
    "objectName" : "",
    "startIndex" : 8,
    "stopIndex" : 38,
    "fragment" : "locate(utf8_binary, utf8_lcase)"
  } ]
}


-- !query
select locate(s, utf8_binary) from t1
-- !query analysis
Project [locate(s#x, utf8_binary#x, 1) AS locate(s, utf8_binary, 1)#x]
+- SubqueryAlias spark_catalog.default.t1
   +- Relation spark_catalog.default.t1[s#x,utf8_binary#x,utf8_lcase#x] parquet


-- !query
select locate(utf8_binary collate utf8_binary, s collate utf8_lcase) from t1
-- !query analysis
org.apache.spark.sql.AnalysisException
{
  "errorClass" : "COLLATION_MISMATCH.EXPLICIT",
  "sqlState" : "42P21",
  "messageParameters" : {
    "explicitTypes" : "\"STRING\", \"STRING COLLATE UTF8_LCASE\""
  }
}


-- !query
select locate(utf8_binary, utf8_lcase collate utf8_binary) from t1
-- !query analysis
Project [locate(utf8_binary#x, collate(utf8_lcase#x, utf8_binary), 1) AS locate(utf8_binary, collate(utf8_lcase, utf8_binary), 1)#x]
+- SubqueryAlias spark_catalog.default.t1
   +- Relation spark_catalog.default.t1[s#x,utf8_binary#x,utf8_lcase#x] parquet


-- !query
select locate(utf8_binary collate utf8_lcase, utf8_lcase collate utf8_lcase, 3) from t1
-- !query analysis
Project [locate(collate(utf8_binary#x, utf8_lcase), collate(utf8_lcase#x, utf8_lcase), 3) AS locate(collate(utf8_binary, utf8_lcase), collate(utf8_lcase, utf8_lcase), 3)#x]
+- SubqueryAlias spark_catalog.default.t1
   +- Relation spark_catalog.default.t1[s#x,utf8_binary#x,utf8_lcase#x] parquet


-- !query
select locate(utf8_binary collate unicode_ai, utf8_lcase collate unicode_ai, 3) from t1
-- !query analysis
org.apache.spark.sql.catalyst.ExtendedAnalysisException
{
  "errorClass" : "DATATYPE_MISMATCH.UNEXPECTED_INPUT_TYPE",
  "sqlState" : "42K09",
  "messageParameters" : {
    "inputSql" : "\"collate(utf8_binary, unicode_ai)\"",
    "inputType" : "\"STRING COLLATE UNICODE_AI\"",
    "paramIndex" : "first",
    "requiredType" : "\"STRING\"",
    "sqlExpr" : "\"locate(collate(utf8_binary, unicode_ai), collate(utf8_lcase, unicode_ai), 3)\""
  },
  "queryContext" : [ {
    "objectType" : "",
    "objectName" : "",
    "startIndex" : 8,
    "stopIndex" : 79,
    "fragment" : "locate(utf8_binary collate unicode_ai, utf8_lcase collate unicode_ai, 3)"
  } ]
}


-- !query
select locate(utf8_binary, 'a'), locate(utf8_lcase, 'a') from t1
-- !query analysis
Project [locate(utf8_binary#x, a, 1) AS locate(utf8_binary, a, 1)#x, locate(utf8_lcase#x, a, 1) AS locate(utf8_lcase, 'a' collate UTF8_LCASE, 1)#x]
+- SubqueryAlias spark_catalog.default.t1
   +- Relation spark_catalog.default.t1[s#x,utf8_binary#x,utf8_lcase#x] parquet


-- !query
select locate(utf8_binary, 'AaAA' collate utf8_lcase, 4), locate(utf8_lcase, 'AAa' collate utf8_binary, 4) from t1
-- !query analysis
Project [locate(cast(utf8_binary#x as string collate UTF8_LCASE), collate(AaAA, utf8_lcase), 4) AS locate(utf8_binary, collate(AaAA, utf8_lcase), 4)#x, locate(cast(utf8_lcase#x as string), collate(AAa, utf8_binary), 4) AS locate(utf8_lcase, collate(AAa, utf8_binary), 4)#x]
+- SubqueryAlias spark_catalog.default.t1
   +- Relation spark_catalog.default.t1[s#x,utf8_binary#x,utf8_lcase#x] parquet


-- !query
select locate(utf8_binary, 'AaAA ' collate utf8_binary_rtrim, 4), locate(utf8_lcase, 'AAa ' collate utf8_binary, 4) from t1
-- !query analysis
Project [locate(cast(utf8_binary#x as string collate UTF8_BINARY_RTRIM), collate(AaAA , utf8_binary_rtrim), 4) AS locate(utf8_binary, collate(AaAA , utf8_binary_rtrim), 4)#x, locate(cast(utf8_lcase#x as string), collate(AAa , utf8_binary), 4) AS locate(utf8_lcase, collate(AAa , utf8_binary), 4)#x]
+- SubqueryAlias spark_catalog.default.t1
   +- Relation spark_catalog.default.t1[s#x,utf8_binary#x,utf8_lcase#x] parquet


-- !query
drop table t1
-- !query analysis
DropTable false, false
+- ResolvedIdentifier V2SessionCatalog(spark_catalog), default.t1


-- !query
drop table t2
-- !query analysis
DropTable false, false
+- ResolvedIdentifier V2SessionCatalog(spark_catalog), default.t2


-- !query
drop table t3
-- !query analysis
DropTable false, false
+- ResolvedIdentifier V2SessionCatalog(spark_catalog), default.t3
