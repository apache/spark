-- Automatically generated by SQLQueryTestSuite
-- !query
create table char_tbl(c char(5), v varchar(6)) using parquet
-- !query analysis
CreateDataSourceTableCommand `spark_catalog`.`default`.`char_tbl`, false


-- !query
desc formatted char_tbl
-- !query analysis
DescribeTableCommand `spark_catalog`.`default`.`char_tbl`, true, [col_name#x, data_type#x, comment#x]


-- !query
desc formatted char_tbl c
-- !query analysis
DescribeColumnCommand `spark_catalog`.`default`.`char_tbl`, [spark_catalog, default, char_tbl, c], true, [info_name#x, info_value#x]


-- !query
show create table char_tbl
-- !query analysis
ShowCreateTable false, [createtab_stmt#x]
+- ResolvedTable V2SessionCatalog(spark_catalog), default.char_tbl, V1Table(default.char_tbl), [c#x, v#x]


-- !query
create table char_tbl2 using parquet as select * from char_tbl
-- !query analysis
CreateDataSourceTableAsSelectCommand `spark_catalog`.`default`.`char_tbl2`, ErrorIfExists, [c, v]
   +- Project [c#x, v#x]
      +- SubqueryAlias spark_catalog.default.char_tbl
         +- Relation spark_catalog.default.char_tbl[c#x,v#x] parquet


-- !query
show create table char_tbl2
-- !query analysis
ShowCreateTable false, [createtab_stmt#x]
+- ResolvedTable V2SessionCatalog(spark_catalog), default.char_tbl2, V1Table(default.char_tbl2), [c#x, v#x]


-- !query
desc formatted char_tbl2
-- !query analysis
DescribeTableCommand `spark_catalog`.`default`.`char_tbl2`, true, [col_name#x, data_type#x, comment#x]


-- !query
desc formatted char_tbl2 c
-- !query analysis
DescribeColumnCommand `spark_catalog`.`default`.`char_tbl2`, [spark_catalog, default, char_tbl2, c], true, [info_name#x, info_value#x]


-- !query
create table char_tbl3 like char_tbl
-- !query analysis
CreateTableLikeCommand `char_tbl3`, `char_tbl`, Storage(), false


-- !query
desc formatted char_tbl3
-- !query analysis
DescribeTableCommand `spark_catalog`.`default`.`char_tbl3`, true, [col_name#x, data_type#x, comment#x]


-- !query
desc formatted char_tbl3 c
-- !query analysis
DescribeColumnCommand `spark_catalog`.`default`.`char_tbl3`, [spark_catalog, default, char_tbl3, c], true, [info_name#x, info_value#x]


-- !query
show create table char_tbl3
-- !query analysis
ShowCreateTable false, [createtab_stmt#x]
+- ResolvedTable V2SessionCatalog(spark_catalog), default.char_tbl3, V1Table(default.char_tbl3), [c#x, v#x]


-- !query
create view char_view as select * from char_tbl
-- !query analysis
org.apache.spark.sql.AnalysisException
{
  "errorClass" : "TABLE_OR_VIEW_ALREADY_EXISTS",
  "sqlState" : "42P07",
  "messageParameters" : {
    "relationName" : "`spark_catalog`.`default`.`char_view`"
  }
}


-- !query
desc formatted char_view
-- !query analysis
DescribeTableCommand `spark_catalog`.`default`.`char_view`, true, [col_name#x, data_type#x, comment#x]


-- !query
desc formatted char_view c
-- !query analysis
DescribeColumnCommand `spark_catalog`.`default`.`char_view`, [c], true, [info_name#x, info_value#x]


-- !query
show create table char_view
-- !query analysis
ShowCreateTableCommand `spark_catalog`.`default`.`char_view`, [createtab_stmt#x]


-- !query
alter table char_tbl rename to char_tbl1
-- !query analysis
AlterTableRenameCommand `spark_catalog`.`default`.`char_tbl`, `char_tbl1`, false


-- !query
desc formatted char_tbl1
-- !query analysis
DescribeTableCommand `spark_catalog`.`default`.`char_tbl1`, true, [col_name#x, data_type#x, comment#x]


-- !query
alter table char_tbl1 change column c type char(6)
-- !query analysis
org.apache.spark.sql.AnalysisException
{
  "errorClass" : "NOT_SUPPORTED_CHANGE_COLUMN",
  "sqlState" : "0A000",
  "messageParameters" : {
    "newName" : "`c`",
    "newType" : "\"CHAR(6)\"",
    "originName" : "`c`",
    "originType" : "\"CHAR(5)\"",
    "table" : "`spark_catalog`.`default`.`char_tbl1`"
  },
  "queryContext" : [ {
    "objectType" : "",
    "objectName" : "",
    "startIndex" : 1,
    "stopIndex" : 50,
    "fragment" : "alter table char_tbl1 change column c type char(6)"
  } ]
}


-- !query
alter table char_tbl1 change column c type char(5)
-- !query analysis
AlterTableChangeColumnCommand `spark_catalog`.`default`.`char_tbl1`, c, StructField(c,CharType(5),true)


-- !query
desc formatted char_tbl1
-- !query analysis
DescribeTableCommand `spark_catalog`.`default`.`char_tbl1`, true, [col_name#x, data_type#x, comment#x]


-- !query
alter table char_tbl1 add columns (d char(5))
-- !query analysis
AlterTableAddColumnsCommand `spark_catalog`.`default`.`char_tbl1`, [StructField(d,CharType(5),true)]


-- !query
desc formatted char_tbl1
-- !query analysis
DescribeTableCommand `spark_catalog`.`default`.`char_tbl1`, true, [col_name#x, data_type#x, comment#x]


-- !query
alter view char_view as select * from char_tbl2
-- !query analysis
AlterViewAsCommand `spark_catalog`.`default`.`char_view`, select * from char_tbl2, true
   +- Project [c#x, v#x]
      +- SubqueryAlias spark_catalog.default.char_tbl2
         +- Project [static_invoke(CharVarcharCodegenUtils.readSidePadding(c#x, 5)) AS c#x, v#x]
            +- Relation spark_catalog.default.char_tbl2[c#x,v#x] parquet


-- !query
desc formatted char_view
-- !query analysis
DescribeTableCommand `spark_catalog`.`default`.`char_view`, true, [col_name#x, data_type#x, comment#x]


-- !query
alter table char_tbl1 SET TBLPROPERTIES('yes'='no')
-- !query analysis
AlterTableSetPropertiesCommand `spark_catalog`.`default`.`char_tbl1`, [yes=no], false


-- !query
desc formatted char_tbl1
-- !query analysis
DescribeTableCommand `spark_catalog`.`default`.`char_tbl1`, true, [col_name#x, data_type#x, comment#x]


-- !query
alter view char_view SET TBLPROPERTIES('yes'='no')
-- !query analysis
AlterTableSetPropertiesCommand `spark_catalog`.`default`.`char_view`, [yes=no], true


-- !query
desc formatted char_view
-- !query analysis
DescribeTableCommand `spark_catalog`.`default`.`char_view`, true, [col_name#x, data_type#x, comment#x]


-- !query
alter table char_tbl1 UNSET TBLPROPERTIES('yes')
-- !query analysis
AlterTableUnsetPropertiesCommand `spark_catalog`.`default`.`char_tbl1`, [yes], false, false


-- !query
desc formatted char_tbl1
-- !query analysis
DescribeTableCommand `spark_catalog`.`default`.`char_tbl1`, true, [col_name#x, data_type#x, comment#x]


-- !query
alter view char_view UNSET TBLPROPERTIES('yes')
-- !query analysis
AlterTableUnsetPropertiesCommand `spark_catalog`.`default`.`char_view`, [yes], false, true


-- !query
desc formatted char_view
-- !query analysis
DescribeTableCommand `spark_catalog`.`default`.`char_view`, true, [col_name#x, data_type#x, comment#x]


-- !query
alter table char_tbl1 SET SERDEPROPERTIES('yes'='no')
-- !query analysis
AlterTableSerDePropertiesCommand `spark_catalog`.`default`.`char_tbl1`, Map(yes -> no)


-- !query
desc formatted char_tbl1
-- !query analysis
DescribeTableCommand `spark_catalog`.`default`.`char_tbl1`, true, [col_name#x, data_type#x, comment#x]


-- !query
create table char_part(c1 char(5), c2 char(2), v1 varchar(6), v2 varchar(2)) using parquet partitioned by (v2, c2)
-- !query analysis
org.apache.spark.sql.catalyst.analysis.TableAlreadyExistsException
{
  "errorClass" : "TABLE_OR_VIEW_ALREADY_EXISTS",
  "sqlState" : "42P07",
  "messageParameters" : {
    "relationName" : "`spark_catalog`.`default`.`char_part`"
  }
}


-- !query
desc formatted char_part
-- !query analysis
DescribeTableCommand `spark_catalog`.`default`.`char_part`, true, [col_name#x, data_type#x, comment#x]


-- !query
alter table char_part change column c1 comment 'char comment'
-- !query analysis
AlterTableChangeColumnCommand `spark_catalog`.`default`.`char_part`, c1, StructField(c1,CharType(5),true)


-- !query
alter table char_part change column v1 comment 'varchar comment'
-- !query analysis
AlterTableChangeColumnCommand `spark_catalog`.`default`.`char_part`, v1, StructField(v1,VarcharType(6),true)


-- !query
alter table char_part add partition (v2='ke', c2='nt') location 'loc1'
-- !query analysis
org.apache.spark.sql.catalyst.analysis.PartitionsAlreadyExistException
{
  "errorClass" : "PARTITIONS_ALREADY_EXIST",
  "sqlState" : "428FT",
  "messageParameters" : {
    "partitionList" : "PARTITION (`v2` = ke, `c2` = nt)",
    "tableName" : "`default`.`char_part`"
  }
}


-- !query
desc formatted char_part
-- !query analysis
DescribeTableCommand `spark_catalog`.`default`.`char_part`, true, [col_name#x, data_type#x, comment#x]


-- !query
alter table char_part partition (v2='ke') rename to partition (v2='nt')
-- !query analysis
org.apache.spark.sql.AnalysisException
{
  "errorClass" : "_LEGACY_ERROR_TEMP_1232",
  "messageParameters" : {
    "partitionColumnNames" : "v2, c2",
    "specKeys" : "v2",
    "tableName" : "`spark_catalog`.`default`.`char_part`"
  }
}


-- !query
desc formatted char_part
-- !query analysis
DescribeTableCommand `spark_catalog`.`default`.`char_part`, true, [col_name#x, data_type#x, comment#x]


-- !query
alter table char_part partition (v2='ke', c2='nt') set location 'loc2'
-- !query analysis
AlterTableSetLocationCommand `spark_catalog`.`default`.`char_part`, Map(v2 -> ke, c2 -> nt), loc2


-- !query
desc formatted char_part
-- !query analysis
DescribeTableCommand `spark_catalog`.`default`.`char_part`, true, [col_name#x, data_type#x, comment#x]


-- !query
MSCK REPAIR TABLE char_part
-- !query analysis
RepairTableCommand `spark_catalog`.`default`.`char_part`, true, false, MSCK REPAIR TABLE


-- !query
desc formatted char_part
-- !query analysis
DescribeTableCommand `spark_catalog`.`default`.`char_part`, true, [col_name#x, data_type#x, comment#x]


-- !query
create temporary view str_view as select c, v from values
    (null, null),
    (null, 'S'),
    ('N', 'N '),
    ('Ne', 'Sp'),
    ('Net  ', 'Spa  '),
    ('NetE', 'Spar'),
    ('NetEa ', 'Spark '),
    ('NetEas ', 'Spark'),
    ('NetEase', 'Spark-') t(c, v)
-- !query analysis
CreateViewCommand `str_view`, select c, v from values
    (null, null),
    (null, 'S'),
    ('N', 'N '),
    ('Ne', 'Sp'),
    ('Net  ', 'Spa  '),
    ('NetE', 'Spar'),
    ('NetEa ', 'Spark '),
    ('NetEas ', 'Spark'),
    ('NetEase', 'Spark-') t(c, v), false, false, LocalTempView, UNSUPPORTED, true
   +- Project [c#x, v#x]
      +- SubqueryAlias t
         +- LocalRelation [c#x, v#x]


-- !query
create table char_tbl4(c7 char(7), c8 char(8), v varchar(6), s string) using parquet
-- !query analysis
CreateDataSourceTableCommand `spark_catalog`.`default`.`char_tbl4`, false


-- !query
insert into char_tbl4 select c, c, v, c from str_view
-- !query analysis
InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/char_tbl4, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/char_tbl4], Append, `spark_catalog`.`default`.`char_tbl4`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/char_tbl4), [c7, c8, v, s]
+- Project [static_invoke(CharVarcharCodegenUtils.charTypeWriteSideCheck(cast(c#x as string), 7)) AS c7#x, static_invoke(CharVarcharCodegenUtils.charTypeWriteSideCheck(cast(c#x as string), 8)) AS c8#x, static_invoke(CharVarcharCodegenUtils.varcharTypeWriteSideCheck(cast(v#x as string), 6)) AS v#x, cast(c#x as string) AS s#x]
   +- Project [c#x, c#x, v#x, c#x]
      +- SubqueryAlias str_view
         +- View (`str_view`, [c#x, v#x])
            +- Project [cast(c#x as string) AS c#x, cast(v#x as string) AS v#x]
               +- Project [c#x, v#x]
                  +- SubqueryAlias t
                     +- LocalRelation [c#x, v#x]


-- !query
select c7, c8, v, s from char_tbl4
-- !query analysis
Project [c7#x, c8#x, v#x, s#x]
+- SubqueryAlias spark_catalog.default.char_tbl4
   +- Project [static_invoke(CharVarcharCodegenUtils.readSidePadding(c7#x, 7)) AS c7#x, static_invoke(CharVarcharCodegenUtils.readSidePadding(c8#x, 8)) AS c8#x, v#x, s#x]
      +- Relation spark_catalog.default.char_tbl4[c7#x,c8#x,v#x,s#x] parquet


-- !query
select c7, c8, v, s from char_tbl4 where c7 = c8
-- !query analysis
Project [c7#x, c8#x, v#x, s#x]
+- Filter (rpad(c7#x, 8,  ) = c8#x)
   +- SubqueryAlias spark_catalog.default.char_tbl4
      +- Project [static_invoke(CharVarcharCodegenUtils.readSidePadding(c7#x, 7)) AS c7#x, static_invoke(CharVarcharCodegenUtils.readSidePadding(c8#x, 8)) AS c8#x, v#x, s#x]
         +- Relation spark_catalog.default.char_tbl4[c7#x,c8#x,v#x,s#x] parquet


-- !query
select c7, c8, v, s from char_tbl4 where c7 = v
-- !query analysis
Project [c7#x, c8#x, v#x, s#x]
+- Filter (c7#x = v#x)
   +- SubqueryAlias spark_catalog.default.char_tbl4
      +- Project [static_invoke(CharVarcharCodegenUtils.readSidePadding(c7#x, 7)) AS c7#x, static_invoke(CharVarcharCodegenUtils.readSidePadding(c8#x, 8)) AS c8#x, v#x, s#x]
         +- Relation spark_catalog.default.char_tbl4[c7#x,c8#x,v#x,s#x] parquet


-- !query
select c7, c8, v, s from char_tbl4 where c7 = s
-- !query analysis
Project [c7#x, c8#x, v#x, s#x]
+- Filter (c7#x = s#x)
   +- SubqueryAlias spark_catalog.default.char_tbl4
      +- Project [static_invoke(CharVarcharCodegenUtils.readSidePadding(c7#x, 7)) AS c7#x, static_invoke(CharVarcharCodegenUtils.readSidePadding(c8#x, 8)) AS c8#x, v#x, s#x]
         +- Relation spark_catalog.default.char_tbl4[c7#x,c8#x,v#x,s#x] parquet


-- !query
select c7, c8, v, s from char_tbl4 where c7 = 'NetEase               '
-- !query analysis
Project [c7#x, c8#x, v#x, s#x]
+- Filter (rpad(c7#x, 22,  ) = NetEase               )
   +- SubqueryAlias spark_catalog.default.char_tbl4
      +- Project [static_invoke(CharVarcharCodegenUtils.readSidePadding(c7#x, 7)) AS c7#x, static_invoke(CharVarcharCodegenUtils.readSidePadding(c8#x, 8)) AS c8#x, v#x, s#x]
         +- Relation spark_catalog.default.char_tbl4[c7#x,c8#x,v#x,s#x] parquet


-- !query
select c7, c8, v, s from char_tbl4 where v = 'Spark '
-- !query analysis
Project [c7#x, c8#x, v#x, s#x]
+- Filter (v#x = Spark )
   +- SubqueryAlias spark_catalog.default.char_tbl4
      +- Project [static_invoke(CharVarcharCodegenUtils.readSidePadding(c7#x, 7)) AS c7#x, static_invoke(CharVarcharCodegenUtils.readSidePadding(c8#x, 8)) AS c8#x, v#x, s#x]
         +- Relation spark_catalog.default.char_tbl4[c7#x,c8#x,v#x,s#x] parquet


-- !query
select c7, c8, v, s from char_tbl4 order by c7
-- !query analysis
Sort [c7#x ASC NULLS FIRST], true
+- Project [c7#x, c8#x, v#x, s#x]
   +- SubqueryAlias spark_catalog.default.char_tbl4
      +- Project [static_invoke(CharVarcharCodegenUtils.readSidePadding(c7#x, 7)) AS c7#x, static_invoke(CharVarcharCodegenUtils.readSidePadding(c8#x, 8)) AS c8#x, v#x, s#x]
         +- Relation spark_catalog.default.char_tbl4[c7#x,c8#x,v#x,s#x] parquet


-- !query
select c7, c8, v, s from char_tbl4 order by v
-- !query analysis
Sort [v#x ASC NULLS FIRST], true
+- Project [c7#x, c8#x, v#x, s#x]
   +- SubqueryAlias spark_catalog.default.char_tbl4
      +- Project [static_invoke(CharVarcharCodegenUtils.readSidePadding(c7#x, 7)) AS c7#x, static_invoke(CharVarcharCodegenUtils.readSidePadding(c8#x, 8)) AS c8#x, v#x, s#x]
         +- Relation spark_catalog.default.char_tbl4[c7#x,c8#x,v#x,s#x] parquet


-- !query
select ascii(c7), ascii(c8), ascii(v), ascii(s) from char_tbl4
-- !query analysis
Project [ascii(c7#x) AS ascii(c7)#x, ascii(c8#x) AS ascii(c8)#x, ascii(v#x) AS ascii(v)#x, ascii(s#x) AS ascii(s)#x]
+- SubqueryAlias spark_catalog.default.char_tbl4
   +- Project [static_invoke(CharVarcharCodegenUtils.readSidePadding(c7#x, 7)) AS c7#x, static_invoke(CharVarcharCodegenUtils.readSidePadding(c8#x, 8)) AS c8#x, v#x, s#x]
      +- Relation spark_catalog.default.char_tbl4[c7#x,c8#x,v#x,s#x] parquet


-- !query
select base64(c7), base64(c8), base64(v), ascii(s) from char_tbl4
-- !query analysis
Project [base64(cast(c7#x as binary)) AS base64(c7)#x, base64(cast(c8#x as binary)) AS base64(c8)#x, base64(cast(v#x as binary)) AS base64(v)#x, ascii(s#x) AS ascii(s)#x]
+- SubqueryAlias spark_catalog.default.char_tbl4
   +- Project [static_invoke(CharVarcharCodegenUtils.readSidePadding(c7#x, 7)) AS c7#x, static_invoke(CharVarcharCodegenUtils.readSidePadding(c8#x, 8)) AS c8#x, v#x, s#x]
      +- Relation spark_catalog.default.char_tbl4[c7#x,c8#x,v#x,s#x] parquet


-- !query
select bit_length(c7), bit_length(c8), bit_length(v), bit_length(s) from char_tbl4
-- !query analysis
Project [bit_length(c7#x) AS bit_length(c7)#x, bit_length(c8#x) AS bit_length(c8)#x, bit_length(v#x) AS bit_length(v)#x, bit_length(s#x) AS bit_length(s)#x]
+- SubqueryAlias spark_catalog.default.char_tbl4
   +- Project [static_invoke(CharVarcharCodegenUtils.readSidePadding(c7#x, 7)) AS c7#x, static_invoke(CharVarcharCodegenUtils.readSidePadding(c8#x, 8)) AS c8#x, v#x, s#x]
      +- Relation spark_catalog.default.char_tbl4[c7#x,c8#x,v#x,s#x] parquet


-- !query
select char_length(c7), char_length(c8), char_length(v), char_length(s) from char_tbl4
-- !query analysis
Project [char_length(c7#x) AS char_length(c7)#x, char_length(c8#x) AS char_length(c8)#x, char_length(v#x) AS char_length(v)#x, char_length(s#x) AS char_length(s)#x]
+- SubqueryAlias spark_catalog.default.char_tbl4
   +- Project [static_invoke(CharVarcharCodegenUtils.readSidePadding(c7#x, 7)) AS c7#x, static_invoke(CharVarcharCodegenUtils.readSidePadding(c8#x, 8)) AS c8#x, v#x, s#x]
      +- Relation spark_catalog.default.char_tbl4[c7#x,c8#x,v#x,s#x] parquet


-- !query
select octet_length(c7), octet_length(c8), octet_length(v), octet_length(s) from char_tbl4
-- !query analysis
Project [octet_length(c7#x) AS octet_length(c7)#x, octet_length(c8#x) AS octet_length(c8)#x, octet_length(v#x) AS octet_length(v)#x, octet_length(s#x) AS octet_length(s)#x]
+- SubqueryAlias spark_catalog.default.char_tbl4
   +- Project [static_invoke(CharVarcharCodegenUtils.readSidePadding(c7#x, 7)) AS c7#x, static_invoke(CharVarcharCodegenUtils.readSidePadding(c8#x, 8)) AS c8#x, v#x, s#x]
      +- Relation spark_catalog.default.char_tbl4[c7#x,c8#x,v#x,s#x] parquet


-- !query
select concat_ws('|', c7, c8), concat_ws('|', c7, v), concat_ws('|', c7, s), concat_ws('|', v, s) from char_tbl4
-- !query analysis
Project [concat_ws(|, c7#x, c8#x) AS concat_ws(|, c7, c8)#x, concat_ws(|, c7#x, v#x) AS concat_ws(|, c7, v)#x, concat_ws(|, c7#x, s#x) AS concat_ws(|, c7, s)#x, concat_ws(|, v#x, s#x) AS concat_ws(|, v, s)#x]
+- SubqueryAlias spark_catalog.default.char_tbl4
   +- Project [static_invoke(CharVarcharCodegenUtils.readSidePadding(c7#x, 7)) AS c7#x, static_invoke(CharVarcharCodegenUtils.readSidePadding(c8#x, 8)) AS c8#x, v#x, s#x]
      +- Relation spark_catalog.default.char_tbl4[c7#x,c8#x,v#x,s#x] parquet


-- !query
select concat(c7, c8), concat(c7, v), concat(c7, s), concat(v, s) from char_tbl4
-- !query analysis
Project [concat(c7#x, c8#x) AS concat(c7, c8)#x, concat(c7#x, v#x) AS concat(c7, v)#x, concat(c7#x, s#x) AS concat(c7, s)#x, concat(v#x, s#x) AS concat(v, s)#x]
+- SubqueryAlias spark_catalog.default.char_tbl4
   +- Project [static_invoke(CharVarcharCodegenUtils.readSidePadding(c7#x, 7)) AS c7#x, static_invoke(CharVarcharCodegenUtils.readSidePadding(c8#x, 8)) AS c8#x, v#x, s#x]
      +- Relation spark_catalog.default.char_tbl4[c7#x,c8#x,v#x,s#x] parquet


-- !query
select like(c7, 'Ne     _'), like(c8, 'Ne     _') from char_tbl4
-- !query analysis
Project [c7#x LIKE Ne     _ AS c7 LIKE Ne     _#x, c8#x LIKE Ne     _ AS c8 LIKE Ne     _#x]
+- SubqueryAlias spark_catalog.default.char_tbl4
   +- Project [static_invoke(CharVarcharCodegenUtils.readSidePadding(c7#x, 7)) AS c7#x, static_invoke(CharVarcharCodegenUtils.readSidePadding(c8#x, 8)) AS c8#x, v#x, s#x]
      +- Relation spark_catalog.default.char_tbl4[c7#x,c8#x,v#x,s#x] parquet


-- !query
select like(v, 'Spark_') from char_tbl4
-- !query analysis
Project [v#x LIKE Spark_ AS v LIKE Spark_#x]
+- SubqueryAlias spark_catalog.default.char_tbl4
   +- Project [static_invoke(CharVarcharCodegenUtils.readSidePadding(c7#x, 7)) AS c7#x, static_invoke(CharVarcharCodegenUtils.readSidePadding(c8#x, 8)) AS c8#x, v#x, s#x]
      +- Relation spark_catalog.default.char_tbl4[c7#x,c8#x,v#x,s#x] parquet


-- !query
select c7 = c8, upper(c7) = upper(c8), lower(c7) = lower(c8) from char_tbl4 where s = 'NetEase'
-- !query analysis
Project [(rpad(c7#x, 8,  ) = c8#x) AS (c7 = c8)#x, (upper(c7#x) = upper(c8#x)) AS (upper(c7) = upper(c8))#x, (lower(c7#x) = lower(c8#x)) AS (lower(c7) = lower(c8))#x]
+- Filter (s#x = NetEase)
   +- SubqueryAlias spark_catalog.default.char_tbl4
      +- Project [static_invoke(CharVarcharCodegenUtils.readSidePadding(c7#x, 7)) AS c7#x, static_invoke(CharVarcharCodegenUtils.readSidePadding(c8#x, 8)) AS c8#x, v#x, s#x]
         +- Relation spark_catalog.default.char_tbl4[c7#x,c8#x,v#x,s#x] parquet


-- !query
select c7 = s, upper(c7) = upper(s), lower(c7) = lower(s) from char_tbl4 where s = 'NetEase'
-- !query analysis
Project [(c7#x = s#x) AS (c7 = s)#x, (upper(c7#x) = upper(s#x)) AS (upper(c7) = upper(s))#x, (lower(c7#x) = lower(s#x)) AS (lower(c7) = lower(s))#x]
+- Filter (s#x = NetEase)
   +- SubqueryAlias spark_catalog.default.char_tbl4
      +- Project [static_invoke(CharVarcharCodegenUtils.readSidePadding(c7#x, 7)) AS c7#x, static_invoke(CharVarcharCodegenUtils.readSidePadding(c8#x, 8)) AS c8#x, v#x, s#x]
         +- Relation spark_catalog.default.char_tbl4[c7#x,c8#x,v#x,s#x] parquet


-- !query
select c7 = 'NetEase', upper(c7) = upper('NetEase'), lower(c7) = lower('NetEase') from char_tbl4 where s = 'NetEase'
-- !query analysis
Project [(c7#x = NetEase) AS (c7 = NetEase)#x, (upper(c7#x) = upper(NetEase)) AS (upper(c7) = upper(NetEase))#x, (lower(c7#x) = lower(NetEase)) AS (lower(c7) = lower(NetEase))#x]
+- Filter (s#x = NetEase)
   +- SubqueryAlias spark_catalog.default.char_tbl4
      +- Project [static_invoke(CharVarcharCodegenUtils.readSidePadding(c7#x, 7)) AS c7#x, static_invoke(CharVarcharCodegenUtils.readSidePadding(c8#x, 8)) AS c8#x, v#x, s#x]
         +- Relation spark_catalog.default.char_tbl4[c7#x,c8#x,v#x,s#x] parquet


-- !query
select printf('Hey, %s%s%s%s', c7, c8, v, s) from char_tbl4
-- !query analysis
Project [printf(Hey, %s%s%s%s, c7#x, c8#x, v#x, s#x) AS printf(Hey, %s%s%s%s, c7, c8, v, s)#x]
+- SubqueryAlias spark_catalog.default.char_tbl4
   +- Project [static_invoke(CharVarcharCodegenUtils.readSidePadding(c7#x, 7)) AS c7#x, static_invoke(CharVarcharCodegenUtils.readSidePadding(c8#x, 8)) AS c8#x, v#x, s#x]
      +- Relation spark_catalog.default.char_tbl4[c7#x,c8#x,v#x,s#x] parquet


-- !query
select repeat(c7, 2), repeat(c8, 2), repeat(v, 2), repeat(s, 2) from char_tbl4
-- !query analysis
Project [repeat(c7#x, 2) AS repeat(c7, 2)#x, repeat(c8#x, 2) AS repeat(c8, 2)#x, repeat(v#x, 2) AS repeat(v, 2)#x, repeat(s#x, 2) AS repeat(s, 2)#x]
+- SubqueryAlias spark_catalog.default.char_tbl4
   +- Project [static_invoke(CharVarcharCodegenUtils.readSidePadding(c7#x, 7)) AS c7#x, static_invoke(CharVarcharCodegenUtils.readSidePadding(c8#x, 8)) AS c8#x, v#x, s#x]
      +- Relation spark_catalog.default.char_tbl4[c7#x,c8#x,v#x,s#x] parquet


-- !query
select replace(c7, 'Net', 'Apache'), replace(c8, 'Net', 'Apache'), replace(v, 'Spark', 'Kyuubi'), replace(s, 'Net', 'Apache') from char_tbl4
-- !query analysis
Project [replace(c7#x, Net, Apache) AS replace(c7, Net, Apache)#x, replace(c8#x, Net, Apache) AS replace(c8, Net, Apache)#x, replace(v#x, Spark, Kyuubi) AS replace(v, Spark, Kyuubi)#x, replace(s#x, Net, Apache) AS replace(s, Net, Apache)#x]
+- SubqueryAlias spark_catalog.default.char_tbl4
   +- Project [static_invoke(CharVarcharCodegenUtils.readSidePadding(c7#x, 7)) AS c7#x, static_invoke(CharVarcharCodegenUtils.readSidePadding(c8#x, 8)) AS c8#x, v#x, s#x]
      +- Relation spark_catalog.default.char_tbl4[c7#x,c8#x,v#x,s#x] parquet


-- !query
select rpad(c7, 10), rpad(c8, 5), rpad(v, 5), rpad(s, 5)  from char_tbl4
-- !query analysis
Project [rpad(c7#x, 10,  ) AS rpad(c7, 10,  )#x, rpad(c8#x, 5,  ) AS rpad(c8, 5,  )#x, rpad(v#x, 5,  ) AS rpad(v, 5,  )#x, rpad(s#x, 5,  ) AS rpad(s, 5,  )#x]
+- SubqueryAlias spark_catalog.default.char_tbl4
   +- Project [static_invoke(CharVarcharCodegenUtils.readSidePadding(c7#x, 7)) AS c7#x, static_invoke(CharVarcharCodegenUtils.readSidePadding(c8#x, 8)) AS c8#x, v#x, s#x]
      +- Relation spark_catalog.default.char_tbl4[c7#x,c8#x,v#x,s#x] parquet


-- !query
select rtrim(c7), rtrim(c8), rtrim(v), rtrim(s) from char_tbl4
-- !query analysis
Project [rtrim(c7#x, None) AS rtrim(c7)#x, rtrim(c8#x, None) AS rtrim(c8)#x, rtrim(v#x, None) AS rtrim(v)#x, rtrim(s#x, None) AS rtrim(s)#x]
+- SubqueryAlias spark_catalog.default.char_tbl4
   +- Project [static_invoke(CharVarcharCodegenUtils.readSidePadding(c7#x, 7)) AS c7#x, static_invoke(CharVarcharCodegenUtils.readSidePadding(c8#x, 8)) AS c8#x, v#x, s#x]
      +- Relation spark_catalog.default.char_tbl4[c7#x,c8#x,v#x,s#x] parquet


-- !query
select split(c7, 'e'), split(c8, 'e'), split(v, 'a'), split(s, 'e') from char_tbl4
-- !query analysis
Project [split(c7#x, e, -1) AS split(c7, e, -1)#x, split(c8#x, e, -1) AS split(c8, e, -1)#x, split(v#x, a, -1) AS split(v, a, -1)#x, split(s#x, e, -1) AS split(s, e, -1)#x]
+- SubqueryAlias spark_catalog.default.char_tbl4
   +- Project [static_invoke(CharVarcharCodegenUtils.readSidePadding(c7#x, 7)) AS c7#x, static_invoke(CharVarcharCodegenUtils.readSidePadding(c8#x, 8)) AS c8#x, v#x, s#x]
      +- Relation spark_catalog.default.char_tbl4[c7#x,c8#x,v#x,s#x] parquet


-- !query
select substring(c7, 2), substring(c8, 2), substring(v, 3), substring(s, 2) from char_tbl4
-- !query analysis
Project [substring(c7#x, 2, 2147483647) AS substring(c7, 2, 2147483647)#x, substring(c8#x, 2, 2147483647) AS substring(c8, 2, 2147483647)#x, substring(v#x, 3, 2147483647) AS substring(v, 3, 2147483647)#x, substring(s#x, 2, 2147483647) AS substring(s, 2, 2147483647)#x]
+- SubqueryAlias spark_catalog.default.char_tbl4
   +- Project [static_invoke(CharVarcharCodegenUtils.readSidePadding(c7#x, 7)) AS c7#x, static_invoke(CharVarcharCodegenUtils.readSidePadding(c8#x, 8)) AS c8#x, v#x, s#x]
      +- Relation spark_catalog.default.char_tbl4[c7#x,c8#x,v#x,s#x] parquet


-- !query
select left(c7, 2), left(c8, 2), left(v, 3), left(s, 2) from char_tbl4
-- !query analysis
Project [left(c7#x, 2) AS left(c7, 2)#x, left(c8#x, 2) AS left(c8, 2)#x, left(v#x, 3) AS left(v, 3)#x, left(s#x, 2) AS left(s, 2)#x]
+- SubqueryAlias spark_catalog.default.char_tbl4
   +- Project [static_invoke(CharVarcharCodegenUtils.readSidePadding(c7#x, 7)) AS c7#x, static_invoke(CharVarcharCodegenUtils.readSidePadding(c8#x, 8)) AS c8#x, v#x, s#x]
      +- Relation spark_catalog.default.char_tbl4[c7#x,c8#x,v#x,s#x] parquet


-- !query
select right(c7, 2), right(c8, 2), right(v, 3), right(s, 2) from char_tbl4
-- !query analysis
Project [right(c7#x, 2) AS right(c7, 2)#x, right(c8#x, 2) AS right(c8, 2)#x, right(v#x, 3) AS right(v, 3)#x, right(s#x, 2) AS right(s, 2)#x]
+- SubqueryAlias spark_catalog.default.char_tbl4
   +- Project [static_invoke(CharVarcharCodegenUtils.readSidePadding(c7#x, 7)) AS c7#x, static_invoke(CharVarcharCodegenUtils.readSidePadding(c8#x, 8)) AS c8#x, v#x, s#x]
      +- Relation spark_catalog.default.char_tbl4[c7#x,c8#x,v#x,s#x] parquet


-- !query
select typeof(c7), typeof(c8), typeof(v), typeof(s) from char_tbl4 limit 1
-- !query analysis
GlobalLimit 1
+- LocalLimit 1
   +- Project [typeof(c7#x) AS typeof(c7)#x, typeof(c8#x) AS typeof(c8)#x, typeof(v#x) AS typeof(v)#x, typeof(s#x) AS typeof(s)#x]
      +- SubqueryAlias spark_catalog.default.char_tbl4
         +- Project [static_invoke(CharVarcharCodegenUtils.readSidePadding(c7#x, 7)) AS c7#x, static_invoke(CharVarcharCodegenUtils.readSidePadding(c8#x, 8)) AS c8#x, v#x, s#x]
            +- Relation spark_catalog.default.char_tbl4[c7#x,c8#x,v#x,s#x] parquet


-- !query
select cast(c7 as char(1)), cast(c8 as char(10)), cast(v as char(1)), cast(v as varchar(1)), cast(s as char(5)) from char_tbl4
-- !query analysis
Project [cast(c7#x as string) AS c7#x, cast(c8#x as string) AS c8#x, cast(v#x as string) AS v#x, cast(v#x as string) AS v#x, cast(s#x as string) AS s#x]
+- SubqueryAlias spark_catalog.default.char_tbl4
   +- Project [static_invoke(CharVarcharCodegenUtils.readSidePadding(c7#x, 7)) AS c7#x, static_invoke(CharVarcharCodegenUtils.readSidePadding(c8#x, 8)) AS c8#x, v#x, s#x]
      +- Relation spark_catalog.default.char_tbl4[c7#x,c8#x,v#x,s#x] parquet


-- !query
drop table char_tbl1
-- !query analysis
DropTable false, false
+- ResolvedIdentifier V2SessionCatalog(spark_catalog), default.char_tbl1


-- !query
drop table char_tbl2
-- !query analysis
DropTable false, false
+- ResolvedIdentifier V2SessionCatalog(spark_catalog), default.char_tbl2


-- !query
drop table char_tbl3
-- !query analysis
DropTable false, false
+- ResolvedIdentifier V2SessionCatalog(spark_catalog), default.char_tbl3


-- !query
drop table char_tbl4
-- !query analysis
DropTable false, false
+- ResolvedIdentifier V2SessionCatalog(spark_catalog), default.char_tbl4


-- !query
select ascii('§'), ascii('÷'), ascii('×10')
-- !query analysis
Project [ascii(§) AS ascii(§)#x, ascii(÷) AS ascii(÷)#x, ascii(×10) AS ascii(×10)#x]
+- OneRowRelation


-- !query
select chr(167), chr(247), chr(215)
-- !query analysis
Project [chr(cast(167 as bigint)) AS chr(167)#x, chr(cast(247 as bigint)) AS chr(247)#x, chr(cast(215 as bigint)) AS chr(215)#x]
+- OneRowRelation


-- !query
SELECT to_varchar(78.12, '$99.99')
-- !query analysis
Project [to_char(78.12, $99.99) AS to_char(78.12, $99.99)#x]
+- OneRowRelation


-- !query
SELECT to_varchar(111.11, '99.9')
-- !query analysis
Project [to_char(111.11, 99.9) AS to_char(111.11, 99.9)#x]
+- OneRowRelation


-- !query
SELECT to_varchar(12454.8, '99,999.9S')
-- !query analysis
Project [to_char(12454.8, 99,999.9S) AS to_char(12454.8, 99,999.9S)#x]
+- OneRowRelation
