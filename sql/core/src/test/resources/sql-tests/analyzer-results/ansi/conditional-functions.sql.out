-- Automatically generated by SQLQueryTestSuite
-- !query
CREATE TABLE conditional_t USING PARQUET AS SELECT c1, c2 FROM VALUES(1d, 0),(2d, 1),(null, 1),(CAST('NaN' AS DOUBLE), 0) AS t(c1, c2)
-- !query analysis
CreateDataSourceTableAsSelectCommand `spark_catalog`.`default`.`conditional_t`, ErrorIfExists, [c1, c2]
   +- Project [c1#x, c2#x]
      +- SubqueryAlias t
         +- LocalRelation [c1#x, c2#x]


-- !query
SELECT nanvl(c2, c1/c2 + c1/c2) FROM conditional_t
-- !query analysis
Project [nanvl(cast(c2#x as double), ((c1#x / cast(c2#x as double)) + (c1#x / cast(c2#x as double)))) AS nanvl(c2, ((c1 / c2) + (c1 / c2)))#x]
+- SubqueryAlias spark_catalog.default.conditional_t
   +- Relation spark_catalog.default.conditional_t[c1#x,c2#x] parquet


-- !query
SELECT nanvl(c2, 1/0) FROM conditional_t
-- !query analysis
Project [nanvl(cast(c2#x as double), (cast(1 as double) / cast(0 as double))) AS nanvl(c2, (1 / 0))#x]
+- SubqueryAlias spark_catalog.default.conditional_t
   +- Relation spark_catalog.default.conditional_t[c1#x,c2#x] parquet


-- !query
SELECT nanvl(1-0, 1/0) FROM conditional_t
-- !query analysis
Project [nanvl(cast((1 - 0) as double), (cast(1 as double) / cast(0 as double))) AS nanvl((1 - 0), (1 / 0))#x]
+- SubqueryAlias spark_catalog.default.conditional_t
   +- Relation spark_catalog.default.conditional_t[c1#x,c2#x] parquet


-- !query
SELECT if(c2 >= 0, 1-0, 1/0) from conditional_t
-- !query analysis
Project [if ((c2#x >= 0)) cast((1 - 0) as double) else (cast(1 as double) / cast(0 as double)) AS (IF((c2 >= 0), (1 - 0), (1 / 0)))#x]
+- SubqueryAlias spark_catalog.default.conditional_t
   +- Relation spark_catalog.default.conditional_t[c1#x,c2#x] parquet


-- !query
SELECT if(1 == 1, 1, 1/0)
-- !query analysis
Project [if ((1 = 1)) cast(1 as double) else (cast(1 as double) / cast(0 as double)) AS (IF((1 = 1), 1, (1 / 0)))#x]
+- OneRowRelation


-- !query
SELECT if(1 != 1, 1/0, 1)
-- !query analysis
Project [if (NOT (1 = 1)) (cast(1 as double) / cast(0 as double)) else cast(1 as double) AS (IF((NOT (1 = 1)), (1 / 0), 1))#x]
+- OneRowRelation


-- !query
SELECT coalesce(c2, 1/0) from conditional_t
-- !query analysis
Project [coalesce(cast(c2#x as double), (cast(1 as double) / cast(0 as double))) AS coalesce(c2, (1 / 0))#x]
+- SubqueryAlias spark_catalog.default.conditional_t
   +- Relation spark_catalog.default.conditional_t[c1#x,c2#x] parquet


-- !query
SELECT coalesce(1, 1/0)
-- !query analysis
Project [coalesce(cast(1 as double), (cast(1 as double) / cast(0 as double))) AS coalesce(1, (1 / 0))#x]
+- OneRowRelation


-- !query
SELECT coalesce(null, 1, 1/0)
-- !query analysis
Project [coalesce(cast(null as double), cast(1 as double), (cast(1 as double) / cast(0 as double))) AS coalesce(NULL, 1, (1 / 0))#x]
+- OneRowRelation


-- !query
SELECT case when c2 >= 0 then 1 else 1/0 end from conditional_t
-- !query analysis
Project [CASE WHEN (c2#x >= 0) THEN cast(1 as double) ELSE (cast(1 as double) / cast(0 as double)) END AS CASE WHEN (c2 >= 0) THEN 1 ELSE (1 / 0) END#x]
+- SubqueryAlias spark_catalog.default.conditional_t
   +- Relation spark_catalog.default.conditional_t[c1#x,c2#x] parquet


-- !query
SELECT case when 1 < 2 then 1 else 1/0 end
-- !query analysis
Project [CASE WHEN (1 < 2) THEN cast(1 as double) ELSE (cast(1 as double) / cast(0 as double)) END AS CASE WHEN (1 < 2) THEN 1 ELSE (1 / 0) END#x]
+- OneRowRelation


-- !query
SELECT case when 1 > 2 then 1/0 else 1 end
-- !query analysis
Project [CASE WHEN (1 > 2) THEN (cast(1 as double) / cast(0 as double)) ELSE cast(1 as double) END AS CASE WHEN (1 > 2) THEN (1 / 0) ELSE 1 END#x]
+- OneRowRelation


-- !query
SELECT NULLIF(1, 1)
-- !query analysis
Project [nullif(1, 1) AS nullif(1, 1)#x]
+- OneRowRelation


-- !query
SELECT NULLIF(1, 2)
-- !query analysis
Project [nullif(1, 2) AS nullif(1, 2)#x]
+- OneRowRelation


-- !query
SELECT NULLIF(NULL, 1)
-- !query analysis
Project [nullif(null, 1) AS nullif(NULL, 1)#x]
+- OneRowRelation


-- !query
SELECT NULLIF(1, NULL)
-- !query analysis
Project [nullif(1, null) AS nullif(1, NULL)#x]
+- OneRowRelation


-- !query
SELECT NULLIF(NULL, NULL)
-- !query analysis
Project [nullif(null, null) AS nullif(NULL, NULL)#x]
+- OneRowRelation


-- !query
SELECT NULLIF('abc', 'abc')
-- !query analysis
Project [nullif(abc, abc) AS nullif(abc, abc)#x]
+- OneRowRelation


-- !query
SELECT NULLIF('abc', 'xyz')
-- !query analysis
Project [nullif(abc, xyz) AS nullif(abc, xyz)#x]
+- OneRowRelation


-- !query
SELECT NULLIF(id, 1) FROM range(10) GROUP BY NULLIF(id, 1)
-- !query analysis
Aggregate [nullif(id#xL, 1)], [nullif(id#xL, 1) AS nullif(id, 1)#xL]
+- Range (0, 10, step=1)


-- !query
SELECT NULLIF(id, 1), COUNT(*) FROM range(10) GROUP BY NULLIF(id, 2)
-- !query analysis
org.apache.spark.sql.catalyst.ExtendedAnalysisException
{
  "errorClass" : "MISSING_AGGREGATION",
  "sqlState" : "42803",
  "messageParameters" : {
    "expression" : "\"id\"",
    "expressionAnyValue" : "\"any_value(id)\""
  }
}


-- !query
SELECT NULLIF(id, 1), COUNT(*) FROM range(10) GROUP BY NULLIF(id, 1) HAVING COUNT(*) > 1
-- !query analysis
Filter (count(1)#xL > cast(1 as bigint))
+- Aggregate [nullif(id#xL, 1)], [nullif(id#xL, 1) AS nullif(id, 1)#xL, count(1) AS count(1)#xL]
   +- Range (0, 10, step=1)


-- !query
SELECT nullifzero(0),
       nullifzero(cast(0 as tinyint)),
       nullifzero(cast(0 as bigint)),
       nullifzero('0'),
       nullifzero(0.0),
       nullifzero(1),
       nullifzero(null)
-- !query analysis
Project [nullifzero(0) AS nullifzero(0)#x, nullifzero(cast(0 as tinyint)) AS nullifzero(CAST(0 AS TINYINT))#x, nullifzero(cast(0 as bigint)) AS nullifzero(CAST(0 AS BIGINT))#xL, nullifzero(0) AS nullifzero(0)#x, nullifzero(0.0) AS nullifzero(0.0)#x, nullifzero(1) AS nullifzero(1)#x, nullifzero(null) AS nullifzero(NULL)#x]
+- OneRowRelation


-- !query
SELECT nullifzero('abc')
-- !query analysis
Project [nullifzero(abc) AS nullifzero(abc)#x]
+- OneRowRelation


-- !query
SELECT zeroifnull(null),
       zeroifnull(1),
       zeroifnull(cast(1 as tinyint)),
       zeroifnull(cast(1 as bigint))
-- !query analysis
Project [zeroifnull(null) AS zeroifnull(NULL)#x, zeroifnull(1) AS zeroifnull(1)#x, zeroifnull(cast(1 as tinyint)) AS zeroifnull(CAST(1 AS TINYINT))#x, zeroifnull(cast(1 as bigint)) AS zeroifnull(CAST(1 AS BIGINT))#xL]
+- OneRowRelation


-- !query
SELECT zeroifnull('abc')
-- !query analysis
Project [zeroifnull(abc) AS zeroifnull(abc)#xL]
+- OneRowRelation


-- !query
DROP TABLE conditional_t
-- !query analysis
DropTable false, false
+- ResolvedIdentifier V2SessionCatalog(spark_catalog), default.conditional_t
