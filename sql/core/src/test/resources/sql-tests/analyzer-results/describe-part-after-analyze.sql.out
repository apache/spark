-- Automatically generated by SQLQueryTestSuite
-- !query
CREATE TABLE t (key STRING, value STRING, ds STRING, hr INT) USING parquet
    PARTITIONED BY (ds, hr)
-- !query analysis
CreateDataSourceTableCommand `spark_catalog`.`default`.`t`, false


-- !query
INSERT INTO TABLE t PARTITION (ds='2017-08-01', hr=10)
VALUES ('k1', 100), ('k2', 200), ('k3', 300)
-- !query analysis
InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/t, [ds=2017-08-01, hr=10], false, [ds#x, hr#x], Parquet, [path=file:[not included in comparison]/{warehouse_dir}/t], Append, `spark_catalog`.`default`.`t`, org.apache.spark.sql.execution.datasources.CatalogFileIndex(file:[not included in comparison]/{warehouse_dir}/t), [key, value, ds, hr]
+- Project [key#x, value#x, cast(2017-08-01 as string) AS ds#x, cast(10 as int) AS hr#x]
   +- Project [cast(col1#x as string) AS key#x, cast(col2#x as string) AS value#x]
      +- LocalRelation [col1#x, col2#x]


-- !query
INSERT INTO TABLE t PARTITION (ds='2017-08-01', hr=11)
VALUES ('k1', 101), ('k2', 201), ('k3', 301), ('k4', 401)
-- !query analysis
InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/t, [ds=2017-08-01, hr=11], false, [ds#x, hr#x], Parquet, [path=file:[not included in comparison]/{warehouse_dir}/t], Append, `spark_catalog`.`default`.`t`, org.apache.spark.sql.execution.datasources.CatalogFileIndex(file:[not included in comparison]/{warehouse_dir}/t), [key, value, ds, hr]
+- Project [key#x, value#x, cast(2017-08-01 as string) AS ds#x, cast(11 as int) AS hr#x]
   +- Project [cast(col1#x as string) AS key#x, cast(col2#x as string) AS value#x]
      +- LocalRelation [col1#x, col2#x]


-- !query
INSERT INTO TABLE t PARTITION (ds='2017-09-01', hr=5)
VALUES ('k1', 102), ('k2', 202)
-- !query analysis
InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/t, [ds=2017-09-01, hr=5], false, [ds#x, hr#x], Parquet, [path=file:[not included in comparison]/{warehouse_dir}/t], Append, `spark_catalog`.`default`.`t`, org.apache.spark.sql.execution.datasources.CatalogFileIndex(file:[not included in comparison]/{warehouse_dir}/t), [key, value, ds, hr]
+- Project [key#x, value#x, cast(2017-09-01 as string) AS ds#x, cast(5 as int) AS hr#x]
   +- Project [cast(col1#x as string) AS key#x, cast(col2#x as string) AS value#x]
      +- LocalRelation [col1#x, col2#x]


-- !query
DESC EXTENDED t PARTITION (ds='2017-08-01', hr=10)
-- !query analysis
DescribeTableCommand `spark_catalog`.`default`.`t`, [ds=2017-08-01, hr=10], true, [col_name#x, data_type#x, comment#x]


-- !query
ANALYZE TABLE t PARTITION (ds='2017-08-01', hr=10) COMPUTE STATISTICS
-- !query analysis
AnalyzePartitionCommand `spark_catalog`.`default`.`t`, [ds=Some(2017-08-01), hr=Some(10)], false


-- !query
DESC EXTENDED t PARTITION (ds='2017-08-01', hr=10)
-- !query analysis
DescribeTableCommand `spark_catalog`.`default`.`t`, [ds=2017-08-01, hr=10], true, [col_name#x, data_type#x, comment#x]


-- !query
ANALYZE TABLE t PARTITION (ds='2017-08-01') COMPUTE STATISTICS
-- !query analysis
AnalyzePartitionCommand `spark_catalog`.`default`.`t`, [ds=Some(2017-08-01)], false


-- !query
DESC EXTENDED t PARTITION (ds='2017-08-01', hr=10)
-- !query analysis
DescribeTableCommand `spark_catalog`.`default`.`t`, [ds=2017-08-01, hr=10], true, [col_name#x, data_type#x, comment#x]


-- !query
DESC EXTENDED t PARTITION (ds='2017-08-01', hr=11)
-- !query analysis
DescribeTableCommand `spark_catalog`.`default`.`t`, [ds=2017-08-01, hr=11], true, [col_name#x, data_type#x, comment#x]


-- !query
ANALYZE TABLE t PARTITION (ds, hr) COMPUTE STATISTICS
-- !query analysis
AnalyzePartitionCommand `spark_catalog`.`default`.`t`, [ds=None, hr=None], false


-- !query
DESC EXTENDED t PARTITION (ds='2017-08-01', hr=10)
-- !query analysis
DescribeTableCommand `spark_catalog`.`default`.`t`, [ds=2017-08-01, hr=10], true, [col_name#x, data_type#x, comment#x]


-- !query
DESC EXTENDED t PARTITION (ds='2017-08-01', hr=11)
-- !query analysis
DescribeTableCommand `spark_catalog`.`default`.`t`, [ds=2017-08-01, hr=11], true, [col_name#x, data_type#x, comment#x]


-- !query
DESC EXTENDED t PARTITION (ds='2017-09-01', hr=5)
-- !query analysis
DescribeTableCommand `spark_catalog`.`default`.`t`, [ds=2017-09-01, hr=5], true, [col_name#x, data_type#x, comment#x]


-- !query
DROP TABLE t
-- !query analysis
DropTable false, false
+- ResolvedIdentifier V2SessionCatalog(spark_catalog), default.t
