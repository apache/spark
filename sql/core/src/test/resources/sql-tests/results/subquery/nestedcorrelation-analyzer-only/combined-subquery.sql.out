-- Automatically generated by SQLQueryTestSuite
-- !query
set spark.sql.optimizer.supportNestedCorrelatedSubqueries.enabled=true
-- !query analysis
SetCommand (spark.sql.optimizer.supportNestedCorrelatedSubqueries.enabled,Some(true))


-- !query
set spark.sql.optimizer.supportNestedCorrelatedSubqueriesForScalarSubqueries.enabled=true
-- !query analysis
SetCommand (spark.sql.optimizer.supportNestedCorrelatedSubqueriesForScalarSubqueries.enabled,Some(true))


-- !query
set spark.sql.optimizer.supportNestedCorrelatedSubqueriesForINSubqueries.enabled=true
-- !query analysis
SetCommand (spark.sql.optimizer.supportNestedCorrelatedSubqueriesForINSubqueries.enabled,Some(true))


-- !query
set spark.sql.optimizer.supportNestedCorrelatedSubqueriesForEXISTSSubqueries.enabled=true
-- !query analysis
SetCommand (spark.sql.optimizer.supportNestedCorrelatedSubqueriesForEXISTSSubqueries.enabled,Some(true))


-- !query
DROP TABLE IF EXISTS tbl
-- !query analysis
DropTable true, false
+- ResolvedIdentifier V2SessionCatalog(spark_catalog), default.tbl


-- !query
CREATE TABLE tbl(a TINYINT, b SMALLINT, c INTEGER, d BIGINT, e VARCHAR(1), f DATE, g TIMESTAMP)
-- !query analysis
CreateDataSourceTableCommand `spark_catalog`.`default`.`tbl`, false


-- !query
INSERT INTO tbl VALUES (1, 2, 3, 4, '5', DATE '1992-01-01', TIMESTAMP '1992-01-01 00:00:00')
-- !query analysis
InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/tbl, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/tbl], Append, `spark_catalog`.`default`.`tbl`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/tbl), [a, b, c, d, e, f, g]
+- Project [cast(col1#x as tinyint) AS a#x, cast(col2#x as smallint) AS b#x, cast(col3#x as int) AS c#x, cast(col4#x as bigint) AS d#xL, static_invoke(CharVarcharCodegenUtils.varcharTypeWriteSideCheck(cast(col5#x as string), 1)) AS e#x, cast(col6#x as date) AS f#x, cast(col7#x as timestamp) AS g#x]
   +- LocalRelation [col1#x, col2#x, col3#x, col4#x, col5#x, col6#x, col7#x]


-- !query
SELECT t1.c+(SELECT t1.b FROM tbl t2 WHERE EXISTS(SELECT t1.b+t2.a)) FROM tbl t1
-- !query analysis
Project [(c#x + cast(scalar-subquery#x [b#x] as int)) AS (c + scalarsubquery(b))#x]
:  +- Project [outer(b#x)]
:     +- Filter exists#x [b#x && a#x]
:        :  +- Project [(outer(b#x) + cast(outer(a#x) as smallint)) AS (outer(t1.b) + outer(t2.a))#x]
:        :     +- OneRowRelation
:        +- SubqueryAlias t2
:           +- SubqueryAlias spark_catalog.default.tbl
:              +- Relation spark_catalog.default.tbl[a#x,b#x,c#x,d#xL,e#x,f#x,g#x] parquet
+- SubqueryAlias t1
   +- SubqueryAlias spark_catalog.default.tbl
      +- Relation spark_catalog.default.tbl[a#x,b#x,c#x,d#xL,e#x,f#x,g#x] parquet


-- !query
SELECT 1 FROM tbl t1 JOIN tbl t2 ON (t1.d=t2.d) WHERE EXISTS(SELECT t1.c FROM tbl t3 WHERE t1.d+t3.c<100 AND EXISTS(SELECT 1 FROM tbl t4 WHERE t2.f < DATE '2000-01-01'))
-- !query analysis
[Analyzer test output redacted due to nondeterminism]
