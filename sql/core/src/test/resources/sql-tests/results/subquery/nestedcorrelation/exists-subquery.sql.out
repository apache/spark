-- Automatically generated by SQLQueryTestSuite
-- !query
set spark.sql.optimizer.supportNestedCorrelatedSubqueries.enabled=true
-- !query analysis
SetCommand (spark.sql.optimizer.supportNestedCorrelatedSubqueries.enabled,Some(true))


-- !query
set spark.sql.optimizer.supportNestedCorrelatedSubqueriesForScalarSubqueries.enabled=true
-- !query analysis
SetCommand (spark.sql.optimizer.supportNestedCorrelatedSubqueriesForScalarSubqueries.enabled,Some(true))


-- !query
set spark.sql.optimizer.supportNestedCorrelatedSubqueriesForINSubqueries.enabled=true
-- !query analysis
SetCommand (spark.sql.optimizer.supportNestedCorrelatedSubqueriesForINSubqueries.enabled,Some(true))


-- !query
set spark.sql.optimizer.supportNestedCorrelatedSubqueriesForEXISTSSubqueries.enabled=true
-- !query analysis
SetCommand (spark.sql.optimizer.supportNestedCorrelatedSubqueriesForEXISTSSubqueries.enabled,Some(true))


-- !query
DROP TABLE IF EXISTS table_integers
-- !query analysis
DropTable true, false
+- ResolvedIdentifier V2SessionCatalog(spark_catalog), default.table_integers


-- !query
CREATE TABLE table_integers(i INTEGER)
-- !query analysis
CreateDataSourceTableCommand `spark_catalog`.`default`.`table_integers`, false


-- !query
INSERT INTO table_integers VALUES (1), (2), (3), (NULL)
-- !query analysis
InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/table_integers, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/table_integers], Append, `spark_catalog`.`default`.`table_integers`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/table_integers), [i]
+- Project [cast(col1#x as int) AS i#x]
   +- LocalRelation [col1#x]


-- !query
SELECT
  i,
  (
    SELECT SUM(ss1.i)
    FROM (
      SELECT s1.i
      FROM table_integers s1
      WHERE EXISTS (
        SELECT 1
        FROM table_integers t2
        WHERE s1.i > t2.i
      )
    ) ss1
  ) AS j
FROM table_integers i1
ORDER BY i
-- !query analysis
Sort [i#x ASC NULLS FIRST], true
+- Project [i#x, scalar-subquery#x [] AS j#xL]
   :  +- Aggregate [sum(i#x) AS sum(i)#xL]
   :     +- SubqueryAlias ss1
   :        +- Project [i#x]
   :           +- Filter exists#x [i#x]
   :              :  +- Project [1 AS 1#x]
   :              :     +- Filter (outer(i#x) > i#x)
   :              :        +- SubqueryAlias t2
   :              :           +- SubqueryAlias spark_catalog.default.table_integers
   :              :              +- Relation spark_catalog.default.table_integers[i#x] parquet
   :              +- SubqueryAlias s1
   :                 +- SubqueryAlias spark_catalog.default.table_integers
   :                    +- Relation spark_catalog.default.table_integers[i#x] parquet
   +- SubqueryAlias i1
      +- SubqueryAlias spark_catalog.default.table_integers
         +- Relation spark_catalog.default.table_integers[i#x] parquet


-- !query
SELECT
  i,
  (
    SELECT SUM(ss2.i)
    FROM (
      SELECT s1.i
      FROM table_integers s1
      WHERE s1.i = i1.i
        AND EXISTS (
          SELECT 1
          FROM table_integers t2
          WHERE t2.i = s1.i
        )
    ) ss2
  ) AS j
FROM table_integers i1
ORDER BY i
-- !query analysis
Sort [i#x ASC NULLS FIRST], true
+- Project [i#x, scalar-subquery#x [i#x] AS j#xL]
   :  +- Aggregate [sum(i#x) AS sum(i)#xL]
   :     +- SubqueryAlias ss2
   :        +- Project [i#x]
   :           +- Filter ((i#x = outer(i#x)) AND exists#x [i#x])
   :              :  +- Project [1 AS 1#x]
   :              :     +- Filter (i#x = outer(i#x))
   :              :        +- SubqueryAlias t2
   :              :           +- SubqueryAlias spark_catalog.default.table_integers
   :              :              +- Relation spark_catalog.default.table_integers[i#x] parquet
   :              +- SubqueryAlias s1
   :                 +- SubqueryAlias spark_catalog.default.table_integers
   :                    +- Relation spark_catalog.default.table_integers[i#x] parquet
   +- SubqueryAlias i1
      +- SubqueryAlias spark_catalog.default.table_integers
         +- Relation spark_catalog.default.table_integers[i#x] parquet


-- !query
SELECT
  i,
  (
    SELECT SUM(ss1.i) + SUM(ss2.i)
    FROM (
      SELECT s1.i
      FROM table_integers s1
      WHERE EXISTS (
        SELECT 1
        FROM table_integers t2
        WHERE s1.i > t2.i
      )
    ) ss1
    LEFT OUTER JOIN (
      SELECT s1.i
      FROM table_integers s1
      WHERE EXISTS (
        SELECT 1
        FROM table_integers t2
        WHERE s1.i = t2.i
      )
    ) ss2
      ON ss1.i = ss2.i
  ) AS j
FROM table_integers i1
ORDER BY i
-- !query analysis
Sort [i#x ASC NULLS FIRST], true
+- Project [i#x, scalar-subquery#x [] AS j#xL]
   :  +- Aggregate [(sum(i#x) + sum(i#x)) AS (sum(i) + sum(i))#xL]
   :     +- Join LeftOuter, (i#x = i#x)
   :        :- SubqueryAlias ss1
   :        :  +- Project [i#x]
   :        :     +- Filter exists#x [i#x]
   :        :        :  +- Project [1 AS 1#x]
   :        :        :     +- Filter (outer(i#x) > i#x)
   :        :        :        +- SubqueryAlias t2
   :        :        :           +- SubqueryAlias spark_catalog.default.table_integers
   :        :        :              +- Relation spark_catalog.default.table_integers[i#x] parquet
   :        :        +- SubqueryAlias s1
   :        :           +- SubqueryAlias spark_catalog.default.table_integers
   :        :              +- Relation spark_catalog.default.table_integers[i#x] parquet
   :        +- SubqueryAlias ss2
   :           +- Project [i#x]
   :              +- Filter exists#x [i#x]
   :                 :  +- Project [1 AS 1#x]
   :                 :     +- Filter (outer(i#x) = i#x)
   :                 :        +- SubqueryAlias t2
   :                 :           +- SubqueryAlias spark_catalog.default.table_integers
   :                 :              +- Relation spark_catalog.default.table_integers[i#x] parquet
   :                 +- SubqueryAlias s1
   :                    +- SubqueryAlias spark_catalog.default.table_integers
   :                       +- Relation spark_catalog.default.table_integers[i#x] parquet
   +- SubqueryAlias i1
      +- SubqueryAlias spark_catalog.default.table_integers
         +- Relation spark_catalog.default.table_integers[i#x] parquet


-- !query
SELECT i, (SELECT SUM(ss2.i) FROM (SELECT i FROM table_integers s1 WHERE CASE WHEN (i=i1.i AND EXISTS (SELECT i FROM table_integers WHERE i=s1.i)) THEN true ELSE false END) ss2) AS j FROM table_integers i1 ORDER BY i
-- !query analysis
Sort [i#x ASC NULLS FIRST], true
+- Project [i#x, scalar-subquery#x [i#x] AS j#xL]
   :  +- Aggregate [sum(i#x) AS sum(i)#xL]
   :     +- SubqueryAlias ss2
   :        +- Project [i#x]
   :           +- Filter CASE WHEN ((i#x = outer(i#x)) AND exists#x [i#x]) THEN true ELSE false END
   :              :  +- Project [i#x]
   :              :     +- Filter (i#x = outer(i#x))
   :              :        +- SubqueryAlias spark_catalog.default.table_integers
   :              :           +- Relation spark_catalog.default.table_integers[i#x] parquet
   :              +- SubqueryAlias s1
   :                 +- SubqueryAlias spark_catalog.default.table_integers
   :                    +- Relation spark_catalog.default.table_integers[i#x] parquet
   +- SubqueryAlias i1
      +- SubqueryAlias spark_catalog.default.table_integers
         +- Relation spark_catalog.default.table_integers[i#x] parquet


-- !query
SELECT i, (SELECT SUM(ss2.i) FROM (SELECT i FROM table_integers s1 WHERE i=i1.i AND EXISTS (SELECT i FROM table_integers WHERE i=s1.i)) ss2) AS j FROM table_integers i1 ORDER BY i
-- !query analysis
Sort [i#x ASC NULLS FIRST], true
+- Project [i#x, scalar-subquery#x [i#x] AS j#xL]
   :  +- Aggregate [sum(i#x) AS sum(i)#xL]
   :     +- SubqueryAlias ss2
   :        +- Project [i#x]
   :           +- Filter ((i#x = outer(i#x)) AND exists#x [i#x])
   :              :  +- Project [i#x]
   :              :     +- Filter (i#x = outer(i#x))
   :              :        +- SubqueryAlias spark_catalog.default.table_integers
   :              :           +- Relation spark_catalog.default.table_integers[i#x] parquet
   :              +- SubqueryAlias s1
   :                 +- SubqueryAlias spark_catalog.default.table_integers
   :                    +- Relation spark_catalog.default.table_integers[i#x] parquet
   +- SubqueryAlias i1
      +- SubqueryAlias spark_catalog.default.table_integers
         +- Relation spark_catalog.default.table_integers[i#x] parquet


-- !query
SELECT i, (SELECT SUM(ss2.i) FROM (SELECT i FROM table_integers s1 WHERE (SELECT i FROM table_integers WHERE i=s1.i) = 1) ss2) AS j FROM table_integers i1 ORDER BY i
-- !query analysis
Sort [i#x ASC NULLS FIRST], true
+- Project [i#x, scalar-subquery#x [] AS j#xL]
   :  +- Aggregate [sum(i#x) AS sum(i)#xL]
   :     +- SubqueryAlias ss2
   :        +- Project [i#x]
   :           +- Filter (scalar-subquery#x [i#x] = 1)
   :              :  +- Project [i#x]
   :              :     +- Filter (i#x = outer(i#x))
   :              :        +- SubqueryAlias spark_catalog.default.table_integers
   :              :           +- Relation spark_catalog.default.table_integers[i#x] parquet
   :              +- SubqueryAlias s1
   :                 +- SubqueryAlias spark_catalog.default.table_integers
   :                    +- Relation spark_catalog.default.table_integers[i#x] parquet
   +- SubqueryAlias i1
      +- SubqueryAlias spark_catalog.default.table_integers
         +- Relation spark_catalog.default.table_integers[i#x] parquet


-- !query
SELECT i, (SELECT i FROM table_integers s1 WHERE i=i1.i AND EXISTS (SELECT i FROM table_integers WHERE i=s1.i)) AS j FROM table_integers i1 ORDER BY i
-- !query analysis
Sort [i#x ASC NULLS FIRST], true
+- Project [i#x, scalar-subquery#x [i#x] AS j#x]
   :  +- Project [i#x]
   :     +- Filter ((i#x = outer(i#x)) AND exists#x [i#x])
   :        :  +- Project [i#x]
   :        :     +- Filter (i#x = outer(i#x))
   :        :        +- SubqueryAlias spark_catalog.default.table_integers
   :        :           +- Relation spark_catalog.default.table_integers[i#x] parquet
   :        +- SubqueryAlias s1
   :           +- SubqueryAlias spark_catalog.default.table_integers
   :              +- Relation spark_catalog.default.table_integers[i#x] parquet
   +- SubqueryAlias i1
      +- SubqueryAlias spark_catalog.default.table_integers
         +- Relation spark_catalog.default.table_integers[i#x] parquet


-- !query
SELECT i, (SELECT SUM(ss2.i) FROM (SELECT i FROM table_integers s1 WHERE i=i1.i OR i=ANY(SELECT i FROM table_integers WHERE i=s1.i)) ss2) AS j FROM table_integers i1 ORDER BY i
-- !query analysis
org.apache.spark.sql.catalyst.parser.ParseException
{
  "errorClass" : "PARSE_SYNTAX_ERROR",
  "sqlState" : "42601",
  "messageParameters" : {
    "error" : "'('",
    "hint" : ""
  }
}


-- !query
SELECT i, (SELECT SUM(ss2.i) FROM (SELECT i FROM table_integers s1 WHERE CASE WHEN (i=i1.i AND EXISTS(SELECT i FROM table_integers WHERE i=s1.i)) THEN true ELSE false END) ss2) AS j FROM table_integers i1 ORDER BY i
-- !query analysis
Sort [i#x ASC NULLS FIRST], true
+- Project [i#x, scalar-subquery#x [i#x] AS j#xL]
   :  +- Aggregate [sum(i#x) AS sum(i)#xL]
   :     +- SubqueryAlias ss2
   :        +- Project [i#x]
   :           +- Filter CASE WHEN ((i#x = outer(i#x)) AND exists#x [i#x]) THEN true ELSE false END
   :              :  +- Project [i#x]
   :              :     +- Filter (i#x = outer(i#x))
   :              :        +- SubqueryAlias spark_catalog.default.table_integers
   :              :           +- Relation spark_catalog.default.table_integers[i#x] parquet
   :              +- SubqueryAlias s1
   :                 +- SubqueryAlias spark_catalog.default.table_integers
   :                    +- Relation spark_catalog.default.table_integers[i#x] parquet
   +- SubqueryAlias i1
      +- SubqueryAlias spark_catalog.default.table_integers
         +- Relation spark_catalog.default.table_integers[i#x] parquet


-- !query
SELECT i, (SELECT SUM(ss2.i) FROM (SELECT i FROM table_integers s1 WHERE i=i1.i AND EXISTS(SELECT i FROM table_integers WHERE i=s1.i)) ss2) AS j FROM table_integers i1 ORDER BY i
-- !query analysis
Sort [i#x ASC NULLS FIRST], true
+- Project [i#x, scalar-subquery#x [i#x] AS j#xL]
   :  +- Aggregate [sum(i#x) AS sum(i)#xL]
   :     +- SubqueryAlias ss2
   :        +- Project [i#x]
   :           +- Filter ((i#x = outer(i#x)) AND exists#x [i#x])
   :              :  +- Project [i#x]
   :              :     +- Filter (i#x = outer(i#x))
   :              :        +- SubqueryAlias spark_catalog.default.table_integers
   :              :           +- Relation spark_catalog.default.table_integers[i#x] parquet
   :              +- SubqueryAlias s1
   :                 +- SubqueryAlias spark_catalog.default.table_integers
   :                    +- Relation spark_catalog.default.table_integers[i#x] parquet
   +- SubqueryAlias i1
      +- SubqueryAlias spark_catalog.default.table_integers
         +- Relation spark_catalog.default.table_integers[i#x] parquet


-- !query
SELECT i, (SELECT SUM(ss1.i) FROM (SELECT i FROM table_integers s1 WHERE EXISTS(SELECT i FROM table_integers WHERE i<>s1.i AND s1.i > i)) ss1) AS j FROM table_integers i1 ORDER BY i
-- !query analysis
Sort [i#x ASC NULLS FIRST], true
+- Project [i#x, scalar-subquery#x [] AS j#xL]
   :  +- Aggregate [sum(i#x) AS sum(i)#xL]
   :     +- SubqueryAlias ss1
   :        +- Project [i#x]
   :           +- Filter exists#x [i#x]
   :              :  +- Project [i#x]
   :              :     +- Filter (NOT (i#x = outer(i#x)) AND (outer(i#x) > i#x))
   :              :        +- SubqueryAlias spark_catalog.default.table_integers
   :              :           +- Relation spark_catalog.default.table_integers[i#x] parquet
   :              +- SubqueryAlias s1
   :                 +- SubqueryAlias spark_catalog.default.table_integers
   :                    +- Relation spark_catalog.default.table_integers[i#x] parquet
   +- SubqueryAlias i1
      +- SubqueryAlias spark_catalog.default.table_integers
         +- Relation spark_catalog.default.table_integers[i#x] parquet


-- !query
SELECT i, (SELECT SUM(ss1.i)+SUM(ss2.i) FROM (SELECT i FROM table_integers s1 WHERE i=i1.i AND EXISTS(SELECT i FROM table_integers WHERE i<>s1.i AND s1.i>i)) ss1 LEFT OUTER JOIN (SELECT i FROM table_integers s1 WHERE EXISTS(SELECT i FROM table_integers WHERE i=s1.i)) ss2 ON ss1.i=ss2.i) AS j FROM table_integers i1 ORDER BY i
-- !query analysis
Sort [i#x ASC NULLS FIRST], true
+- Project [i#x, scalar-subquery#x [i#x] AS j#xL]
   :  +- Aggregate [(sum(i#x) + sum(i#x)) AS (sum(i) + sum(i))#xL]
   :     +- Join LeftOuter, (i#x = i#x)
   :        :- SubqueryAlias ss1
   :        :  +- Project [i#x]
   :        :     +- Filter ((i#x = outer(i#x)) AND exists#x [i#x])
   :        :        :  +- Project [i#x]
   :        :        :     +- Filter (NOT (i#x = outer(i#x)) AND (outer(i#x) > i#x))
   :        :        :        +- SubqueryAlias spark_catalog.default.table_integers
   :        :        :           +- Relation spark_catalog.default.table_integers[i#x] parquet
   :        :        +- SubqueryAlias s1
   :        :           +- SubqueryAlias spark_catalog.default.table_integers
   :        :              +- Relation spark_catalog.default.table_integers[i#x] parquet
   :        +- SubqueryAlias ss2
   :           +- Project [i#x]
   :              +- Filter exists#x [i#x]
   :                 :  +- Project [i#x]
   :                 :     +- Filter (i#x = outer(i#x))
   :                 :        +- SubqueryAlias spark_catalog.default.table_integers
   :                 :           +- Relation spark_catalog.default.table_integers[i#x] parquet
   :                 +- SubqueryAlias s1
   :                    +- SubqueryAlias spark_catalog.default.table_integers
   :                       +- Relation spark_catalog.default.table_integers[i#x] parquet
   +- SubqueryAlias i1
      +- SubqueryAlias spark_catalog.default.table_integers
         +- Relation spark_catalog.default.table_integers[i#x] parquet


-- !query
DROP TABLE IF EXISTS tbl_ProductSales
-- !query analysis
DropTable true, false
+- ResolvedIdentifier V2SessionCatalog(spark_catalog), default.tbl_ProductSales


-- !query
DROP TABLE IF EXISTS another_T
-- !query analysis
DropTable true, false
+- ResolvedIdentifier V2SessionCatalog(spark_catalog), default.another_T


-- !query
CREATE TABLE tbl_ProductSales (ColID int, Product_Category  varchar(64), Product_Name  varchar(64), TotalSales int)
-- !query analysis
CreateDataSourceTableCommand `spark_catalog`.`default`.`tbl_ProductSales`, false


-- !query
CREATE TABLE another_T (col1 INT, col2 INT, col3 INT, col4 INT, col5 INT, col6 INT, col7 INT, col8 INT)
-- !query analysis
CreateDataSourceTableCommand `spark_catalog`.`default`.`another_T`, false


-- !query
INSERT INTO tbl_ProductSales VALUES (1,'Game','Mobo Game',200),(2,'Game','PKO Game',400),(3,'Fashion','Shirt',500),(4,'Fashion','Shorts',100)
-- !query analysis
InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/tbl_productsales, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/tbl_productsales], Append, `spark_catalog`.`default`.`tbl_productsales`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/tbl_productsales), [ColID, Product_Category, Product_Name, TotalSales]
+- Project [cast(col1#x as int) AS ColID#x, static_invoke(CharVarcharCodegenUtils.varcharTypeWriteSideCheck(cast(col2#x as string), 64)) AS Product_Category#x, static_invoke(CharVarcharCodegenUtils.varcharTypeWriteSideCheck(cast(col3#x as string), 64)) AS Product_Name#x, cast(col4#x as int) AS TotalSales#x]
   +- LocalRelation [col1#x, col2#x, col3#x, col4#x]


-- !query
INSERT INTO another_T VALUES (1,2,3,4,5,6,7,8), (11,22,33,44,55,66,77,88), (111,222,333,444,555,666,777,888), (1111,2222,3333,4444,5555,6666,7777,8888)
-- !query analysis
InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/another_t, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/another_t], Append, `spark_catalog`.`default`.`another_t`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/another_t), [col1, col2, col3, col4, col5, col6, col7, col8]
+- LocalRelation [col1#x, col2#x, col3#x, col4#x, col5#x, col6#x, col7#x, col8#x]


-- !query
SELECT (SELECT MIN(ColID) FROM tbl_ProductSales INNER JOIN another_T t2 ON EXISTS (SELECT MAX(t1.col1 + t3.col4) AS mymax FROM another_T t3 HAVING t1.col7 <> mymax)) FROM another_T t1
-- !query analysis
org.apache.spark.sql.AnalysisException
{
  "errorClass" : "UNSUPPORTED_SUBQUERY_EXPRESSION_CATEGORY.AGGREGATE_FUNCTION_MIXED_OUTER_LOCAL_REFERENCES",
  "sqlState" : "0A000",
  "messageParameters" : {
    "function" : "max((outer(t1.col1) + t3.col4))"
  },
  "queryContext" : [ {
    "objectType" : "",
    "objectName" : "",
    "startIndex" : 91,
    "stopIndex" : 112,
    "fragment" : "MAX(t1.col1 + t3.col4)"
  } ]
}
