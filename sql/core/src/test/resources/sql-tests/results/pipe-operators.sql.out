-- Automatically generated by SQLQueryTestSuite
-- !query
drop table if exists t
-- !query schema
struct<>
-- !query output



-- !query
create table t(x int, y string) using csv
-- !query schema
struct<>
-- !query output



-- !query
insert into t values (0, 'abc'), (1, 'def')
-- !query schema
struct<>
-- !query output



-- !query
drop table if exists other
-- !query schema
struct<>
-- !query output



-- !query
create table other(a int, b int) using json
-- !query schema
struct<>
-- !query output



-- !query
insert into other values (1, 1), (1, 2), (2, 4)
-- !query schema
struct<>
-- !query output



-- !query
drop table if exists st
-- !query schema
struct<>
-- !query output



-- !query
create table st(x int, col struct<i1:int, i2:int>) using parquet
-- !query schema
struct<>
-- !query output



-- !query
insert into st values (1, (2, 3))
-- !query schema
struct<>
-- !query output



-- !query
create temporary view courseSales as select * from values
  ("dotNET", 2012, 10000),
  ("Java", 2012, 20000),
  ("dotNET", 2012, 5000),
  ("dotNET", 2013, 48000),
  ("Java", 2013, 30000)
  as courseSales(course, year, earnings)
-- !query schema
struct<>
-- !query output



-- !query
create temporary view courseEarnings as select * from values
  ("dotNET", 15000, 48000, 22500),
  ("Java", 20000, 30000, NULL)
  as courseEarnings(course, `2012`, `2013`, `2014`)
-- !query schema
struct<>
-- !query output



-- !query
create temporary view courseEarningsAndSales as select * from values
  ("dotNET", 15000, NULL, 48000, 1, 22500, 1),
  ("Java", 20000, 1, 30000, 2, NULL, NULL)
  as courseEarningsAndSales(
    course, earnings2012, sales2012, earnings2013, sales2013, earnings2014, sales2014)
-- !query schema
struct<>
-- !query output



-- !query
create temporary view yearsWithComplexTypes as select * from values
  (2012, array(1, 1), map('1', 1), struct(1, 'a')),
  (2013, array(2, 2), map('2', 2), struct(2, 'b'))
  as yearsWithComplexTypes(y, a, m, s)
-- !query schema
struct<>
-- !query output



-- !query
create temporary view join_test_t1 as select * from values (1) as grouping(a)
-- !query schema
struct<>
-- !query output



-- !query
create temporary view join_test_t2 as select * from values (1) as grouping(a)
-- !query schema
struct<>
-- !query output



-- !query
create temporary view join_test_t3 as select * from values (1) as grouping(a)
-- !query schema
struct<>
-- !query output



-- !query
create temporary view join_test_empty_table as select a from join_test_t2 where false
-- !query schema
struct<>
-- !query output



-- !query
create temporary view lateral_test_t1(c1, c2)
  as values (0, 1), (1, 2)
-- !query schema
struct<>
-- !query output



-- !query
create temporary view lateral_test_t2(c1, c2)
  as values (0, 2), (0, 3)
-- !query schema
struct<>
-- !query output



-- !query
create temporary view lateral_test_t3(c1, c2)
  as values (0, array(0, 1)), (1, array(2)), (2, array()), (null, array(4))
-- !query schema
struct<>
-- !query output



-- !query
create temporary view lateral_test_t4(c1, c2)
  as values (0, 1), (0, 2), (1, 1), (1, 3)
-- !query schema
struct<>
-- !query output



-- !query
create temporary view natural_join_test_t1 as select * from values
  ("one", 1), ("two", 2), ("three", 3) as natural_join_test_t1(k, v1)
-- !query schema
struct<>
-- !query output



-- !query
create temporary view natural_join_test_t2 as select * from values
  ("one", 1), ("two", 22), ("one", 5) as natural_join_test_t2(k, v2)
-- !query schema
struct<>
-- !query output



-- !query
create temporary view natural_join_test_t3 as select * from values
  ("one", 4), ("two", 5), ("one", 6) as natural_join_test_t3(k, v3)
-- !query schema
struct<>
-- !query output



-- !query
create temporary view windowTestData as select * from values
  (null, 1L, 1.0D, date("2017-08-01"), timestamp_seconds(1501545600), "a"),
  (1, 1L, 1.0D, date("2017-08-01"), timestamp_seconds(1501545600), "a"),
  (1, 2L, 2.5D, date("2017-08-02"), timestamp_seconds(1502000000), "a"),
  (2, 2147483650L, 100.001D, date("2020-12-31"), timestamp_seconds(1609372800), "a"),
  (1, null, 1.0D, date("2017-08-01"), timestamp_seconds(1501545600), "b"),
  (2, 3L, 3.3D, date("2017-08-03"), timestamp_seconds(1503000000), "b"),
  (3, 2147483650L, 100.001D, date("2020-12-31"), timestamp_seconds(1609372800), "b"),
  (null, null, null, null, null, null),
  (3, 1L, 1.0D, date("2017-08-01"), timestamp_seconds(1501545600), null)
  AS testData(val, val_long, val_double, val_date, val_timestamp, cate)
-- !query schema
struct<>
-- !query output



-- !query
from t
-- !query schema
struct<x:int,y:string>
-- !query output
0	abc
1	def


-- !query
table t
-- !query schema
struct<x:int,y:string>
-- !query output
0	abc
1	def


-- !query
from t
|> select 1 as x
-- !query schema
struct<x:int>
-- !query output
1
1


-- !query
from t as t_alias
|> select t_alias.x
-- !query schema
struct<x:int>
-- !query output
0
1


-- !query
from t as t_alias
|> select t_alias.x as tx, t_alias.y as ty
|> where ty = 'def'
|> select tx
-- !query schema
struct<tx:int>
-- !query output
1


-- !query
from t, other
|> select t.x + other.a as z
-- !query schema
struct<z:int>
-- !query output
1
1
2
2
2
3


-- !query
from t join other on (t.x = other.a)
|> select t.x + other.a as z
-- !query schema
struct<z:int>
-- !query output
2
2


-- !query
from t lateral view explode(array(100, 101)) as ly
|> select t.x + ly as z
-- !query schema
struct<z:int>
-- !query output
100
101
101
102


-- !query
from st
|> select col.i1
-- !query schema
struct<i1:int>
-- !query output
2


-- !query
from st as st_alias
|> select st_alias.col.i1
-- !query schema
struct<i1:int>
-- !query output
2


-- !query
from values (0), (1) tab(col)
|> select col as x
-- !query schema
struct<x:int>
-- !query output
0
1


-- !query
from t
|> from t
-- !query schema
struct<>
-- !query output
org.apache.spark.sql.catalyst.parser.ParseException
{
  "errorClass" : "PARSE_SYNTAX_ERROR",
  "sqlState" : "42601",
  "messageParameters" : {
    "error" : "'from'",
    "hint" : ""
  }
}


-- !query
table t
|> select 1 as x
-- !query schema
struct<x:int>
-- !query output
1
1


-- !query
table t
|> select x, y
-- !query schema
struct<x:int,y:string>
-- !query output
0	abc
1	def


-- !query
table t
|> select x, y
|> select x + length(y) as z
-- !query schema
struct<z:int>
-- !query output
3
4


-- !query
values (0), (1) tab(col)
|> select col * 2 as result
-- !query schema
struct<result:int>
-- !query output
0
2


-- !query
(select * from t union all select * from t)
|> select x + length(y) as result
-- !query schema
struct<result:int>
-- !query output
3
3
4
4


-- !query
(table t
 |> select x, y
 |> select x)
union all
select x from t where x < 1
-- !query schema
struct<x:int>
-- !query output
0
0
1


-- !query
(select col from st)
|> select col.i1
-- !query schema
struct<i1:int>
-- !query output
2


-- !query
table st
|> select st.col.i1
-- !query schema
struct<i1:int>
-- !query output
2


-- !query
table t
|> select (select a from other where x = a limit 1) as result
-- !query schema
struct<result:int>
-- !query output
1
NULL


-- !query
select (values (0) tab(col) |> select col) as result
-- !query schema
struct<result:int>
-- !query output
0


-- !query
table t
|> select (select any_value(a) from other where x = a limit 1) as result
-- !query schema
struct<result:int>
-- !query output
1
NULL


-- !query
table t
|> select x + length(x) as z, z + 1 as plus_one
-- !query schema
struct<z:int,plus_one:int>
-- !query output
1	2
2	3


-- !query
table t
|> select first_value(x) over (partition by y) as result
-- !query schema
struct<result:int>
-- !query output
0
1


-- !query
select 1 x, 2 y, 3 z
|> select 1 + sum(x) over (),
     avg(y) over (),
     x,
     avg(x+1) over (partition by y order by z) AS a2
|> select a2
-- !query schema
struct<a2:double>
-- !query output
2.0


-- !query
table t
|> select x, count(*) over ()
|> select x
-- !query schema
struct<x:int>
-- !query output
0
1


-- !query
table t
|> select distinct x, y
-- !query schema
struct<x:int,y:string>
-- !query output
0	abc
1	def


-- !query
table t
|> select *
-- !query schema
struct<x:int,y:string>
-- !query output
0	abc
1	def


-- !query
table t
|> select * except (y)
-- !query schema
struct<x:int>
-- !query output
0
1


-- !query
table t
|> select /*+ repartition(3) */ *
-- !query schema
struct<x:int,y:string>
-- !query output
0	abc
1	def


-- !query
table t
|> select /*+ repartition(3) */ distinct x
-- !query schema
struct<x:int>
-- !query output
0
1


-- !query
table t
|> select /*+ repartition(3) */ all x
-- !query schema
struct<x:int>
-- !query output
0
1


-- !query
table t
|> select sum(x) as result
-- !query schema
struct<>
-- !query output
org.apache.spark.sql.AnalysisException
{
  "errorClass" : "PIPE_OPERATOR_CONTAINS_AGGREGATE_FUNCTION",
  "sqlState" : "0A000",
  "messageParameters" : {
    "clause" : "SELECT",
    "expr" : "sum(x#x)"
  },
  "queryContext" : [ {
    "objectType" : "",
    "objectName" : "",
    "startIndex" : 19,
    "stopIndex" : 24,
    "fragment" : "sum(x)"
  } ]
}


-- !query
table t
|> select y, length(y) + sum(x) as result
-- !query schema
struct<>
-- !query output
org.apache.spark.sql.AnalysisException
{
  "errorClass" : "PIPE_OPERATOR_CONTAINS_AGGREGATE_FUNCTION",
  "sqlState" : "0A000",
  "messageParameters" : {
    "clause" : "SELECT",
    "expr" : "sum(x#x)"
  },
  "queryContext" : [ {
    "objectType" : "",
    "objectName" : "",
    "startIndex" : 34,
    "stopIndex" : 39,
    "fragment" : "sum(x)"
  } ]
}


-- !query
from t
|> select sum(x)
-- !query schema
struct<>
-- !query output
org.apache.spark.sql.AnalysisException
{
  "errorClass" : "PIPE_OPERATOR_CONTAINS_AGGREGATE_FUNCTION",
  "sqlState" : "0A000",
  "messageParameters" : {
    "clause" : "SELECT",
    "expr" : "sum(x#x)"
  },
  "queryContext" : [ {
    "objectType" : "",
    "objectName" : "",
    "startIndex" : 18,
    "stopIndex" : 23,
    "fragment" : "sum(x)"
  } ]
}


-- !query
from t as t_alias
|> select y, sum(x)
-- !query schema
struct<>
-- !query output
org.apache.spark.sql.AnalysisException
{
  "errorClass" : "PIPE_OPERATOR_CONTAINS_AGGREGATE_FUNCTION",
  "sqlState" : "0A000",
  "messageParameters" : {
    "clause" : "SELECT",
    "expr" : "sum(x#x)"
  },
  "queryContext" : [ {
    "objectType" : "",
    "objectName" : "",
    "startIndex" : 32,
    "stopIndex" : 37,
    "fragment" : "sum(x)"
  } ]
}


-- !query
from t as t_alias
|> select y, sum(x) group by y
-- !query schema
struct<>
-- !query output
org.apache.spark.sql.catalyst.parser.ParseException
{
  "errorClass" : "PARSE_SYNTAX_ERROR",
  "sqlState" : "42601",
  "messageParameters" : {
    "error" : "'group'",
    "hint" : ""
  }
}


-- !query
table t
|> extend 1 as z
-- !query schema
struct<x:int,y:string,z:int>
-- !query output
0	abc	1
1	def	1


-- !query
table t
|> extend 1
-- !query schema
struct<x:int,y:string,1:int>
-- !query output
0	abc	1
1	def	1


-- !query
table t
|> extend x as z
-- !query schema
struct<x:int,y:string,z:int>
-- !query output
0	abc	0
1	def	1


-- !query
table t
|> extend x + length(y) as z
-- !query schema
struct<x:int,y:string,z:int>
-- !query output
0	abc	3
1	def	4


-- !query
table t
|> extend x + length(y) as z, x + 1 as zz
-- !query schema
struct<x:int,y:string,z:int,zz:int>
-- !query output
0	abc	3	1
1	def	4	2


-- !query
table t
|> extend x + length(y) as z
|> extend z + 1 as zz
-- !query schema
struct<x:int,y:string,z:int,zz:int>
-- !query output
0	abc	3	4
1	def	4	5


-- !query
select col from st
|> extend col.i1 as z
-- !query schema
struct<col:struct<i1:int,i2:int>,z:int>
-- !query output
{"i1":2,"i2":3}	2


-- !query
table t
|> extend (select a from other where x = a limit 1) as z
-- !query schema
struct<x:int,y:string,z:int>
-- !query output
0	abc	NULL
1	def	1


-- !query
table t
|> where exists (
    table other
    |> extend t.x
    |> select * except (a, b))
-- !query schema
struct<x:int,y:string>
-- !query output
0	abc
1	def


-- !query
table t
|> extend 1 as x
-- !query schema
struct<x:int,y:string,x:int>
-- !query output
0	abc	1
1	def	1


-- !query
table t
|> extend first_value(x) over (partition by y) as result
-- !query schema
struct<x:int,y:string,result:int>
-- !query output
0	abc	0
1	def	1


-- !query
table t
|> extend x + length(y) as z, z + 1 as plus_one
-- !query schema
struct<x:int,y:string,z:int,plus_one:int>
-- !query output
0	abc	3	4
1	def	4	5


-- !query
table t
|> extend sum(x) as z
-- !query schema
struct<>
-- !query output
org.apache.spark.sql.AnalysisException
{
  "errorClass" : "PIPE_OPERATOR_CONTAINS_AGGREGATE_FUNCTION",
  "sqlState" : "0A000",
  "messageParameters" : {
    "clause" : "EXTEND",
    "expr" : "sum(x#x)"
  },
  "queryContext" : [ {
    "objectType" : "",
    "objectName" : "",
    "startIndex" : 19,
    "stopIndex" : 24,
    "fragment" : "sum(x)"
  } ]
}


-- !query
table t
|> extend distinct x as z
-- !query schema
struct<>
-- !query output
org.apache.spark.sql.catalyst.parser.ParseException
{
  "errorClass" : "PARSE_SYNTAX_ERROR",
  "sqlState" : "42601",
  "messageParameters" : {
    "error" : "'as'",
    "hint" : ""
  }
}


-- !query
table t
|> extend *
-- !query schema
struct<>
-- !query output
org.apache.spark.sql.AnalysisException
{
  "errorClass" : "INVALID_USAGE_OF_STAR_OR_REGEX",
  "sqlState" : "42000",
  "messageParameters" : {
    "elem" : "'*'",
    "prettyName" : "expression `pipeexpression`"
  }
}


-- !query
table t
|> set x = 1
-- !query schema
struct<x:int,y:string>
-- !query output
1	abc
1	def


-- !query
table t
|> set y = x
-- !query schema
struct<x:int,y:int>
-- !query output
0	0
1	1


-- !query
table t
|> extend 1 as z
|> set z = x + length(y)
-- !query schema
struct<x:int,y:string,z:int>
-- !query output
0	abc	3
1	def	4


-- !query
table t
|> extend 1 as z
|> extend 2 as zz
|> set z = x + length(y), zz = x + 1
-- !query schema
struct<x:int,y:string,z:int,zz:int>
-- !query output
0	abc	3	1
1	def	4	2


-- !query
table other
|> extend 3 as c
|> set a = b, b = c
-- !query schema
struct<a:int,b:int,c:int>
-- !query output
1	3	3
2	3	3
4	3	3


-- !query
table t
|> extend 1 as z
|> extend 2 as zz
|> set z = x + length(y), zz = z + 1
-- !query schema
struct<x:int,y:string,z:int,zz:int>
-- !query output
0	abc	3	4
1	def	4	5


-- !query
table t
|> extend 1 as z
|> set z = x + length(y)
|> set z = z + 1
-- !query schema
struct<x:int,y:string,z:int>
-- !query output
0	abc	4
1	def	5


-- !query
table t
|> extend 1 as z
|> set z = x + length(y), z = z + 1
-- !query schema
struct<x:int,y:string,z:int>
-- !query output
0	abc	4
1	def	5


-- !query
select col from st
|> extend 1 as z
|> set z = col.i1
-- !query schema
struct<col:struct<i1:int,i2:int>,z:int>
-- !query output
{"i1":2,"i2":3}	2


-- !query
table t
|> set y = (select a from other where x = a limit 1)
-- !query schema
struct<x:int,y:int>
-- !query output
0	NULL
1	1


-- !query
table t
|> extend 1 as `x.y.z`
|> set `x.y.z` = x + length(y)
-- !query schema
struct<x:int,y:string,x.y.z:int>
-- !query output
0	abc	3
1	def	4


-- !query
table t
|> extend 1 as z
|> set z = first_value(x) over (partition by y)
-- !query schema
struct<x:int,y:string,z:int>
-- !query output
0	abc	0
1	def	1


-- !query
values (0), (1) lhs(a)
|> inner join values (1), (2) rhs(a) using (a)
|> extend lhs.a + rhs.a as z1
|> extend lhs.a - rhs.a as z2
|> drop z1
|> where z2 = 0
|> order by lhs.a, rhs.a, z2
|> set z2 = 4
|> limit 2
|> select lhs.a, rhs.a, z2
-- !query schema
struct<a:int,a:int,z2:int>
-- !query output
1	1	4


-- !query
table t
|> set z = 1
-- !query schema
struct<>
-- !query output
org.apache.spark.sql.AnalysisException
{
  "errorClass" : "UNRESOLVED_COLUMN.WITH_SUGGESTION",
  "sqlState" : "42703",
  "messageParameters" : {
    "objectName" : "`z`",
    "proposal" : "`x`, `y`"
  },
  "queryContext" : [ {
    "objectType" : "",
    "objectName" : "",
    "startIndex" : 1,
    "stopIndex" : 20,
    "fragment" : "table t\n|> set z = 1"
  } ]
}


-- !query
table t
|> set x = 1 as z
-- !query schema
struct<>
-- !query output
org.apache.spark.sql.catalyst.parser.ParseException
{
  "errorClass" : "PARSE_SYNTAX_ERROR",
  "sqlState" : "42601",
  "messageParameters" : {
    "error" : "'as'",
    "hint" : ""
  }
}


-- !query
select col from st
|> set col.i1 = 42
-- !query schema
struct<>
-- !query output
org.apache.spark.sql.catalyst.parser.ParseException
{
  "errorClass" : "_LEGACY_ERROR_TEMP_0035",
  "messageParameters" : {
    "message" : "SQL pipe syntax |> SET operator with multi-part assignment key (only single-part keys are allowed)"
  },
  "queryContext" : [ {
    "objectType" : "",
    "objectName" : "",
    "startIndex" : 27,
    "stopIndex" : 37,
    "fragment" : "col.i1 = 42"
  } ]
}


-- !query
table t
|> drop y
-- !query schema
struct<x:int>
-- !query output
0
1


-- !query
select 1 as x, 2 as y, 3 as z
|> drop z, y
-- !query schema
struct<x:int>
-- !query output
1


-- !query
select 1 as x, 2 as y, 3 as z
|> drop z
|> drop y
-- !query schema
struct<x:int>
-- !query output
1


-- !query
select x from t
|> drop x
-- !query schema
struct<>
-- !query output



-- !query
table t
|> extend 1 as `x.y.z`
|> drop `x.y.z`
-- !query schema
struct<x:int,y:string>
-- !query output
0	abc
1	def


-- !query
table t
|> drop z
-- !query schema
struct<>
-- !query output
org.apache.spark.sql.AnalysisException
{
  "errorClass" : "UNRESOLVED_COLUMN.WITH_SUGGESTION",
  "sqlState" : "42703",
  "messageParameters" : {
    "objectName" : "`z`",
    "proposal" : "`x`, `y`"
  },
  "queryContext" : [ {
    "objectType" : "",
    "objectName" : "",
    "startIndex" : 1,
    "stopIndex" : 17,
    "fragment" : "table t\n|> drop z"
  } ]
}


-- !query
table st
|> drop col.i1
-- !query schema
struct<>
-- !query output
org.apache.spark.sql.catalyst.parser.ParseException
{
  "errorClass" : "PARSE_SYNTAX_ERROR",
  "sqlState" : "42601",
  "messageParameters" : {
    "error" : "'.'",
    "hint" : ""
  }
}


-- !query
table st
|> drop `col.i1`
-- !query schema
struct<>
-- !query output
org.apache.spark.sql.AnalysisException
{
  "errorClass" : "UNRESOLVED_COLUMN.WITH_SUGGESTION",
  "sqlState" : "42703",
  "messageParameters" : {
    "objectName" : "`col.i1`",
    "proposal" : "`col`, `x`"
  },
  "queryContext" : [ {
    "objectType" : "",
    "objectName" : "",
    "startIndex" : 1,
    "stopIndex" : 25,
    "fragment" : "table st\n|> drop `col.i1`"
  } ]
}


-- !query
select 1 as x, 2 as y, 3 as z
|> drop z, y, z
-- !query schema
struct<>
-- !query output
org.apache.spark.sql.AnalysisException
{
  "errorClass" : "EXCEPT_OVERLAPPING_COLUMNS",
  "sqlState" : "42702",
  "messageParameters" : {
    "columns" : "z, y, z"
  },
  "queryContext" : [ {
    "objectType" : "",
    "objectName" : "",
    "startIndex" : 1,
    "stopIndex" : 45,
    "fragment" : "select 1 as x, 2 as y, 3 as z\n|> drop z, y, z"
  } ]
}


-- !query
table t
|> as u
|> select u.x, u.y
-- !query schema
struct<x:int,y:string>
-- !query output
0	abc
1	def


-- !query
select 1 as x, 2 as y
|> as u
|> select u.x, u.y
-- !query schema
struct<x:int,y:int>
-- !query output
1	2


-- !query
table t
|> as `u.v`
|> select `u.v`.x, `u.v`.y
-- !query schema
struct<x:int,y:string>
-- !query output
0	abc
1	def


-- !query
table t
|> as u
|> as v
|> select v.x, v.y
-- !query schema
struct<x:int,y:string>
-- !query output
0	abc
1	def


-- !query
table t
|> as u
|> where u.x = 1
-- !query schema
struct<x:int,y:string>
-- !query output
1	def


-- !query
table t
|> as u, v
-- !query schema
struct<>
-- !query output
org.apache.spark.sql.catalyst.parser.ParseException
{
  "errorClass" : "PARSE_SYNTAX_ERROR",
  "sqlState" : "42601",
  "messageParameters" : {
    "error" : "','",
    "hint" : ""
  }
}


-- !query
table t
|> as 1 + 2
-- !query schema
struct<>
-- !query output
org.apache.spark.sql.catalyst.parser.ParseException
{
  "errorClass" : "PARSE_SYNTAX_ERROR",
  "sqlState" : "42601",
  "messageParameters" : {
    "error" : "'1'",
    "hint" : ""
  }
}


-- !query
table t
|> as u-v
-- !query schema
struct<>
-- !query output
org.apache.spark.sql.catalyst.parser.ParseException
{
  "errorClass" : "INVALID_IDENTIFIER",
  "sqlState" : "42602",
  "messageParameters" : {
    "ident" : "u-v"
  }
}


-- !query
table t
|> as u@v
-- !query schema
struct<>
-- !query output
org.apache.spark.sql.catalyst.parser.ParseException
{
  "errorClass" : "PARSE_SYNTAX_ERROR",
  "sqlState" : "42601",
  "messageParameters" : {
    "error" : "'@'",
    "hint" : ""
  }
}


-- !query
table t
|> as u#######v
-- !query schema
struct<>
-- !query output
org.apache.spark.sql.catalyst.parser.ParseException
{
  "errorClass" : "PARSE_SYNTAX_ERROR",
  "sqlState" : "42601",
  "messageParameters" : {
    "error" : "'#'",
    "hint" : ""
  }
}


-- !query
table t
|> where true
-- !query schema
struct<x:int,y:string>
-- !query output
0	abc
1	def


-- !query
table t
|> where x + length(y) < 4
-- !query schema
struct<x:int,y:string>
-- !query output
0	abc


-- !query
table t
|> where x + length(y) < 4
|> where x + length(y) < 3
-- !query schema
struct<x:int,y:string>
-- !query output



-- !query
(select x, sum(length(y)) as sum_len from t group by x)
|> where x = 1
-- !query schema
struct<x:int,sum_len:bigint>
-- !query output
1	3


-- !query
table t
|> where t.x = 1
-- !query schema
struct<x:int,y:string>
-- !query output
1	def


-- !query
table t
|> where spark_catalog.default.t.x = 1
-- !query schema
struct<x:int,y:string>
-- !query output
1	def


-- !query
(select col from st)
|> where col.i1 = 1
-- !query schema
struct<col:struct<i1:int,i2:int>>
-- !query output



-- !query
table st
|> where st.col.i1 = 2
-- !query schema
struct<x:int,col:struct<i1:int,i2:int>>
-- !query output
1	{"i1":2,"i2":3}


-- !query
table t
|> where exists (select a from other where x = a limit 1)
-- !query schema
struct<x:int,y:string>
-- !query output
1	def


-- !query
table t
|> where (select any_value(a) from other where x = a limit 1) = 1
-- !query schema
struct<x:int,y:string>
-- !query output
1	def


-- !query
table t
|> where sum(x) = 1
-- !query schema
struct<>
-- !query output
org.apache.spark.sql.catalyst.ExtendedAnalysisException
{
  "errorClass" : "INVALID_WHERE_CONDITION",
  "sqlState" : "42903",
  "messageParameters" : {
    "condition" : "\"(sum(x) = 1)\"",
    "expressionList" : "sum(spark_catalog.default.t.x)"
  },
  "queryContext" : [ {
    "objectType" : "",
    "objectName" : "",
    "startIndex" : 1,
    "stopIndex" : 27,
    "fragment" : "table t\n|> where sum(x) = 1"
  } ]
}


-- !query
table t
|> where y = 'abc' or length(y) + sum(x) = 1
-- !query schema
struct<>
-- !query output
org.apache.spark.sql.catalyst.ExtendedAnalysisException
{
  "errorClass" : "INVALID_WHERE_CONDITION",
  "sqlState" : "42903",
  "messageParameters" : {
    "condition" : "\"((y = abc) OR ((length(y) + sum(x)) = 1))\"",
    "expressionList" : "sum(spark_catalog.default.t.x)"
  },
  "queryContext" : [ {
    "objectType" : "",
    "objectName" : "",
    "startIndex" : 1,
    "stopIndex" : 52,
    "fragment" : "table t\n|> where y = 'abc' or length(y) + sum(x) = 1"
  } ]
}


-- !query
table t
|> where sum(x) over (partition by y) = 1
-- !query schema
struct<>
-- !query output
org.apache.spark.sql.AnalysisException
{
  "errorClass" : "_LEGACY_ERROR_TEMP_1034",
  "messageParameters" : {
    "clauseName" : "WHERE"
  }
}


-- !query
table t
|> where sum(x) over w = 1
   window w as (partition by y)
-- !query schema
struct<>
-- !query output
org.apache.spark.sql.catalyst.parser.ParseException
{
  "errorClass" : "NOT_ALLOWED_IN_PIPE_OPERATOR_WHERE.WINDOW_CLAUSE",
  "sqlState" : "42601",
  "queryContext" : [ {
    "objectType" : "",
    "objectName" : "",
    "startIndex" : 1,
    "stopIndex" : 66,
    "fragment" : "table t\n|> where sum(x) over w = 1\n   window w as (partition by y)"
  } ]
}


-- !query
select * from t where sum(x) over (partition by y) = 1
-- !query schema
struct<>
-- !query output
org.apache.spark.sql.AnalysisException
{
  "errorClass" : "_LEGACY_ERROR_TEMP_1034",
  "messageParameters" : {
    "clauseName" : "WHERE"
  }
}


-- !query
table t
|> select x, length(y) as z
|> where x + length(y) < 4
-- !query schema
struct<>
-- !query output
org.apache.spark.sql.catalyst.ExtendedAnalysisException
{
  "errorClass" : "UNRESOLVED_COLUMN.WITH_SUGGESTION",
  "sqlState" : "42703",
  "messageParameters" : {
    "objectName" : "`y`",
    "proposal" : "`z`, `spark_catalog`.`default`.`t`.`x`"
  },
  "queryContext" : [ {
    "objectType" : "",
    "objectName" : "",
    "startIndex" : 57,
    "stopIndex" : 57,
    "fragment" : "y"
  } ]
}


-- !query
table t
|> select x, length(y) as z
|> limit 1000
|> where x + length(y) < 4
-- !query schema
struct<>
-- !query output
org.apache.spark.sql.catalyst.ExtendedAnalysisException
{
  "errorClass" : "UNRESOLVED_COLUMN.WITH_SUGGESTION",
  "sqlState" : "42703",
  "messageParameters" : {
    "objectName" : "`y`",
    "proposal" : "`z`, `spark_catalog`.`default`.`t`.`x`"
  },
  "queryContext" : [ {
    "objectType" : "",
    "objectName" : "",
    "startIndex" : 71,
    "stopIndex" : 71,
    "fragment" : "y"
  } ]
}


-- !query
table t
|> select x, length(y) as z
|> limit 1000 offset 1
|> where x + length(y) < 4
-- !query schema
struct<>
-- !query output
org.apache.spark.sql.catalyst.ExtendedAnalysisException
{
  "errorClass" : "UNRESOLVED_COLUMN.WITH_SUGGESTION",
  "sqlState" : "42703",
  "messageParameters" : {
    "objectName" : "`y`",
    "proposal" : "`z`, `spark_catalog`.`default`.`t`.`x`"
  },
  "queryContext" : [ {
    "objectType" : "",
    "objectName" : "",
    "startIndex" : 80,
    "stopIndex" : 80,
    "fragment" : "y"
  } ]
}


-- !query
table t
|> select x, length(y) as z
|> order by x, y
|> where x + length(y) < 4
-- !query schema
struct<>
-- !query output
org.apache.spark.sql.catalyst.ExtendedAnalysisException
{
  "errorClass" : "UNRESOLVED_COLUMN.WITH_SUGGESTION",
  "sqlState" : "42703",
  "messageParameters" : {
    "objectName" : "`y`",
    "proposal" : "`z`, `spark_catalog`.`default`.`t`.`x`"
  },
  "queryContext" : [ {
    "objectType" : "",
    "objectName" : "",
    "startIndex" : 52,
    "stopIndex" : 52,
    "fragment" : "y"
  } ]
}


-- !query
(select x, sum(length(y)) as sum_len from t group by x)
|> where sum(length(y)) = 3
-- !query schema
struct<>
-- !query output
org.apache.spark.sql.catalyst.ExtendedAnalysisException
{
  "errorClass" : "UNRESOLVED_COLUMN.WITH_SUGGESTION",
  "sqlState" : "42703",
  "messageParameters" : {
    "objectName" : "`y`",
    "proposal" : "`sum_len`, `spark_catalog`.`default`.`t`.`x`"
  },
  "queryContext" : [ {
    "objectType" : "",
    "objectName" : "",
    "startIndex" : 77,
    "stopIndex" : 77,
    "fragment" : "y"
  } ]
}


-- !query
table courseSales
|> select `year`, course, earnings
|> pivot (
     sum(earnings)
     for course in ('dotNET', 'Java')
  )
-- !query schema
struct<year:int,dotNET:bigint,Java:bigint>
-- !query output
2012	15000	20000
2013	48000	30000


-- !query
table courseSales
|> select `year` as y, course as c, earnings as e
|> pivot (
     sum(e) as s, avg(e) as a
     for y in (2012 as firstYear, 2013 as secondYear)
   )
-- !query schema
struct<c:string,firstYear_s:bigint,firstYear_a:double,secondYear_s:bigint,secondYear_a:double>
-- !query output
Java	20000	20000.0	30000	30000.0
dotNET	15000	7500.0	48000	48000.0


-- !query
select course, `year`, y, a
from courseSales
join yearsWithComplexTypes on `year` = y
|> pivot (
     max(a)
     for (y, course) in ((2012, 'dotNET'), (2013, 'Java'))
   )
-- !query schema
struct<year:int,{2012, dotNET}:array<int>,{2013, Java}:array<int>>
-- !query output
2012	[1,1]	NULL
2013	NULL	[2,2]


-- !query
select earnings, `year`, s
from courseSales
join yearsWithComplexTypes on `year` = y
|> pivot (
     sum(earnings)
     for s in ((1, 'a'), (2, 'b'))
   )
-- !query schema
struct<year:int,{1, a}:bigint,{2, b}:bigint>
-- !query output
2012	35000	NULL
2013	NULL	78000


-- !query
table courseEarnings
|> unpivot (
     earningsYear for `year` in (`2012`, `2013`, `2014`)
   )
-- !query schema
struct<course:string,year:string,earningsYear:int>
-- !query output
Java	2012	20000
Java	2013	30000
dotNET	2012	15000
dotNET	2013	48000
dotNET	2014	22500


-- !query
table courseEarnings
|> unpivot include nulls (
     earningsYear for `year` in (`2012`, `2013`, `2014`)
   )
-- !query schema
struct<course:string,year:string,earningsYear:int>
-- !query output
Java	2012	20000
Java	2013	30000
Java	2014	NULL
dotNET	2012	15000
dotNET	2013	48000
dotNET	2014	22500


-- !query
table courseEarningsAndSales
|> unpivot include nulls (
     (earnings, sales) for `year` in (
       (earnings2012, sales2012) as `2012`,
       (earnings2013, sales2013) as `2013`,
       (earnings2014, sales2014) as `2014`)
   )
-- !query schema
struct<course:string,year:string,earnings:int,sales:int>
-- !query output
Java	2012	20000	1
Java	2013	30000	2
Java	2014	NULL	NULL
dotNET	2012	15000	NULL
dotNET	2013	48000	1
dotNET	2014	22500	1


-- !query
table courseSales
|> select course, earnings
|> pivot (
     sum(earnings)
     for `year` in (2012, 2013)
   )
-- !query schema
struct<>
-- !query output
org.apache.spark.sql.catalyst.ExtendedAnalysisException
{
  "errorClass" : "UNRESOLVED_COLUMN.WITH_SUGGESTION",
  "sqlState" : "42703",
  "messageParameters" : {
    "objectName" : "`year`",
    "proposal" : "`course`, `earnings`"
  },
  "queryContext" : [ {
    "objectType" : "",
    "objectName" : "",
    "startIndex" : 49,
    "stopIndex" : 111,
    "fragment" : "pivot (\n     sum(earnings)\n     for `year` in (2012, 2013)\n   )"
  } ]
}


-- !query
table courseSales
|> pivot (
     sum(earnings)
     for `year` in (course, 2013)
   )
-- !query schema
struct<>
-- !query output
org.apache.spark.sql.AnalysisException
{
  "errorClass" : "NON_LITERAL_PIVOT_VALUES",
  "sqlState" : "42K08",
  "messageParameters" : {
    "expression" : "\"course\""
  }
}


-- !query
table courseSales
|> select course, earnings
|> pivot (
     sum(earnings)
     for `year` in (2012, 2013)
   )
   unpivot (
     earningsYear for `year` in (`2012`, `2013`, `2014`)
   )
-- !query schema
struct<>
-- !query output
org.apache.spark.sql.catalyst.parser.ParseException
{
  "errorClass" : "NOT_ALLOWED_IN_FROM.UNPIVOT_WITH_PIVOT",
  "sqlState" : "42601",
  "queryContext" : [ {
    "objectType" : "",
    "objectName" : "",
    "startIndex" : 1,
    "stopIndex" : 186,
    "fragment" : "table courseSales\n|> select course, earnings\n|> pivot (\n     sum(earnings)\n     for `year` in (2012, 2013)\n   )\n   unpivot (\n     earningsYear for `year` in (`2012`, `2013`, `2014`)\n   )"
  } ]
}


-- !query
table courseSales
|> select course, earnings
|> unpivot (
     earningsYear for `year` in (`2012`, `2013`, `2014`)
   )
   pivot (
     sum(earnings)
     for `year` in (2012, 2013)
   )
-- !query schema
struct<>
-- !query output
org.apache.spark.sql.catalyst.parser.ParseException
{
  "errorClass" : "NOT_ALLOWED_IN_FROM.UNPIVOT_WITH_PIVOT",
  "sqlState" : "42601",
  "queryContext" : [ {
    "objectType" : "",
    "objectName" : "",
    "startIndex" : 1,
    "stopIndex" : 186,
    "fragment" : "table courseSales\n|> select course, earnings\n|> unpivot (\n     earningsYear for `year` in (`2012`, `2013`, `2014`)\n   )\n   pivot (\n     sum(earnings)\n     for `year` in (2012, 2013)\n   )"
  } ]
}


-- !query
table courseSales
|> select course, earnings
|> pivot (
     sum(earnings)
     for `year` in (2012, 2013)
   )
   pivot (
     sum(earnings)
     for `year` in (2012, 2013)
   )
-- !query schema
struct<>
-- !query output
org.apache.spark.sql.catalyst.parser.ParseException
{
  "errorClass" : "PARSE_SYNTAX_ERROR",
  "sqlState" : "42601",
  "messageParameters" : {
    "error" : "'pivot'",
    "hint" : ""
  }
}


-- !query
table courseSales
|> select course, earnings
|> unpivot (
     earningsYear for `year` in (`2012`, `2013`, `2014`)
   )
   unpivot (
     earningsYear for `year` in (`2012`, `2013`, `2014`)
   )
   pivot (
     sum(earnings)
     for `year` in (2012, 2013)
   )
-- !query schema
struct<>
-- !query output
org.apache.spark.sql.catalyst.parser.ParseException
{
  "errorClass" : "PARSE_SYNTAX_ERROR",
  "sqlState" : "42601",
  "messageParameters" : {
    "error" : "'unpivot'",
    "hint" : ""
  }
}


-- !query
table t
|> tablesample (100 percent) repeatable (0)
-- !query schema
struct<x:int,y:string>
-- !query output
0	abc
1	def


-- !query
table t
|> tablesample (2 rows) repeatable (0)
-- !query schema
struct<x:int,y:string>
-- !query output
0	abc
1	def


-- !query
table t
|> tablesample (bucket 1 out of 1) repeatable (0)
-- !query schema
struct<x:int,y:string>
-- !query output
0	abc
1	def


-- !query
table t
|> tablesample (100 percent) repeatable (0)
|> tablesample (5 rows) repeatable (0)
|> tablesample (bucket 1 out of 1) repeatable (0)
-- !query schema
struct<x:int,y:string>
-- !query output
0	abc
1	def


-- !query
table t
|> tablesample ()
-- !query schema
struct<>
-- !query output
org.apache.spark.sql.catalyst.parser.ParseException
{
  "errorClass" : "_LEGACY_ERROR_TEMP_0014",
  "queryContext" : [ {
    "objectType" : "",
    "objectName" : "",
    "startIndex" : 12,
    "stopIndex" : 25,
    "fragment" : "tablesample ()"
  } ]
}


-- !query
table t
|> tablesample (-100 percent) repeatable (0)
-- !query schema
struct<>
-- !query output
org.apache.spark.sql.catalyst.parser.ParseException
{
  "errorClass" : "_LEGACY_ERROR_TEMP_0064",
  "messageParameters" : {
    "msg" : "Sampling fraction (-1.0) must be on interval [0, 1]"
  },
  "queryContext" : [ {
    "objectType" : "",
    "objectName" : "",
    "startIndex" : 12,
    "stopIndex" : 52,
    "fragment" : "tablesample (-100 percent) repeatable (0)"
  } ]
}


-- !query
table t
|> tablesample (-5 rows)
-- !query schema
struct<>
-- !query output
org.apache.spark.sql.catalyst.ExtendedAnalysisException
{
  "errorClass" : "INVALID_LIMIT_LIKE_EXPRESSION.IS_NEGATIVE",
  "sqlState" : "42K0E",
  "messageParameters" : {
    "expr" : "\"-5\"",
    "name" : "limit",
    "v" : "-5"
  },
  "queryContext" : [ {
    "objectType" : "",
    "objectName" : "",
    "startIndex" : 25,
    "stopIndex" : 26,
    "fragment" : "-5"
  } ]
}


-- !query
table t
|> tablesample (x rows)
-- !query schema
struct<>
-- !query output
org.apache.spark.sql.catalyst.ExtendedAnalysisException
{
  "errorClass" : "INVALID_LIMIT_LIKE_EXPRESSION.IS_UNFOLDABLE",
  "sqlState" : "42K0E",
  "messageParameters" : {
    "expr" : "\"x\"",
    "name" : "limit"
  },
  "queryContext" : [ {
    "objectType" : "",
    "objectName" : "",
    "startIndex" : 25,
    "stopIndex" : 25,
    "fragment" : "x"
  } ]
}


-- !query
table t
|> tablesample (bucket 2 out of 1)
-- !query schema
struct<>
-- !query output
org.apache.spark.sql.catalyst.parser.ParseException
{
  "errorClass" : "_LEGACY_ERROR_TEMP_0064",
  "messageParameters" : {
    "msg" : "Sampling fraction (2.0) must be on interval [0, 1]"
  },
  "queryContext" : [ {
    "objectType" : "",
    "objectName" : "",
    "startIndex" : 12,
    "stopIndex" : 42,
    "fragment" : "tablesample (bucket 2 out of 1)"
  } ]
}


-- !query
table t
|> tablesample (200b) repeatable (0)
-- !query schema
struct<>
-- !query output
org.apache.spark.sql.catalyst.parser.ParseException
{
  "errorClass" : "_LEGACY_ERROR_TEMP_0015",
  "messageParameters" : {
    "msg" : "byteLengthLiteral"
  },
  "queryContext" : [ {
    "objectType" : "",
    "objectName" : "",
    "startIndex" : 12,
    "stopIndex" : 44,
    "fragment" : "tablesample (200b) repeatable (0)"
  } ]
}


-- !query
table t
|> tablesample (200) repeatable (0)
-- !query schema
struct<>
-- !query output
org.apache.spark.sql.catalyst.parser.ParseException
{
  "errorClass" : "_LEGACY_ERROR_TEMP_0016",
  "messageParameters" : {
    "bytesStr" : "200"
  },
  "queryContext" : [ {
    "objectType" : "",
    "objectName" : "",
    "startIndex" : 12,
    "stopIndex" : 43,
    "fragment" : "tablesample (200) repeatable (0)"
  } ]
}


-- !query
table join_test_t1
|> inner join join_test_empty_table
-- !query schema
struct<a:int,a:int>
-- !query output



-- !query
table join_test_t1
|> cross join join_test_empty_table
-- !query schema
struct<a:int,a:int>
-- !query output



-- !query
table join_test_t1
|> left outer join join_test_empty_table
-- !query schema
struct<a:int,a:int>
-- !query output
1	NULL


-- !query
table join_test_t1
|> right outer join join_test_empty_table
-- !query schema
struct<a:int,a:int>
-- !query output



-- !query
table join_test_t1
|> full outer join join_test_empty_table using (a)
-- !query schema
struct<a:int>
-- !query output
1


-- !query
table join_test_t1
|> full outer join join_test_empty_table on (join_test_t1.a = join_test_empty_table.a)
-- !query schema
struct<a:int,a:int>
-- !query output
1	NULL


-- !query
table join_test_t1
|> left semi join join_test_empty_table
-- !query schema
struct<a:int>
-- !query output



-- !query
table join_test_t1
|> left anti join join_test_empty_table
-- !query schema
struct<a:int>
-- !query output
1


-- !query
select * from join_test_t1 where true
|> inner join join_test_empty_table
-- !query schema
struct<a:int,a:int>
-- !query output



-- !query
select 1 as x, 2 as y
|> inner join (select 1 as x, 4 as y) using (x)
-- !query schema
struct<x:int,y:int,y:int>
-- !query output
1	2	4


-- !query
table join_test_t1
|> inner join (join_test_t2 jt2 inner join join_test_t3 jt3 using (a)) using (a)
|> select a, join_test_t1.a, jt2.a, jt3.a
-- !query schema
struct<a:int,a:int,a:int,a:int>
-- !query output
1	1	1	1


-- !query
table join_test_t1
|> inner join join_test_t2 tablesample (100 percent) repeatable (0) jt2 using (a)
-- !query schema
struct<a:int>
-- !query output
1


-- !query
table join_test_t1
|> inner join (select 1 as a) tablesample (100 percent) repeatable (0) jt2 using (a)
-- !query schema
struct<a:int>
-- !query output
1


-- !query
table join_test_t1
|> join join_test_t1 using (a)
-- !query schema
struct<a:int>
-- !query output
1


-- !query
table lateral_test_t1
|> join lateral (select c1)
-- !query schema
struct<c1:int,c2:int,c1:int>
-- !query output
0	1	0
1	2	1


-- !query
table lateral_test_t1
|> join lateral (select c1 from lateral_test_t2)
-- !query schema
struct<c1:int,c2:int,c1:int>
-- !query output
0	1	0
0	1	0
1	2	0
1	2	0


-- !query
table lateral_test_t1
|> join lateral (select lateral_test_t1.c1 from lateral_test_t2)
-- !query schema
struct<c1:int,c2:int,c1:int>
-- !query output
0	1	0
0	1	0
1	2	1
1	2	1


-- !query
table lateral_test_t1
|> join lateral (select lateral_test_t1.c1 + t2.c1 from lateral_test_t2 t2)
-- !query schema
struct<c1:int,c2:int,(outer(lateral_test_t1.c1) + c1):int>
-- !query output
0	1	0
0	1	0
1	2	1
1	2	1


-- !query
table lateral_test_t1
|> join lateral (select *)
-- !query schema
struct<c1:int,c2:int>
-- !query output
0	1
1	2


-- !query
table lateral_test_t1
|> join lateral (select * from lateral_test_t2)
-- !query schema
struct<c1:int,c2:int,c1:int,c2:int>
-- !query output
0	1	0	2
0	1	0	3
1	2	0	2
1	2	0	3


-- !query
table lateral_test_t1
|> join lateral (select lateral_test_t1.* from lateral_test_t2)
-- !query schema
struct<c1:int,c2:int,c1:int,c2:int>
-- !query output
0	1	0	1
0	1	0	1
1	2	1	2
1	2	1	2


-- !query
table lateral_test_t1
|> join lateral (select lateral_test_t1.*, t2.* from lateral_test_t2 t2)
-- !query schema
struct<c1:int,c2:int,c1:int,c2:int,c1:int,c2:int>
-- !query output
0	1	0	1	0	2
0	1	0	1	0	3
1	2	1	2	0	2
1	2	1	2	0	3


-- !query
table lateral_test_t1
|> join lateral_test_t2
|> join lateral (select lateral_test_t1.c2 + lateral_test_t2.c2)
-- !query schema
struct<c1:int,c2:int,c1:int,c2:int,(outer(lateral_test_t1.c2) + outer(lateral_test_t2.c2)):int>
-- !query output
0	1	0	2	3
0	1	0	3	4
1	2	0	2	4
1	2	0	3	5


-- !query
table natural_join_test_t1
|> natural join natural_join_test_t2
|> where k = "one"
-- !query schema
struct<k:string,v1:int,v2:int>
-- !query output
one	1	1
one	1	5


-- !query
table natural_join_test_t1
|> natural join natural_join_test_t2 nt2
|> select natural_join_test_t1.*
-- !query schema
struct<k:string,v1:int>
-- !query output
one	1
one	1
two	2


-- !query
table natural_join_test_t1
|> natural join natural_join_test_t2 nt2
|> natural join natural_join_test_t3 nt3
|> select natural_join_test_t1.*, nt2.*, nt3.*
-- !query schema
struct<k:string,v1:int,k:string,v2:int,k:string,v3:int>
-- !query output
one	1	one	1	one	4
one	1	one	1	one	6
one	1	one	5	one	4
one	1	one	5	one	6
two	2	two	22	two	5


-- !query
table join_test_t1
|> inner join join_test_empty_table
   inner join join_test_empty_table
-- !query schema
struct<>
-- !query output
org.apache.spark.sql.catalyst.parser.ParseException
{
  "errorClass" : "PARSE_SYNTAX_ERROR",
  "sqlState" : "42601",
  "messageParameters" : {
    "error" : "'inner'",
    "hint" : ""
  }
}


-- !query
table join_test_t1
|> select 1 + 2 as result
|> full outer join join_test_empty_table on (join_test_t1.a = join_test_empty_table.a)
-- !query schema
struct<>
-- !query output
org.apache.spark.sql.catalyst.ExtendedAnalysisException
{
  "errorClass" : "UNRESOLVED_COLUMN.WITH_SUGGESTION",
  "sqlState" : "42703",
  "messageParameters" : {
    "objectName" : "`join_test_t1`.`a`",
    "proposal" : "`result`, `join_test_empty_table`.`a`"
  },
  "queryContext" : [ {
    "objectType" : "",
    "objectName" : "",
    "startIndex" : 91,
    "stopIndex" : 104,
    "fragment" : "join_test_t1.a"
  } ]
}


-- !query
table join_test_t1 jt
|> cross join (select * from jt)
-- !query schema
struct<>
-- !query output
org.apache.spark.sql.catalyst.parser.ParseException
{
  "errorClass" : "PARSE_SYNTAX_ERROR",
  "sqlState" : "42601",
  "messageParameters" : {
    "error" : "'jt'",
    "hint" : ""
  }
}


-- !query
table t
|> union all table t
-- !query schema
struct<x:int,y:string>
-- !query output
0	abc
0	abc
1	def
1	def


-- !query
table t
|> union table t
-- !query schema
struct<x:int,y:string>
-- !query output
0	abc
1	def


-- !query
(select * from t)
|> union all table t
-- !query schema
struct<x:int,y:string>
-- !query output
0	abc
0	abc
1	def
1	def


-- !query
(select * from t)
|> union table t
-- !query schema
struct<x:int,y:string>
-- !query output
0	abc
1	def


-- !query
values (0, 'abc') tab(x, y)
|> union all table t
-- !query schema
struct<x:int,y:string>
-- !query output
0	abc
0	abc
1	def


-- !query
values (2, 'xyz') tab(x, y)
|> union table t
|> where x = 0
-- !query schema
struct<x:int,y:string>
-- !query output
0	abc


-- !query
values (2, 'xyz') tab(x, y)
|> union table t
|> drop x
-- !query schema
struct<y:string>
-- !query output
abc
def
xyz


-- !query
(select * from t)
|> union all (select * from t)
-- !query schema
struct<x:int,y:string>
-- !query output
0	abc
0	abc
1	def
1	def


-- !query
table t
|> except all table t
-- !query schema
struct<x:int,y:string>
-- !query output



-- !query
table t
|> except table t
-- !query schema
struct<x:int,y:string>
-- !query output



-- !query
table t
|> intersect all table t
-- !query schema
struct<x:int,y:string>
-- !query output
0	abc
1	def


-- !query
table t
|> intersect table t
-- !query schema
struct<x:int,y:string>
-- !query output
0	abc
1	def


-- !query
table t
|> minus all table t
-- !query schema
struct<x:int,y:string>
-- !query output



-- !query
table t
|> minus table t
-- !query schema
struct<x:int,y:string>
-- !query output



-- !query
table t
|> select x
|> union all table t
-- !query schema
struct<>
-- !query output
org.apache.spark.sql.catalyst.ExtendedAnalysisException
{
  "errorClass" : "NUM_COLUMNS_MISMATCH",
  "sqlState" : "42826",
  "messageParameters" : {
    "firstNumColumns" : "1",
    "invalidNumColumns" : "2",
    "invalidOrdinalNum" : "second",
    "operator" : "UNION"
  },
  "queryContext" : [ {
    "objectType" : "",
    "objectName" : "",
    "startIndex" : 1,
    "stopIndex" : 40,
    "fragment" : "table t\n|> select x\n|> union all table t"
  } ]
}


-- !query
table t
|> union all table st
-- !query schema
struct<>
-- !query output
org.apache.spark.sql.catalyst.ExtendedAnalysisException
{
  "errorClass" : "INCOMPATIBLE_COLUMN_TYPE",
  "sqlState" : "42825",
  "messageParameters" : {
    "columnOrdinalNumber" : "second",
    "dataType1" : "\"STRUCT<i1: INT, i2: INT>\"",
    "dataType2" : "\"STRING\"",
    "hint" : "",
    "operator" : "UNION",
    "tableOrdinalNumber" : "second"
  },
  "queryContext" : [ {
    "objectType" : "",
    "objectName" : "",
    "startIndex" : 1,
    "stopIndex" : 29,
    "fragment" : "table t\n|> union all table st"
  } ]
}


-- !query
table t
|> order by x
-- !query schema
struct<x:int,y:string>
-- !query output
0	abc
1	def


-- !query
(select * from t)
|> order by x
-- !query schema
struct<x:int,y:string>
-- !query output
0	abc
1	def


-- !query
values (0, 'abc') tab(x, y)
|> order by x
-- !query schema
struct<x:int,y:string>
-- !query output
0	abc


-- !query
table t
|> order by x
|> limit 1
-- !query schema
struct<x:int,y:string>
-- !query output
0	abc


-- !query
table t
|> where x = 1
|> select y
|> limit 2 offset 1
-- !query schema
struct<y:string>
-- !query output



-- !query
table t
|> where x = 1
|> select y
|> offset 1
-- !query schema
struct<y:string>
-- !query output



-- !query
table t
|> limit all offset 0
-- !query schema
struct<x:int,y:string>
-- !query output
0	abc
1	def


-- !query
table t
|> distribute by x
-- !query schema
struct<x:int,y:string>
-- !query output
0	abc
1	def


-- !query
table t
|> cluster by x
-- !query schema
struct<x:int,y:string>
-- !query output
0	abc
1	def


-- !query
table t
|> sort by x distribute by x
-- !query schema
struct<x:int,y:string>
-- !query output
0	abc
1	def


-- !query
table t
|> order by x desc
order by y
-- !query schema
struct<x:int,y:string>
-- !query output
0	abc
1	def


-- !query
table t
|> order by x desc order by x + y
order by y
-- !query schema
struct<>
-- !query output
org.apache.spark.sql.catalyst.parser.ParseException
{
  "errorClass" : "PARSE_SYNTAX_ERROR",
  "sqlState" : "42601",
  "messageParameters" : {
    "error" : "'order'",
    "hint" : ""
  }
}


-- !query
table t
|> select 1 + 2 as result
|> order by x
-- !query schema
struct<>
-- !query output
org.apache.spark.sql.catalyst.ExtendedAnalysisException
{
  "errorClass" : "UNRESOLVED_COLUMN.WITH_SUGGESTION",
  "sqlState" : "42703",
  "messageParameters" : {
    "objectName" : "`x`",
    "proposal" : "`result`"
  },
  "queryContext" : [ {
    "objectType" : "",
    "objectName" : "",
    "startIndex" : 47,
    "stopIndex" : 47,
    "fragment" : "x"
  } ]
}


-- !query
table t
|> select 1 + 2 as result
|> distribute by x
-- !query schema
struct<>
-- !query output
org.apache.spark.sql.catalyst.ExtendedAnalysisException
{
  "errorClass" : "UNRESOLVED_COLUMN.WITH_SUGGESTION",
  "sqlState" : "42703",
  "messageParameters" : {
    "objectName" : "`x`",
    "proposal" : "`result`"
  },
  "queryContext" : [ {
    "objectType" : "",
    "objectName" : "",
    "startIndex" : 52,
    "stopIndex" : 52,
    "fragment" : "x"
  } ]
}


-- !query
table t
|> order by x limit 1
-- !query schema
struct<>
-- !query output
org.apache.spark.sql.catalyst.parser.ParseException
{
  "errorClass" : "MULTIPLE_QUERY_RESULT_CLAUSES_WITH_PIPE_OPERATORS",
  "sqlState" : "42000",
  "messageParameters" : {
    "clause1" : "ORDER BY",
    "clause2" : "LIMIT"
  },
  "queryContext" : [ {
    "objectType" : "",
    "objectName" : "",
    "startIndex" : 12,
    "stopIndex" : 29,
    "fragment" : "order by x limit 1"
  } ]
}


-- !query
table t
|> order by x sort by x
-- !query schema
struct<>
-- !query output
org.apache.spark.sql.catalyst.parser.ParseException
{
  "errorClass" : "UNSUPPORTED_FEATURE.COMBINATION_QUERY_RESULT_CLAUSES",
  "sqlState" : "0A000",
  "queryContext" : [ {
    "objectType" : "",
    "objectName" : "",
    "startIndex" : 12,
    "stopIndex" : 31,
    "fragment" : "order by x sort by x"
  } ]
}


-- !query
table other
|> aggregate sum(b) as result group by a
-- !query schema
struct<a:int,result:bigint>
-- !query output
1	3
2	4


-- !query
table other
|> aggregate sum(b) as result group by a
|> select result
-- !query schema
struct<result:bigint>
-- !query output
3
4


-- !query
table other
|> aggregate sum(b) group by a + 1 as gkey
|> select gkey
-- !query schema
struct<gkey:int>
-- !query output
2
3


-- !query
select 1 as x, 2 as y
|> aggregate group by x, y
-- !query schema
struct<x:int,y:int>
-- !query output
1	2


-- !query
select 3 as x, 4 as y
|> aggregate group by 1, 2
-- !query schema
struct<x:int,y:int>
-- !query output
3	4


-- !query
values (3, 4) as tab(x, y)
|> aggregate sum(y) group by 1
-- !query schema
struct<x:int,sum(y):bigint>
-- !query output
3	4


-- !query
values (3, 4), (5, 4) as tab(x, y)
|> aggregate sum(y) group by 1
-- !query schema
struct<x:int,sum(y):bigint>
-- !query output
3	4
5	4


-- !query
select 3 as x, 4 as y
|> aggregate sum(y) group by 1, 1
-- !query schema
struct<x:int,x:int,sum(y):bigint>
-- !query output
3	3	4


-- !query
select 1 as `1`, 2 as `2`
|> aggregate sum(`2`) group by `1`
-- !query schema
struct<1:int,sum(2):bigint>
-- !query output
1	2


-- !query
select 3 as x, 4 as y
|> aggregate sum(y) group by 2
-- !query schema
struct<y:int,sum(y):bigint>
-- !query output
4	4


-- !query
select 3 as x, 4 as y, 5 as z
|> aggregate sum(y) group by 2
-- !query schema
struct<y:int,sum(y):bigint>
-- !query output
4	4


-- !query
select 3 as x, 4 as y, 5 as z
|> aggregate sum(y) group by 3
-- !query schema
struct<z:int,sum(y):bigint>
-- !query output
5	4


-- !query
select 3 as x, 4 as y, 5 as z
|> aggregate sum(y) group by 2, 3
-- !query schema
struct<y:int,z:int,sum(y):bigint>
-- !query output
4	5	4


-- !query
select 3 as x, 4 as y, 5 as z
|> aggregate sum(y) group by 1, 2, 3
-- !query schema
struct<x:int,y:int,z:int,sum(y):bigint>
-- !query output
3	4	5	4


-- !query
select 3 as x, 4 as y, 5 as z
|> aggregate sum(y) group by x, 2, 3
-- !query schema
struct<x:int,y:int,z:int,sum(y):bigint>
-- !query output
3	4	5	4


-- !query
table t
|> aggregate sum(x)
-- !query schema
struct<sum(x):bigint>
-- !query output
1


-- !query
table t
|> aggregate sum(x) + 1 as result_plus_one
-- !query schema
struct<result_plus_one:bigint>
-- !query output
2


-- !query
table other
|> aggregate group by a
|> where a = 1
-- !query schema
struct<a:int>
-- !query output
1


-- !query
select 1 as x, 2 as y, 3 as z
|> aggregate group by x, y, x + y as z
-- !query schema
struct<x:int,y:int,z:int>
-- !query output
1	2	3


-- !query
select 1 as x, 2 as y, 3 as z
|> aggregate group by x as z, x + y as z
-- !query schema
struct<z:int,z:int>
-- !query output
1	3


-- !query
select 1 as x, 2 as y, named_struct('z', 3) as st
|> aggregate group by x, y, x, x, st.z, (st).z, 1 + x, 2 + x
-- !query schema
struct<x:int,y:int,x:int,x:int,z:int,z:int,(1 + x):int,(2 + x):int>
-- !query output
1	2	1	1	3	3	2	3


-- !query
select 1 x, 2 y, 3 z
|> aggregate sum(z) z group by x, y
|> aggregate avg(z) z group by x
|> aggregate count(distinct z) c
-- !query schema
struct<c:bigint>
-- !query output
1


-- !query
select 1 x, 3 z
|> aggregate count(*) group by x, z, x
|> select x
-- !query schema
struct<x:int>
-- !query output
1


-- !query
table other
|> aggregate a + count(b) group by a
-- !query schema
struct<a:int,(a + count(b)):bigint>
-- !query output
1	3
2	3


-- !query
table other
|> aggregate a group by a
-- !query schema
struct<>
-- !query output
org.apache.spark.sql.AnalysisException
{
  "errorClass" : "PIPE_OPERATOR_AGGREGATE_EXPRESSION_CONTAINS_NO_AGGREGATE_FUNCTION",
  "sqlState" : "0A000",
  "messageParameters" : {
    "expr" : "a#x"
  },
  "queryContext" : [ {
    "objectType" : "",
    "objectName" : "",
    "startIndex" : 37,
    "stopIndex" : 37,
    "fragment" : "a"
  } ]
}


-- !query
select 3 as x, 4 as y
|> aggregate group by all
-- !query schema
struct<>
-- !query output
org.apache.spark.sql.catalyst.parser.ParseException
{
  "errorClass" : "UNSUPPORTED_FEATURE.PIPE_OPERATOR_AGGREGATE_UNSUPPORTED_CASE",
  "sqlState" : "0A000",
  "messageParameters" : {
    "case" : "GROUP BY ALL"
  },
  "queryContext" : [ {
    "objectType" : "",
    "objectName" : "",
    "startIndex" : 1,
    "stopIndex" : 47,
    "fragment" : "select 3 as x, 4 as y\n|> aggregate group by all"
  } ]
}


-- !query
table courseSales
|> aggregate sum(earnings) group by rollup(course, `year`)
|> where course = 'dotNET' and `year` = '2013'
-- !query schema
struct<>
-- !query output
org.apache.spark.sql.catalyst.parser.ParseException
{
  "errorClass" : "UNSUPPORTED_FEATURE.PIPE_OPERATOR_AGGREGATE_UNSUPPORTED_CASE",
  "sqlState" : "0A000",
  "messageParameters" : {
    "case" : "GROUP BY ROLLUP"
  },
  "queryContext" : [ {
    "objectType" : "",
    "objectName" : "",
    "startIndex" : 1,
    "stopIndex" : 123,
    "fragment" : "table courseSales\n|> aggregate sum(earnings) group by rollup(course, `year`)\n|> where course = 'dotNET' and `year` = '2013'"
  } ]
}


-- !query
table courseSales
|> aggregate sum(earnings) group by cube(course, `year`)
|> where course = 'dotNET' and `year` = '2013'
-- !query schema
struct<>
-- !query output
org.apache.spark.sql.catalyst.parser.ParseException
{
  "errorClass" : "UNSUPPORTED_FEATURE.PIPE_OPERATOR_AGGREGATE_UNSUPPORTED_CASE",
  "sqlState" : "0A000",
  "messageParameters" : {
    "case" : "GROUP BY CUBE"
  },
  "queryContext" : [ {
    "objectType" : "",
    "objectName" : "",
    "startIndex" : 1,
    "stopIndex" : 121,
    "fragment" : "table courseSales\n|> aggregate sum(earnings) group by cube(course, `year`)\n|> where course = 'dotNET' and `year` = '2013'"
  } ]
}


-- !query
table courseSales
|> aggregate sum(earnings) group by course, `year` grouping sets(course, `year`)
|> where course = 'dotNET' and `year` = '2013'
-- !query schema
struct<>
-- !query output
org.apache.spark.sql.catalyst.parser.ParseException
{
  "errorClass" : "UNSUPPORTED_FEATURE.PIPE_OPERATOR_AGGREGATE_UNSUPPORTED_CASE",
  "sqlState" : "0A000",
  "messageParameters" : {
    "case" : "GROUPING SETS"
  },
  "queryContext" : [ {
    "objectType" : "",
    "objectName" : "",
    "startIndex" : 1,
    "stopIndex" : 145,
    "fragment" : "table courseSales\n|> aggregate sum(earnings) group by course, `year` grouping sets(course, `year`)\n|> where course = 'dotNET' and `year` = '2013'"
  } ]
}


-- !query
table courseSales
|> aggregate sum(earnings), grouping(course) + 1
   group by course
|> where course = 'dotNET' and `year` = '2013'
-- !query schema
struct<>
-- !query output
org.apache.spark.sql.catalyst.parser.ParseException
{
  "errorClass" : "UNSUPPORTED_FEATURE.PIPE_OPERATOR_AGGREGATE_UNSUPPORTED_CASE",
  "sqlState" : "0A000",
  "messageParameters" : {
    "case" : "GROUPING"
  },
  "queryContext" : [ {
    "objectType" : "",
    "objectName" : "",
    "startIndex" : 1,
    "stopIndex" : 132,
    "fragment" : "table courseSales\n|> aggregate sum(earnings), grouping(course) + 1\n   group by course\n|> where course = 'dotNET' and `year` = '2013'"
  } ]
}


-- !query
table courseSales
|> aggregate sum(earnings), grouping_id(course)
   group by course
|> where course = 'dotNET' and `year` = '2013'
-- !query schema
struct<>
-- !query output
org.apache.spark.sql.catalyst.parser.ParseException
{
  "errorClass" : "UNSUPPORTED_FEATURE.PIPE_OPERATOR_AGGREGATE_UNSUPPORTED_CASE",
  "sqlState" : "0A000",
  "messageParameters" : {
    "case" : "GROUPING_ID"
  },
  "queryContext" : [ {
    "objectType" : "",
    "objectName" : "",
    "startIndex" : 1,
    "stopIndex" : 131,
    "fragment" : "table courseSales\n|> aggregate sum(earnings), grouping_id(course)\n   group by course\n|> where course = 'dotNET' and `year` = '2013'"
  } ]
}


-- !query
select 1 as x, 2 as y
|> aggregate group by ()
-- !query schema
struct<>
-- !query output
org.apache.spark.sql.catalyst.parser.ParseException
{
  "errorClass" : "PARSE_SYNTAX_ERROR",
  "sqlState" : "42601",
  "messageParameters" : {
    "error" : "')'",
    "hint" : ""
  }
}


-- !query
table other
|> aggregate a
-- !query schema
struct<>
-- !query output
org.apache.spark.sql.AnalysisException
{
  "errorClass" : "PIPE_OPERATOR_AGGREGATE_EXPRESSION_CONTAINS_NO_AGGREGATE_FUNCTION",
  "sqlState" : "0A000",
  "messageParameters" : {
    "expr" : "a#x"
  },
  "queryContext" : [ {
    "objectType" : "",
    "objectName" : "",
    "startIndex" : 26,
    "stopIndex" : 26,
    "fragment" : "a"
  } ]
}


-- !query
table other
|> select sum(a) as result
-- !query schema
struct<>
-- !query output
org.apache.spark.sql.AnalysisException
{
  "errorClass" : "PIPE_OPERATOR_CONTAINS_AGGREGATE_FUNCTION",
  "sqlState" : "0A000",
  "messageParameters" : {
    "clause" : "SELECT",
    "expr" : "sum(a#x)"
  },
  "queryContext" : [ {
    "objectType" : "",
    "objectName" : "",
    "startIndex" : 23,
    "stopIndex" : 28,
    "fragment" : "sum(a)"
  } ]
}


-- !query
table other
|> aggregate
-- !query schema
struct<>
-- !query output
org.apache.spark.sql.catalyst.parser.ParseException
{
  "errorClass" : "_LEGACY_ERROR_TEMP_0035",
  "messageParameters" : {
    "message" : "The AGGREGATE clause requires a list of aggregate expressions or a list of grouping expressions, or both"
  },
  "queryContext" : [ {
    "objectType" : "",
    "objectName" : "",
    "startIndex" : 1,
    "stopIndex" : 24,
    "fragment" : "table other\n|> aggregate"
  } ]
}


-- !query
table other
|> aggregate group by
-- !query schema
struct<>
-- !query output
org.apache.spark.sql.catalyst.ExtendedAnalysisException
{
  "errorClass" : "UNRESOLVED_COLUMN.WITH_SUGGESTION",
  "sqlState" : "42703",
  "messageParameters" : {
    "objectName" : "`group`",
    "proposal" : "`a`, `b`"
  },
  "queryContext" : [ {
    "objectType" : "",
    "objectName" : "",
    "startIndex" : 26,
    "stopIndex" : 30,
    "fragment" : "group"
  } ]
}


-- !query
table other
|> group by a
-- !query schema
struct<>
-- !query output
org.apache.spark.sql.catalyst.parser.ParseException
{
  "errorClass" : "PARSE_SYNTAX_ERROR",
  "sqlState" : "42601",
  "messageParameters" : {
    "error" : "'group'",
    "hint" : ""
  }
}


-- !query
table other
|> aggregate sum(a) over () group by b
-- !query schema
struct<>
-- !query output
org.apache.spark.sql.catalyst.parser.ParseException
{
  "errorClass" : "UNSUPPORTED_FEATURE.PIPE_OPERATOR_AGGREGATE_UNSUPPORTED_CASE",
  "sqlState" : "0A000",
  "messageParameters" : {
    "case" : "window functions; please update the query to move the window functions to a subsequent |> SELECT operator instead"
  },
  "queryContext" : [ {
    "objectType" : "",
    "objectName" : "",
    "startIndex" : 1,
    "stopIndex" : 50,
    "fragment" : "table other\n|> aggregate sum(a) over () group by b"
  } ]
}


-- !query
select 1 x, 2 y, 3 z
|> aggregate count(*) AS c, sum(x) AS x group by x
|> where c = 1
|> where x = 1
-- !query schema
struct<>
-- !query output
org.apache.spark.sql.AnalysisException
{
  "errorClass" : "AMBIGUOUS_REFERENCE",
  "sqlState" : "42704",
  "messageParameters" : {
    "name" : "`x`",
    "referenceNames" : "[`x`, `x`]"
  },
  "queryContext" : [ {
    "objectType" : "",
    "objectName" : "",
    "startIndex" : 97,
    "stopIndex" : 97,
    "fragment" : "x"
  } ]
}


-- !query
table windowTestData
|> select cate, sum(val) over w
   window w as (partition by cate order by val)
-- !query schema
struct<cate:string,sum(val) OVER (PARTITION BY cate ORDER BY val ASC NULLS FIRST RANGE BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW):bigint>
-- !query output
NULL	3
NULL	NULL
a	2
a	2
a	4
a	NULL
b	1
b	3
b	6


-- !query
table windowTestData
|> select cate, sum(val) over w
   window w as (order by val_timestamp range between unbounded preceding and current row)
-- !query schema
struct<cate:string,sum(val) OVER (ORDER BY val_timestamp ASC NULLS FIRST RANGE BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW):bigint>
-- !query output
NULL	5
NULL	NULL
a	13
a	5
a	5
a	6
b	13
b	5
b	8


-- !query
table windowTestData
|> select cate, val
    window w as (partition by cate order by val)
-- !query schema
struct<cate:string,val:int>
-- !query output
NULL	3
NULL	NULL
a	1
a	1
a	2
a	NULL
b	1
b	2
b	3


-- !query
table windowTestData
|> select cate, val, sum(val) over w as sum_val
   window w as (partition by cate)
|> select cate, val, sum_val, first_value(cate) over w
   window w as (order by val)
-- !query schema
struct<cate:string,val:int,sum_val:bigint,first_value(cate) OVER (ORDER BY val ASC NULLS FIRST RANGE BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW):string>
-- !query output
NULL	3	3	a
NULL	NULL	3	a
a	1	4	a
a	1	4	a
a	2	4	a
a	NULL	4	a
b	1	6	a
b	2	6	a
b	3	6	a


-- !query
table windowTestData
|> select cate, val, sum(val) over w1, first_value(cate) over w2
   window w1 as (partition by cate), w2 as (order by val)
-- !query schema
struct<cate:string,val:int,sum(val) OVER (PARTITION BY cate ROWS BETWEEN UNBOUNDED PRECEDING AND UNBOUNDED FOLLOWING):bigint,first_value(cate) OVER (ORDER BY val ASC NULLS FIRST RANGE BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW):string>
-- !query output
NULL	3	3	a
NULL	NULL	3	a
a	1	4	a
a	1	4	a
a	2	4	a
a	NULL	4	a
b	1	6	a
b	2	6	a
b	3	6	a


-- !query
table windowTestData
|> select cate, val, sum(val) over w, first_value(val) over w
   window w1 as (partition by cate order by val)
-- !query schema
struct<>
-- !query output
org.apache.spark.sql.AnalysisException
{
  "errorClass" : "MISSING_WINDOW_SPECIFICATION",
  "sqlState" : "42P20",
  "messageParameters" : {
    "docroot" : "https://spark.apache.org/docs/latest",
    "windowName" : "w"
  }
}


-- !query
(select col from st)
|> select col.i1, sum(col.i2) over w
   window w as (partition by col.i1 order by col.i2)
-- !query schema
struct<i1:int,sum(col.i2) OVER (PARTITION BY col.i1 ORDER BY col.i2 ASC NULLS FIRST RANGE BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW):bigint>
-- !query output
2	3


-- !query
table st
|> select st.col.i1, sum(st.col.i2) over w
   window w as (partition by st.col.i1 order by st.col.i2)
-- !query schema
struct<i1:int,sum(col.i2) OVER (PARTITION BY col.i1 ORDER BY col.i2 ASC NULLS FIRST RANGE BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW):bigint>
-- !query output
2	3


-- !query
table st
|> select spark_catalog.default.st.col.i1, sum(spark_catalog.default.st.col.i2) over w
   window w as (partition by spark_catalog.default.st.col.i1 order by spark_catalog.default.st.col.i2)
-- !query schema
struct<i1:int,sum(col.i2) OVER (PARTITION BY col.i1 ORDER BY col.i2 ASC NULLS FIRST RANGE BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW):bigint>
-- !query output
2	3


-- !query
table windowTestData
|> select cate, sum(val) over val
   window val as (partition by cate order by val)
-- !query schema
struct<cate:string,sum(val) OVER (PARTITION BY cate ORDER BY val ASC NULLS FIRST RANGE BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW):bigint>
-- !query output
NULL	3
NULL	NULL
a	2
a	2
a	4
a	NULL
b	1
b	3
b	6


-- !query
table windowTestData
|> select cate, sum(val) over w
-- !query schema
struct<>
-- !query output
org.apache.spark.sql.catalyst.ExtendedAnalysisException
{
  "errorClass" : "MISSING_WINDOW_SPECIFICATION",
  "sqlState" : "42P20",
  "messageParameters" : {
    "docroot" : "https://spark.apache.org/docs/latest",
    "windowName" : "w"
  }
}


-- !query
table windowTestData
|> select cate, val, sum(val) over w1, first_value(cate) over w2
   window w1 as (partition by cate)
   window w2 as (order by val)
-- !query schema
struct<>
-- !query output
org.apache.spark.sql.AnalysisException
{
  "errorClass" : "MISSING_WINDOW_SPECIFICATION",
  "sqlState" : "42P20",
  "messageParameters" : {
    "docroot" : "https://spark.apache.org/docs/latest",
    "windowName" : "w2"
  }
}


-- !query
table windowTestData
|> select cate, val, sum(val) over w as sum_val
   window w as (partition by cate order by val)
|> select cate, val, sum_val, first_value(cate) over w
-- !query schema
struct<>
-- !query output
org.apache.spark.sql.catalyst.ExtendedAnalysisException
{
  "errorClass" : "MISSING_WINDOW_SPECIFICATION",
  "sqlState" : "42P20",
  "messageParameters" : {
    "docroot" : "https://spark.apache.org/docs/latest",
    "windowName" : "w"
  }
}


-- !query
table windowTestData
|> select cate, val, first_value(cate) over w as first_val
|> select cate, val, sum(val) over w as sum_val
   window w as (order by val)
-- !query schema
struct<>
-- !query output
org.apache.spark.sql.catalyst.ExtendedAnalysisException
{
  "errorClass" : "MISSING_WINDOW_SPECIFICATION",
  "sqlState" : "42P20",
  "messageParameters" : {
    "docroot" : "https://spark.apache.org/docs/latest",
    "windowName" : "w"
  }
}


-- !query
with customer_total_return as
(select
    sr_customer_sk as ctr_customer_sk,
    sr_store_sk as ctr_store_sk,
    sum(sr_return_amt) as ctr_total_return
  from store_returns, date_dim
  where sr_returned_date_sk = d_date_sk and d_year = 2000
  group by sr_customer_sk, sr_store_sk)
select c_customer_id
from customer_total_return ctr1, store, customer
where ctr1.ctr_total_return >
  (select avg(ctr_total_return) * 1.2
  from customer_total_return ctr2
  where ctr1.ctr_store_sk = ctr2.ctr_store_sk)
  and s_store_sk = ctr1.ctr_store_sk
  and s_state = 'tn'
  and ctr1.ctr_customer_sk = c_customer_sk
order by c_customer_id
limit 100
-- !query schema
struct<c_customer_id:string>
-- !query output



-- !query
with customer_total_return as
  (from store_returns
  |> join date_dim
  |> where sr_returned_date_sk = d_date_sk and d_year = 2000
  |> aggregate sum(sr_return_amt) as ctr_total_return
       group by sr_customer_sk as ctr_customer_sk, sr_store_sk as ctr_store_sk)
from customer_total_return ctr1
|> join store
|> join customer
|> where ctr1.ctr_total_return >
     (table customer_total_return
      |> as ctr2
      |> where ctr1.ctr_store_sk = ctr2.ctr_store_sk
      |> aggregate avg(ctr_total_return) * 1.2)
     and s_store_sk = ctr1.ctr_store_sk
     and s_state = 'tn'
     and ctr1.ctr_customer_sk = c_customer_sk
|> order by c_customer_id
|> limit 100
|> select c_customer_id
-- !query schema
struct<c_customer_id:string>
-- !query output



-- !query
with wscs as
( select
    sold_date_sk,
    sales_price
  from (select
    ws_sold_date_sk sold_date_sk,
    ws_ext_sales_price sales_price
  from web_sales) x
  union all
  (select
    cs_sold_date_sk sold_date_sk,
    cs_ext_sales_price sales_price
  from catalog_sales)),
    wswscs as
  ( select
    d_week_seq,
    sum(case when (d_day_name = 'sunday')
      then sales_price
        else null end)
    sun_sales,
    sum(case when (d_day_name = 'monday')
      then sales_price
        else null end)
    mon_sales,
    sum(case when (d_day_name = 'tuesday')
      then sales_price
        else null end)
    tue_sales,
    sum(case when (d_day_name = 'wednesday')
      then sales_price
        else null end)
    wed_sales,
    sum(case when (d_day_name = 'thursday')
      then sales_price
        else null end)
    thu_sales,
    sum(case when (d_day_name = 'friday')
      then sales_price
        else null end)
    fri_sales,
    sum(case when (d_day_name = 'saturday')
      then sales_price
        else null end)
    sat_sales
  from wscs, date_dim
  where d_date_sk = sold_date_sk
  group by d_week_seq)
select
  d_week_seq1,
  round(sun_sales1 / sun_sales2, 2),
  round(mon_sales1 / mon_sales2, 2),
  round(tue_sales1 / tue_sales2, 2),
  round(wed_sales1 / wed_sales2, 2),
  round(thu_sales1 / thu_sales2, 2),
  round(fri_sales1 / fri_sales2, 2),
  round(sat_sales1 / sat_sales2, 2)
from
  (select
    wswscs.d_week_seq d_week_seq1,
    sun_sales sun_sales1,
    mon_sales mon_sales1,
    tue_sales tue_sales1,
    wed_sales wed_sales1,
    thu_sales thu_sales1,
    fri_sales fri_sales1,
    sat_sales sat_sales1
  from wswscs, date_dim
  where date_dim.d_week_seq = wswscs.d_week_seq and d_year = 2001) y,
  (select
    wswscs.d_week_seq d_week_seq2,
    sun_sales sun_sales2,
    mon_sales mon_sales2,
    tue_sales tue_sales2,
    wed_sales wed_sales2,
    thu_sales thu_sales2,
    fri_sales fri_sales2,
    sat_sales sat_sales2
  from wswscs, date_dim
  where date_dim.d_week_seq = wswscs.d_week_seq and d_year = 2001 + 1) z
where d_week_seq1 = d_week_seq2 - 53
order by d_week_seq1
-- !query schema
struct<d_week_seq1:int,round((sun_sales1 / sun_sales2), 2):decimal(20,2),round((mon_sales1 / mon_sales2), 2):decimal(20,2),round((tue_sales1 / tue_sales2), 2):decimal(20,2),round((wed_sales1 / wed_sales2), 2):decimal(20,2),round((thu_sales1 / thu_sales2), 2):decimal(20,2),round((fri_sales1 / fri_sales2), 2):decimal(20,2),round((sat_sales1 / sat_sales2), 2):decimal(20,2)>
-- !query output



-- !query
with wscs as
  (table web_sales
  |> select
       ws_sold_date_sk sold_date_sk,
       ws_ext_sales_price sales_price
  |> as x
  |> union all (
       table catalog_sales
       |> select
            cs_sold_date_sk sold_date_sk,
            cs_ext_sales_price sales_price)
  |> select
       sold_date_sk,
       sales_price),
wswscs as
  (table wscs
  |> join date_dim
  |> where d_date_sk = sold_date_sk
  |> aggregate
      sum(case when (d_day_name = 'sunday')
        then sales_price
          else null end)
      sun_sales,
      sum(case when (d_day_name = 'monday')
        then sales_price
          else null end)
      mon_sales,
      sum(case when (d_day_name = 'tuesday')
        then sales_price
          else null end)
      tue_sales,
      sum(case when (d_day_name = 'wednesday')
        then sales_price
          else null end)
      wed_sales,
      sum(case when (d_day_name = 'thursday')
        then sales_price
          else null end)
      thu_sales,
      sum(case when (d_day_name = 'friday')
        then sales_price
          else null end)
      fri_sales,
      sum(case when (d_day_name = 'saturday')
        then sales_price
          else null end)
      sat_sales
      group by d_week_seq)
table wswscs
|> join date_dim
|> where date_dim.d_week_seq = wswscs.d_week_seq AND d_year = 2001
|> select
     wswscs.d_week_seq d_week_seq1,
     sun_sales sun_sales1,
     mon_sales mon_sales1,
     tue_sales tue_sales1,
     wed_sales wed_sales1,
     thu_sales thu_sales1,
     fri_sales fri_sales1,
     sat_sales sat_sales1
|> as y
|> join (
     table wswscs
     |> join date_dim
     |> where date_dim.d_week_seq = wswscs.d_week_seq AND d_year = 2001 + 1
     |> select
          wswscs.d_week_seq d_week_seq2,
          sun_sales sun_sales2,
          mon_sales mon_sales2,
          tue_sales tue_sales2,
          wed_sales wed_sales2,
          thu_sales thu_sales2,
          fri_sales fri_sales2,
          sat_sales sat_sales2
     |> as z)
|> where d_week_seq1 = d_week_seq2 - 53
|> order by d_week_seq1
|> select
     d_week_seq1,
     round(sun_sales1 / sun_sales2, 2),
     round(mon_sales1 / mon_sales2, 2),
     round(tue_sales1 / tue_sales2, 2),
     round(wed_sales1 / wed_sales2, 2),
     round(thu_sales1 / thu_sales2, 2),
     round(fri_sales1 / fri_sales2, 2),
     round(sat_sales1 / sat_sales2, 2)
-- !query schema
struct<d_week_seq1:int,round((sun_sales1 / sun_sales2), 2):decimal(20,2),round((mon_sales1 / mon_sales2), 2):decimal(20,2),round((tue_sales1 / tue_sales2), 2):decimal(20,2),round((wed_sales1 / wed_sales2), 2):decimal(20,2),round((thu_sales1 / thu_sales2), 2):decimal(20,2),round((fri_sales1 / fri_sales2), 2):decimal(20,2),round((sat_sales1 / sat_sales2), 2):decimal(20,2)>
-- !query output



-- !query
select
  dt.d_year,
  item.i_brand_id brand_id,
  item.i_brand brand,
  sum(ss_ext_sales_price) sum_agg
from date_dim dt, store_sales, item
where dt.d_date_sk = store_sales.ss_sold_date_sk
  and store_sales.ss_item_sk = item.i_item_sk
  and item.i_manufact_id = 128
  and dt.d_moy = 11
group by dt.d_year, item.i_brand, item.i_brand_id
order by dt.d_year, sum_agg desc, brand_id
limit 100
-- !query schema
struct<d_year:int,brand_id:int,brand:string,sum_agg:decimal(17,2)>
-- !query output



-- !query
table date_dim
|> as dt
|> join store_sales
|> join item
|> where dt.d_date_sk = store_sales.ss_sold_date_sk
     and store_sales.ss_item_sk = item.i_item_sk
     and item.i_manufact_id = 128
     and dt.d_moy = 11
|> aggregate sum(ss_ext_sales_price) sum_agg
     group by dt.d_year d_year, item.i_brand_id brand_id, item.i_brand brand
|> order by d_year, sum_agg desc, brand_id
|> limit 100
-- !query schema
struct<d_year:int,brand_id:int,brand:string,sum_agg:decimal(17,2)>
-- !query output



-- !query
select
  i_item_desc,
  i_category,
  i_class,
  i_current_price,
  sum(ws_ext_sales_price) as itemrevenue,
  sum(ws_ext_sales_price) * 100 / sum(sum(ws_ext_sales_price))
  over
  (partition by i_class) as revenueratio
from
  web_sales, item, date_dim
where
  ws_item_sk = i_item_sk
    and i_category in ('sports', 'books', 'home')
    and ws_sold_date_sk = d_date_sk
    and d_date between cast('1999-02-22' as date)
  and (cast('1999-02-22' as date) + interval 30 days)
group by
  i_item_id, i_item_desc, i_category, i_class, i_current_price
order by
  i_category, i_class, i_item_id, i_item_desc, revenueratio
limit 100
-- !query schema
struct<i_item_desc:string,i_category:string,i_class:string,i_current_price:decimal(7,2),itemrevenue:decimal(17,2),revenueratio:decimal(38,17)>
-- !query output



-- !query
table web_sales
|> join item
|> join date_dim
|> where ws_item_sk = i_item_sk
     and i_category in ('sports', 'books', 'home')
     and ws_sold_date_sk = d_date_sk
     and d_date between cast('1999-02-22' as date)
     and (cast('1999-02-22' as date) + interval 30 days)
|> aggregate sum(ws_ext_sales_price) AS itemrevenue
     group by i_item_id, i_item_desc, i_category, i_class, i_current_price
|> extend
     itemrevenue * 100 / sum(itemrevenue)
       over (partition by i_class) as revenueratio
|> order by i_category, i_class, i_item_id, i_item_desc, revenueratio
|> select i_item_desc, i_category, i_class, i_current_price, itemrevenue, revenueratio
|> limit 100
-- !query schema
struct<i_item_desc:string,i_category:string,i_class:string,i_current_price:decimal(7,2),itemrevenue:decimal(17,2),revenueratio:decimal(38,17)>
-- !query output



-- !query
select
  asceding.rnk,
  i1.i_product_name best_performing,
  i2.i_product_name worst_performing
from (select *
from (select
  item_sk,
  rank()
  over (
    order by rank_col asc) rnk
from (select
  ss_item_sk item_sk,
  avg(ss_net_profit) rank_col
from store_sales ss1
where ss_store_sk = 4
group by ss_item_sk
having avg(ss_net_profit) > 0.9 * (select avg(ss_net_profit) rank_col
from store_sales
where ss_store_sk = 4
  and ss_addr_sk is null
group by ss_store_sk)) v1) v11
where rnk < 11) asceding,
  (select *
  from (select
    item_sk,
    rank()
    over (
      order by rank_col desc) rnk
  from (select
    ss_item_sk item_sk,
    avg(ss_net_profit) rank_col
  from store_sales ss1
  where ss_store_sk = 4
  group by ss_item_sk
  having avg(ss_net_profit) > 0.9 * (select avg(ss_net_profit) rank_col
  from store_sales
  where ss_store_sk = 4
    and ss_addr_sk is null
  group by ss_store_sk)) v2) v21
  where rnk < 11) descending,
  item i1, item i2
where asceding.rnk = descending.rnk
  and i1.i_item_sk = asceding.item_sk
  and i2.i_item_sk = descending.item_sk
order by asceding.rnk
limit 100
-- !query schema
struct<rnk:int,best_performing:string,worst_performing:string>
-- !query output



-- !query
from store_sales ss1
|> where ss_store_sk = 4
|> aggregate avg(ss_net_profit) rank_col
     group by ss_item_sk as item_sk
|> where rank_col > 0.9 * (
     from store_sales
     |> where ss_store_sk = 4
          and ss_addr_sk is null
     |> aggregate avg(ss_net_profit) rank_col
          group by ss_store_sk
     |> select rank_col)
|> as v1
|> select
     item_sk,
     rank() over (
       order by rank_col asc) rnk
|> as v11
|> where rnk < 11
|> as asceding
|> join (
     from store_sales ss1
     |> where ss_store_sk = 4
     |> aggregate avg(ss_net_profit) rank_col
          group by ss_item_sk as item_sk
     |> where rank_col > 0.9 * (
          table store_sales
          |> where ss_store_sk = 4
               and ss_addr_sk is null
          |> aggregate avg(ss_net_profit) rank_col
               group by ss_store_sk
          |> select rank_col)
     |> as v2
     |> select
          item_sk,
          rank() over (
            order by rank_col asc) rnk
     |> as v21
     |> where rnk < 11) descending
|> join item i1
|> join item i2
|> where asceding.rnk = descending.rnk
     and i1.i_item_sk = asceding.item_sk
     and i2.i_item_sk = descending.item_sk
|> order by asceding.rnk
|> select
     asceding.rnk,
     i1.i_product_name best_performing,
     i2.i_product_name worst_performing
-- !query schema
struct<rnk:int,best_performing:string,worst_performing:string>
-- !query output



-- !query
with web_v1 as (
  select
    ws_item_sk item_sk,
    d_date,
    sum(sum(ws_sales_price))
    over (partition by ws_item_sk
      order by d_date
      rows between unbounded preceding and current row) cume_sales
  from web_sales, date_dim
  where ws_sold_date_sk = d_date_sk
    and d_month_seq between 1200 and 1200 + 11
    and ws_item_sk is not null
  group by ws_item_sk, d_date),
    store_v1 as (
    select
      ss_item_sk item_sk,
      d_date,
      sum(sum(ss_sales_price))
      over (partition by ss_item_sk
        order by d_date
        rows between unbounded preceding and current row) cume_sales
    from store_sales, date_dim
    where ss_sold_date_sk = d_date_sk
      and d_month_seq between 1200 and 1200 + 11
      and ss_item_sk is not null
    group by ss_item_sk, d_date)
select *
from (select
  item_sk,
  d_date,
  web_sales,
  store_sales,
  max(web_sales)
  over (partition by item_sk
    order by d_date
    rows between unbounded preceding and current row) web_cumulative,
  max(store_sales)
  over (partition by item_sk
    order by d_date
    rows between unbounded preceding and current row) store_cumulative
from (select
  case when web.item_sk is not null
    then web.item_sk
  else store.item_sk end item_sk,
  case when web.d_date is not null
    then web.d_date
  else store.d_date end d_date,
  web.cume_sales web_sales,
  store.cume_sales store_sales
from web_v1 web full outer join store_v1 store on (web.item_sk = store.item_sk
  and web.d_date = store.d_date)
     ) x) y
where web_cumulative > store_cumulative
order by item_sk, d_date
limit 100
-- !query schema
struct<item_sk:int,d_date:date,web_sales:decimal(27,2),store_sales:decimal(27,2),web_cumulative:decimal(27,2),store_cumulative:decimal(27,2)>
-- !query output



-- !query
with web_v1 as (
  table web_sales
  |> join date_dim
  |> where ws_sold_date_sk = d_date_sk
       and d_month_seq between 1200 and 1200 + 11
       and ws_item_sk is not null
  |> aggregate sum(ws_sales_price) as sum_ws_sales_price
       group by ws_item_sk as item_sk, d_date
  |> extend sum(sum_ws_sales_price)
       over (partition by item_sk
         order by d_date
         rows between unbounded preceding and current row)
       as cume_sales),
store_v1 as (
  table store_sales
  |> join date_dim
  |> where ss_sold_date_sk = d_date_sk
       and d_month_seq between 1200 and 1200 + 11
       and ss_item_sk is not null
  |> aggregate sum(ss_sales_price) as sum_ss_sales_price
       group by ss_item_sk as item_sk, d_date
  |> extend sum(sum_ss_sales_price)
       over (partition by item_sk
           order by d_date
           rows between unbounded preceding and current row)
       as cume_sales)
table web_v1
|> as web
|> full outer join store_v1 store
     on (web.item_sk = store.item_sk and web.d_date = store.d_date)
|> select
     case when web.item_sk is not null
       then web.item_sk
       else store.item_sk end item_sk,
     case when web.d_date is not null
       then web.d_date
       else store.d_date end d_date,
     web.cume_sales web_sales,
     store.cume_sales store_sales
|> as x
|> select
     item_sk,
     d_date,
     web_sales,
     store_sales,
     max(web_sales)
       over (partition by item_sk
         order by d_date
         rows between unbounded preceding and current row) web_cumulative,
     max(store_sales)
       over (partition by item_sk
         order by d_date
         rows between unbounded preceding and current row) store_cumulative
|> as y
|> where web_cumulative > store_cumulative
|> order by item_sk, d_date
|> limit 100
-- !query schema
struct<item_sk:int,d_date:date,web_sales:decimal(27,2),store_sales:decimal(27,2),web_cumulative:decimal(27,2),store_cumulative:decimal(27,2)>
-- !query output



-- !query
drop table t
-- !query schema
struct<>
-- !query output



-- !query
drop table other
-- !query schema
struct<>
-- !query output



-- !query
drop table st
-- !query schema
struct<>
-- !query output

