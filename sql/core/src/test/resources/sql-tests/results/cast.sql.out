-- Automatically generated by SQLQueryTestSuite
-- !query
SELECT CAST('1.23' AS int)
-- !query schema
struct<>
-- !query output
org.apache.spark.SparkNumberFormatException
{
  "errorClass" : "CAST_INVALID_INPUT",
  "sqlState" : "22018",
  "messageParameters" : {
    "ansiConfig" : "\"spark.sql.ansi.enabled\"",
    "expression" : "'1.23'",
    "sourceType" : "\"STRING\"",
    "targetType" : "\"INT\""
  },
  "queryContext" : [ {
    "objectType" : "",
    "objectName" : "",
    "startIndex" : 8,
    "stopIndex" : 26,
    "fragment" : "CAST('1.23' AS int)"
  } ]
}


-- !query
SELECT CAST('1.23' AS long)
-- !query schema
struct<>
-- !query output
org.apache.spark.SparkNumberFormatException
{
  "errorClass" : "CAST_INVALID_INPUT",
  "sqlState" : "22018",
  "messageParameters" : {
    "ansiConfig" : "\"spark.sql.ansi.enabled\"",
    "expression" : "'1.23'",
    "sourceType" : "\"STRING\"",
    "targetType" : "\"BIGINT\""
  },
  "queryContext" : [ {
    "objectType" : "",
    "objectName" : "",
    "startIndex" : 8,
    "stopIndex" : 27,
    "fragment" : "CAST('1.23' AS long)"
  } ]
}


-- !query
SELECT CAST('-4.56' AS int)
-- !query schema
struct<>
-- !query output
org.apache.spark.SparkNumberFormatException
{
  "errorClass" : "CAST_INVALID_INPUT",
  "sqlState" : "22018",
  "messageParameters" : {
    "ansiConfig" : "\"spark.sql.ansi.enabled\"",
    "expression" : "'-4.56'",
    "sourceType" : "\"STRING\"",
    "targetType" : "\"INT\""
  },
  "queryContext" : [ {
    "objectType" : "",
    "objectName" : "",
    "startIndex" : 8,
    "stopIndex" : 27,
    "fragment" : "CAST('-4.56' AS int)"
  } ]
}


-- !query
SELECT CAST('-4.56' AS long)
-- !query schema
struct<>
-- !query output
org.apache.spark.SparkNumberFormatException
{
  "errorClass" : "CAST_INVALID_INPUT",
  "sqlState" : "22018",
  "messageParameters" : {
    "ansiConfig" : "\"spark.sql.ansi.enabled\"",
    "expression" : "'-4.56'",
    "sourceType" : "\"STRING\"",
    "targetType" : "\"BIGINT\""
  },
  "queryContext" : [ {
    "objectType" : "",
    "objectName" : "",
    "startIndex" : 8,
    "stopIndex" : 28,
    "fragment" : "CAST('-4.56' AS long)"
  } ]
}


-- !query
SELECT CAST('abc' AS int)
-- !query schema
struct<>
-- !query output
org.apache.spark.SparkNumberFormatException
{
  "errorClass" : "CAST_INVALID_INPUT",
  "sqlState" : "22018",
  "messageParameters" : {
    "ansiConfig" : "\"spark.sql.ansi.enabled\"",
    "expression" : "'abc'",
    "sourceType" : "\"STRING\"",
    "targetType" : "\"INT\""
  },
  "queryContext" : [ {
    "objectType" : "",
    "objectName" : "",
    "startIndex" : 8,
    "stopIndex" : 25,
    "fragment" : "CAST('abc' AS int)"
  } ]
}


-- !query
SELECT CAST('abc' AS long)
-- !query schema
struct<>
-- !query output
org.apache.spark.SparkNumberFormatException
{
  "errorClass" : "CAST_INVALID_INPUT",
  "sqlState" : "22018",
  "messageParameters" : {
    "ansiConfig" : "\"spark.sql.ansi.enabled\"",
    "expression" : "'abc'",
    "sourceType" : "\"STRING\"",
    "targetType" : "\"BIGINT\""
  },
  "queryContext" : [ {
    "objectType" : "",
    "objectName" : "",
    "startIndex" : 8,
    "stopIndex" : 26,
    "fragment" : "CAST('abc' AS long)"
  } ]
}


-- !query
SELECT CAST('abc' AS float)
-- !query schema
struct<>
-- !query output
org.apache.spark.SparkNumberFormatException
{
  "errorClass" : "CAST_INVALID_INPUT",
  "sqlState" : "22018",
  "messageParameters" : {
    "ansiConfig" : "\"spark.sql.ansi.enabled\"",
    "expression" : "'abc'",
    "sourceType" : "\"STRING\"",
    "targetType" : "\"FLOAT\""
  },
  "queryContext" : [ {
    "objectType" : "",
    "objectName" : "",
    "startIndex" : 8,
    "stopIndex" : 27,
    "fragment" : "CAST('abc' AS float)"
  } ]
}


-- !query
SELECT CAST('abc' AS double)
-- !query schema
struct<>
-- !query output
org.apache.spark.SparkNumberFormatException
{
  "errorClass" : "CAST_INVALID_INPUT",
  "sqlState" : "22018",
  "messageParameters" : {
    "ansiConfig" : "\"spark.sql.ansi.enabled\"",
    "expression" : "'abc'",
    "sourceType" : "\"STRING\"",
    "targetType" : "\"DOUBLE\""
  },
  "queryContext" : [ {
    "objectType" : "",
    "objectName" : "",
    "startIndex" : 8,
    "stopIndex" : 28,
    "fragment" : "CAST('abc' AS double)"
  } ]
}


-- !query
SELECT CAST('1234567890123' AS int)
-- !query schema
struct<>
-- !query output
org.apache.spark.SparkNumberFormatException
{
  "errorClass" : "CAST_INVALID_INPUT",
  "sqlState" : "22018",
  "messageParameters" : {
    "ansiConfig" : "\"spark.sql.ansi.enabled\"",
    "expression" : "'1234567890123'",
    "sourceType" : "\"STRING\"",
    "targetType" : "\"INT\""
  },
  "queryContext" : [ {
    "objectType" : "",
    "objectName" : "",
    "startIndex" : 8,
    "stopIndex" : 35,
    "fragment" : "CAST('1234567890123' AS int)"
  } ]
}


-- !query
SELECT CAST('12345678901234567890123' AS long)
-- !query schema
struct<>
-- !query output
org.apache.spark.SparkNumberFormatException
{
  "errorClass" : "CAST_INVALID_INPUT",
  "sqlState" : "22018",
  "messageParameters" : {
    "ansiConfig" : "\"spark.sql.ansi.enabled\"",
    "expression" : "'12345678901234567890123'",
    "sourceType" : "\"STRING\"",
    "targetType" : "\"BIGINT\""
  },
  "queryContext" : [ {
    "objectType" : "",
    "objectName" : "",
    "startIndex" : 8,
    "stopIndex" : 46,
    "fragment" : "CAST('12345678901234567890123' AS long)"
  } ]
}


-- !query
SELECT CAST('' AS int)
-- !query schema
struct<>
-- !query output
org.apache.spark.SparkNumberFormatException
{
  "errorClass" : "CAST_INVALID_INPUT",
  "sqlState" : "22018",
  "messageParameters" : {
    "ansiConfig" : "\"spark.sql.ansi.enabled\"",
    "expression" : "''",
    "sourceType" : "\"STRING\"",
    "targetType" : "\"INT\""
  },
  "queryContext" : [ {
    "objectType" : "",
    "objectName" : "",
    "startIndex" : 8,
    "stopIndex" : 22,
    "fragment" : "CAST('' AS int)"
  } ]
}


-- !query
SELECT CAST('' AS long)
-- !query schema
struct<>
-- !query output
org.apache.spark.SparkNumberFormatException
{
  "errorClass" : "CAST_INVALID_INPUT",
  "sqlState" : "22018",
  "messageParameters" : {
    "ansiConfig" : "\"spark.sql.ansi.enabled\"",
    "expression" : "''",
    "sourceType" : "\"STRING\"",
    "targetType" : "\"BIGINT\""
  },
  "queryContext" : [ {
    "objectType" : "",
    "objectName" : "",
    "startIndex" : 8,
    "stopIndex" : 23,
    "fragment" : "CAST('' AS long)"
  } ]
}


-- !query
SELECT CAST('' AS float)
-- !query schema
struct<>
-- !query output
org.apache.spark.SparkNumberFormatException
{
  "errorClass" : "CAST_INVALID_INPUT",
  "sqlState" : "22018",
  "messageParameters" : {
    "ansiConfig" : "\"spark.sql.ansi.enabled\"",
    "expression" : "''",
    "sourceType" : "\"STRING\"",
    "targetType" : "\"FLOAT\""
  },
  "queryContext" : [ {
    "objectType" : "",
    "objectName" : "",
    "startIndex" : 8,
    "stopIndex" : 24,
    "fragment" : "CAST('' AS float)"
  } ]
}


-- !query
SELECT CAST('' AS double)
-- !query schema
struct<>
-- !query output
org.apache.spark.SparkNumberFormatException
{
  "errorClass" : "CAST_INVALID_INPUT",
  "sqlState" : "22018",
  "messageParameters" : {
    "ansiConfig" : "\"spark.sql.ansi.enabled\"",
    "expression" : "''",
    "sourceType" : "\"STRING\"",
    "targetType" : "\"DOUBLE\""
  },
  "queryContext" : [ {
    "objectType" : "",
    "objectName" : "",
    "startIndex" : 8,
    "stopIndex" : 25,
    "fragment" : "CAST('' AS double)"
  } ]
}


-- !query
SELECT CAST(NULL AS int)
-- !query schema
struct<CAST(NULL AS INT):int>
-- !query output
NULL


-- !query
SELECT CAST(NULL AS long)
-- !query schema
struct<CAST(NULL AS BIGINT):bigint>
-- !query output
NULL


-- !query
SELECT CAST('123.a' AS int)
-- !query schema
struct<>
-- !query output
org.apache.spark.SparkNumberFormatException
{
  "errorClass" : "CAST_INVALID_INPUT",
  "sqlState" : "22018",
  "messageParameters" : {
    "ansiConfig" : "\"spark.sql.ansi.enabled\"",
    "expression" : "'123.a'",
    "sourceType" : "\"STRING\"",
    "targetType" : "\"INT\""
  },
  "queryContext" : [ {
    "objectType" : "",
    "objectName" : "",
    "startIndex" : 8,
    "stopIndex" : 27,
    "fragment" : "CAST('123.a' AS int)"
  } ]
}


-- !query
SELECT CAST('123.a' AS long)
-- !query schema
struct<>
-- !query output
org.apache.spark.SparkNumberFormatException
{
  "errorClass" : "CAST_INVALID_INPUT",
  "sqlState" : "22018",
  "messageParameters" : {
    "ansiConfig" : "\"spark.sql.ansi.enabled\"",
    "expression" : "'123.a'",
    "sourceType" : "\"STRING\"",
    "targetType" : "\"BIGINT\""
  },
  "queryContext" : [ {
    "objectType" : "",
    "objectName" : "",
    "startIndex" : 8,
    "stopIndex" : 28,
    "fragment" : "CAST('123.a' AS long)"
  } ]
}


-- !query
SELECT CAST('123.a' AS float)
-- !query schema
struct<>
-- !query output
org.apache.spark.SparkNumberFormatException
{
  "errorClass" : "CAST_INVALID_INPUT",
  "sqlState" : "22018",
  "messageParameters" : {
    "ansiConfig" : "\"spark.sql.ansi.enabled\"",
    "expression" : "'123.a'",
    "sourceType" : "\"STRING\"",
    "targetType" : "\"FLOAT\""
  },
  "queryContext" : [ {
    "objectType" : "",
    "objectName" : "",
    "startIndex" : 8,
    "stopIndex" : 29,
    "fragment" : "CAST('123.a' AS float)"
  } ]
}


-- !query
SELECT CAST('123.a' AS double)
-- !query schema
struct<>
-- !query output
org.apache.spark.SparkNumberFormatException
{
  "errorClass" : "CAST_INVALID_INPUT",
  "sqlState" : "22018",
  "messageParameters" : {
    "ansiConfig" : "\"spark.sql.ansi.enabled\"",
    "expression" : "'123.a'",
    "sourceType" : "\"STRING\"",
    "targetType" : "\"DOUBLE\""
  },
  "queryContext" : [ {
    "objectType" : "",
    "objectName" : "",
    "startIndex" : 8,
    "stopIndex" : 30,
    "fragment" : "CAST('123.a' AS double)"
  } ]
}


-- !query
SELECT CAST('-2147483648' AS int)
-- !query schema
struct<CAST(-2147483648 AS INT):int>
-- !query output
-2147483648


-- !query
SELECT CAST('-2147483649' AS int)
-- !query schema
struct<>
-- !query output
org.apache.spark.SparkNumberFormatException
{
  "errorClass" : "CAST_INVALID_INPUT",
  "sqlState" : "22018",
  "messageParameters" : {
    "ansiConfig" : "\"spark.sql.ansi.enabled\"",
    "expression" : "'-2147483649'",
    "sourceType" : "\"STRING\"",
    "targetType" : "\"INT\""
  },
  "queryContext" : [ {
    "objectType" : "",
    "objectName" : "",
    "startIndex" : 8,
    "stopIndex" : 33,
    "fragment" : "CAST('-2147483649' AS int)"
  } ]
}


-- !query
SELECT CAST('2147483647' AS int)
-- !query schema
struct<CAST(2147483647 AS INT):int>
-- !query output
2147483647


-- !query
SELECT CAST('2147483648' AS int)
-- !query schema
struct<>
-- !query output
org.apache.spark.SparkNumberFormatException
{
  "errorClass" : "CAST_INVALID_INPUT",
  "sqlState" : "22018",
  "messageParameters" : {
    "ansiConfig" : "\"spark.sql.ansi.enabled\"",
    "expression" : "'2147483648'",
    "sourceType" : "\"STRING\"",
    "targetType" : "\"INT\""
  },
  "queryContext" : [ {
    "objectType" : "",
    "objectName" : "",
    "startIndex" : 8,
    "stopIndex" : 32,
    "fragment" : "CAST('2147483648' AS int)"
  } ]
}


-- !query
SELECT CAST('-9223372036854775808' AS long)
-- !query schema
struct<CAST(-9223372036854775808 AS BIGINT):bigint>
-- !query output
-9223372036854775808


-- !query
SELECT CAST('-9223372036854775809' AS long)
-- !query schema
struct<>
-- !query output
org.apache.spark.SparkNumberFormatException
{
  "errorClass" : "CAST_INVALID_INPUT",
  "sqlState" : "22018",
  "messageParameters" : {
    "ansiConfig" : "\"spark.sql.ansi.enabled\"",
    "expression" : "'-9223372036854775809'",
    "sourceType" : "\"STRING\"",
    "targetType" : "\"BIGINT\""
  },
  "queryContext" : [ {
    "objectType" : "",
    "objectName" : "",
    "startIndex" : 8,
    "stopIndex" : 43,
    "fragment" : "CAST('-9223372036854775809' AS long)"
  } ]
}


-- !query
SELECT CAST('9223372036854775807' AS long)
-- !query schema
struct<CAST(9223372036854775807 AS BIGINT):bigint>
-- !query output
9223372036854775807


-- !query
SELECT CAST('9223372036854775808' AS long)
-- !query schema
struct<>
-- !query output
org.apache.spark.SparkNumberFormatException
{
  "errorClass" : "CAST_INVALID_INPUT",
  "sqlState" : "22018",
  "messageParameters" : {
    "ansiConfig" : "\"spark.sql.ansi.enabled\"",
    "expression" : "'9223372036854775808'",
    "sourceType" : "\"STRING\"",
    "targetType" : "\"BIGINT\""
  },
  "queryContext" : [ {
    "objectType" : "",
    "objectName" : "",
    "startIndex" : 8,
    "stopIndex" : 42,
    "fragment" : "CAST('9223372036854775808' AS long)"
  } ]
}


-- !query
SELECT HEX(CAST('abc' AS binary))
-- !query schema
struct<hex(CAST(abc AS BINARY)):string>
-- !query output
616263


-- !query
SELECT HEX(CAST(CAST(123 AS byte) AS binary))
-- !query schema
struct<>
-- !query output
org.apache.spark.sql.catalyst.ExtendedAnalysisException
{
  "errorClass" : "DATATYPE_MISMATCH.CAST_WITH_CONF_SUGGESTION",
  "sqlState" : "42K09",
  "messageParameters" : {
    "config" : "\"spark.sql.ansi.enabled\"",
    "configVal" : "'false'",
    "sqlExpr" : "\"CAST(CAST(123 AS TINYINT) AS BINARY)\"",
    "srcType" : "\"TINYINT\"",
    "targetType" : "\"BINARY\""
  },
  "queryContext" : [ {
    "objectType" : "",
    "objectName" : "",
    "startIndex" : 12,
    "stopIndex" : 44,
    "fragment" : "CAST(CAST(123 AS byte) AS binary)"
  } ]
}


-- !query
SELECT HEX(CAST(CAST(-123 AS byte) AS binary))
-- !query schema
struct<>
-- !query output
org.apache.spark.sql.catalyst.ExtendedAnalysisException
{
  "errorClass" : "DATATYPE_MISMATCH.CAST_WITH_CONF_SUGGESTION",
  "sqlState" : "42K09",
  "messageParameters" : {
    "config" : "\"spark.sql.ansi.enabled\"",
    "configVal" : "'false'",
    "sqlExpr" : "\"CAST(CAST(-123 AS TINYINT) AS BINARY)\"",
    "srcType" : "\"TINYINT\"",
    "targetType" : "\"BINARY\""
  },
  "queryContext" : [ {
    "objectType" : "",
    "objectName" : "",
    "startIndex" : 12,
    "stopIndex" : 45,
    "fragment" : "CAST(CAST(-123 AS byte) AS binary)"
  } ]
}


-- !query
SELECT HEX(CAST(123S AS binary))
-- !query schema
struct<>
-- !query output
org.apache.spark.sql.catalyst.ExtendedAnalysisException
{
  "errorClass" : "DATATYPE_MISMATCH.CAST_WITH_CONF_SUGGESTION",
  "sqlState" : "42K09",
  "messageParameters" : {
    "config" : "\"spark.sql.ansi.enabled\"",
    "configVal" : "'false'",
    "sqlExpr" : "\"CAST(123 AS BINARY)\"",
    "srcType" : "\"SMALLINT\"",
    "targetType" : "\"BINARY\""
  },
  "queryContext" : [ {
    "objectType" : "",
    "objectName" : "",
    "startIndex" : 12,
    "stopIndex" : 31,
    "fragment" : "CAST(123S AS binary)"
  } ]
}


-- !query
SELECT HEX(CAST(-123S AS binary))
-- !query schema
struct<>
-- !query output
org.apache.spark.sql.catalyst.ExtendedAnalysisException
{
  "errorClass" : "DATATYPE_MISMATCH.CAST_WITH_CONF_SUGGESTION",
  "sqlState" : "42K09",
  "messageParameters" : {
    "config" : "\"spark.sql.ansi.enabled\"",
    "configVal" : "'false'",
    "sqlExpr" : "\"CAST(-123 AS BINARY)\"",
    "srcType" : "\"SMALLINT\"",
    "targetType" : "\"BINARY\""
  },
  "queryContext" : [ {
    "objectType" : "",
    "objectName" : "",
    "startIndex" : 12,
    "stopIndex" : 32,
    "fragment" : "CAST(-123S AS binary)"
  } ]
}


-- !query
SELECT HEX(CAST(123 AS binary))
-- !query schema
struct<>
-- !query output
org.apache.spark.sql.catalyst.ExtendedAnalysisException
{
  "errorClass" : "DATATYPE_MISMATCH.CAST_WITH_CONF_SUGGESTION",
  "sqlState" : "42K09",
  "messageParameters" : {
    "config" : "\"spark.sql.ansi.enabled\"",
    "configVal" : "'false'",
    "sqlExpr" : "\"CAST(123 AS BINARY)\"",
    "srcType" : "\"INT\"",
    "targetType" : "\"BINARY\""
  },
  "queryContext" : [ {
    "objectType" : "",
    "objectName" : "",
    "startIndex" : 12,
    "stopIndex" : 30,
    "fragment" : "CAST(123 AS binary)"
  } ]
}


-- !query
SELECT HEX(CAST(-123 AS binary))
-- !query schema
struct<>
-- !query output
org.apache.spark.sql.catalyst.ExtendedAnalysisException
{
  "errorClass" : "DATATYPE_MISMATCH.CAST_WITH_CONF_SUGGESTION",
  "sqlState" : "42K09",
  "messageParameters" : {
    "config" : "\"spark.sql.ansi.enabled\"",
    "configVal" : "'false'",
    "sqlExpr" : "\"CAST(-123 AS BINARY)\"",
    "srcType" : "\"INT\"",
    "targetType" : "\"BINARY\""
  },
  "queryContext" : [ {
    "objectType" : "",
    "objectName" : "",
    "startIndex" : 12,
    "stopIndex" : 31,
    "fragment" : "CAST(-123 AS binary)"
  } ]
}


-- !query
SELECT HEX(CAST(123L AS binary))
-- !query schema
struct<>
-- !query output
org.apache.spark.sql.catalyst.ExtendedAnalysisException
{
  "errorClass" : "DATATYPE_MISMATCH.CAST_WITH_CONF_SUGGESTION",
  "sqlState" : "42K09",
  "messageParameters" : {
    "config" : "\"spark.sql.ansi.enabled\"",
    "configVal" : "'false'",
    "sqlExpr" : "\"CAST(123 AS BINARY)\"",
    "srcType" : "\"BIGINT\"",
    "targetType" : "\"BINARY\""
  },
  "queryContext" : [ {
    "objectType" : "",
    "objectName" : "",
    "startIndex" : 12,
    "stopIndex" : 31,
    "fragment" : "CAST(123L AS binary)"
  } ]
}


-- !query
SELECT HEX(CAST(-123L AS binary))
-- !query schema
struct<>
-- !query output
org.apache.spark.sql.catalyst.ExtendedAnalysisException
{
  "errorClass" : "DATATYPE_MISMATCH.CAST_WITH_CONF_SUGGESTION",
  "sqlState" : "42K09",
  "messageParameters" : {
    "config" : "\"spark.sql.ansi.enabled\"",
    "configVal" : "'false'",
    "sqlExpr" : "\"CAST(-123 AS BINARY)\"",
    "srcType" : "\"BIGINT\"",
    "targetType" : "\"BINARY\""
  },
  "queryContext" : [ {
    "objectType" : "",
    "objectName" : "",
    "startIndex" : 12,
    "stopIndex" : 32,
    "fragment" : "CAST(-123L AS binary)"
  } ]
}


-- !query
DESC FUNCTION boolean
-- !query schema
struct<function_desc:string>
-- !query output
Class: org.apache.spark.sql.catalyst.expressions.Cast
Function: boolean
Usage: boolean(expr) - Casts the value `expr` to the target data type `boolean`.


-- !query
DESC FUNCTION EXTENDED boolean
-- !query schema
struct<function_desc:string>
-- !query output
Class: org.apache.spark.sql.catalyst.expressions.Cast
Extended Usage:
    No example/argument for boolean.

    Since: 2.0.1

Function: boolean
Usage: boolean(expr) - Casts the value `expr` to the target data type `boolean`.


-- !query
SELECT CAST('interval 3 month 1 hour' AS interval)
-- !query schema
struct<CAST(interval 3 month 1 hour AS INTERVAL):interval>
-- !query output
3 months 1 hours


-- !query
SELECT CAST("interval '3-1' year to month" AS interval year to month)
-- !query schema
struct<CAST(interval '3-1' year to month AS INTERVAL YEAR TO MONTH):interval year to month>
-- !query output
3-1


-- !query
SELECT CAST("interval '3 00:00:01' day to second" AS interval day to second)
-- !query schema
struct<CAST(interval '3 00:00:01' day to second AS INTERVAL DAY TO SECOND):interval day to second>
-- !query output
3 00:00:01.000000000


-- !query
SELECT CAST(interval 3 month 1 hour AS string)
-- !query schema
struct<>
-- !query output
org.apache.spark.sql.catalyst.parser.ParseException
{
  "errorClass" : "_LEGACY_ERROR_TEMP_0029",
  "messageParameters" : {
    "literal" : "interval 3 month 1 hour"
  },
  "queryContext" : [ {
    "objectType" : "",
    "objectName" : "",
    "startIndex" : 13,
    "stopIndex" : 35,
    "fragment" : "interval 3 month 1 hour"
  } ]
}


-- !query
SELECT CAST(interval 3 year 1 month AS string)
-- !query schema
struct<CAST(INTERVAL '3-1' YEAR TO MONTH AS STRING):string>
-- !query output
INTERVAL '3-1' YEAR TO MONTH


-- !query
SELECT CAST(interval 3 day 1 second AS string)
-- !query schema
struct<CAST(INTERVAL '3 00:00:01' DAY TO SECOND AS STRING):string>
-- !query output
INTERVAL '3 00:00:01' DAY TO SECOND


-- !query
select cast(' 1' as tinyint)
-- !query schema
struct<CAST( 1 AS TINYINT):tinyint>
-- !query output
1


-- !query
select cast(' 1\t' as tinyint)
-- !query schema
struct<CAST( 1	 AS TINYINT):tinyint>
-- !query output
1


-- !query
select cast(' 1' as smallint)
-- !query schema
struct<CAST( 1 AS SMALLINT):smallint>
-- !query output
1


-- !query
select cast(' 1' as INT)
-- !query schema
struct<CAST( 1 AS INT):int>
-- !query output
1


-- !query
select cast(' 1' as bigint)
-- !query schema
struct<CAST( 1 AS BIGINT):bigint>
-- !query output
1


-- !query
select cast(' 1' as float)
-- !query schema
struct<CAST( 1 AS FLOAT):float>
-- !query output
1.0


-- !query
select cast(' 1 ' as DOUBLE)
-- !query schema
struct<CAST( 1  AS DOUBLE):double>
-- !query output
1.0


-- !query
select cast('1.0 ' as DEC)
-- !query schema
struct<CAST(1.0  AS DECIMAL(10,0)):decimal(10,0)>
-- !query output
1


-- !query
select cast('1中文' as tinyint)
-- !query schema
struct<>
-- !query output
org.apache.spark.SparkNumberFormatException
{
  "errorClass" : "CAST_INVALID_INPUT",
  "sqlState" : "22018",
  "messageParameters" : {
    "ansiConfig" : "\"spark.sql.ansi.enabled\"",
    "expression" : "'1中文'",
    "sourceType" : "\"STRING\"",
    "targetType" : "\"TINYINT\""
  },
  "queryContext" : [ {
    "objectType" : "",
    "objectName" : "",
    "startIndex" : 8,
    "stopIndex" : 29,
    "fragment" : "cast('1中文' as tinyint)"
  } ]
}


-- !query
select cast('1中文' as smallint)
-- !query schema
struct<>
-- !query output
org.apache.spark.SparkNumberFormatException
{
  "errorClass" : "CAST_INVALID_INPUT",
  "sqlState" : "22018",
  "messageParameters" : {
    "ansiConfig" : "\"spark.sql.ansi.enabled\"",
    "expression" : "'1中文'",
    "sourceType" : "\"STRING\"",
    "targetType" : "\"SMALLINT\""
  },
  "queryContext" : [ {
    "objectType" : "",
    "objectName" : "",
    "startIndex" : 8,
    "stopIndex" : 30,
    "fragment" : "cast('1中文' as smallint)"
  } ]
}


-- !query
select cast('1中文' as INT)
-- !query schema
struct<>
-- !query output
org.apache.spark.SparkNumberFormatException
{
  "errorClass" : "CAST_INVALID_INPUT",
  "sqlState" : "22018",
  "messageParameters" : {
    "ansiConfig" : "\"spark.sql.ansi.enabled\"",
    "expression" : "'1中文'",
    "sourceType" : "\"STRING\"",
    "targetType" : "\"INT\""
  },
  "queryContext" : [ {
    "objectType" : "",
    "objectName" : "",
    "startIndex" : 8,
    "stopIndex" : 25,
    "fragment" : "cast('1中文' as INT)"
  } ]
}


-- !query
select cast('中文1' as bigint)
-- !query schema
struct<>
-- !query output
org.apache.spark.SparkNumberFormatException
{
  "errorClass" : "CAST_INVALID_INPUT",
  "sqlState" : "22018",
  "messageParameters" : {
    "ansiConfig" : "\"spark.sql.ansi.enabled\"",
    "expression" : "'中文1'",
    "sourceType" : "\"STRING\"",
    "targetType" : "\"BIGINT\""
  },
  "queryContext" : [ {
    "objectType" : "",
    "objectName" : "",
    "startIndex" : 8,
    "stopIndex" : 28,
    "fragment" : "cast('中文1' as bigint)"
  } ]
}


-- !query
select cast('1中文' as bigint)
-- !query schema
struct<>
-- !query output
org.apache.spark.SparkNumberFormatException
{
  "errorClass" : "CAST_INVALID_INPUT",
  "sqlState" : "22018",
  "messageParameters" : {
    "ansiConfig" : "\"spark.sql.ansi.enabled\"",
    "expression" : "'1中文'",
    "sourceType" : "\"STRING\"",
    "targetType" : "\"BIGINT\""
  },
  "queryContext" : [ {
    "objectType" : "",
    "objectName" : "",
    "startIndex" : 8,
    "stopIndex" : 28,
    "fragment" : "cast('1中文' as bigint)"
  } ]
}


-- !query
select cast('\t\t true \n\r ' as boolean)
-- !query schema
struct<CAST(		 true 
  AS BOOLEAN):boolean>
-- !query output
true


-- !query
select cast('\t\n false \t\r' as boolean)
-- !query schema
struct<CAST(	
 false 	 AS BOOLEAN):boolean>
-- !query output
false


-- !query
select cast('\t\n xyz \t\r' as boolean)
-- !query schema
struct<>
-- !query output
org.apache.spark.SparkRuntimeException
{
  "errorClass" : "CAST_INVALID_INPUT",
  "sqlState" : "22018",
  "messageParameters" : {
    "ansiConfig" : "\"spark.sql.ansi.enabled\"",
    "expression" : "'\t\n xyz \t\r'",
    "sourceType" : "\"STRING\"",
    "targetType" : "\"BOOLEAN\""
  },
  "queryContext" : [ {
    "objectType" : "",
    "objectName" : "",
    "startIndex" : 8,
    "stopIndex" : 39,
    "fragment" : "cast('\\t\\n xyz \\t\\r' as boolean)"
  } ]
}


-- !query
select cast('23.45' as decimal(4, 2))
-- !query schema
struct<CAST(23.45 AS DECIMAL(4,2)):decimal(4,2)>
-- !query output
23.45


-- !query
select cast('123.45' as decimal(4, 2))
-- !query schema
struct<>
-- !query output
org.apache.spark.SparkArithmeticException
{
  "errorClass" : "NUMERIC_VALUE_OUT_OF_RANGE.WITH_SUGGESTION",
  "sqlState" : "22003",
  "messageParameters" : {
    "config" : "\"spark.sql.ansi.enabled\"",
    "precision" : "4",
    "scale" : "2",
    "value" : "123.45"
  },
  "queryContext" : [ {
    "objectType" : "",
    "objectName" : "",
    "startIndex" : 8,
    "stopIndex" : 38,
    "fragment" : "cast('123.45' as decimal(4, 2))"
  } ]
}


-- !query
select cast('xyz' as decimal(4, 2))
-- !query schema
struct<>
-- !query output
org.apache.spark.SparkNumberFormatException
{
  "errorClass" : "CAST_INVALID_INPUT",
  "sqlState" : "22018",
  "messageParameters" : {
    "ansiConfig" : "\"spark.sql.ansi.enabled\"",
    "expression" : "'xyz'",
    "sourceType" : "\"STRING\"",
    "targetType" : "\"DECIMAL(4,2)\""
  },
  "queryContext" : [ {
    "objectType" : "",
    "objectName" : "",
    "startIndex" : 8,
    "stopIndex" : 35,
    "fragment" : "cast('xyz' as decimal(4, 2))"
  } ]
}


-- !query
select cast('2022-01-01' as date)
-- !query schema
struct<CAST(2022-01-01 AS DATE):date>
-- !query output
2022-01-01


-- !query
select cast('a' as date)
-- !query schema
struct<>
-- !query output
org.apache.spark.SparkDateTimeException
{
  "errorClass" : "CAST_INVALID_INPUT",
  "sqlState" : "22018",
  "messageParameters" : {
    "ansiConfig" : "\"spark.sql.ansi.enabled\"",
    "expression" : "'a'",
    "sourceType" : "\"STRING\"",
    "targetType" : "\"DATE\""
  },
  "queryContext" : [ {
    "objectType" : "",
    "objectName" : "",
    "startIndex" : 8,
    "stopIndex" : 24,
    "fragment" : "cast('a' as date)"
  } ]
}


-- !query
select cast('2022-01-01 00:00:00' as timestamp)
-- !query schema
struct<CAST(2022-01-01 00:00:00 AS TIMESTAMP):timestamp>
-- !query output
2022-01-01 00:00:00


-- !query
select cast('a' as timestamp)
-- !query schema
struct<>
-- !query output
org.apache.spark.SparkDateTimeException
{
  "errorClass" : "CAST_INVALID_INPUT",
  "sqlState" : "22018",
  "messageParameters" : {
    "ansiConfig" : "\"spark.sql.ansi.enabled\"",
    "expression" : "'a'",
    "sourceType" : "\"STRING\"",
    "targetType" : "\"TIMESTAMP\""
  },
  "queryContext" : [ {
    "objectType" : "",
    "objectName" : "",
    "startIndex" : 8,
    "stopIndex" : 29,
    "fragment" : "cast('a' as timestamp)"
  } ]
}


-- !query
select cast('2022-01-01 00:00:00' as timestamp_ntz)
-- !query schema
struct<CAST(2022-01-01 00:00:00 AS TIMESTAMP_NTZ):timestamp_ntz>
-- !query output
2022-01-01 00:00:00


-- !query
select cast('a' as timestamp_ntz)
-- !query schema
struct<>
-- !query output
org.apache.spark.SparkDateTimeException
{
  "errorClass" : "CAST_INVALID_INPUT",
  "sqlState" : "22018",
  "messageParameters" : {
    "ansiConfig" : "\"spark.sql.ansi.enabled\"",
    "expression" : "'a'",
    "sourceType" : "\"STRING\"",
    "targetType" : "\"TIMESTAMP_NTZ\""
  },
  "queryContext" : [ {
    "objectType" : "",
    "objectName" : "",
    "startIndex" : 8,
    "stopIndex" : 33,
    "fragment" : "cast('a' as timestamp_ntz)"
  } ]
}


-- !query
select cast(cast('inf' as double) as timestamp)
-- !query schema
struct<>
-- !query output
org.apache.spark.SparkDateTimeException
{
  "errorClass" : "CAST_INVALID_INPUT",
  "sqlState" : "22018",
  "messageParameters" : {
    "ansiConfig" : "\"spark.sql.ansi.enabled\"",
    "expression" : "Infinity",
    "sourceType" : "\"DOUBLE\"",
    "targetType" : "\"TIMESTAMP\""
  },
  "queryContext" : [ {
    "objectType" : "",
    "objectName" : "",
    "startIndex" : 8,
    "stopIndex" : 47,
    "fragment" : "cast(cast('inf' as double) as timestamp)"
  } ]
}


-- !query
select cast(cast('inf' as float) as timestamp)
-- !query schema
struct<>
-- !query output
org.apache.spark.SparkDateTimeException
{
  "errorClass" : "CAST_INVALID_INPUT",
  "sqlState" : "22018",
  "messageParameters" : {
    "ansiConfig" : "\"spark.sql.ansi.enabled\"",
    "expression" : "Infinity",
    "sourceType" : "\"DOUBLE\"",
    "targetType" : "\"TIMESTAMP\""
  },
  "queryContext" : [ {
    "objectType" : "",
    "objectName" : "",
    "startIndex" : 8,
    "stopIndex" : 46,
    "fragment" : "cast(cast('inf' as float) as timestamp)"
  } ]
}


-- !query
select cast(interval '1' year as tinyint)
-- !query schema
struct<CAST(INTERVAL '1' YEAR AS TINYINT):tinyint>
-- !query output
1


-- !query
select cast(interval '-10-2' year to month as smallint)
-- !query schema
struct<CAST(INTERVAL '-10-2' YEAR TO MONTH AS SMALLINT):smallint>
-- !query output
-122


-- !query
select cast(interval '1000' month as int)
-- !query schema
struct<CAST(INTERVAL '1000' MONTH AS INT):int>
-- !query output
1000


-- !query
select cast(interval -'10.123456' second as tinyint)
-- !query schema
struct<CAST(INTERVAL '-10.123456' SECOND AS TINYINT):tinyint>
-- !query output
-10


-- !query
select cast(interval '23:59:59' hour to second as smallint)
-- !query schema
struct<>
-- !query output
org.apache.spark.SparkArithmeticException
{
  "errorClass" : "CAST_OVERFLOW",
  "sqlState" : "22003",
  "messageParameters" : {
    "ansiConfig" : "\"spark.sql.ansi.enabled\"",
    "sourceType" : "\"INTERVAL HOUR TO SECOND\"",
    "targetType" : "\"SMALLINT\"",
    "value" : "INTERVAL '23:59:59' HOUR TO SECOND"
  }
}


-- !query
select cast(interval -'1 02:03:04.123' day to second as int)
-- !query schema
struct<CAST(INTERVAL '-1 02:03:04.123' DAY TO SECOND AS INT):int>
-- !query output
-93784


-- !query
select cast(interval '10' day as bigint)
-- !query schema
struct<CAST(INTERVAL '10' DAY AS BIGINT):bigint>
-- !query output
10


-- !query
select cast(interval '-1000' month as tinyint)
-- !query schema
struct<>
-- !query output
org.apache.spark.SparkArithmeticException
{
  "errorClass" : "CAST_OVERFLOW",
  "sqlState" : "22003",
  "messageParameters" : {
    "ansiConfig" : "\"spark.sql.ansi.enabled\"",
    "sourceType" : "\"INTERVAL MONTH\"",
    "targetType" : "\"TINYINT\"",
    "value" : "INTERVAL '-1000' MONTH"
  }
}


-- !query
select cast(interval '1000000' second as smallint)
-- !query schema
struct<>
-- !query output
org.apache.spark.SparkArithmeticException
{
  "errorClass" : "CAST_OVERFLOW",
  "sqlState" : "22003",
  "messageParameters" : {
    "ansiConfig" : "\"spark.sql.ansi.enabled\"",
    "sourceType" : "\"INTERVAL SECOND\"",
    "targetType" : "\"SMALLINT\"",
    "value" : "INTERVAL '1000000' SECOND"
  }
}


-- !query
select cast(1Y as interval year)
-- !query schema
struct<CAST(1 AS INTERVAL YEAR):interval year>
-- !query output
1-0


-- !query
select cast(-122S as interval year to month)
-- !query schema
struct<CAST(-122 AS INTERVAL YEAR TO MONTH):interval year to month>
-- !query output
-10-2


-- !query
select cast(ym as interval year to month) from values(-122S) as t(ym)
-- !query schema
struct<ym:interval year to month>
-- !query output
-10-2


-- !query
select cast(1000 as interval month)
-- !query schema
struct<CAST(1000 AS INTERVAL MONTH):interval month>
-- !query output
83-4


-- !query
select cast(-10L as interval second)
-- !query schema
struct<CAST(-10 AS INTERVAL SECOND):interval second>
-- !query output
-0 00:00:10.000000000


-- !query
select cast(100Y as interval hour to second)
-- !query schema
struct<CAST(100 AS INTERVAL HOUR TO SECOND):interval hour to second>
-- !query output
0 00:01:40.000000000


-- !query
select cast(dt as interval hour to second) from values(100Y) as t(dt)
-- !query schema
struct<dt:interval hour to second>
-- !query output
0 00:01:40.000000000


-- !query
select cast(-1000S as interval day to second)
-- !query schema
struct<CAST(-1000 AS INTERVAL DAY TO SECOND):interval day to second>
-- !query output
-0 00:16:40.000000000


-- !query
select cast(10 as interval day)
-- !query schema
struct<CAST(10 AS INTERVAL DAY):interval day>
-- !query output
10 00:00:00.000000000


-- !query
select cast(2147483647 as interval year)
-- !query schema
struct<>
-- !query output
org.apache.spark.SparkArithmeticException
{
  "errorClass" : "CAST_OVERFLOW",
  "sqlState" : "22003",
  "messageParameters" : {
    "ansiConfig" : "\"spark.sql.ansi.enabled\"",
    "sourceType" : "\"INT\"",
    "targetType" : "\"INTERVAL YEAR\"",
    "value" : "2147483647"
  }
}


-- !query
select cast(-9223372036854775808L as interval day)
-- !query schema
struct<>
-- !query output
org.apache.spark.SparkArithmeticException
{
  "errorClass" : "CAST_OVERFLOW",
  "sqlState" : "22003",
  "messageParameters" : {
    "ansiConfig" : "\"spark.sql.ansi.enabled\"",
    "sourceType" : "\"BIGINT\"",
    "targetType" : "\"INTERVAL DAY\"",
    "value" : "-9223372036854775808L"
  }
}


-- !query
select cast(interval '-1' year as decimal(10, 0))
-- !query schema
struct<CAST(INTERVAL '-1' YEAR AS DECIMAL(10,0)):decimal(10,0)>
-- !query output
-1


-- !query
select cast(interval '1.000001' second as decimal(10, 6))
-- !query schema
struct<CAST(INTERVAL '01.000001' SECOND AS DECIMAL(10,6)):decimal(10,6)>
-- !query output
1.000001


-- !query
select cast(interval '08:11:10.001' hour to second as decimal(10, 4))
-- !query schema
struct<CAST(INTERVAL '08:11:10.001' HOUR TO SECOND AS DECIMAL(10,4)):decimal(10,4)>
-- !query output
29470.0010


-- !query
select cast(interval '1 01:02:03.1' day to second as decimal(8, 1))
-- !query schema
struct<CAST(INTERVAL '1 01:02:03.1' DAY TO SECOND AS DECIMAL(8,1)):decimal(8,1)>
-- !query output
90123.1


-- !query
select cast(interval '10.123' second as decimal(4, 2))
-- !query schema
struct<CAST(INTERVAL '10.123' SECOND AS DECIMAL(4,2)):decimal(4,2)>
-- !query output
10.12


-- !query
select cast(interval '10.005' second as decimal(4, 2))
-- !query schema
struct<CAST(INTERVAL '10.005' SECOND AS DECIMAL(4,2)):decimal(4,2)>
-- !query output
10.01


-- !query
select cast(interval '10.123' second as decimal(5, 2))
-- !query schema
struct<CAST(INTERVAL '10.123' SECOND AS DECIMAL(5,2)):decimal(5,2)>
-- !query output
10.12


-- !query
select cast(interval '10.123' second as decimal(1, 0))
-- !query schema
struct<>
-- !query output
org.apache.spark.SparkArithmeticException
{
  "errorClass" : "NUMERIC_VALUE_OUT_OF_RANGE.WITH_SUGGESTION",
  "sqlState" : "22003",
  "messageParameters" : {
    "config" : "\"spark.sql.ansi.enabled\"",
    "precision" : "1",
    "scale" : "0",
    "value" : "10.123000"
  },
  "queryContext" : [ {
    "objectType" : "",
    "objectName" : "",
    "startIndex" : 8,
    "stopIndex" : 54,
    "fragment" : "cast(interval '10.123' second as decimal(1, 0))"
  } ]
}


-- !query
select cast(10.123456BD as interval day to second)
-- !query schema
struct<CAST(10.123456 AS INTERVAL DAY TO SECOND):interval day to second>
-- !query output
0 00:00:10.123456000


-- !query
select cast(80.654321BD as interval hour to minute)
-- !query schema
struct<CAST(80.654321 AS INTERVAL HOUR TO MINUTE):interval hour to minute>
-- !query output
0 01:20:00.000000000


-- !query
select cast(-10.123456BD as interval year to month)
-- !query schema
struct<CAST(-10.123456 AS INTERVAL YEAR TO MONTH):interval year to month>
-- !query output
-0-10


-- !query
select cast(10.654321BD as interval month)
-- !query schema
struct<CAST(10.654321 AS INTERVAL MONTH):interval month>
-- !query output
0-11


-- !query
SELECT '1.23' :: int
-- !query schema
struct<>
-- !query output
org.apache.spark.SparkNumberFormatException
{
  "errorClass" : "CAST_INVALID_INPUT",
  "sqlState" : "22018",
  "messageParameters" : {
    "ansiConfig" : "\"spark.sql.ansi.enabled\"",
    "expression" : "'1.23'",
    "sourceType" : "\"STRING\"",
    "targetType" : "\"INT\""
  },
  "queryContext" : [ {
    "objectType" : "",
    "objectName" : "",
    "startIndex" : 8,
    "stopIndex" : 20,
    "fragment" : "'1.23' :: int"
  } ]
}


-- !query
SELECT 'abc' :: int
-- !query schema
struct<>
-- !query output
org.apache.spark.SparkNumberFormatException
{
  "errorClass" : "CAST_INVALID_INPUT",
  "sqlState" : "22018",
  "messageParameters" : {
    "ansiConfig" : "\"spark.sql.ansi.enabled\"",
    "expression" : "'abc'",
    "sourceType" : "\"STRING\"",
    "targetType" : "\"INT\""
  },
  "queryContext" : [ {
    "objectType" : "",
    "objectName" : "",
    "startIndex" : 8,
    "stopIndex" : 19,
    "fragment" : "'abc' :: int"
  } ]
}


-- !query
SELECT '12345678901234567890123' :: long
-- !query schema
struct<>
-- !query output
org.apache.spark.SparkNumberFormatException
{
  "errorClass" : "CAST_INVALID_INPUT",
  "sqlState" : "22018",
  "messageParameters" : {
    "ansiConfig" : "\"spark.sql.ansi.enabled\"",
    "expression" : "'12345678901234567890123'",
    "sourceType" : "\"STRING\"",
    "targetType" : "\"BIGINT\""
  },
  "queryContext" : [ {
    "objectType" : "",
    "objectName" : "",
    "startIndex" : 8,
    "stopIndex" : 40,
    "fragment" : "'12345678901234567890123' :: long"
  } ]
}


-- !query
SELECT '' :: int
-- !query schema
struct<>
-- !query output
org.apache.spark.SparkNumberFormatException
{
  "errorClass" : "CAST_INVALID_INPUT",
  "sqlState" : "22018",
  "messageParameters" : {
    "ansiConfig" : "\"spark.sql.ansi.enabled\"",
    "expression" : "''",
    "sourceType" : "\"STRING\"",
    "targetType" : "\"INT\""
  },
  "queryContext" : [ {
    "objectType" : "",
    "objectName" : "",
    "startIndex" : 8,
    "stopIndex" : 16,
    "fragment" : "'' :: int"
  } ]
}


-- !query
SELECT NULL :: int
-- !query schema
struct<CAST(NULL AS INT):int>
-- !query output
NULL


-- !query
SELECT '123.a' :: int
-- !query schema
struct<>
-- !query output
org.apache.spark.SparkNumberFormatException
{
  "errorClass" : "CAST_INVALID_INPUT",
  "sqlState" : "22018",
  "messageParameters" : {
    "ansiConfig" : "\"spark.sql.ansi.enabled\"",
    "expression" : "'123.a'",
    "sourceType" : "\"STRING\"",
    "targetType" : "\"INT\""
  },
  "queryContext" : [ {
    "objectType" : "",
    "objectName" : "",
    "startIndex" : 8,
    "stopIndex" : 21,
    "fragment" : "'123.a' :: int"
  } ]
}


-- !query
SELECT '-2147483648' :: int
-- !query schema
struct<CAST(-2147483648 AS INT):int>
-- !query output
-2147483648


-- !query
SELECT HEX('abc' :: binary)
-- !query schema
struct<hex(CAST(abc AS BINARY)):string>
-- !query output
616263


-- !query
SELECT HEX((123 :: byte) :: binary)
-- !query schema
struct<>
-- !query output
org.apache.spark.sql.catalyst.ExtendedAnalysisException
{
  "errorClass" : "DATATYPE_MISMATCH.CAST_WITH_CONF_SUGGESTION",
  "sqlState" : "42K09",
  "messageParameters" : {
    "config" : "\"spark.sql.ansi.enabled\"",
    "configVal" : "'false'",
    "sqlExpr" : "\"CAST(CAST(123 AS TINYINT) AS BINARY)\"",
    "srcType" : "\"TINYINT\"",
    "targetType" : "\"BINARY\""
  },
  "queryContext" : [ {
    "objectType" : "",
    "objectName" : "",
    "startIndex" : 12,
    "stopIndex" : 34,
    "fragment" : "(123 :: byte) :: binary"
  } ]
}


-- !query
SELECT 'interval 3 month 1 hour' :: interval
-- !query schema
struct<CAST(interval 3 month 1 hour AS INTERVAL):interval>
-- !query output
3 months 1 hours


-- !query
SELECT interval 3 day 1 second :: string
-- !query schema
struct<CAST(INTERVAL '3 00:00:01' DAY TO SECOND AS STRING):string>
-- !query output
INTERVAL '3 00:00:01' DAY TO SECOND


-- !query
select ' 1 ' :: DOUBLE
-- !query schema
struct<CAST( 1  AS DOUBLE):double>
-- !query output
1.0


-- !query
select '1.0 ' :: DEC
-- !query schema
struct<CAST(1.0  AS DECIMAL(10,0)):decimal(10,0)>
-- !query output
1


-- !query
select '\t\t true \n\r ' :: boolean
-- !query schema
struct<CAST(		 true 
  AS BOOLEAN):boolean>
-- !query output
true


-- !query
select '2022-01-01 00:00:00' :: timestamp
-- !query schema
struct<CAST(2022-01-01 00:00:00 AS TIMESTAMP):timestamp>
-- !query output
2022-01-01 00:00:00


-- !query
select interval '-10-2' year to month :: smallint
-- !query schema
struct<CAST(INTERVAL '-10-2' YEAR TO MONTH AS SMALLINT):smallint>
-- !query output
-122


-- !query
select -10L :: interval second
-- !query schema
struct<CAST(-10 AS INTERVAL SECOND):interval second>
-- !query output
-0 00:00:10.000000000


-- !query
select interval '08:11:10.001' hour to second :: decimal(10, 4)
-- !query schema
struct<CAST(INTERVAL '08:11:10.001' HOUR TO SECOND AS DECIMAL(10,4)):decimal(10,4)>
-- !query output
29470.0010


-- !query
select 10.123456BD :: interval day to second
-- !query schema
struct<CAST(10.123456 AS INTERVAL DAY TO SECOND):interval day to second>
-- !query output
0 00:00:10.123456000


-- !query
SELECT '1.23' :: int :: long
-- !query schema
struct<>
-- !query output
org.apache.spark.SparkNumberFormatException
{
  "errorClass" : "CAST_INVALID_INPUT",
  "sqlState" : "22018",
  "messageParameters" : {
    "ansiConfig" : "\"spark.sql.ansi.enabled\"",
    "expression" : "'1.23'",
    "sourceType" : "\"STRING\"",
    "targetType" : "\"INT\""
  },
  "queryContext" : [ {
    "objectType" : "",
    "objectName" : "",
    "startIndex" : 8,
    "stopIndex" : 20,
    "fragment" : "'1.23' :: int"
  } ]
}


-- !query
SELECT '2147483648' :: long :: int
-- !query schema
struct<>
-- !query output
org.apache.spark.SparkArithmeticException
{
  "errorClass" : "CAST_OVERFLOW",
  "sqlState" : "22003",
  "messageParameters" : {
    "ansiConfig" : "\"spark.sql.ansi.enabled\"",
    "sourceType" : "\"BIGINT\"",
    "targetType" : "\"INT\"",
    "value" : "2147483648L"
  }
}


-- !query
SELECT CAST('2147483648' :: long AS int)
-- !query schema
struct<>
-- !query output
org.apache.spark.SparkArithmeticException
{
  "errorClass" : "CAST_OVERFLOW",
  "sqlState" : "22003",
  "messageParameters" : {
    "ansiConfig" : "\"spark.sql.ansi.enabled\"",
    "sourceType" : "\"BIGINT\"",
    "targetType" : "\"INT\"",
    "value" : "2147483648L"
  }
}


-- !query
SELECT map(1, '123', 2, '456')[1] :: int
-- !query schema
struct<CAST(map(1, 123, 2, 456)[1] AS INT):int>
-- !query output
123


-- !query
SELECT '2147483648' :: BINT
-- !query schema
struct<>
-- !query output
org.apache.spark.sql.catalyst.parser.ParseException
{
  "errorClass" : "UNSUPPORTED_DATATYPE",
  "sqlState" : "0A000",
  "messageParameters" : {
    "typeName" : "\"BINT\""
  },
  "queryContext" : [ {
    "objectType" : "",
    "objectName" : "",
    "startIndex" : 24,
    "stopIndex" : 27,
    "fragment" : "BINT"
  } ]
}


-- !query
SELECT '2147483648' :: SELECT
-- !query schema
struct<>
-- !query output
org.apache.spark.sql.catalyst.parser.ParseException
{
  "errorClass" : "UNSUPPORTED_DATATYPE",
  "sqlState" : "0A000",
  "messageParameters" : {
    "typeName" : "\"SELECT\""
  },
  "queryContext" : [ {
    "objectType" : "",
    "objectName" : "",
    "startIndex" : 24,
    "stopIndex" : 29,
    "fragment" : "SELECT"
  } ]
}


-- !query
SELECT FALSE IS NOT NULL :: string
-- !query schema
struct<>
-- !query output
org.apache.spark.sql.catalyst.parser.ParseException
{
  "errorClass" : "PARSE_SYNTAX_ERROR",
  "sqlState" : "42601",
  "messageParameters" : {
    "error" : "'::'",
    "hint" : ""
  }
}
