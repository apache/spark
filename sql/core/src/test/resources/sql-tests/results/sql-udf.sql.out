-- Automatically generated by SQLQueryTestSuite
-- !query
CREATE FUNCTION foo1a0() RETURNS INT RETURN 1
-- !query schema
struct<>
-- !query output



-- !query
SELECT foo1a0()
-- !query schema
struct<spark_catalog.default.foo1a0():int>
-- !query output
1


-- !query
SELECT foo1a0(1)
-- !query schema
struct<>
-- !query output
org.apache.spark.sql.AnalysisException
{
  "errorClass" : "WRONG_NUM_ARGS.WITHOUT_SUGGESTION",
  "sqlState" : "42605",
  "messageParameters" : {
    "actualNum" : "1",
    "docroot" : "https://spark.apache.org/docs/latest",
    "expectedNum" : "0",
    "functionName" : "`spark_catalog`.`default`.`foo1a0`"
  },
  "queryContext" : [ {
    "objectType" : "",
    "objectName" : "",
    "startIndex" : 8,
    "stopIndex" : 16,
    "fragment" : "foo1a0(1)"
  } ]
}


-- !query
CREATE FUNCTION foo1a1(a INT) RETURNS INT RETURN 1
-- !query schema
struct<>
-- !query output



-- !query
SELECT foo1a1(1)
-- !query schema
struct<spark_catalog.default.foo1a1(1):int>
-- !query output
1


-- !query
SELECT foo1a1(1, 2)
-- !query schema
struct<>
-- !query output
org.apache.spark.sql.AnalysisException
{
  "errorClass" : "WRONG_NUM_ARGS.WITHOUT_SUGGESTION",
  "sqlState" : "42605",
  "messageParameters" : {
    "actualNum" : "2",
    "docroot" : "https://spark.apache.org/docs/latest",
    "expectedNum" : "1",
    "functionName" : "`spark_catalog`.`default`.`foo1a1`"
  },
  "queryContext" : [ {
    "objectType" : "",
    "objectName" : "",
    "startIndex" : 8,
    "stopIndex" : 19,
    "fragment" : "foo1a1(1, 2)"
  } ]
}


-- !query
CREATE FUNCTION foo1a2(a INT, b INT, c INT, d INT) RETURNS INT RETURN 1
-- !query schema
struct<>
-- !query output



-- !query
SELECT foo1a2(1, 2, 3, 4)
-- !query schema
struct<spark_catalog.default.foo1a2(1, 2, 3, 4):int>
-- !query output
1


-- !query
CREATE FUNCTION foo1b0() RETURNS TABLE (c1 INT) RETURN SELECT 1
-- !query schema
struct<>
-- !query output



-- !query
SELECT * FROM foo1b0()
-- !query schema
struct<c1:int>
-- !query output
1


-- !query
CREATE FUNCTION foo1b1(a INT) RETURNS TABLE (c1 INT) RETURN SELECT 1
-- !query schema
struct<>
-- !query output



-- !query
SELECT * FROM foo1b1(1)
-- !query schema
struct<c1:int>
-- !query output
1


-- !query
CREATE FUNCTION foo1b2(a INT, b INT, c INT, d INT) RETURNS TABLE(c1 INT) RETURN SELECT 1
-- !query schema
struct<>
-- !query output



-- !query
SELECT * FROM foo1b2(1, 2, 3, 4)
-- !query schema
struct<c1:int>
-- !query output
1


-- !query
CREATE FUNCTION foo1c1(duplicate INT, DUPLICATE INT) RETURNS INT RETURN 1
-- !query schema
struct<>
-- !query output
org.apache.spark.sql.AnalysisException
{
  "errorClass" : "DUPLICATE_ROUTINE_PARAMETER_NAMES",
  "sqlState" : "42734",
  "messageParameters" : {
    "names" : "`duplicate`",
    "routineName" : "foo1c1"
  }
}


-- !query
CREATE FUNCTION foo1c2(a INT, b INT, thisisaduplicate INT, c INT, d INT, e INT, f INT, thisIsaDuplicate INT, g INT)
    RETURNS TABLE (a INT) RETURN SELECT 1
-- !query schema
struct<>
-- !query output
org.apache.spark.sql.AnalysisException
{
  "errorClass" : "DUPLICATE_ROUTINE_PARAMETER_NAMES",
  "sqlState" : "42734",
  "messageParameters" : {
    "names" : "`thisisaduplicate`",
    "routineName" : "foo1c2"
  }
}


-- !query
CREATE OR REPLACE FUNCTION foo1d1(a INT DEFAULT NULL) RETURNS INT RETURN a
-- !query schema
struct<>
-- !query output



-- !query
SELECT foo1d1(5), foo1d1()
-- !query schema
struct<spark_catalog.default.foo1d1(5):int,spark_catalog.default.foo1d1():int>
-- !query output
5	NULL


-- !query
CREATE OR REPLACE FUNCTION foo1d1(a INT DEFAULT 10) RETURNS INT RETURN a
-- !query schema
struct<>
-- !query output



-- !query
SELECT foo1d1(5), foo1d1()
-- !query schema
struct<spark_catalog.default.foo1d1(5):int,spark_catalog.default.foo1d1():int>
-- !query output
5	10


-- !query
CREATE OR REPLACE FUNCTION foo1d1(a INT DEFAULT length(substr(current_database(), 1, 1))) RETURNS INT RETURN a
-- !query schema
struct<>
-- !query output



-- !query
SELECT foo1d1(5), foo1d1()
-- !query schema
struct<spark_catalog.default.foo1d1(5):int,spark_catalog.default.foo1d1():int>
-- !query output
5	1


-- !query
CREATE OR REPLACE FUNCTION foo1d1(a INT DEFAULT '5' || length(substr(current_database(), 1, 1)))
  RETURNS INT RETURN a
-- !query schema
struct<>
-- !query output



-- !query
SELECT foo1d1(5), foo1d1()
-- !query schema
struct<spark_catalog.default.foo1d1(5):int,spark_catalog.default.foo1d1():int>
-- !query output
5	51


-- !query
CREATE OR REPLACE FUNCTION foo1d1(a INT DEFAULT RAND()::INT) RETURNS INT RETURN a
-- !query schema
struct<>
-- !query output



-- !query
SELECT foo1d1(5), foo1d1()
-- !query schema
struct<spark_catalog.default.foo1d1(5):int,spark_catalog.default.foo1d1():int>
-- !query output
5	0


-- !query
CREATE OR REPLACE FUNCTION foo1d1(a INT DEFAULT array(55, 17))
  RETURNS INT RETURN a
-- !query schema
struct<>
-- !query output
org.apache.spark.sql.catalyst.ExtendedAnalysisException
{
  "errorClass" : "DATATYPE_MISMATCH.CAST_WITHOUT_SUGGESTION",
  "sqlState" : "42K09",
  "messageParameters" : {
    "sqlExpr" : "\"array(55, 17)\"",
    "srcType" : "\"ARRAY<INT>\"",
    "targetType" : "\"INT\""
  },
  "queryContext" : [ {
    "objectType" : "",
    "objectName" : "",
    "startIndex" : 1,
    "stopIndex" : 85,
    "fragment" : "CREATE OR REPLACE FUNCTION foo1d1(a INT DEFAULT array(55, 17))\n  RETURNS INT RETURN a"
  } ]
}


-- !query
CREATE OR REPLACE FUNCTION foo1d1(a INT DEFAULT (SELECT max(c1) FROM VALUES (1) AS T(c1)))
  RETURNS INT RETURN a
-- !query schema
struct<>
-- !query output
org.apache.spark.sql.AnalysisException
{
  "errorClass" : "USER_DEFINED_FUNCTIONS.NOT_A_VALID_DEFAULT_EXPRESSION",
  "sqlState" : "42601",
  "messageParameters" : {
    "functionName" : "foo1d1",
    "parameterName" : "a"
  }
}


-- !query
CREATE OR REPLACE FUNCTION foo1d2(a INT, b INT DEFAULT 7, c INT DEFAULT 8, d INT DEFAULT 9 COMMENT 'test')
  RETURNS STRING RETURN a || ' ' || b || ' ' || c || ' ' || d
-- !query schema
struct<>
-- !query output



-- !query
SELECT foo1d2(1, 2, 3, 4), foo1d2(1, 2, 3), foo1d2(1, 2), foo1d2(1)
-- !query schema
struct<spark_catalog.default.foo1d2(1, 2, 3, 4):string,spark_catalog.default.foo1d2(1, 2, 3):string,spark_catalog.default.foo1d2(1, 2):string,spark_catalog.default.foo1d2(1):string>
-- !query output
1 2 3 4	1 2 3 9	1 2 8 9	1 7 8 9


-- !query
SELECT foo1d2()
-- !query schema
struct<>
-- !query output
org.apache.spark.sql.AnalysisException
{
  "errorClass" : "WRONG_NUM_ARGS.WITHOUT_SUGGESTION",
  "sqlState" : "42605",
  "messageParameters" : {
    "actualNum" : "0",
    "docroot" : "https://spark.apache.org/docs/latest",
    "expectedNum" : "4",
    "functionName" : "`spark_catalog`.`default`.`foo1d2`"
  },
  "queryContext" : [ {
    "objectType" : "",
    "objectName" : "",
    "startIndex" : 8,
    "stopIndex" : 15,
    "fragment" : "foo1d2()"
  } ]
}


-- !query
SELECT foo1d2(1, 2, 3, 4, 5)
-- !query schema
struct<>
-- !query output
org.apache.spark.sql.AnalysisException
{
  "errorClass" : "WRONG_NUM_ARGS.WITHOUT_SUGGESTION",
  "sqlState" : "42605",
  "messageParameters" : {
    "actualNum" : "5",
    "docroot" : "https://spark.apache.org/docs/latest",
    "expectedNum" : "4",
    "functionName" : "`spark_catalog`.`default`.`foo1d2`"
  },
  "queryContext" : [ {
    "objectType" : "",
    "objectName" : "",
    "startIndex" : 8,
    "stopIndex" : 28,
    "fragment" : "foo1d2(1, 2, 3, 4, 5)"
  } ]
}


-- !query
CREATE OR REPLACE FUNCTION foo1d2(a INT DEFAULT 5, b INT , c INT DEFAULT 8, d INT DEFAULT 9 COMMENT 'test')
  RETURNS STRING RETURN a || ' ' || b || ' ' || c || ' ' || d
-- !query schema
struct<>
-- !query output
org.apache.spark.sql.AnalysisException
{
  "errorClass" : "USER_DEFINED_FUNCTIONS.NOT_A_VALID_DEFAULT_PARAMETER_POSITION",
  "sqlState" : "42601",
  "messageParameters" : {
    "functionName" : "foo1d2",
    "nextParameterName" : "b",
    "parameterName" : "a"
  }
}


-- !query
CREATE OR REPLACE FUNCTION foo1d2(a INT, b INT DEFAULT 7, c INT DEFAULT 8, d INT COMMENT 'test')
  RETURNS STRING RETURN a || ' ' || b || ' ' || c || ' ' || d
-- !query schema
struct<>
-- !query output
org.apache.spark.sql.AnalysisException
{
  "errorClass" : "USER_DEFINED_FUNCTIONS.NOT_A_VALID_DEFAULT_PARAMETER_POSITION",
  "sqlState" : "42601",
  "messageParameters" : {
    "functionName" : "foo1d2",
    "nextParameterName" : "d",
    "parameterName" : "c"
  }
}


-- !query
CREATE OR REPLACE TEMPORARY FUNCTION foo1d3(a INT DEFAULT 7 COMMENT 'hello') RETURNS INT RETURN a
-- !query schema
struct<>
-- !query output



-- !query
SELECT foo1d3(5), foo1d3()
-- !query schema
struct<foo1d3(5):int,foo1d3():int>
-- !query output
5	7


-- !query
CREATE OR REPLACE FUNCTION foo1d4(a INT, b INT DEFAULT a) RETURNS INT RETURN a + b
-- !query schema
struct<>
-- !query output
org.apache.spark.sql.AnalysisException
{
  "errorClass" : "UNRESOLVED_COLUMN.WITHOUT_SUGGESTION",
  "sqlState" : "42703",
  "messageParameters" : {
    "objectName" : "`a`"
  },
  "queryContext" : [ {
    "objectType" : "",
    "objectName" : "",
    "startIndex" : 1,
    "stopIndex" : 1,
    "fragment" : "a"
  } ]
}


-- !query
CREATE OR REPLACE FUNCTION foo1d4(a INT, b INT DEFAULT 3) RETURNS INT RETURN a + b
-- !query schema
struct<>
-- !query output



-- !query
CREATE OR REPLACE FUNCTION foo1d5(a INT, b INT DEFAULT foo1d4(6)) RETURNS INT RETURN a + b
-- !query schema
struct<>
-- !query output



-- !query
SELECT foo1d5(10), foo1d5(10, 2)
-- !query schema
struct<spark_catalog.default.foo1d5(10):int,spark_catalog.default.foo1d5(10, 2):int>
-- !query output
19	12


-- !query
CREATE OR REPLACE FUNCTION foo1d5(a INT, b INT) RETURNS INT RETURN a + foo1d4(b)
-- !query schema
struct<>
-- !query output



-- !query
SELECT foo1d5(10, 2)
-- !query schema
struct<spark_catalog.default.foo1d5(10, 2):int>
-- !query output
15


-- !query
CREATE OR REPLACE FUNCTION foo1d6(a INT, b INT DEFAULT 7) RETURNS TABLE(a INT, b INT) RETURN SELECT a, b
-- !query schema
struct<>
-- !query output



-- !query
SELECT * FROM foo1d6(5)
-- !query schema
struct<a:int,b:int>
-- !query output
5	7


-- !query
SELECT * FROM foo1d6(5, 2)
-- !query schema
struct<a:int,b:int>
-- !query output
5	2


-- !query
CREATE FUNCTION foo1e1(x INT NOT NULL, y INT) RETURNS INT RETURN 1
-- !query schema
struct<>
-- !query output
org.apache.spark.sql.AnalysisException
{
  "errorClass" : "USER_DEFINED_FUNCTIONS.NOT_NULL_ON_FUNCTION_PARAMETERS",
  "sqlState" : "42601",
  "messageParameters" : {
    "input" : "x INT NOT NULL, y INT"
  }
}


-- !query
CREATE FUNCTION foo1e2(x INT, y INT NOT NULL) RETURNS TABLE (x INT) RETURN SELECT 1
-- !query schema
struct<>
-- !query output
org.apache.spark.sql.AnalysisException
{
  "errorClass" : "USER_DEFINED_FUNCTIONS.NOT_NULL_ON_FUNCTION_PARAMETERS",
  "sqlState" : "42601",
  "messageParameters" : {
    "input" : "x INT, y INT NOT NULL"
  }
}


-- !query
CREATE FUNCTION foo1e3(x INT, y INT) RETURNS TABLE (x INT NOT NULL) RETURN SELECT 1
-- !query schema
struct<>
-- !query output
org.apache.spark.sql.AnalysisException
{
  "errorClass" : "USER_DEFINED_FUNCTIONS.NOT_NULL_ON_FUNCTION_PARAMETERS",
  "sqlState" : "42601",
  "messageParameters" : {
    "input" : "x INT NOT NULL"
  }
}


-- !query
CREATE FUNCTION foo1f1(x INT, y INT GENERATED ALWAYS AS (x + 10)) RETURNS INT RETURN y + 1
-- !query schema
struct<>
-- !query output
org.apache.spark.sql.catalyst.parser.ParseException
{
  "errorClass" : "INVALID_SQL_SYNTAX.CREATE_FUNC_WITH_GENERATED_COLUMNS_AS_PARAMETERS",
  "sqlState" : "42000",
  "queryContext" : [ {
    "objectType" : "",
    "objectName" : "",
    "startIndex" : 1,
    "stopIndex" : 90,
    "fragment" : "CREATE FUNCTION foo1f1(x INT, y INT GENERATED ALWAYS AS (x + 10)) RETURNS INT RETURN y + 1"
  } ]
}


-- !query
CREATE FUNCTION foo1f2(id BIGINT GENERATED ALWAYS AS IDENTITY) RETURNS BIGINT RETURN id + 1
-- !query schema
struct<>
-- !query output
org.apache.spark.sql.catalyst.parser.ParseException
{
  "errorClass" : "INVALID_SQL_SYNTAX.CREATE_FUNC_WITH_GENERATED_COLUMNS_AS_PARAMETERS",
  "sqlState" : "42000",
  "queryContext" : [ {
    "objectType" : "",
    "objectName" : "",
    "startIndex" : 1,
    "stopIndex" : 91,
    "fragment" : "CREATE FUNCTION foo1f2(id BIGINT GENERATED ALWAYS AS IDENTITY) RETURNS BIGINT RETURN id + 1"
  } ]
}


-- !query
CREATE FUNCTION foo1g1(x INT, y INT UNIQUE) RETURNS INT RETURN y + 1
-- !query schema
struct<>
-- !query output
org.apache.spark.sql.catalyst.parser.ParseException
{
  "errorClass" : "INVALID_SQL_SYNTAX.CREATE_FUNC_WITH_COLUMN_CONSTRAINTS",
  "sqlState" : "42000",
  "queryContext" : [ {
    "objectType" : "",
    "objectName" : "",
    "startIndex" : 1,
    "stopIndex" : 68,
    "fragment" : "CREATE FUNCTION foo1g1(x INT, y INT UNIQUE) RETURNS INT RETURN y + 1"
  } ]
}


-- !query
CREATE FUNCTION foo1g2(id BIGINT CHECK (true)) RETURNS BIGINT RETURN id + 1
-- !query schema
struct<>
-- !query output
org.apache.spark.sql.catalyst.parser.ParseException
{
  "errorClass" : "INVALID_SQL_SYNTAX.CREATE_FUNC_WITH_COLUMN_CONSTRAINTS",
  "sqlState" : "42000",
  "queryContext" : [ {
    "objectType" : "",
    "objectName" : "",
    "startIndex" : 1,
    "stopIndex" : 75,
    "fragment" : "CREATE FUNCTION foo1g2(id BIGINT CHECK (true)) RETURNS BIGINT RETURN id + 1"
  } ]
}


-- !query
CREATE FUNCTION foo2a0() RETURNS TABLE() RETURN SELECT 1
-- !query schema
struct<>
-- !query output
org.apache.spark.sql.catalyst.parser.ParseException
{
  "errorClass" : "PARSE_SYNTAX_ERROR",
  "sqlState" : "42601",
  "messageParameters" : {
    "error" : "')'",
    "hint" : ""
  }
}


-- !query
CREATE FUNCTION foo2a2() RETURNS TABLE(c1 INT, c2 INT) RETURN SELECT 1, 2
-- !query schema
struct<>
-- !query output



-- !query
SELECT * FROM foo2a2()
-- !query schema
struct<c1:int,c2:int>
-- !query output
1	2


-- !query
CREATE FUNCTION foo2a4() RETURNS TABLE(c1 INT, c2 INT, c3 INT, c4 INT) RETURN SELECT 1, 2, 3, 4
-- !query schema
struct<>
-- !query output



-- !query
SELECT * FROM foo2a2()
-- !query schema
struct<c1:int,c2:int>
-- !query output
1	2


-- !query
CREATE FUNCTION foo2b1() RETURNS TABLE(DuPLiCatE INT, duplicate INT) RETURN SELECT 1, 2
-- !query schema
struct<>
-- !query output
org.apache.spark.sql.AnalysisException
{
  "errorClass" : "DUPLICATE_ROUTINE_RETURNS_COLUMNS",
  "sqlState" : "42711",
  "messageParameters" : {
    "columns" : "`duplicate`",
    "routineName" : "foo2b1"
  }
}


-- !query
CREATE FUNCTION foo2b2() RETURNS TABLE(a INT, b INT, duplicate INT, c INT, d INT, e INT, DUPLICATE INT)
RETURN SELECT 1, 2, 3, 4, 5, 6, 7
-- !query schema
struct<>
-- !query output
org.apache.spark.sql.AnalysisException
{
  "errorClass" : "DUPLICATE_ROUTINE_RETURNS_COLUMNS",
  "sqlState" : "42711",
  "messageParameters" : {
    "columns" : "`duplicate`",
    "routineName" : "foo2b2"
  }
}


-- !query
CREATE FUNCTION foo2c1() RETURNS TABLE(c1 INT DEFAULT 5) RETURN SELECT 1, 2
-- !query schema
struct<>
-- !query output
org.apache.spark.sql.catalyst.parser.ParseException
{
  "errorClass" : "PARSE_SYNTAX_ERROR",
  "sqlState" : "42601",
  "messageParameters" : {
    "error" : "'DEFAULT'",
    "hint" : ""
  }
}


-- !query
CREATE FUNCTION foo31() RETURNS INT RETURN (SELECT 1, 2)
-- !query schema
struct<>
-- !query output
org.apache.spark.sql.catalyst.ExtendedAnalysisException
{
  "errorClass" : "INVALID_SUBQUERY_EXPRESSION.SCALAR_SUBQUERY_RETURN_MORE_THAN_ONE_OUTPUT_COLUMN",
  "sqlState" : "42823",
  "messageParameters" : {
    "number" : "2"
  },
  "queryContext" : [ {
    "objectType" : "",
    "objectName" : "",
    "startIndex" : 1,
    "stopIndex" : 56,
    "fragment" : "CREATE FUNCTION foo31() RETURNS INT RETURN (SELECT 1, 2)"
  } ]
}


-- !query
CREATE FUNCTION foo32() RETURNS TABLE(a INT) RETURN SELECT 1, 2
-- !query schema
struct<>
-- !query output
org.apache.spark.sql.AnalysisException
{
  "errorClass" : "USER_DEFINED_FUNCTIONS.RETURN_COLUMN_COUNT_MISMATCH",
  "sqlState" : "42601",
  "messageParameters" : {
    "name" : "spark_catalog.default.foo32",
    "outputSize" : "2",
    "returnParamSize" : "1"
  }
}


-- !query
CREATE FUNCTION foo33() RETURNS TABLE(a INT, b INT) RETURN SELECT 1
-- !query schema
struct<>
-- !query output
org.apache.spark.sql.AnalysisException
{
  "errorClass" : "USER_DEFINED_FUNCTIONS.RETURN_COLUMN_COUNT_MISMATCH",
  "sqlState" : "42601",
  "messageParameters" : {
    "name" : "spark_catalog.default.foo33",
    "outputSize" : "1",
    "returnParamSize" : "2"
  }
}


-- !query
CREATE FUNCTION foo41() RETURNS INT RETURN SELECT 1
-- !query schema
struct<>
-- !query output



-- !query
CREATE FUNCTION foo42() RETURNS TABLE(a INT) RETURN 1
-- !query schema
struct<>
-- !query output
org.apache.spark.sql.AnalysisException
{
  "errorClass" : "USER_DEFINED_FUNCTIONS.SQL_TABLE_UDF_BODY_MUST_BE_A_QUERY",
  "sqlState" : "42601",
  "messageParameters" : {
    "name" : "foo42"
  }
}


-- !query
CREATE FUNCTION foo2_1a(a INT) RETURNS INT RETURN a
-- !query schema
struct<>
-- !query output



-- !query
SELECT foo2_1a(5)
-- !query schema
struct<spark_catalog.default.foo2_1a(5):int>
-- !query output
5


-- !query
CREATE FUNCTION foo2_1b(a INT, b INT) RETURNS INT RETURN a + b
-- !query schema
struct<>
-- !query output



-- !query
SELECT foo2_1b(5, 6)
-- !query schema
struct<spark_catalog.default.foo2_1b(5, 6):int>
-- !query output
11


-- !query
CREATE FUNCTION foo2_1c(a INT, b INT) RETURNS INT RETURN 10 * (a + b) + 100 * (a -b)
-- !query schema
struct<>
-- !query output



-- !query
SELECT foo2_1c(5, 6)
-- !query schema
struct<spark_catalog.default.foo2_1c(5, 6):int>
-- !query output
10


-- !query
CREATE FUNCTION foo2_1d(a INT, b INT) RETURNS INT RETURN ABS(a) - LENGTH(CAST(b AS VARCHAR(10)))
-- !query schema
struct<>
-- !query output



-- !query
SELECT foo2_1d(-5, 6)
-- !query schema
struct<spark_catalog.default.foo2_1d(-5, 6):int>
-- !query output
4


-- !query
CREATE FUNCTION foo2_2a(a INT) RETURNS INT RETURN SELECT a
-- !query schema
struct<>
-- !query output



-- !query
SELECT foo2_2a(5)
-- !query schema
struct<spark_catalog.default.foo2_2a(5):int>
-- !query output
5


-- !query
CREATE FUNCTION foo2_2b(a INT) RETURNS INT RETURN 1 + (SELECT a)
-- !query schema
struct<>
-- !query output



-- !query
SELECT foo2_2b(5)
-- !query schema
struct<spark_catalog.default.foo2_2b(5):int>
-- !query output
6


-- !query
CREATE FUNCTION foo2_2c(a INT) RETURNS INT RETURN 1 + (SELECT (SELECT a))
-- !query schema
struct<>
-- !query output
org.apache.spark.sql.catalyst.ExtendedAnalysisException
{
  "errorClass" : "UNRESOLVED_COLUMN.WITHOUT_SUGGESTION",
  "sqlState" : "42703",
  "messageParameters" : {
    "objectName" : "`a`"
  },
  "queryContext" : [ {
    "objectType" : "",
    "objectName" : "",
    "startIndex" : 21,
    "stopIndex" : 21,
    "fragment" : "a"
  } ]
}


-- !query
CREATE FUNCTION foo2_2d(a INT) RETURNS INT RETURN 1 + (SELECT (SELECT (SELECT (SELECT a))))
-- !query schema
struct<>
-- !query output
org.apache.spark.sql.catalyst.ExtendedAnalysisException
{
  "errorClass" : "UNRESOLVED_COLUMN.WITHOUT_SUGGESTION",
  "sqlState" : "42703",
  "messageParameters" : {
    "objectName" : "`a`"
  },
  "queryContext" : [ {
    "objectType" : "",
    "objectName" : "",
    "startIndex" : 37,
    "stopIndex" : 37,
    "fragment" : "a"
  } ]
}


-- !query
CREATE FUNCTION foo2_2e(a INT) RETURNS INT RETURN
SELECT a FROM (VALUES 1) AS V(c1) WHERE c1 = 2
UNION ALL
SELECT a + 1 FROM (VALUES 1) AS V(c1)
-- !query schema
struct<>
-- !query output



-- !query
CREATE FUNCTION foo2_2f(a INT) RETURNS INT RETURN
SELECT a FROM (VALUES 1) AS V(c1)
EXCEPT
SELECT a + 1 FROM (VALUES 1) AS V(a)
-- !query schema
struct<>
-- !query output



-- !query
CREATE FUNCTION foo2_2g(a INT) RETURNS INT RETURN
SELECT a FROM (VALUES 1) AS V(c1)
INTERSECT
SELECT a FROM (VALUES 1) AS V(a)
-- !query schema
struct<>
-- !query output



-- !query
DROP TABLE IF EXISTS t1
-- !query schema
struct<>
-- !query output



-- !query
DROP TABLE IF EXISTS t2
-- !query schema
struct<>
-- !query output



-- !query
DROP TABLE IF EXISTS ts
-- !query schema
struct<>
-- !query output



-- !query
DROP TABLE IF EXISTS tm
-- !query schema
struct<>
-- !query output



-- !query
DROP TABLE IF EXISTS ta
-- !query schema
struct<>
-- !query output



-- !query
DROP TABLE IF EXISTS V1
-- !query schema
struct<>
-- !query output



-- !query
DROP TABLE IF EXISTS V2
-- !query schema
struct<>
-- !query output



-- !query
DROP VIEW IF EXISTS t1
-- !query schema
struct<>
-- !query output



-- !query
DROP VIEW IF EXISTS t2
-- !query schema
struct<>
-- !query output



-- !query
DROP VIEW IF EXISTS ts
-- !query schema
struct<>
-- !query output



-- !query
DROP VIEW IF EXISTS tm
-- !query schema
struct<>
-- !query output



-- !query
DROP VIEW IF EXISTS ta
-- !query schema
struct<>
-- !query output



-- !query
DROP VIEW IF EXISTS V1
-- !query schema
struct<>
-- !query output



-- !query
DROP VIEW IF EXISTS V2
-- !query schema
struct<>
-- !query output



-- !query
CREATE FUNCTION foo2_3(a INT, b INT) RETURNS INT RETURN a + b
-- !query schema
struct<>
-- !query output



-- !query
CREATE VIEW V1(c1, c2) AS VALUES (1, 2), (3, 4), (5, 6)
-- !query schema
struct<>
-- !query output



-- !query
CREATE VIEW V2(c1, c2) AS VALUES (-1, -2), (-3, -4), (-5, -6)
-- !query schema
struct<>
-- !query output



-- !query
SELECT foo2_3(c1, c2), foo2_3(c2, 1), foo2_3(c1, c2) - foo2_3(c2, c1 - 1) FROM V1 ORDER BY 1, 2, 3
-- !query schema
struct<spark_catalog.default.foo2_3(c1, c2):int,spark_catalog.default.foo2_3(c2, 1):int,(spark_catalog.default.foo2_3(c1, c2) - spark_catalog.default.foo2_3(c2, (c1 - 1))):int>
-- !query output
3	3	1
7	5	1
11	7	1


-- !query
SELECT * FROM V1 WHERE foo2_3(c1, 0) = c1 AND foo2_3(c1, c2) < 8
-- !query schema
struct<c1:int,c2:int>
-- !query output
1	2
3	4


-- !query
SELECT foo2_3(SUM(c1), SUM(c2)), SUM(c1) + SUM(c2), SUM(foo2_3(c1, c2) + foo2_3(c2, c1) - foo2_3(c2, c1))
FROM V1
-- !query schema
struct<spark_catalog.default.foo2_3(sum(c1), sum(c2)):int,(sum(c1) + sum(c2)):bigint,sum(((spark_catalog.default.foo2_3(c1, c2) + spark_catalog.default.foo2_3(c2, c1)) - spark_catalog.default.foo2_3(c2, c1))):bigint>
-- !query output
21	21	21


-- !query
CREATE FUNCTION foo2_4a(a ARRAY<STRING>) RETURNS STRING RETURN
SELECT array_sort(a, (i, j) -> rank[i] - rank[j])[0] FROM (SELECT MAP('a', 1, 'b', 2) rank)
-- !query schema
struct<>
-- !query output



-- !query
SELECT foo2_4a(ARRAY('a', 'b'))
-- !query schema
struct<spark_catalog.default.foo2_4a(array(a, b)):string>
-- !query output
a


-- !query
CREATE FUNCTION foo2_4b(m MAP<STRING, STRING>, k STRING) RETURNS STRING RETURN
SELECT v || ' ' || v FROM (SELECT upper(m[k]) AS v)
-- !query schema
struct<>
-- !query output



-- !query
SELECT foo2_4b(map('a', 'hello', 'b', 'world'), 'a')
-- !query schema
struct<spark_catalog.default.foo2_4b(map(a, hello, b, world), a):string>
-- !query output
HELLO HELLO


-- !query
DROP VIEW V2
-- !query schema
struct<>
-- !query output



-- !query
DROP VIEW V1
-- !query schema
struct<>
-- !query output



-- !query
CREATE VIEW t1(c1, c2) AS VALUES (0, 1), (0, 2), (1, 2)
-- !query schema
struct<>
-- !query output



-- !query
CREATE VIEW t2(c1, c2) AS VALUES (0, 2), (0, 3)
-- !query schema
struct<>
-- !query output



-- !query
CREATE VIEW ts(x) AS VALUES NAMED_STRUCT('a', 1, 'b', 2)
-- !query schema
struct<>
-- !query output



-- !query
CREATE VIEW tm(x) AS VALUES MAP('a', 1, 'b', 2)
-- !query schema
struct<>
-- !query output



-- !query
CREATE VIEW ta(x) AS VALUES ARRAY(1, 2, 3)
-- !query schema
struct<>
-- !query output



-- !query
CREATE FUNCTION foo3_1a(a DOUBLE, b DOUBLE) RETURNS DOUBLE RETURN a * b
-- !query schema
struct<>
-- !query output



-- !query
CREATE FUNCTION foo3_1b(x INT) RETURNS INT RETURN x
-- !query schema
struct<>
-- !query output



-- !query
CREATE FUNCTION foo3_1c(x INT) RETURNS INT RETURN SELECT x
-- !query schema
struct<>
-- !query output



-- !query
CREATE FUNCTION foo3_1d(x INT) RETURNS INT RETURN (SELECT SUM(c2) FROM t2 WHERE c1 = x)
-- !query schema
struct<>
-- !query output



-- !query
CREATE FUNCTION foo3_1e() RETURNS INT RETURN foo3_1d(0)
-- !query schema
struct<>
-- !query output



-- !query
CREATE FUNCTION foo3_1f() RETURNS INT RETURN SELECT SUM(c2) FROM t2 WHERE c1 = 0
-- !query schema
struct<>
-- !query output



-- !query
CREATE FUNCTION foo3_1g(x INT) RETURNS INT RETURN SELECT (SELECT x)
-- !query schema
struct<>
-- !query output



-- !query
SELECT a, b, foo3_1a(a + 1, b + 1) FROM t1 AS t(a, b)
-- !query schema
struct<a:int,b:int,spark_catalog.default.foo3_1a((a + 1), (b + 1)):double>
-- !query output
0	1	2.0
0	2	3.0
1	2	6.0


-- !query
SELECT x, foo3_1c(x) FROM t1 AS t(x, y)
-- !query schema
struct<x:int,spark_catalog.default.foo3_1c(x):int>
-- !query output
0	0
0	0
1	1


-- !query
SELECT c1, foo3_1d(c1) FROM t1
-- !query schema
struct<c1:int,spark_catalog.default.foo3_1d(c1):int>
-- !query output
0	5
0	5
1	NULL


-- !query
SELECT c1, foo3_1a(foo3_1b(c1), foo3_1b(c1)) FROM t1
-- !query schema
struct<c1:int,spark_catalog.default.foo3_1a(spark_catalog.default.foo3_1b(c1), spark_catalog.default.foo3_1b(c1)):double>
-- !query output
0	0.0
0	0.0
1	1.0


-- !query
SELECT c1, foo3_1d(foo3_1c(foo3_1b(c1))) FROM t1
-- !query schema
struct<c1:int,spark_catalog.default.foo3_1d(spark_catalog.default.foo3_1c(spark_catalog.default.foo3_1b(c1))):int>
-- !query output
0	5
0	5
1	NULL


-- !query
SELECT c1, foo3_1a(foo3_1c(foo3_1b(c1)), foo3_1d(foo3_1b(c1))) FROM t1
-- !query schema
struct<c1:int,spark_catalog.default.foo3_1a(spark_catalog.default.foo3_1c(spark_catalog.default.foo3_1b(c1)), spark_catalog.default.foo3_1d(spark_catalog.default.foo3_1b(c1))):double>
-- !query output
0	0.0
0	0.0
1	NULL


-- !query
SELECT foo3_1c(foo3_1e()) FROM t1
-- !query schema
struct<spark_catalog.default.foo3_1c(spark_catalog.default.foo3_1e()):int>
-- !query output
5
5
5


-- !query
SELECT foo3_1a(MAX(c1), MAX(c2)) FROM t1
-- !query schema
struct<spark_catalog.default.foo3_1a(max(c1), max(c2)):double>
-- !query output
2.0


-- !query
SELECT foo3_1a(MAX(c1), c2) FROM t1 GROUP BY c2
-- !query schema
struct<spark_catalog.default.foo3_1a(max(c1), c2):double>
-- !query output
0.0
2.0


-- !query
SELECT foo3_1a(c1, c2) FROM t1 GROUP BY c1, c2
-- !query schema
struct<spark_catalog.default.foo3_1a(c1, c2):double>
-- !query output
0.0
0.0
2.0


-- !query
SELECT MAX(foo3_1a(c1, c2)) FROM t1 GROUP BY c1, c2
-- !query schema
struct<max(spark_catalog.default.foo3_1a(c1, c2)):double>
-- !query output
0.0
0.0
2.0


-- !query
SELECT MAX(c1) + foo3_1b(MAX(c1)) FROM t1 GROUP BY c2
-- !query schema
struct<(max(c1) + spark_catalog.default.foo3_1b(max(c1))):int>
-- !query output
0
2


-- !query
SELECT c1, SUM(foo3_1c(c2)) FROM t1 GROUP BY c1
-- !query schema
struct<c1:int,sum(spark_catalog.default.foo3_1c(c2)):bigint>
-- !query output
0	3
1	2


-- !query
SELECT c1, SUM(foo3_1d(c2)) FROM t1 GROUP BY c1
-- !query schema
struct<c1:int,sum(spark_catalog.default.foo3_1d(c2)):bigint>
-- !query output
0	NULL
1	NULL


-- !query
SELECT foo3_1c(c1), foo3_1d(c1) FROM t1 GROUP BY c1
-- !query schema
struct<spark_catalog.default.foo3_1c(c1):int,spark_catalog.default.foo3_1d(c1):int>
-- !query output
0	5
1	NULL


-- !query
SELECT foo3_1a(SUM(c1), rand(0) * 0) FROM t1
-- !query schema
struct<spark_catalog.default.foo3_1a(sum(c1), (rand(0) * 0)):double>
-- !query output
0.0


-- !query
SELECT foo3_1a(SUM(c1) + rand(0) * 0, SUM(c2)) FROM t1
-- !query schema
struct<spark_catalog.default.foo3_1a((sum(c1) + (rand(0) * 0)), sum(c2)):double>
-- !query output
5.0


-- !query
SELECT foo3_1b(SUM(c1) + rand(0) * 0) FROM t1
-- !query schema
struct<spark_catalog.default.foo3_1b((sum(c1) + (rand(0) * 0))):int>
-- !query output
1


-- !query
SELECT foo3_1b(SUM(1) + rand(0) * 0) FROM t1 GROUP BY c2
-- !query schema
struct<spark_catalog.default.foo3_1b((sum(1) + (rand(0) * 0))):int>
-- !query output
1
2


-- !query
SELECT foo3_1c(SUM(c2) + rand(0) * 0) FROM t1 GROUP by c1
-- !query schema
struct<spark_catalog.default.foo3_1c((sum(c2) + (rand(0) * 0))):int>
-- !query output
2
3


-- !query
SELECT foo3_1b(foo3_1b(MAX(c2))) FROM t1
-- !query schema
struct<spark_catalog.default.foo3_1b(spark_catalog.default.foo3_1b(max(c2))):int>
-- !query output
2


-- !query
SELECT foo3_1b(MAX(foo3_1b(c2))) FROM t1
-- !query schema
struct<spark_catalog.default.foo3_1b(max(spark_catalog.default.foo3_1b(c2))):int>
-- !query output
2


-- !query
SELECT foo3_1a(foo3_1b(c1), MAX(c2)) FROM t1 GROUP BY c1
-- !query schema
struct<spark_catalog.default.foo3_1a(spark_catalog.default.foo3_1b(c1), max(c2)):double>
-- !query output
0.0
2.0


-- !query
SELECT c1, foo3_1b(c1) FROM t1 GROUP BY c1
-- !query schema
struct<c1:int,spark_catalog.default.foo3_1b(c1):int>
-- !query output
0	0
1	1


-- !query
SELECT c1, foo3_1b(c1 + 1) FROM t1 GROUP BY c1
-- !query schema
struct<c1:int,spark_catalog.default.foo3_1b((c1 + 1)):int>
-- !query output
0	1
1	2


-- !query
SELECT c1, foo3_1b(c1 + rand(0) * 0) FROM t1 GROUP BY c1
-- !query schema
struct<c1:int,spark_catalog.default.foo3_1b((c1 + (rand(0) * 0))):int>
-- !query output
0	0
1	1


-- !query
SELECT c1, foo3_1a(c1, MIN(c2)) FROM t1 GROUP BY c1
-- !query schema
struct<c1:int,spark_catalog.default.foo3_1a(c1, min(c2)):double>
-- !query output
0	0.0
1	2.0


-- !query
SELECT c1, foo3_1a(c1 + 1, MIN(c2 + 1)) FROM t1 GROUP BY c1
-- !query schema
struct<c1:int,spark_catalog.default.foo3_1a((c1 + 1), min((c2 + 1))):double>
-- !query output
0	2.0
1	6.0


-- !query
SELECT c1, c2, foo3_1a(c1, c2) FROM t1 GROUP BY c1, c2
-- !query schema
struct<c1:int,c2:int,spark_catalog.default.foo3_1a(c1, c2):double>
-- !query output
0	1	0.0
0	2	0.0
1	2	2.0


-- !query
SELECT c1, c2, foo3_1a(1, 2) FROM t1 GROUP BY c1, c2
-- !query schema
struct<c1:int,c2:int,spark_catalog.default.foo3_1a(1, 2):double>
-- !query output
0	1	2.0
0	2	2.0
1	2	2.0


-- !query
SELECT c1 + c2, foo3_1b(c1 + c2 + 1) FROM t1 GROUP BY c1 + c2
-- !query schema
struct<(c1 + c2):int,spark_catalog.default.foo3_1b(((c1 + c2) + 1)):int>
-- !query output
1	2
2	3
3	4


-- !query
SELECT COUNT(*) + foo3_1b(c1) + foo3_1b(SUM(c2)) + SUM(foo3_1b(c2)) FROM t1 GROUP BY c1
-- !query schema
struct<(((count(1) + spark_catalog.default.foo3_1b(c1)) + spark_catalog.default.foo3_1b(sum(c2))) + sum(spark_catalog.default.foo3_1b(c2))):bigint>
-- !query output
6
8


-- !query
SELECT c1, COUNT(*), foo3_1b(SUM(c2)) FROM t1 GROUP BY c1 HAVING COUNT(*) > 0
-- !query schema
struct<c1:int,count(1):bigint,spark_catalog.default.foo3_1b(sum(c2)):int>
-- !query output
0	2	3
1	1	2


-- !query
SELECT c1, COUNT(*), foo3_1b(SUM(c2)) FROM t1 GROUP BY c1 HAVING foo3_1b(SUM(c2)) > 0
-- !query schema
struct<c1:int,count(1):bigint,spark_catalog.default.foo3_1b(sum(c2)):int>
-- !query output
0	2	3
1	1	2


-- !query
SELECT c1, COUNT(*), foo3_1b(SUM(c2)) FROM t1 GROUP BY c1 HAVING SUM(foo3_1b(c2)) > 0
-- !query schema
struct<c1:int,count(1):bigint,spark_catalog.default.foo3_1b(sum(c2)):int>
-- !query output
0	2	3
1	1	2


-- !query
SELECT foo3_1b(c1), MIN(c2) FROM t1 GROUP BY 1
-- !query schema
struct<spark_catalog.default.foo3_1b(c1):int,min(c2):int>
-- !query output
0	1
1	2


-- !query
SELECT foo3_1a(c1 + rand(0) * 0, c2) FROM t1 GROUP BY 1
-- !query schema
struct<spark_catalog.default.foo3_1a((c1 + (rand(0) * 0)), c2):double>
-- !query output
0.0
2.0


-- !query
SELECT c1, c2, foo3_1a(c1, c2) FROM t1 GROUP BY c1, c2, 3
-- !query schema
struct<c1:int,c2:int,spark_catalog.default.foo3_1a(c1, c2):double>
-- !query output
0	1	0.0
0	2	0.0
1	2	2.0


-- !query
SELECT c1, (SELECT c1), (SELECT foo3_1b(c1)), SUM(c2) FROM t1 GROUP BY 1, 2, 3
-- !query schema
struct<c1:int,scalarsubquery(c1):int,scalarsubquery(c1):int,sum(c2):bigint>
-- !query output
0	0	0	3
1	1	1	2


-- !query
SELECT c1, SUM(c2) + foo3_1a(MIN(c2), MAX(c2)) + (SELECT SUM(c2)) FROM t1 GROUP BY c1
-- !query schema
struct<>
-- !query output
org.apache.spark.sql.catalyst.ExtendedAnalysisException
{
  "errorClass" : "UNSUPPORTED_SUBQUERY_EXPRESSION_CATEGORY.CORRELATED_REFERENCE",
  "sqlState" : "0A000",
  "messageParameters" : {
    "sqlExprs" : "\"sum(c2) AS `sum(outer(spark_catalog.default.t1.c2))`\""
  },
  "queryContext" : [ {
    "objectType" : "",
    "objectName" : "",
    "startIndex" : 51,
    "stopIndex" : 64,
    "fragment" : "SELECT SUM(c2)"
  } ]
}


-- !query
SELECT foo3_1b(SUM(c1)) + (SELECT foo3_1b(SUM(c1))) FROM t1
-- !query schema
struct<>
-- !query output
org.apache.spark.sql.catalyst.ExtendedAnalysisException
{
  "errorClass" : "UNSUPPORTED_SUBQUERY_EXPRESSION_CATEGORY.CORRELATED_REFERENCE",
  "sqlState" : "0A000",
  "messageParameters" : {
    "sqlExprs" : "\"sum(c1) AS `sum(outer(spark_catalog.default.t1.c1))`\""
  },
  "queryContext" : [ {
    "objectType" : "",
    "objectName" : "",
    "startIndex" : 27,
    "stopIndex" : 51,
    "fragment" : "(SELECT foo3_1b(SUM(c1)))"
  } ]
}


-- !query
SELECT SUM(foo3_1b(SUM(c1))) FROM t1
-- !query schema
struct<>
-- !query output
org.apache.spark.sql.AnalysisException
{
  "errorClass" : "NESTED_AGGREGATE_FUNCTION",
  "sqlState" : "42607",
  "queryContext" : [ {
    "objectType" : "",
    "objectName" : "",
    "startIndex" : 12,
    "stopIndex" : 27,
    "fragment" : "foo3_1b(SUM(c1))"
  } ]
}


-- !query
SELECT foo3_1b(SUM(c1)) + (SELECT SUM(SUM(c1))) FROM t1
-- !query schema
struct<>
-- !query output
org.apache.spark.sql.catalyst.ExtendedAnalysisException
{
  "errorClass" : "NESTED_AGGREGATE_FUNCTION",
  "sqlState" : "42607",
  "queryContext" : [ {
    "objectType" : "",
    "objectName" : "",
    "startIndex" : 39,
    "stopIndex" : 45,
    "fragment" : "SUM(c1)"
  } ]
}


-- !query
SELECT foo3_1b(SUM(c1) + SUM(SUM(c1))) FROM t1
-- !query schema
struct<>
-- !query output
org.apache.spark.sql.catalyst.ExtendedAnalysisException
{
  "errorClass" : "NESTED_AGGREGATE_FUNCTION",
  "sqlState" : "42607",
  "queryContext" : [ {
    "objectType" : "",
    "objectName" : "",
    "startIndex" : 30,
    "stopIndex" : 36,
    "fragment" : "SUM(c1)"
  } ]
}


-- !query
SELECT foo3_1b(SUM(c1 + rand(0) * 0)) FROM t1
-- !query schema
struct<>
-- !query output
org.apache.spark.sql.catalyst.ExtendedAnalysisException
{
  "errorClass" : "AGGREGATE_FUNCTION_WITH_NONDETERMINISTIC_EXPRESSION",
  "sqlState" : "42845",
  "messageParameters" : {
    "sqlExpr" : "\"sum((c1 + (rand(0) * 0)))\""
  },
  "queryContext" : [ {
    "objectType" : "",
    "objectName" : "",
    "startIndex" : 20,
    "stopIndex" : 35,
    "fragment" : "c1 + rand(0) * 0"
  } ]
}


-- !query
SELECT SUM(foo3_1b(c1) + rand(0) * 0) FROM t1
-- !query schema
struct<>
-- !query output
org.apache.spark.sql.catalyst.ExtendedAnalysisException
{
  "errorClass" : "AGGREGATE_FUNCTION_WITH_NONDETERMINISTIC_EXPRESSION",
  "sqlState" : "42845",
  "messageParameters" : {
    "sqlExpr" : "\"sum((spark_catalog.default.foo3_1b(foo3_1b.x) + (rand(0) * 0)))\""
  },
  "queryContext" : [ {
    "objectType" : "",
    "objectName" : "",
    "startIndex" : 12,
    "stopIndex" : 36,
    "fragment" : "foo3_1b(c1) + rand(0) * 0"
  } ]
}


-- !query
SELECT SUM(foo3_1b(c1 + rand(0) * 0)) FROM t1
-- !query schema
struct<sum(spark_catalog.default.foo3_1b((c1 + (rand(0) * 0)))):bigint>
-- !query output
1


-- !query
SELECT foo3_1b(SUM(c1) + foo3_1b(SUM(c1))) FROM t1
-- !query schema
struct<spark_catalog.default.foo3_1b((sum(c1) + spark_catalog.default.foo3_1b(sum(c1)))):int>
-- !query output
2


-- !query
SELECT foo3_1b(SUM(c2) + foo3_1b(SUM(c1))) AS foo FROM t1 HAVING foo > 0
-- !query schema
struct<foo:int>
-- !query output
6


-- !query
SELECT c1, COUNT(*), foo3_1b(SUM(c2) + foo3_1b(SUM(c2))) FROM t1 GROUP BY c1 HAVING COUNT(*) > 0
-- !query schema
struct<c1:int,count(1):bigint,spark_catalog.default.foo3_1b((sum(c2) + spark_catalog.default.foo3_1b(sum(c2)))):int>
-- !query output
0	2	6
1	1	4


-- !query
SELECT foo3_1a(c1, MAX(c2)) FROM t1 GROUP BY c1, 1
-- !query schema
struct<>
-- !query output
org.apache.spark.sql.AnalysisException
{
  "errorClass" : "GROUP_BY_POS_AGGREGATE",
  "sqlState" : "42903",
  "messageParameters" : {
    "aggExpr" : "spark_catalog.default.foo3_1a(spark_catalog.default.t1.c1, max(spark_catalog.default.t1.c2)) AS `spark_catalog.default.foo3_1a(c1, max(c2))`",
    "index" : "1"
  },
  "queryContext" : [ {
    "objectType" : "",
    "objectName" : "",
    "startIndex" : 50,
    "stopIndex" : 50,
    "fragment" : "1"
  } ]
}


-- !query
WITH cte AS (SELECT foo3_1a(c1, c2) FROM t1)
SELECT * FROM cte
-- !query schema
struct<spark_catalog.default.foo3_1a(c1, c2):double>
-- !query output
0.0
0.0
2.0


-- !query
SELECT SUM(c2) FROM t1 GROUP BY foo3_1b(c1)
-- !query schema
struct<sum(c2):bigint>
-- !query output
2
3


-- !query
SELECT foo3_1b(c1), SUM(c2) FROM t1 GROUP BY 1
-- !query schema
struct<spark_catalog.default.foo3_1b(c1):int,sum(c2):bigint>
-- !query output
0	3
1	2


-- !query
SELECT foo3_1b(c1), c2, GROUPING(foo3_1b(c1)), SUM(c1) FROM t1 GROUP BY ROLLUP(foo3_1b(c1), c2)
-- !query schema
struct<spark_catalog.default.foo3_1b(c1):int,c2:int,grouping(spark_catalog.default.foo3_1b(c1)):tinyint,sum(c1):bigint>
-- !query output
0	1	0	0
0	2	0	0
0	NULL	0	0
1	2	0	1
1	NULL	0	1
NULL	NULL	1	1


-- !query
SELECT c1, SUM(c2) FROM t1 GROUP BY c1 HAVING foo3_1b(SUM(c2)) > 1
-- !query schema
struct<c1:int,sum(c2):bigint>
-- !query output
0	3
1	2


-- !query
SELECT c1, SUM(c2) FROM t1 GROUP BY CUBE(c1) HAVING foo3_1b(GROUPING(c1)) = 0
-- !query schema
struct<c1:int,sum(c2):bigint>
-- !query output
0	3
1	2


-- !query
SELECT * FROM t1 JOIN t2 ON foo3_1a(t1.c1, t2.c2) >= 2
-- !query schema
struct<c1:int,c2:int,c1:int,c2:int>
-- !query output
1	2	0	2
1	2	0	3


-- !query
SELECT * FROM t1 JOIN t2 ON foo3_1b(t1.c2) = foo3_1b(t2.c2)
-- !query schema
struct<c1:int,c2:int,c1:int,c2:int>
-- !query output
0	2	0	2
1	2	0	2


-- !query
SELECT * FROM t1 JOIN t2 ON foo3_1b(t1.c1 + t2.c1 + 2) > 2
-- !query schema
struct<c1:int,c2:int,c1:int,c2:int>
-- !query output
1	2	0	2
1	2	0	3


-- !query
SELECT * FROM t1 JOIN t2 ON foo3_1a(foo3_1b(t1.c1), t2.c2) >= 2
-- !query schema
struct<c1:int,c2:int,c1:int,c2:int>
-- !query output
1	2	0	2
1	2	0	3


-- !query
SELECT * FROM t1 JOIN t2 ON foo3_1f() > 0
-- !query schema
struct<c1:int,c2:int,c1:int,c2:int>
-- !query output
0	1	0	2
0	1	0	3
0	2	0	2
0	2	0	3
1	2	0	2
1	2	0	3


-- !query
SELECT * FROM t1 JOIN t2 ON foo3_1b(t1.c1 + rand(0) * 0) > 1
-- !query schema
struct<>
-- !query output
org.apache.spark.sql.catalyst.ExtendedAnalysisException
{
  "errorClass" : "INVALID_NON_DETERMINISTIC_EXPRESSIONS",
  "sqlState" : "42K0E",
  "messageParameters" : {
    "sqlExprs" : "\"(spark_catalog.default.foo3_1b(foo3_1b.x) > 1)\""
  }
}


-- !query
SELECT * FROM t1 JOIN t2 ON foo3_1c(t1.c1) = 2
-- !query schema
struct<c1:int,c2:int,c1:int,c2:int>
-- !query output



-- !query
SELECT * FROM t1 JOIN t2 ON foo3_1g(t1.c1) = 2
-- !query schema
struct<>
-- !query output
org.apache.spark.sql.catalyst.ExtendedAnalysisException
{
  "errorClass" : "UNSUPPORTED_SUBQUERY_EXPRESSION_CATEGORY.UNSUPPORTED_CORRELATED_SCALAR_SUBQUERY",
  "sqlState" : "0A000",
  "messageParameters" : {
    "treeNode" : "Join Inner, (spark_catalog.default.foo3_1g(x#x) = 2)\n:  +- Project [outer(x#x)]\n:     +- OneRowRelation\n:- SubqueryAlias spark_catalog.default.t1\n:  +- View (`spark_catalog`.`default`.`t1`, [c1#x, c2#x])\n:     +- Project [cast(col1#x as int) AS c1#x, cast(col2#x as int) AS c2#x]\n:        +- LocalRelation [col1#x, col2#x]\n+- SubqueryAlias spark_catalog.default.t2\n   +- View (`spark_catalog`.`default`.`t2`, [c1#x, c2#x])\n      +- Project [cast(col1#x as int) AS c1#x, cast(col2#x as int) AS c2#x]\n         +- LocalRelation [col1#x, col2#x]\n"
  }
}


-- !query
SELECT * FROM t1 ORDER BY foo3_1b(c1)
-- !query schema
struct<>
-- !query output
org.apache.spark.sql.AnalysisException
{
  "errorClass" : "UNSUPPORTED_SQL_UDF_USAGE",
  "sqlState" : "0A000",
  "messageParameters" : {
    "functionName" : "`spark_catalog`.`default`.`foo3_1b`",
    "nodeName" : "Sort"
  },
  "queryContext" : [ {
    "objectType" : "",
    "objectName" : "",
    "startIndex" : 27,
    "stopIndex" : 37,
    "fragment" : "foo3_1b(c1)"
  } ]
}


-- !query
SELECT * FROM t1 LIMIT foo3_1b(1)
-- !query schema
struct<>
-- !query output
org.apache.spark.sql.AnalysisException
{
  "errorClass" : "UNSUPPORTED_SQL_UDF_USAGE",
  "sqlState" : "0A000",
  "messageParameters" : {
    "functionName" : "`spark_catalog`.`default`.`foo3_1b`",
    "nodeName" : "GlobalLimit"
  },
  "queryContext" : [ {
    "objectType" : "",
    "objectName" : "",
    "startIndex" : 24,
    "stopIndex" : 33,
    "fragment" : "foo3_1b(1)"
  } ]
}


-- !query
SELECT * FROM ta LATERAL VIEW EXPLODE(ARRAY(foo3_1b(x[0]), foo3_1b(x[1]))) AS t
-- !query schema
struct<>
-- !query output
org.apache.spark.sql.AnalysisException
{
  "errorClass" : "UNSUPPORTED_SQL_UDF_USAGE",
  "sqlState" : "0A000",
  "messageParameters" : {
    "functionName" : "`spark_catalog`.`default`.`foo3_1b`",
    "nodeName" : "Generate"
  },
  "queryContext" : [ {
    "objectType" : "",
    "objectName" : "",
    "startIndex" : 45,
    "stopIndex" : 57,
    "fragment" : "foo3_1b(x[0])"
  } ]
}


-- !query
SELECT CASE WHEN foo3_1b(rand(0) * 0 < 1 THEN 1 ELSE -1 END
-- !query schema
struct<>
-- !query output
org.apache.spark.sql.catalyst.parser.ParseException
{
  "errorClass" : "PARSE_SYNTAX_ERROR",
  "sqlState" : "42601",
  "messageParameters" : {
    "error" : "'foo3_1b'",
    "hint" : ""
  }
}


-- !query
SELECT (SELECT SUM(c2) FROM t2 WHERE c1 = foo3_1b(t1.c1)) FROM t1
-- !query schema
struct<scalarsubquery(c1):bigint>
-- !query output
5
5
NULL


-- !query
SELECT foo3_1b((SELECT SUM(c1) FROM t1))
-- !query schema
struct<spark_catalog.default.foo3_1b(scalarsubquery()):int>
-- !query output
1


-- !query
SELECT foo3_1a(c1, (SELECT MIN(c1) FROM t1)) FROM t1
-- !query schema
struct<spark_catalog.default.foo3_1a(c1, scalarsubquery()):double>
-- !query output
0.0
0.0
0.0


-- !query
SELECT foo3_1b((SELECT SUM(c1))) FROM t1
-- !query schema
struct<>
-- !query output
org.apache.spark.sql.catalyst.ExtendedAnalysisException
{
  "errorClass" : "UNSUPPORTED_SUBQUERY_EXPRESSION_CATEGORY.CORRELATED_REFERENCE",
  "sqlState" : "0A000",
  "messageParameters" : {
    "sqlExprs" : "\"sum(c1) AS `sum(outer(spark_catalog.default.t1.c1))`\""
  },
  "queryContext" : [ {
    "objectType" : "",
    "objectName" : "",
    "startIndex" : 17,
    "stopIndex" : 30,
    "fragment" : "SELECT SUM(c1)"
  } ]
}


-- !query
SELECT foo3_1b((SELECT SUM(c1) FROM t1 WHERE c2 = t2.c2)) FROM t2
-- !query schema
struct<spark_catalog.default.foo3_1b(scalarsubquery(c2)):int>
-- !query output
1
NULL


-- !query
SELECT c2, AVG(foo3_1b((SELECT COUNT(*) FROM t1 WHERE c2 = t2.c2))) OVER (PARTITION BY c1) AS r FROM t2
-- !query schema
struct<c2:int,r:double>
-- !query output
2	1.0
3	1.0


-- !query
CREATE FUNCTION foo3_1x(x STRUCT<a: INT, b: INT>) RETURNS INT RETURN x.a + x.b
-- !query schema
struct<>
-- !query output



-- !query
CREATE FUNCTION foo3_1y(x ARRAY<INT>) RETURNS INT RETURN aggregate(x, BIGINT(0), (x, y) -> x + y)
-- !query schema
struct<>
-- !query output



-- !query
SELECT foo3_1a(x.a, x.b) FROM ts
-- !query schema
struct<spark_catalog.default.foo3_1a(x.a, x.b):double>
-- !query output
2.0


-- !query
SELECT foo3_1x(x) FROM ts
-- !query schema
struct<spark_catalog.default.foo3_1x(x):int>
-- !query output
3


-- !query
SELECT foo3_1a(x['a'], x['b']) FROM tm
-- !query schema
struct<spark_catalog.default.foo3_1a(x[a], x[b]):double>
-- !query output
2.0


-- !query
SELECT foo3_1a(x[0], x[1]) FROM ta
-- !query schema
struct<spark_catalog.default.foo3_1a(x[0], x[1]):double>
-- !query output
2.0


-- !query
SELECT foo3_1y(x) FROM ta
-- !query schema
struct<spark_catalog.default.foo3_1y(x):int>
-- !query output
6


-- !query
CREATE FUNCTION foo3_2a() RETURNS INT RETURN FLOOR(RAND() * 6) + 1
-- !query schema
struct<>
-- !query output



-- !query
SELECT CASE WHEN foo3_2a() > 6 THEN FALSE ELSE TRUE END
-- !query schema
struct<CASE WHEN (spark_catalog.default.foo3_2a() > 6) THEN false ELSE true END:boolean>
-- !query output
true


-- !query
SELECT * FROM t1 JOIN t2 ON foo3_2a() = 1
-- !query schema
struct<>
-- !query output
org.apache.spark.sql.catalyst.ExtendedAnalysisException
{
  "errorClass" : "INVALID_NON_DETERMINISTIC_EXPRESSIONS",
  "sqlState" : "42K0E",
  "messageParameters" : {
    "sqlExprs" : "\"(spark_catalog.default.foo3_2a() = 1)\""
  }
}


-- !query
CREATE FUNCTION foo3_2b1(x INT) RETURNS BOOLEAN RETURN x IN (SELECT 1)
-- !query schema
struct<>
-- !query output



-- !query
SELECT * FROM t1 WHERE foo3_2b1(c1)
-- !query schema
struct<c1:int,c2:int>
-- !query output
1	2


-- !query
CREATE FUNCTION foo3_2b2(x INT) RETURNS INT RETURN IF(x IN (SELECT 1), 1, 0)
-- !query schema
struct<>
-- !query output



-- !query
SELECT * FROM t1 WHERE foo3_2b2(c1) = 0
-- !query schema
struct<c1:int,c2:int>
-- !query output
0	1
0	2


-- !query
SELECT foo3_2b2(c1) FROM t1
-- !query schema
struct<spark_catalog.default.foo3_2b2(c1):int>
-- !query output
0
0
1


-- !query
CREATE FUNCTION foo3_2b3(x INT) RETURNS BOOLEAN RETURN x IN (SELECT c1 FROM t2)
-- !query schema
struct<>
-- !query output



-- !query
SELECT * FROM t1 WHERE foo3_2b3(c1)
-- !query schema
struct<c1:int,c2:int>
-- !query output
0	1
0	2


-- !query
CREATE FUNCTION foo3_2b4(x INT) RETURNS BOOLEAN RETURN x NOT IN (SELECT c2 FROM t2 WHERE x = c1)
-- !query schema
struct<>
-- !query output



-- !query
SELECT * FROM t1 WHERE foo3_2b4(c1)
-- !query schema
struct<c1:int,c2:int>
-- !query output
0	1
0	2
1	2


-- !query
CREATE FUNCTION foo3_2b5(x INT) RETURNS BOOLEAN RETURN SUM(1) + IF(x IN (SELECT 1), 1, 0)
-- !query schema
struct<>
-- !query output
org.apache.spark.sql.AnalysisException
{
  "errorClass" : "USER_DEFINED_FUNCTIONS.CANNOT_CONTAIN_COMPLEX_FUNCTIONS",
  "sqlState" : "42601",
  "messageParameters" : {
    "queryText" : "SUM(1) + IF(x IN (SELECT 1), 1, 0)"
  }
}


-- !query
CREATE FUNCTION foo3_2b5(x INT) RETURNS BOOLEAN RETURN y IN (SELECT 1)
-- !query schema
struct<>
-- !query output
org.apache.spark.sql.catalyst.ExtendedAnalysisException
{
  "errorClass" : "UNRESOLVED_COLUMN.WITH_SUGGESTION",
  "sqlState" : "42703",
  "messageParameters" : {
    "objectName" : "`y`",
    "proposal" : "`x`"
  },
  "queryContext" : [ {
    "objectType" : "",
    "objectName" : "",
    "startIndex" : 1,
    "stopIndex" : 1,
    "fragment" : "y"
  } ]
}


-- !query
CREATE FUNCTION foo3_2b5(x INT) RETURNS BOOLEAN RETURN x IN (SELECT x WHERE x = 1)
-- !query schema
struct<>
-- !query output



-- !query
CREATE FUNCTION foo3_2c1(x INT) RETURNS BOOLEAN RETURN EXISTS(SELECT 1)
-- !query schema
struct<>
-- !query output



-- !query
SELECT * FROM t1 WHERE foo3_2c1(c1)
-- !query schema
struct<c1:int,c2:int>
-- !query output
0	1
0	2
1	2


-- !query
CREATE FUNCTION foo3_2c2(x INT) RETURNS BOOLEAN RETURN NOT EXISTS(SELECT * FROM t2 WHERE c1 = x)
-- !query schema
struct<>
-- !query output



-- !query
SELECT * FROM t1 WHERE foo3_2c2(c1)
-- !query schema
struct<c1:int,c2:int>
-- !query output
1	2


-- !query
CREATE FUNCTION foo3_2d1(x INT) RETURNS INT RETURN SELECT (SELECT x)
-- !query schema
struct<>
-- !query output



-- !query
CREATE FUNCTION foo3_2d2(x INT) RETURNS INT RETURN SELECT (SELECT 1 WHERE EXISTS (SELECT * FROM t2 WHERE c1 = x))
-- !query schema
struct<>
-- !query output
org.apache.spark.sql.catalyst.ExtendedAnalysisException
{
  "errorClass" : "UNRESOLVED_COLUMN.WITH_SUGGESTION",
  "sqlState" : "42703",
  "messageParameters" : {
    "objectName" : "`x`",
    "proposal" : "`c1`, `c2`"
  },
  "queryContext" : [ {
    "objectType" : "",
    "objectName" : "",
    "startIndex" : 60,
    "stopIndex" : 60,
    "fragment" : "x"
  } ]
}


-- !query
CREATE FUNCTION foo3_2e1(
    occurrences ARRAY<STRUCT<start_time: TIMESTAMP, occurrence_id: STRING>>,
    instance_start_time TIMESTAMP
) RETURNS STRING RETURN
WITH t AS (
    SELECT transform(occurrences, x -> named_struct(
        'diff', abs(unix_millis(x.start_time) - unix_millis(instance_start_time)),
        'id', x.occurrence_id
    )) AS diffs
)
SELECT CASE WHEN occurrences IS NULL OR size(occurrences) = 0
       THEN NULL
       ELSE sort_array(diffs)[0].id END AS id
FROM t
-- !query schema
struct<>
-- !query output



-- !query
SELECT foo3_2e1(
    ARRAY(STRUCT('2022-01-01 10:11:12', '1'), STRUCT('2022-01-01 10:11:15', '2')),
    '2022-01-01')
-- !query schema
struct<spark_catalog.default.foo3_2e1(array(struct(2022-01-01 10:11:12, 1), struct(2022-01-01 10:11:15, 2)), 2022-01-01):string>
-- !query output
1


-- !query
SET spark.sql.ansi.enabled=true
-- !query schema
struct<key:string,value:string>
-- !query output
spark.sql.ansi.enabled	true


-- !query
CREATE FUNCTION foo3_3a(x INT) RETURNS DOUBLE RETURN 1 / x
-- !query schema
struct<>
-- !query output



-- !query
CREATE FUNCTION foo3_3at(x INT) RETURNS TABLE (a DOUBLE) RETURN SELECT 1 / x
-- !query schema
struct<>
-- !query output



-- !query
CREATE TEMPORARY FUNCTION foo3_3b(x INT) RETURNS DOUBLE RETURN 1 / x
-- !query schema
struct<>
-- !query output



-- !query
SET spark.sql.ansi.enabled=false
-- !query schema
struct<key:string,value:string>
-- !query output
spark.sql.ansi.enabled	false


-- !query
SELECT foo3_3a(0)
-- !query schema
struct<>
-- !query output
org.apache.spark.SparkArithmeticException
{
  "errorClass" : "DIVIDE_BY_ZERO",
  "sqlState" : "22012",
  "messageParameters" : {
    "config" : "\"spark.sql.ansi.enabled\""
  },
  "queryContext" : [ {
    "objectType" : "",
    "objectName" : "",
    "startIndex" : 1,
    "stopIndex" : 5,
    "fragment" : "1 / x"
  } ]
}


-- !query
SELECT foo3_3b(0)
-- !query schema
struct<>
-- !query output
org.apache.spark.SparkArithmeticException
{
  "errorClass" : "DIVIDE_BY_ZERO",
  "sqlState" : "22012",
  "messageParameters" : {
    "config" : "\"spark.sql.ansi.enabled\""
  },
  "queryContext" : [ {
    "objectType" : "",
    "objectName" : "",
    "startIndex" : 1,
    "stopIndex" : 5,
    "fragment" : "1 / x"
  } ]
}


-- !query
SELECT * FROM foo3_3at(0)
-- !query schema
struct<>
-- !query output
org.apache.spark.SparkArithmeticException
{
  "errorClass" : "DIVIDE_BY_ZERO",
  "sqlState" : "22012",
  "messageParameters" : {
    "config" : "\"spark.sql.ansi.enabled\""
  },
  "queryContext" : [ {
    "objectType" : "",
    "objectName" : "",
    "startIndex" : 8,
    "stopIndex" : 12,
    "fragment" : "1 / x"
  } ]
}


-- !query
CREATE OR REPLACE FUNCTION foo3_3a(x INT) RETURNS DOUBLE RETURN 1 / x
-- !query schema
struct<>
-- !query output



-- !query
CREATE OR REPLACE FUNCTION foo3_3at(x INT) RETURNS TABLE (a DOUBLE) RETURN SELECT 1 / x
-- !query schema
struct<>
-- !query output
org.apache.spark.sql.catalyst.analysis.FunctionAlreadyExistsException
{
  "errorClass" : "ROUTINE_ALREADY_EXISTS",
  "sqlState" : "42723",
  "messageParameters" : {
    "existingRoutineType" : "routine",
    "newRoutineType" : "routine",
    "routineName" : "`default`.`foo3_3at`"
  }
}


-- !query
CREATE OR REPLACE TEMPORARY FUNCTION foo3_3b(x INT) RETURNS DOUBLE RETURN 1 / x
-- !query schema
struct<>
-- !query output



-- !query
SELECT foo3_3a(0)
-- !query schema
struct<spark_catalog.default.foo3_3a(0):double>
-- !query output
NULL


-- !query
SELECT foo3_3b(0)
-- !query schema
struct<foo3_3b(0):double>
-- !query output
NULL


-- !query
SELECT * FROM foo3_3at(0)
-- !query schema
struct<>
-- !query output
org.apache.spark.SparkArithmeticException
{
  "errorClass" : "DIVIDE_BY_ZERO",
  "sqlState" : "22012",
  "messageParameters" : {
    "config" : "\"spark.sql.ansi.enabled\""
  },
  "queryContext" : [ {
    "objectType" : "",
    "objectName" : "",
    "startIndex" : 8,
    "stopIndex" : 12,
    "fragment" : "1 / x"
  } ]
}


-- !query
CREATE FUNCTION foo3_3c() RETURNS INT RETURN CAST('a' AS INT)
-- !query schema
struct<>
-- !query output



-- !query
CREATE FUNCTION foo3_3ct() RETURNS TABLE (a INT) RETURN SELECT CAST('a' AS INT)
-- !query schema
struct<>
-- !query output



-- !query
CREATE FUNCTION foo3_3d() RETURNS INT RETURN 'a' + 1
-- !query schema
struct<>
-- !query output



-- !query
CREATE FUNCTION foo3_3dt() RETURNS TABLE (a INT) RETURN SELECT 'a' + 1
-- !query schema
struct<>
-- !query output



-- !query
SELECT foo3_3c()
-- !query schema
struct<spark_catalog.default.foo3_3c():int>
-- !query output
NULL


-- !query
SELECT foo3_3d()
-- !query schema
struct<spark_catalog.default.foo3_3d():int>
-- !query output
NULL


-- !query
SELECT * FROM foo3_3ct()
-- !query schema
struct<a:int>
-- !query output
NULL


-- !query
SELECT * FROM foo3_3dt()
-- !query schema
struct<a:int>
-- !query output
NULL


-- !query
SET spark.sql.ansi.enabled=true
-- !query schema
struct<key:string,value:string>
-- !query output
spark.sql.ansi.enabled	true


-- !query
SELECT foo3_3c()
-- !query schema
struct<spark_catalog.default.foo3_3c():int>
-- !query output
NULL


-- !query
SELECT foo3_3d()
-- !query schema
struct<spark_catalog.default.foo3_3d():int>
-- !query output
NULL


-- !query
SELECT * FROM foo3_3ct()
-- !query schema
struct<a:int>
-- !query output
NULL


-- !query
SELECT * FROM foo3_3dt()
-- !query schema
struct<a:int>
-- !query output
NULL


-- !query
RESET spark.sql.ansi.enabled
-- !query schema
struct<>
-- !query output



-- !query
CREATE FUNCTION foo4_0() RETURNS TABLE (x INT) RETURN SELECT 1
-- !query schema
struct<>
-- !query output



-- !query
CREATE FUNCTION foo4_1(x INT) RETURNS TABLE (a INT) RETURN SELECT x
-- !query schema
struct<>
-- !query output



-- !query
CREATE FUNCTION foo4_2(x INT) RETURNS TABLE (a INT) RETURN SELECT c2 FROM t2 WHERE c1 = x
-- !query schema
struct<>
-- !query output



-- !query
CREATE FUNCTION foo4_3(x INT) RETURNS TABLE (a INT, cnt INT) RETURN SELECT c1, COUNT(*) FROM t2 WHERE c1 = x GROUP BY c1
-- !query schema
struct<>
-- !query output



-- !query
SELECT * FROM foo4_0()
-- !query schema
struct<x:int>
-- !query output
1


-- !query
SELECT * FROM foo4_1(1)
-- !query schema
struct<a:int>
-- !query output
1


-- !query
SELECT * FROM foo4_2(2)
-- !query schema
struct<a:int>
-- !query output



-- !query
SELECT * FROM foo4_3(0)
-- !query schema
struct<a:int,cnt:int>
-- !query output
0	2


-- !query
SELECT * FROM foo4_1(rand(0) * 0)
-- !query schema
struct<a:int>
-- !query output
0


-- !query
SELECT * FROM foo4_1(x => 1)
-- !query schema
struct<a:int>
-- !query output
1


-- !query
SELECT * FROM t1, LATERAL foo4_1(c1)
-- !query schema
struct<c1:int,c2:int,a:int>
-- !query output
0	1	0
0	2	0
1	2	1


-- !query
SELECT * FROM t1, LATERAL foo4_2(c1)
-- !query schema
struct<c1:int,c2:int,a:int>
-- !query output
0	1	2
0	1	3
0	2	2
0	2	3


-- !query
SELECT * FROM t1 JOIN LATERAL foo4_2(c1) ON t1.c2 = foo4_2.a
-- !query schema
struct<c1:int,c2:int,a:int>
-- !query output
0	2	2


-- !query
SELECT * FROM t1, LATERAL foo4_3(c1)
-- !query schema
struct<c1:int,c2:int,a:int,cnt:int>
-- !query output
0	1	0	2
0	2	0	2


-- !query
SELECT * FROM t1, LATERAL (SELECT cnt FROM foo4_3(c1))
-- !query schema
struct<c1:int,c2:int,cnt:int>
-- !query output
0	1	2
0	2	2


-- !query
SELECT * FROM t1, LATERAL foo4_1(c1 + rand(0) * 0)
-- !query schema
struct<>
-- !query output
org.apache.spark.sql.catalyst.ExtendedAnalysisException
{
  "errorClass" : "UNSUPPORTED_SUBQUERY_EXPRESSION_CATEGORY.NON_DETERMINISTIC_LATERAL_SUBQUERIES",
  "sqlState" : "0A000",
  "messageParameters" : {
    "treeNode" : "LateralJoin lateral-subquery#x [c1#x], Inner\n:  +- SQLFunctionNode spark_catalog.default.foo4_1\n:     +- SubqueryAlias foo4_1\n:        +- Project [cast(x#x as int) AS a#x]\n:           +- LateralJoin lateral-subquery#x [x#x], Inner\n:              :  +- Project [outer(x#x) AS x#x]\n:              :     +- OneRowRelation\n:              +- Project [cast((cast(outer(c1#x) as double) + (rand(number) * cast(0 as double))) as int) AS x#x]\n:                 +- OneRowRelation\n+- SubqueryAlias spark_catalog.default.t1\n   +- View (`spark_catalog`.`default`.`t1`, [c1#x, c2#x])\n      +- Project [cast(col1#x as int) AS c1#x, cast(col2#x as int) AS c2#x]\n         +- LocalRelation [col1#x, col2#x]\n"
  },
  "queryContext" : [ {
    "objectType" : "",
    "objectName" : "",
    "startIndex" : 10,
    "stopIndex" : 50,
    "fragment" : "FROM t1, LATERAL foo4_1(c1 + rand(0) * 0)"
  } ]
}


-- !query
SELECT * FROM t1 JOIN foo4_1(1) AS foo4_1(x) ON t1.c1 = foo4_1.x
-- !query schema
struct<c1:int,c2:int,x:int>
-- !query output
1	2	1


-- !query
SELECT * FROM t1, LATERAL foo4_1(c1), LATERAL foo4_2(foo4_1.a + c1)
-- !query schema
struct<c1:int,c2:int,a:int,a:int>
-- !query output
0	1	0	2
0	1	0	3
0	2	0	2
0	2	0	3


-- !query
SELECT (SELECT MAX(a) FROM foo4_1(c1)) FROM t1
-- !query schema
struct<scalarsubquery(c1):int>
-- !query output
0
0
1


-- !query
SELECT (SELECT MAX(a) FROM foo4_1(c1) WHERE a = c2) FROM t1
-- !query schema
struct<scalarsubquery(c2, c1):int>
-- !query output
NULL
NULL
NULL


-- !query
SELECT (SELECT MAX(cnt) FROM foo4_3(c1)) FROM t1
-- !query schema
struct<scalarsubquery(c1):int>
-- !query output
2
2
NULL


-- !query
DROP VIEW t1
-- !query schema
struct<>
-- !query output



-- !query
DROP VIEW t2
-- !query schema
struct<>
-- !query output

