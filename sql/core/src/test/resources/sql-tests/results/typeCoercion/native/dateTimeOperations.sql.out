-- Automatically generated by SQLQueryTestSuite
-- !query
CREATE TEMPORARY VIEW t AS SELECT 1
-- !query schema
struct<>
-- !query output



-- !query
select cast(1 as tinyint) + interval 2 day
-- !query schema
struct<>
-- !query output
org.apache.spark.sql.catalyst.ExtendedAnalysisException
{
  "errorClass" : "DATATYPE_MISMATCH.UNEXPECTED_INPUT_TYPE",
  "sqlState" : "42K09",
  "messageParameters" : {
    "inputSql" : "\"CAST(1 AS TINYINT)\"",
    "inputType" : "\"TINYINT\"",
    "paramIndex" : "first",
    "requiredType" : "\"(TIMESTAMP OR TIMESTAMP WITHOUT TIME ZONE)\"",
    "sqlExpr" : "\"CAST(1 AS TINYINT) + INTERVAL '2' DAY\""
  },
  "queryContext" : [ {
    "objectType" : "",
    "objectName" : "",
    "startIndex" : 8,
    "stopIndex" : 42,
    "fragment" : "cast(1 as tinyint) + interval 2 day"
  } ]
}


-- !query
select cast(1 as smallint) + interval 2 day
-- !query schema
struct<>
-- !query output
org.apache.spark.sql.catalyst.ExtendedAnalysisException
{
  "errorClass" : "DATATYPE_MISMATCH.UNEXPECTED_INPUT_TYPE",
  "sqlState" : "42K09",
  "messageParameters" : {
    "inputSql" : "\"CAST(1 AS SMALLINT)\"",
    "inputType" : "\"SMALLINT\"",
    "paramIndex" : "first",
    "requiredType" : "\"(TIMESTAMP OR TIMESTAMP WITHOUT TIME ZONE)\"",
    "sqlExpr" : "\"CAST(1 AS SMALLINT) + INTERVAL '2' DAY\""
  },
  "queryContext" : [ {
    "objectType" : "",
    "objectName" : "",
    "startIndex" : 8,
    "stopIndex" : 43,
    "fragment" : "cast(1 as smallint) + interval 2 day"
  } ]
}


-- !query
select cast(1 as int) + interval 2 day
-- !query schema
struct<>
-- !query output
org.apache.spark.sql.catalyst.ExtendedAnalysisException
{
  "errorClass" : "DATATYPE_MISMATCH.UNEXPECTED_INPUT_TYPE",
  "sqlState" : "42K09",
  "messageParameters" : {
    "inputSql" : "\"CAST(1 AS INT)\"",
    "inputType" : "\"INT\"",
    "paramIndex" : "first",
    "requiredType" : "\"(TIMESTAMP OR TIMESTAMP WITHOUT TIME ZONE)\"",
    "sqlExpr" : "\"CAST(1 AS INT) + INTERVAL '2' DAY\""
  },
  "queryContext" : [ {
    "objectType" : "",
    "objectName" : "",
    "startIndex" : 8,
    "stopIndex" : 38,
    "fragment" : "cast(1 as int) + interval 2 day"
  } ]
}


-- !query
select cast(1 as bigint) + interval 2 day
-- !query schema
struct<>
-- !query output
org.apache.spark.sql.catalyst.ExtendedAnalysisException
{
  "errorClass" : "DATATYPE_MISMATCH.UNEXPECTED_INPUT_TYPE",
  "sqlState" : "42K09",
  "messageParameters" : {
    "inputSql" : "\"CAST(1 AS BIGINT)\"",
    "inputType" : "\"BIGINT\"",
    "paramIndex" : "first",
    "requiredType" : "\"(TIMESTAMP OR TIMESTAMP WITHOUT TIME ZONE)\"",
    "sqlExpr" : "\"CAST(1 AS BIGINT) + INTERVAL '2' DAY\""
  },
  "queryContext" : [ {
    "objectType" : "",
    "objectName" : "",
    "startIndex" : 8,
    "stopIndex" : 41,
    "fragment" : "cast(1 as bigint) + interval 2 day"
  } ]
}


-- !query
select cast(1 as float) + interval 2 day
-- !query schema
struct<>
-- !query output
org.apache.spark.sql.catalyst.ExtendedAnalysisException
{
  "errorClass" : "DATATYPE_MISMATCH.UNEXPECTED_INPUT_TYPE",
  "sqlState" : "42K09",
  "messageParameters" : {
    "inputSql" : "\"CAST(1 AS FLOAT)\"",
    "inputType" : "\"FLOAT\"",
    "paramIndex" : "first",
    "requiredType" : "\"(TIMESTAMP OR TIMESTAMP WITHOUT TIME ZONE)\"",
    "sqlExpr" : "\"CAST(1 AS FLOAT) + INTERVAL '2' DAY\""
  },
  "queryContext" : [ {
    "objectType" : "",
    "objectName" : "",
    "startIndex" : 8,
    "stopIndex" : 40,
    "fragment" : "cast(1 as float) + interval 2 day"
  } ]
}


-- !query
select cast(1 as double) + interval 2 day
-- !query schema
struct<>
-- !query output
org.apache.spark.sql.catalyst.ExtendedAnalysisException
{
  "errorClass" : "DATATYPE_MISMATCH.UNEXPECTED_INPUT_TYPE",
  "sqlState" : "42K09",
  "messageParameters" : {
    "inputSql" : "\"CAST(1 AS DOUBLE)\"",
    "inputType" : "\"DOUBLE\"",
    "paramIndex" : "first",
    "requiredType" : "\"(TIMESTAMP OR TIMESTAMP WITHOUT TIME ZONE)\"",
    "sqlExpr" : "\"CAST(1 AS DOUBLE) + INTERVAL '2' DAY\""
  },
  "queryContext" : [ {
    "objectType" : "",
    "objectName" : "",
    "startIndex" : 8,
    "stopIndex" : 41,
    "fragment" : "cast(1 as double) + interval 2 day"
  } ]
}


-- !query
select cast(1 as decimal(10, 0)) + interval 2 day
-- !query schema
struct<>
-- !query output
org.apache.spark.sql.catalyst.ExtendedAnalysisException
{
  "errorClass" : "DATATYPE_MISMATCH.UNEXPECTED_INPUT_TYPE",
  "sqlState" : "42K09",
  "messageParameters" : {
    "inputSql" : "\"CAST(1 AS DECIMAL(10,0))\"",
    "inputType" : "\"DECIMAL(10,0)\"",
    "paramIndex" : "first",
    "requiredType" : "\"(TIMESTAMP OR TIMESTAMP WITHOUT TIME ZONE)\"",
    "sqlExpr" : "\"CAST(1 AS DECIMAL(10,0)) + INTERVAL '2' DAY\""
  },
  "queryContext" : [ {
    "objectType" : "",
    "objectName" : "",
    "startIndex" : 8,
    "stopIndex" : 49,
    "fragment" : "cast(1 as decimal(10, 0)) + interval 2 day"
  } ]
}


-- !query
select cast('2017-12-11' as string) + interval 2 day
-- !query schema
struct<CAST(2017-12-11 AS STRING) + INTERVAL '2' DAY:string>
-- !query output
2017-12-13 00:00:00


-- !query
select cast('2017-12-11 09:30:00' as string) + interval 2 day
-- !query schema
struct<CAST(2017-12-11 09:30:00 AS STRING) + INTERVAL '2' DAY:string>
-- !query output
2017-12-13 09:30:00


-- !query
select cast('1' as binary) + interval 2 day
-- !query schema
struct<>
-- !query output
org.apache.spark.sql.catalyst.ExtendedAnalysisException
{
  "errorClass" : "DATATYPE_MISMATCH.UNEXPECTED_INPUT_TYPE",
  "sqlState" : "42K09",
  "messageParameters" : {
    "inputSql" : "\"CAST(1 AS BINARY)\"",
    "inputType" : "\"BINARY\"",
    "paramIndex" : "first",
    "requiredType" : "\"(TIMESTAMP OR TIMESTAMP WITHOUT TIME ZONE)\"",
    "sqlExpr" : "\"CAST(1 AS BINARY) + INTERVAL '2' DAY\""
  },
  "queryContext" : [ {
    "objectType" : "",
    "objectName" : "",
    "startIndex" : 8,
    "stopIndex" : 43,
    "fragment" : "cast('1' as binary) + interval 2 day"
  } ]
}


-- !query
select cast(1 as boolean) + interval 2 day
-- !query schema
struct<>
-- !query output
org.apache.spark.sql.catalyst.ExtendedAnalysisException
{
  "errorClass" : "DATATYPE_MISMATCH.UNEXPECTED_INPUT_TYPE",
  "sqlState" : "42K09",
  "messageParameters" : {
    "inputSql" : "\"CAST(1 AS BOOLEAN)\"",
    "inputType" : "\"BOOLEAN\"",
    "paramIndex" : "first",
    "requiredType" : "\"(TIMESTAMP OR TIMESTAMP WITHOUT TIME ZONE)\"",
    "sqlExpr" : "\"CAST(1 AS BOOLEAN) + INTERVAL '2' DAY\""
  },
  "queryContext" : [ {
    "objectType" : "",
    "objectName" : "",
    "startIndex" : 8,
    "stopIndex" : 42,
    "fragment" : "cast(1 as boolean) + interval 2 day"
  } ]
}


-- !query
select cast('2017-12-11 09:30:00.0' as timestamp) + interval 2 day
-- !query schema
struct<CAST(2017-12-11 09:30:00.0 AS TIMESTAMP) + INTERVAL '2' DAY:timestamp>
-- !query output
2017-12-13 09:30:00


-- !query
select cast('2017-12-11 09:30:00' as date) + interval 2 day
-- !query schema
struct<date_add(CAST(2017-12-11 09:30:00 AS DATE), extractansiintervaldays(INTERVAL '2' DAY)):date>
-- !query output
2017-12-13


-- !query
select interval 2 day + cast(1 as tinyint)
-- !query schema
struct<>
-- !query output
org.apache.spark.sql.catalyst.ExtendedAnalysisException
{
  "errorClass" : "DATATYPE_MISMATCH.UNEXPECTED_INPUT_TYPE",
  "sqlState" : "42K09",
  "messageParameters" : {
    "inputSql" : "\"CAST(1 AS TINYINT)\"",
    "inputType" : "\"TINYINT\"",
    "paramIndex" : "first",
    "requiredType" : "\"(TIMESTAMP OR TIMESTAMP WITHOUT TIME ZONE)\"",
    "sqlExpr" : "\"CAST(1 AS TINYINT) + INTERVAL '2' DAY\""
  },
  "queryContext" : [ {
    "objectType" : "",
    "objectName" : "",
    "startIndex" : 8,
    "stopIndex" : 42,
    "fragment" : "interval 2 day + cast(1 as tinyint)"
  } ]
}


-- !query
select interval 2 day + cast(1 as smallint)
-- !query schema
struct<>
-- !query output
org.apache.spark.sql.catalyst.ExtendedAnalysisException
{
  "errorClass" : "DATATYPE_MISMATCH.UNEXPECTED_INPUT_TYPE",
  "sqlState" : "42K09",
  "messageParameters" : {
    "inputSql" : "\"CAST(1 AS SMALLINT)\"",
    "inputType" : "\"SMALLINT\"",
    "paramIndex" : "first",
    "requiredType" : "\"(TIMESTAMP OR TIMESTAMP WITHOUT TIME ZONE)\"",
    "sqlExpr" : "\"CAST(1 AS SMALLINT) + INTERVAL '2' DAY\""
  },
  "queryContext" : [ {
    "objectType" : "",
    "objectName" : "",
    "startIndex" : 8,
    "stopIndex" : 43,
    "fragment" : "interval 2 day + cast(1 as smallint)"
  } ]
}


-- !query
select interval 2 day + cast(1 as int)
-- !query schema
struct<>
-- !query output
org.apache.spark.sql.catalyst.ExtendedAnalysisException
{
  "errorClass" : "DATATYPE_MISMATCH.UNEXPECTED_INPUT_TYPE",
  "sqlState" : "42K09",
  "messageParameters" : {
    "inputSql" : "\"CAST(1 AS INT)\"",
    "inputType" : "\"INT\"",
    "paramIndex" : "first",
    "requiredType" : "\"(TIMESTAMP OR TIMESTAMP WITHOUT TIME ZONE)\"",
    "sqlExpr" : "\"CAST(1 AS INT) + INTERVAL '2' DAY\""
  },
  "queryContext" : [ {
    "objectType" : "",
    "objectName" : "",
    "startIndex" : 8,
    "stopIndex" : 38,
    "fragment" : "interval 2 day + cast(1 as int)"
  } ]
}


-- !query
select interval 2 day + cast(1 as bigint)
-- !query schema
struct<>
-- !query output
org.apache.spark.sql.catalyst.ExtendedAnalysisException
{
  "errorClass" : "DATATYPE_MISMATCH.UNEXPECTED_INPUT_TYPE",
  "sqlState" : "42K09",
  "messageParameters" : {
    "inputSql" : "\"CAST(1 AS BIGINT)\"",
    "inputType" : "\"BIGINT\"",
    "paramIndex" : "first",
    "requiredType" : "\"(TIMESTAMP OR TIMESTAMP WITHOUT TIME ZONE)\"",
    "sqlExpr" : "\"CAST(1 AS BIGINT) + INTERVAL '2' DAY\""
  },
  "queryContext" : [ {
    "objectType" : "",
    "objectName" : "",
    "startIndex" : 8,
    "stopIndex" : 41,
    "fragment" : "interval 2 day + cast(1 as bigint)"
  } ]
}


-- !query
select interval 2 day + cast(1 as float)
-- !query schema
struct<>
-- !query output
org.apache.spark.sql.catalyst.ExtendedAnalysisException
{
  "errorClass" : "DATATYPE_MISMATCH.UNEXPECTED_INPUT_TYPE",
  "sqlState" : "42K09",
  "messageParameters" : {
    "inputSql" : "\"CAST(1 AS FLOAT)\"",
    "inputType" : "\"FLOAT\"",
    "paramIndex" : "first",
    "requiredType" : "\"(TIMESTAMP OR TIMESTAMP WITHOUT TIME ZONE)\"",
    "sqlExpr" : "\"CAST(1 AS FLOAT) + INTERVAL '2' DAY\""
  },
  "queryContext" : [ {
    "objectType" : "",
    "objectName" : "",
    "startIndex" : 8,
    "stopIndex" : 40,
    "fragment" : "interval 2 day + cast(1 as float)"
  } ]
}


-- !query
select interval 2 day + cast(1 as double)
-- !query schema
struct<>
-- !query output
org.apache.spark.sql.catalyst.ExtendedAnalysisException
{
  "errorClass" : "DATATYPE_MISMATCH.UNEXPECTED_INPUT_TYPE",
  "sqlState" : "42K09",
  "messageParameters" : {
    "inputSql" : "\"CAST(1 AS DOUBLE)\"",
    "inputType" : "\"DOUBLE\"",
    "paramIndex" : "first",
    "requiredType" : "\"(TIMESTAMP OR TIMESTAMP WITHOUT TIME ZONE)\"",
    "sqlExpr" : "\"CAST(1 AS DOUBLE) + INTERVAL '2' DAY\""
  },
  "queryContext" : [ {
    "objectType" : "",
    "objectName" : "",
    "startIndex" : 8,
    "stopIndex" : 41,
    "fragment" : "interval 2 day + cast(1 as double)"
  } ]
}


-- !query
select interval 2 day + cast(1 as decimal(10, 0))
-- !query schema
struct<>
-- !query output
org.apache.spark.sql.catalyst.ExtendedAnalysisException
{
  "errorClass" : "DATATYPE_MISMATCH.UNEXPECTED_INPUT_TYPE",
  "sqlState" : "42K09",
  "messageParameters" : {
    "inputSql" : "\"CAST(1 AS DECIMAL(10,0))\"",
    "inputType" : "\"DECIMAL(10,0)\"",
    "paramIndex" : "first",
    "requiredType" : "\"(TIMESTAMP OR TIMESTAMP WITHOUT TIME ZONE)\"",
    "sqlExpr" : "\"CAST(1 AS DECIMAL(10,0)) + INTERVAL '2' DAY\""
  },
  "queryContext" : [ {
    "objectType" : "",
    "objectName" : "",
    "startIndex" : 8,
    "stopIndex" : 49,
    "fragment" : "interval 2 day + cast(1 as decimal(10, 0))"
  } ]
}


-- !query
select interval 2 day + cast('2017-12-11' as string)
-- !query schema
struct<CAST(2017-12-11 AS STRING) + INTERVAL '2' DAY:string>
-- !query output
2017-12-13 00:00:00


-- !query
select interval 2 day + cast('2017-12-11 09:30:00' as string)
-- !query schema
struct<CAST(2017-12-11 09:30:00 AS STRING) + INTERVAL '2' DAY:string>
-- !query output
2017-12-13 09:30:00


-- !query
select interval 2 day + cast('1' as binary)
-- !query schema
struct<>
-- !query output
org.apache.spark.sql.catalyst.ExtendedAnalysisException
{
  "errorClass" : "DATATYPE_MISMATCH.UNEXPECTED_INPUT_TYPE",
  "sqlState" : "42K09",
  "messageParameters" : {
    "inputSql" : "\"CAST(1 AS BINARY)\"",
    "inputType" : "\"BINARY\"",
    "paramIndex" : "first",
    "requiredType" : "\"(TIMESTAMP OR TIMESTAMP WITHOUT TIME ZONE)\"",
    "sqlExpr" : "\"CAST(1 AS BINARY) + INTERVAL '2' DAY\""
  },
  "queryContext" : [ {
    "objectType" : "",
    "objectName" : "",
    "startIndex" : 8,
    "stopIndex" : 43,
    "fragment" : "interval 2 day + cast('1' as binary)"
  } ]
}


-- !query
select interval 2 day + cast(1 as boolean)
-- !query schema
struct<>
-- !query output
org.apache.spark.sql.catalyst.ExtendedAnalysisException
{
  "errorClass" : "DATATYPE_MISMATCH.UNEXPECTED_INPUT_TYPE",
  "sqlState" : "42K09",
  "messageParameters" : {
    "inputSql" : "\"CAST(1 AS BOOLEAN)\"",
    "inputType" : "\"BOOLEAN\"",
    "paramIndex" : "first",
    "requiredType" : "\"(TIMESTAMP OR TIMESTAMP WITHOUT TIME ZONE)\"",
    "sqlExpr" : "\"CAST(1 AS BOOLEAN) + INTERVAL '2' DAY\""
  },
  "queryContext" : [ {
    "objectType" : "",
    "objectName" : "",
    "startIndex" : 8,
    "stopIndex" : 42,
    "fragment" : "interval 2 day + cast(1 as boolean)"
  } ]
}


-- !query
select interval 2 day + cast('2017-12-11 09:30:00.0' as timestamp)
-- !query schema
struct<CAST(2017-12-11 09:30:00.0 AS TIMESTAMP) + INTERVAL '2' DAY:timestamp>
-- !query output
2017-12-13 09:30:00


-- !query
select interval 2 day + cast('2017-12-11 09:30:00' as date)
-- !query schema
struct<date_add(CAST(2017-12-11 09:30:00 AS DATE), extractansiintervaldays(INTERVAL '2' DAY)):date>
-- !query output
2017-12-13


-- !query
select cast(1 as tinyint) - interval 2 day
-- !query schema
struct<>
-- !query output
org.apache.spark.sql.catalyst.ExtendedAnalysisException
{
  "errorClass" : "DATATYPE_MISMATCH.UNEXPECTED_INPUT_TYPE",
  "sqlState" : "42K09",
  "messageParameters" : {
    "inputSql" : "\"CAST(1 AS TINYINT)\"",
    "inputType" : "\"TINYINT\"",
    "paramIndex" : "first",
    "requiredType" : "\"(TIMESTAMP OR TIMESTAMP WITHOUT TIME ZONE)\"",
    "sqlExpr" : "\"CAST(1 AS TINYINT) + (- INTERVAL '2' DAY)\""
  },
  "queryContext" : [ {
    "objectType" : "",
    "objectName" : "",
    "startIndex" : 8,
    "stopIndex" : 42,
    "fragment" : "cast(1 as tinyint) - interval 2 day"
  } ]
}


-- !query
select cast(1 as smallint) - interval 2 day
-- !query schema
struct<>
-- !query output
org.apache.spark.sql.catalyst.ExtendedAnalysisException
{
  "errorClass" : "DATATYPE_MISMATCH.UNEXPECTED_INPUT_TYPE",
  "sqlState" : "42K09",
  "messageParameters" : {
    "inputSql" : "\"CAST(1 AS SMALLINT)\"",
    "inputType" : "\"SMALLINT\"",
    "paramIndex" : "first",
    "requiredType" : "\"(TIMESTAMP OR TIMESTAMP WITHOUT TIME ZONE)\"",
    "sqlExpr" : "\"CAST(1 AS SMALLINT) + (- INTERVAL '2' DAY)\""
  },
  "queryContext" : [ {
    "objectType" : "",
    "objectName" : "",
    "startIndex" : 8,
    "stopIndex" : 43,
    "fragment" : "cast(1 as smallint) - interval 2 day"
  } ]
}


-- !query
select cast(1 as int) - interval 2 day
-- !query schema
struct<>
-- !query output
org.apache.spark.sql.catalyst.ExtendedAnalysisException
{
  "errorClass" : "DATATYPE_MISMATCH.UNEXPECTED_INPUT_TYPE",
  "sqlState" : "42K09",
  "messageParameters" : {
    "inputSql" : "\"CAST(1 AS INT)\"",
    "inputType" : "\"INT\"",
    "paramIndex" : "first",
    "requiredType" : "\"(TIMESTAMP OR TIMESTAMP WITHOUT TIME ZONE)\"",
    "sqlExpr" : "\"CAST(1 AS INT) + (- INTERVAL '2' DAY)\""
  },
  "queryContext" : [ {
    "objectType" : "",
    "objectName" : "",
    "startIndex" : 8,
    "stopIndex" : 38,
    "fragment" : "cast(1 as int) - interval 2 day"
  } ]
}


-- !query
select cast(1 as bigint) - interval 2 day
-- !query schema
struct<>
-- !query output
org.apache.spark.sql.catalyst.ExtendedAnalysisException
{
  "errorClass" : "DATATYPE_MISMATCH.UNEXPECTED_INPUT_TYPE",
  "sqlState" : "42K09",
  "messageParameters" : {
    "inputSql" : "\"CAST(1 AS BIGINT)\"",
    "inputType" : "\"BIGINT\"",
    "paramIndex" : "first",
    "requiredType" : "\"(TIMESTAMP OR TIMESTAMP WITHOUT TIME ZONE)\"",
    "sqlExpr" : "\"CAST(1 AS BIGINT) + (- INTERVAL '2' DAY)\""
  },
  "queryContext" : [ {
    "objectType" : "",
    "objectName" : "",
    "startIndex" : 8,
    "stopIndex" : 41,
    "fragment" : "cast(1 as bigint) - interval 2 day"
  } ]
}


-- !query
select cast(1 as float) - interval 2 day
-- !query schema
struct<>
-- !query output
org.apache.spark.sql.catalyst.ExtendedAnalysisException
{
  "errorClass" : "DATATYPE_MISMATCH.UNEXPECTED_INPUT_TYPE",
  "sqlState" : "42K09",
  "messageParameters" : {
    "inputSql" : "\"CAST(1 AS FLOAT)\"",
    "inputType" : "\"FLOAT\"",
    "paramIndex" : "first",
    "requiredType" : "\"(TIMESTAMP OR TIMESTAMP WITHOUT TIME ZONE)\"",
    "sqlExpr" : "\"CAST(1 AS FLOAT) + (- INTERVAL '2' DAY)\""
  },
  "queryContext" : [ {
    "objectType" : "",
    "objectName" : "",
    "startIndex" : 8,
    "stopIndex" : 40,
    "fragment" : "cast(1 as float) - interval 2 day"
  } ]
}


-- !query
select cast(1 as double) - interval 2 day
-- !query schema
struct<>
-- !query output
org.apache.spark.sql.catalyst.ExtendedAnalysisException
{
  "errorClass" : "DATATYPE_MISMATCH.UNEXPECTED_INPUT_TYPE",
  "sqlState" : "42K09",
  "messageParameters" : {
    "inputSql" : "\"CAST(1 AS DOUBLE)\"",
    "inputType" : "\"DOUBLE\"",
    "paramIndex" : "first",
    "requiredType" : "\"(TIMESTAMP OR TIMESTAMP WITHOUT TIME ZONE)\"",
    "sqlExpr" : "\"CAST(1 AS DOUBLE) + (- INTERVAL '2' DAY)\""
  },
  "queryContext" : [ {
    "objectType" : "",
    "objectName" : "",
    "startIndex" : 8,
    "stopIndex" : 41,
    "fragment" : "cast(1 as double) - interval 2 day"
  } ]
}


-- !query
select cast(1 as decimal(10, 0)) - interval 2 day
-- !query schema
struct<>
-- !query output
org.apache.spark.sql.catalyst.ExtendedAnalysisException
{
  "errorClass" : "DATATYPE_MISMATCH.UNEXPECTED_INPUT_TYPE",
  "sqlState" : "42K09",
  "messageParameters" : {
    "inputSql" : "\"CAST(1 AS DECIMAL(10,0))\"",
    "inputType" : "\"DECIMAL(10,0)\"",
    "paramIndex" : "first",
    "requiredType" : "\"(TIMESTAMP OR TIMESTAMP WITHOUT TIME ZONE)\"",
    "sqlExpr" : "\"CAST(1 AS DECIMAL(10,0)) + (- INTERVAL '2' DAY)\""
  },
  "queryContext" : [ {
    "objectType" : "",
    "objectName" : "",
    "startIndex" : 8,
    "stopIndex" : 49,
    "fragment" : "cast(1 as decimal(10, 0)) - interval 2 day"
  } ]
}


-- !query
select cast('2017-12-11' as string) - interval 2 day
-- !query schema
struct<CAST(2017-12-11 AS STRING) - INTERVAL '2' DAY:string>
-- !query output
2017-12-09 00:00:00


-- !query
select cast('2017-12-11 09:30:00' as string) - interval 2 day
-- !query schema
struct<CAST(2017-12-11 09:30:00 AS STRING) - INTERVAL '2' DAY:string>
-- !query output
2017-12-09 09:30:00


-- !query
select cast('1' as binary) - interval 2 day
-- !query schema
struct<>
-- !query output
org.apache.spark.sql.catalyst.ExtendedAnalysisException
{
  "errorClass" : "DATATYPE_MISMATCH.UNEXPECTED_INPUT_TYPE",
  "sqlState" : "42K09",
  "messageParameters" : {
    "inputSql" : "\"CAST(1 AS BINARY)\"",
    "inputType" : "\"BINARY\"",
    "paramIndex" : "first",
    "requiredType" : "\"(TIMESTAMP OR TIMESTAMP WITHOUT TIME ZONE)\"",
    "sqlExpr" : "\"CAST(1 AS BINARY) + (- INTERVAL '2' DAY)\""
  },
  "queryContext" : [ {
    "objectType" : "",
    "objectName" : "",
    "startIndex" : 8,
    "stopIndex" : 43,
    "fragment" : "cast('1' as binary) - interval 2 day"
  } ]
}


-- !query
select cast(1 as boolean) - interval 2 day
-- !query schema
struct<>
-- !query output
org.apache.spark.sql.catalyst.ExtendedAnalysisException
{
  "errorClass" : "DATATYPE_MISMATCH.UNEXPECTED_INPUT_TYPE",
  "sqlState" : "42K09",
  "messageParameters" : {
    "inputSql" : "\"CAST(1 AS BOOLEAN)\"",
    "inputType" : "\"BOOLEAN\"",
    "paramIndex" : "first",
    "requiredType" : "\"(TIMESTAMP OR TIMESTAMP WITHOUT TIME ZONE)\"",
    "sqlExpr" : "\"CAST(1 AS BOOLEAN) + (- INTERVAL '2' DAY)\""
  },
  "queryContext" : [ {
    "objectType" : "",
    "objectName" : "",
    "startIndex" : 8,
    "stopIndex" : 42,
    "fragment" : "cast(1 as boolean) - interval 2 day"
  } ]
}


-- !query
select cast('2017-12-11 09:30:00.0' as timestamp) - interval 2 day
-- !query schema
struct<CAST(2017-12-11 09:30:00.0 AS TIMESTAMP) - INTERVAL '2' DAY:timestamp>
-- !query output
2017-12-09 09:30:00


-- !query
select cast('2017-12-11 09:30:00' as date) - interval 2 day
-- !query schema
struct<date_add(CAST(2017-12-11 09:30:00 AS DATE), (- extractansiintervaldays(INTERVAL '2' DAY))):date>
-- !query output
2017-12-09
