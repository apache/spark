-- Automatically generated by SQLQueryTestSuite
-- !query
SELECT try_sum(col) FROM VALUES (5), (10), (15) AS tab(col)
-- !query schema
struct<try_sum(col):bigint>
-- !query output
30


-- !query
SELECT try_sum(col) FROM VALUES (5.0), (10.0), (15.0) AS tab(col)
-- !query schema
struct<try_sum(col):decimal(13,1)>
-- !query output
30.0


-- !query
SELECT try_sum(col) FROM VALUES (NULL), (10), (15) AS tab(col)
-- !query schema
struct<try_sum(col):bigint>
-- !query output
25


-- !query
SELECT try_sum(col) FROM VALUES (NULL), (NULL) AS tab(col)
-- !query schema
struct<try_sum(col):double>
-- !query output
NULL


-- !query
SELECT try_sum(col) FROM VALUES (9223372036854775807L), (1L) AS tab(col)
-- !query schema
struct<try_sum(col):bigint>
-- !query output
NULL


-- !query
SELECT try_sum(col) FROM VALUES (98765432109876543210987654321098765432BD), (98765432109876543210987654321098765432BD) AS tab(col)
-- !query schema
struct<try_sum(col):decimal(38,0)>
-- !query output
NULL


-- !query
SELECT try_sum(col) FROM VALUES (interval '1 months'), (interval '1 months') AS tab(col)
-- !query schema
struct<try_sum(col):interval month>
-- !query output
0-2


-- !query
SELECT try_sum(col) FROM VALUES (interval '2147483647 months'), (interval '1 months') AS tab(col)
-- !query schema
struct<try_sum(col):interval month>
-- !query output
NULL


-- !query
SELECT try_sum(col) FROM VALUES (interval '1 seconds'), (interval '1 seconds') AS tab(col)
-- !query schema
struct<try_sum(col):interval second>
-- !query output
0 00:00:02.000000000


-- !query
SELECT try_sum(col) FROM VALUES (interval '106751991 DAYS'), (interval '1 DAYS') AS tab(col)
-- !query schema
struct<try_sum(col):interval day>
-- !query output
NULL


-- !query
SELECT try_sum(col / 0) FROM VALUES (5), (10), (15) AS tab(col)
-- !query schema
struct<>
-- !query output
org.apache.spark.SparkArithmeticException
{
  "errorClass" : "DIVIDE_BY_ZERO",
  "sqlState" : "22012",
  "messageParameters" : {
    "config" : "\"spark.sql.ansi.enabled\""
  },
  "queryContext" : [ {
    "objectType" : "",
    "objectName" : "",
    "startIndex" : 16,
    "stopIndex" : 22,
    "fragment" : "col / 0"
  } ]
}


-- !query
SELECT try_sum(col / 0) FROM VALUES (5.0), (10.0), (15.0) AS tab(col)
-- !query schema
struct<>
-- !query output
org.apache.spark.SparkArithmeticException
{
  "errorClass" : "DIVIDE_BY_ZERO",
  "sqlState" : "22012",
  "messageParameters" : {
    "config" : "\"spark.sql.ansi.enabled\""
  },
  "queryContext" : [ {
    "objectType" : "",
    "objectName" : "",
    "startIndex" : 16,
    "stopIndex" : 22,
    "fragment" : "col / 0"
  } ]
}


-- !query
SELECT try_sum(col / 0) FROM VALUES (NULL), (10), (15) AS tab(col)
-- !query schema
struct<>
-- !query output
org.apache.spark.SparkArithmeticException
{
  "errorClass" : "DIVIDE_BY_ZERO",
  "sqlState" : "22012",
  "messageParameters" : {
    "config" : "\"spark.sql.ansi.enabled\""
  },
  "queryContext" : [ {
    "objectType" : "",
    "objectName" : "",
    "startIndex" : 16,
    "stopIndex" : 22,
    "fragment" : "col / 0"
  } ]
}


-- !query
SELECT try_sum(col + 1L) FROM VALUES (9223372036854775807L), (1L) AS tab(col)
-- !query schema
struct<>
-- !query output
org.apache.spark.SparkArithmeticException
{
  "errorClass" : "ARITHMETIC_OVERFLOW",
  "sqlState" : "22003",
  "messageParameters" : {
    "message" : "long overflow",
    "try_alternative" : " Use 'try_add' to tolerate overflow and return NULL instead."
  },
  "queryContext" : [ {
    "objectType" : "",
    "objectName" : "",
    "startIndex" : 16,
    "stopIndex" : 23,
    "fragment" : "col + 1L"
  } ]
}


-- !query
SELECT try_sum(col / 0) FROM VALUES (interval '1 months'), (interval '1 months') AS tab(col)
-- !query schema
struct<>
-- !query output
org.apache.spark.SparkArithmeticException
{
  "errorClass" : "INTERVAL_DIVIDED_BY_ZERO",
  "sqlState" : "22012",
  "queryContext" : [ {
    "objectType" : "",
    "objectName" : "",
    "startIndex" : 16,
    "stopIndex" : 22,
    "fragment" : "col / 0"
  } ]
}


-- !query
SELECT try_sum(col / 0) FROM VALUES (interval '1 seconds'), (interval '1 seconds') AS tab(col)
-- !query schema
struct<>
-- !query output
org.apache.spark.SparkArithmeticException
{
  "errorClass" : "INTERVAL_DIVIDED_BY_ZERO",
  "sqlState" : "22012",
  "queryContext" : [ {
    "objectType" : "",
    "objectName" : "",
    "startIndex" : 16,
    "stopIndex" : 22,
    "fragment" : "col / 0"
  } ]
}


-- !query
SELECT try_avg(col) FROM VALUES (5), (10), (15) AS tab(col)
-- !query schema
struct<try_avg(col):double>
-- !query output
10.0


-- !query
SELECT try_avg(col) FROM VALUES (5.0), (10.0), (15.0) AS tab(col)
-- !query schema
struct<try_avg(col):decimal(7,5)>
-- !query output
10.00000


-- !query
SELECT try_avg(col) FROM VALUES (NULL), (10), (15) AS tab(col)
-- !query schema
struct<try_avg(col):double>
-- !query output
12.5


-- !query
SELECT try_avg(col) FROM VALUES (NULL), (NULL) AS tab(col)
-- !query schema
struct<try_avg(col):double>
-- !query output
NULL


-- !query
SELECT try_avg(col) FROM VALUES (9223372036854775807L), (1L) AS tab(col)
-- !query schema
struct<try_avg(col):double>
-- !query output
4.611686018427388E18


-- !query
SELECT try_avg(col) FROM VALUES (98765432109876543210987654321098765432BD), (98765432109876543210987654321098765432BD) AS tab(col)
-- !query schema
struct<try_avg(col):decimal(38,4)>
-- !query output
NULL


-- !query
SELECT try_avg(col) FROM VALUES (interval '1 months'), (interval '1 months') AS tab(col)
-- !query schema
struct<try_avg(col):interval year to month>
-- !query output
0-1


-- !query
SELECT try_avg(col) FROM VALUES (interval '2147483647 months'), (interval '1 months') AS tab(col)
-- !query schema
struct<try_avg(col):interval year to month>
-- !query output
NULL


-- !query
SELECT try_avg(col) FROM VALUES (interval '1 seconds'), (interval '1 seconds') AS tab(col)
-- !query schema
struct<try_avg(col):interval day to second>
-- !query output
0 00:00:01.000000000


-- !query
SELECT try_avg(col) FROM VALUES (interval '106751991 DAYS'), (interval '1 DAYS') AS tab(col)
-- !query schema
struct<try_avg(col):interval day to second>
-- !query output
NULL


-- !query
SELECT try_avg(col / 0) FROM VALUES (5), (10), (15) AS tab(col)
-- !query schema
struct<>
-- !query output
org.apache.spark.SparkArithmeticException
{
  "errorClass" : "DIVIDE_BY_ZERO",
  "sqlState" : "22012",
  "messageParameters" : {
    "config" : "\"spark.sql.ansi.enabled\""
  },
  "queryContext" : [ {
    "objectType" : "",
    "objectName" : "",
    "startIndex" : 16,
    "stopIndex" : 22,
    "fragment" : "col / 0"
  } ]
}


-- !query
SELECT try_avg(col / 0) FROM VALUES (5.0), (10.0), (15.0) AS tab(col)
-- !query schema
struct<>
-- !query output
org.apache.spark.SparkArithmeticException
{
  "errorClass" : "DIVIDE_BY_ZERO",
  "sqlState" : "22012",
  "messageParameters" : {
    "config" : "\"spark.sql.ansi.enabled\""
  },
  "queryContext" : [ {
    "objectType" : "",
    "objectName" : "",
    "startIndex" : 16,
    "stopIndex" : 22,
    "fragment" : "col / 0"
  } ]
}


-- !query
SELECT try_avg(col / 0) FROM VALUES (NULL), (10), (15) AS tab(col)
-- !query schema
struct<>
-- !query output
org.apache.spark.SparkArithmeticException
{
  "errorClass" : "DIVIDE_BY_ZERO",
  "sqlState" : "22012",
  "messageParameters" : {
    "config" : "\"spark.sql.ansi.enabled\""
  },
  "queryContext" : [ {
    "objectType" : "",
    "objectName" : "",
    "startIndex" : 16,
    "stopIndex" : 22,
    "fragment" : "col / 0"
  } ]
}


-- !query
SELECT try_avg(col + 1L) FROM VALUES (9223372036854775807L), (1L) AS tab(col)
-- !query schema
struct<>
-- !query output
org.apache.spark.SparkArithmeticException
{
  "errorClass" : "ARITHMETIC_OVERFLOW",
  "sqlState" : "22003",
  "messageParameters" : {
    "message" : "long overflow",
    "try_alternative" : " Use 'try_add' to tolerate overflow and return NULL instead."
  },
  "queryContext" : [ {
    "objectType" : "",
    "objectName" : "",
    "startIndex" : 16,
    "stopIndex" : 23,
    "fragment" : "col + 1L"
  } ]
}


-- !query
SELECT try_avg(col / 0) FROM VALUES (interval '1 months'), (interval '1 months') AS tab(col)
-- !query schema
struct<>
-- !query output
org.apache.spark.SparkArithmeticException
{
  "errorClass" : "INTERVAL_DIVIDED_BY_ZERO",
  "sqlState" : "22012",
  "queryContext" : [ {
    "objectType" : "",
    "objectName" : "",
    "startIndex" : 16,
    "stopIndex" : 22,
    "fragment" : "col / 0"
  } ]
}


-- !query
SELECT try_avg(col / 0) FROM VALUES (interval '1 seconds'), (interval '1 seconds') AS tab(col)
-- !query schema
struct<>
-- !query output
org.apache.spark.SparkArithmeticException
{
  "errorClass" : "INTERVAL_DIVIDED_BY_ZERO",
  "sqlState" : "22012",
  "queryContext" : [ {
    "objectType" : "",
    "objectName" : "",
    "startIndex" : 16,
    "stopIndex" : 22,
    "fragment" : "col / 0"
  } ]
}
