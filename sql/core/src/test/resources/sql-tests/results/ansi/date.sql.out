-- Automatically generated by SQLQueryTestSuite
-- !query
create temporary view date_view as select '2011-11-11' date_str, '1' int_str
-- !query schema
struct<>
-- !query output



-- !query
select date '2019-01-01\t'
-- !query schema
struct<DATE '2019-01-01':date>
-- !query output
2019-01-01


-- !query
select date '2020-01-01中文'
-- !query schema
struct<>
-- !query output
org.apache.spark.sql.catalyst.parser.ParseException

Cannot parse the DATE value: 2020-01-01中文(line 1, pos 7)

== SQL ==
select date '2020-01-01中文'
-------^^^


-- !query
select make_date(2019, 1, 1), make_date(12, 12, 12)
-- !query schema
struct<make_date(2019, 1, 1):date,make_date(12, 12, 12):date>
-- !query output
2019-01-01	0012-12-12


-- !query
select make_date(2000, 13, 1)
-- !query schema
struct<>
-- !query output
java.time.DateTimeException
Invalid value for MonthOfYear (valid values 1 - 12): 13. If necessary set spark.sql.ansi.enabled to false to bypass this error.


-- !query
select make_date(2000, 1, 33)
-- !query schema
struct<>
-- !query output
java.time.DateTimeException
Invalid value for DayOfMonth (valid values 1 - 28/31): 33. If necessary set spark.sql.ansi.enabled to false to bypass this error.


-- !query
select date'015'
-- !query schema
struct<>
-- !query output
org.apache.spark.sql.catalyst.parser.ParseException

Cannot parse the DATE value: 015(line 1, pos 7)

== SQL ==
select date'015'
-------^^^


-- !query
select date'2021-4294967297-11'
-- !query schema
struct<>
-- !query output
org.apache.spark.sql.catalyst.parser.ParseException

Cannot parse the DATE value: 2021-4294967297-11(line 1, pos 7)

== SQL ==
select date'2021-4294967297-11'
-------^^^


-- !query
select current_date = current_date
-- !query schema
struct<(current_date() = current_date()):boolean>
-- !query output
true


-- !query
select current_date() = current_date()
-- !query schema
struct<(current_date() = current_date()):boolean>
-- !query output
true


-- !query
select DATE_FROM_UNIX_DATE(0), DATE_FROM_UNIX_DATE(1000), DATE_FROM_UNIX_DATE(null)
-- !query schema
struct<date_from_unix_date(0):date,date_from_unix_date(1000):date,date_from_unix_date(NULL):date>
-- !query output
1970-01-01	1972-09-27	NULL


-- !query
select UNIX_DATE(DATE('1970-01-01')), UNIX_DATE(DATE('2020-12-04')), UNIX_DATE(null)
-- !query schema
struct<unix_date(1970-01-01):int,unix_date(2020-12-04):int,unix_date(NULL):int>
-- !query output
0	18600	NULL


-- !query
select to_date(null), to_date('2016-12-31'), to_date('2016-12-31', 'yyyy-MM-dd')
-- !query schema
struct<to_date(NULL):date,to_date(2016-12-31):date,to_date(2016-12-31, yyyy-MM-dd):date>
-- !query output
NULL	2016-12-31	2016-12-31


-- !query
select to_date("16", "dd")
-- !query schema
struct<to_date(16, dd):date>
-- !query output
1970-01-16


-- !query
select to_date("02-29", "MM-dd")
-- !query schema
struct<>
-- !query output
java.time.DateTimeException
Invalid date 'February 29' as '1970' is not a leap year. If necessary set spark.sql.ansi.enabled to false to bypass this error.


-- !query
select dayofweek('2007-02-03'), dayofweek('2009-07-30'), dayofweek('2017-05-27'), dayofweek(null),
  dayofweek('1582-10-15 13:10:15'), dayofweek(timestamp_ltz'1582-10-15 13:10:15'), dayofweek(timestamp_ntz'1582-10-15 13:10:15')
-- !query schema
struct<dayofweek(2007-02-03):int,dayofweek(2009-07-30):int,dayofweek(2017-05-27):int,dayofweek(NULL):int,dayofweek(1582-10-15 13:10:15):int,dayofweek(TIMESTAMP '1582-10-15 13:10:15'):int,dayofweek(TIMESTAMP_NTZ '1582-10-15 13:10:15'):int>
-- !query output
7	5	7	NULL	6	6	6


-- !query
select weekday('2007-02-03'), weekday('2009-07-30'), weekday('2017-05-27'), weekday(null),
  weekday('1582-10-15 13:10:15'), weekday(timestamp_ltz'1582-10-15 13:10:15'), weekday(timestamp_ntz'1582-10-15 13:10:15')
-- !query schema
struct<weekday(2007-02-03):int,weekday(2009-07-30):int,weekday(2017-05-27):int,weekday(NULL):int,weekday(1582-10-15 13:10:15):int,weekday(TIMESTAMP '1582-10-15 13:10:15'):int,weekday(TIMESTAMP_NTZ '1582-10-15 13:10:15'):int>
-- !query output
5	3	5	NULL	4	4	4


-- !query
select year('1500-01-01'), year('1582-10-15 13:10:15'), year(timestamp_ltz'1582-10-15 13:10:15'), year(timestamp_ntz'1582-10-15 13:10:15')
-- !query schema
struct<year(1500-01-01):int,year(1582-10-15 13:10:15):int,year(TIMESTAMP '1582-10-15 13:10:15'):int,year(TIMESTAMP_NTZ '1582-10-15 13:10:15'):int>
-- !query output
1500	1582	1582	1582


-- !query
select month('1500-01-01'), month('1582-10-15 13:10:15'), month(timestamp_ltz'1582-10-15 13:10:15'), month(timestamp_ntz'1582-10-15 13:10:15')
-- !query schema
struct<month(1500-01-01):int,month(1582-10-15 13:10:15):int,month(TIMESTAMP '1582-10-15 13:10:15'):int,month(TIMESTAMP_NTZ '1582-10-15 13:10:15'):int>
-- !query output
1	10	10	10


-- !query
select dayOfYear('1500-01-01'), dayOfYear('1582-10-15 13:10:15'), dayOfYear(timestamp_ltz'1582-10-15 13:10:15'), dayOfYear(timestamp_ntz'1582-10-15 13:10:15')
-- !query schema
struct<dayofyear(1500-01-01):int,dayofyear(1582-10-15 13:10:15):int,dayofyear(TIMESTAMP '1582-10-15 13:10:15'):int,dayofyear(TIMESTAMP_NTZ '1582-10-15 13:10:15'):int>
-- !query output
1	288	288	288


-- !query
select next_day("2015-07-23", "Mon")
-- !query schema
struct<next_day(2015-07-23, Mon):date>
-- !query output
2015-07-27


-- !query
select next_day("2015-07-23", "xx")
-- !query schema
struct<>
-- !query output
java.lang.IllegalArgumentException
Illegal input for day of week: xx. If necessary set spark.sql.ansi.enabled to false to bypass this error.


-- !query
select next_day("2015-07-23 12:12:12", "Mon")
-- !query schema
struct<next_day(2015-07-23 12:12:12, Mon):date>
-- !query output
2015-07-27


-- !query
select next_day(timestamp_ltz"2015-07-23 12:12:12", "Mon")
-- !query schema
struct<next_day(TIMESTAMP '2015-07-23 12:12:12', Mon):date>
-- !query output
2015-07-27


-- !query
select next_day(timestamp_ntz"2015-07-23 12:12:12", "Mon")
-- !query schema
struct<next_day(TIMESTAMP_NTZ '2015-07-23 12:12:12', Mon):date>
-- !query output
2015-07-27


-- !query
select next_day("xx", "Mon")
-- !query schema
struct<>
-- !query output
org.apache.spark.SparkDateTimeException
[CAST_INVALID_INPUT] The value 'xx' of the type "STRING" cannot be cast to "DATE" because it is malformed. Correct the value as per the syntax, or change its target type. Use `try_cast` to tolerate malformed input and return NULL instead. If necessary set "spark.sql.ansi.enabled" to "false" to bypass this error.
== SQL(line 1, position 8) ==
select next_day("xx", "Mon")
       ^^^^^^^^^^^^^^^^^^^^^


-- !query
select next_day(null, "Mon")
-- !query schema
struct<next_day(NULL, Mon):date>
-- !query output
NULL


-- !query
select next_day(null, "xx")
-- !query schema
struct<next_day(NULL, xx):date>
-- !query output
NULL


-- !query
select date_add(date'2011-11-11', 1)
-- !query schema
struct<date_add(DATE '2011-11-11', 1):date>
-- !query output
2011-11-12


-- !query
select date_add('2011-11-11', 1)
-- !query schema
struct<date_add(2011-11-11, 1):date>
-- !query output
2011-11-12


-- !query
select date_add('2011-11-11', 1Y)
-- !query schema
struct<date_add(2011-11-11, 1):date>
-- !query output
2011-11-12


-- !query
select date_add('2011-11-11', 1S)
-- !query schema
struct<date_add(2011-11-11, 1):date>
-- !query output
2011-11-12


-- !query
select date_add('2011-11-11', 1L)
-- !query schema
struct<>
-- !query output
org.apache.spark.sql.AnalysisException
cannot resolve 'date_add(CAST('2011-11-11' AS DATE), 1L)' due to data type mismatch: argument 2 requires (int or smallint or tinyint) type, however, '1L' is of bigint type.; line 1 pos 7


-- !query
select date_add('2011-11-11', 1.0)
-- !query schema
struct<>
-- !query output
org.apache.spark.sql.AnalysisException
cannot resolve 'date_add(CAST('2011-11-11' AS DATE), 1.0BD)' due to data type mismatch: argument 2 requires (int or smallint or tinyint) type, however, '1.0BD' is of decimal(2,1) type.; line 1 pos 7


-- !query
select date_add('2011-11-11', 1E1)
-- !query schema
struct<>
-- !query output
org.apache.spark.sql.AnalysisException
cannot resolve 'date_add(CAST('2011-11-11' AS DATE), 10.0D)' due to data type mismatch: argument 2 requires (int or smallint or tinyint) type, however, '10.0D' is of double type.; line 1 pos 7


-- !query
select date_add('2011-11-11', '1')
-- !query schema
struct<date_add(2011-11-11, 1):date>
-- !query output
2011-11-12


-- !query
select date_add('2011-11-11', '1.2')
-- !query schema
struct<>
-- !query output
org.apache.spark.SparkNumberFormatException
[CAST_INVALID_INPUT] The value '1.2' of the type "STRING" cannot be cast to "INT" because it is malformed. Correct the value as per the syntax, or change its target type. Use `try_cast` to tolerate malformed input and return NULL instead. If necessary set "spark.sql.ansi.enabled" to "false" to bypass this error.
== SQL(line 1, position 8) ==
select date_add('2011-11-11', '1.2')
       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^


-- !query
select date_add(null, 1)
-- !query schema
struct<date_add(NULL, 1):date>
-- !query output
NULL


-- !query
select date_add(date'2011-11-11', null)
-- !query schema
struct<date_add(DATE '2011-11-11', NULL):date>
-- !query output
NULL


-- !query
select date_add(timestamp_ltz'2011-11-11 12:12:12', 1)
-- !query schema
struct<date_add(TIMESTAMP '2011-11-11 12:12:12', 1):date>
-- !query output
2011-11-12


-- !query
select date_add(timestamp_ntz'2011-11-11 12:12:12', 1)
-- !query schema
struct<date_add(TIMESTAMP_NTZ '2011-11-11 12:12:12', 1):date>
-- !query output
2011-11-12


-- !query
select date_sub(date'2011-11-11', 1)
-- !query schema
struct<date_sub(DATE '2011-11-11', 1):date>
-- !query output
2011-11-10


-- !query
select date_sub('2011-11-11', 1)
-- !query schema
struct<date_sub(2011-11-11, 1):date>
-- !query output
2011-11-10


-- !query
select date_sub('2011-11-11', 1Y)
-- !query schema
struct<date_sub(2011-11-11, 1):date>
-- !query output
2011-11-10


-- !query
select date_sub('2011-11-11', 1S)
-- !query schema
struct<date_sub(2011-11-11, 1):date>
-- !query output
2011-11-10


-- !query
select date_sub('2011-11-11', 1L)
-- !query schema
struct<>
-- !query output
org.apache.spark.sql.AnalysisException
cannot resolve 'date_sub(CAST('2011-11-11' AS DATE), 1L)' due to data type mismatch: argument 2 requires (int or smallint or tinyint) type, however, '1L' is of bigint type.; line 1 pos 7


-- !query
select date_sub('2011-11-11', 1.0)
-- !query schema
struct<>
-- !query output
org.apache.spark.sql.AnalysisException
cannot resolve 'date_sub(CAST('2011-11-11' AS DATE), 1.0BD)' due to data type mismatch: argument 2 requires (int or smallint or tinyint) type, however, '1.0BD' is of decimal(2,1) type.; line 1 pos 7


-- !query
select date_sub('2011-11-11', 1E1)
-- !query schema
struct<>
-- !query output
org.apache.spark.sql.AnalysisException
cannot resolve 'date_sub(CAST('2011-11-11' AS DATE), 10.0D)' due to data type mismatch: argument 2 requires (int or smallint or tinyint) type, however, '10.0D' is of double type.; line 1 pos 7


-- !query
select date_sub(date'2011-11-11', '1')
-- !query schema
struct<date_sub(DATE '2011-11-11', 1):date>
-- !query output
2011-11-10


-- !query
select date_sub(date'2011-11-11', '1.2')
-- !query schema
struct<>
-- !query output
org.apache.spark.SparkNumberFormatException
[CAST_INVALID_INPUT] The value '1.2' of the type "STRING" cannot be cast to "INT" because it is malformed. Correct the value as per the syntax, or change its target type. Use `try_cast` to tolerate malformed input and return NULL instead. If necessary set "spark.sql.ansi.enabled" to "false" to bypass this error.
== SQL(line 1, position 8) ==
select date_sub(date'2011-11-11', '1.2')
       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^


-- !query
select date_sub(null, 1)
-- !query schema
struct<date_sub(NULL, 1):date>
-- !query output
NULL


-- !query
select date_sub(date'2011-11-11', null)
-- !query schema
struct<date_sub(DATE '2011-11-11', NULL):date>
-- !query output
NULL


-- !query
select date_sub(timestamp_ltz'2011-11-11 12:12:12', 1)
-- !query schema
struct<date_sub(TIMESTAMP '2011-11-11 12:12:12', 1):date>
-- !query output
2011-11-10


-- !query
select date_sub(timestamp_ntz'2011-11-11 12:12:12', 1)
-- !query schema
struct<date_sub(TIMESTAMP_NTZ '2011-11-11 12:12:12', 1):date>
-- !query output
2011-11-10


-- !query
select date_add('2011-11-11', int_str) from date_view
-- !query schema
struct<date_add(2011-11-11, int_str):date>
-- !query output
2011-11-12


-- !query
select date_sub('2011-11-11', int_str) from date_view
-- !query schema
struct<date_sub(2011-11-11, int_str):date>
-- !query output
2011-11-10


-- !query
select date_add(date_str, 1) from date_view
-- !query schema
struct<date_add(date_str, 1):date>
-- !query output
2011-11-12


-- !query
select date_sub(date_str, 1) from date_view
-- !query schema
struct<date_sub(date_str, 1):date>
-- !query output
2011-11-10


-- !query
select date '2011-11-11' + 1E1
-- !query schema
struct<>
-- !query output
org.apache.spark.sql.AnalysisException
cannot resolve 'date_add(DATE '2011-11-11', 10.0D)' due to data type mismatch: argument 2 requires (int or smallint or tinyint) type, however, '10.0D' is of double type.; line 1 pos 7


-- !query
select date '2001-09-28' + 7Y
-- !query schema
struct<date_add(DATE '2001-09-28', 7):date>
-- !query output
2001-10-05


-- !query
select 7S + date '2001-09-28'
-- !query schema
struct<date_add(DATE '2001-09-28', 7):date>
-- !query output
2001-10-05


-- !query
select date '2001-10-01' - 7
-- !query schema
struct<date_sub(DATE '2001-10-01', 7):date>
-- !query output
2001-09-24


-- !query
select date '2001-10-01' - date '2001-09-28'
-- !query schema
struct<(DATE '2001-10-01' - DATE '2001-09-28'):interval day>
-- !query output
3 00:00:00.000000000


-- !query
select date '2001-10-01' - '2001-09-28'
-- !query schema
struct<(DATE '2001-10-01' - 2001-09-28):interval day>
-- !query output
3 00:00:00.000000000


-- !query
select '2001-10-01' - date '2001-09-28'
-- !query schema
struct<(2001-10-01 - DATE '2001-09-28'):interval day>
-- !query output
3 00:00:00.000000000


-- !query
select date '2001-09-28' - null
-- !query schema
struct<date_sub(DATE '2001-09-28', NULL):date>
-- !query output
NULL


-- !query
select null - date '2019-10-06'
-- !query schema
struct<(NULL - DATE '2019-10-06'):interval day>
-- !query output
NULL


-- !query
select date_str - date '2001-09-28' from date_view
-- !query schema
struct<(date_str - DATE '2001-09-28'):interval day>
-- !query output
3696 00:00:00.000000000


-- !query
select date '2001-09-28' - date_str from date_view
-- !query schema
struct<(DATE '2001-09-28' - date_str):interval day>
-- !query output
-3696 00:00:00.000000000


-- !query
select date'2011-11-11' + '1'
-- !query schema
struct<>
-- !query output
org.apache.spark.sql.AnalysisException
cannot resolve 'date_add(DATE '2011-11-11', CAST('1' AS DATE))' due to data type mismatch: argument 2 requires (int or smallint or tinyint) type, however, 'CAST('1' AS DATE)' is of date type.; line 1 pos 7


-- !query
select '1' + date'2011-11-11'
-- !query schema
struct<>
-- !query output
org.apache.spark.sql.AnalysisException
cannot resolve 'date_add(CAST('1' AS DATE), DATE '2011-11-11')' due to data type mismatch: argument 2 requires (int or smallint or tinyint) type, however, 'DATE '2011-11-11'' is of date type.; line 1 pos 7


-- !query
select date'2011-11-11' + null
-- !query schema
struct<date_add(DATE '2011-11-11', NULL):date>
-- !query output
NULL


-- !query
select null + date'2011-11-11'
-- !query schema
struct<date_add(DATE '2011-11-11', NULL):date>
-- !query output
NULL


-- !query
select date '2012-01-01' - interval '2-2' year to month,
       date '2011-11-11' - interval '2' day,
       date '2012-01-01' + interval '-2-2' year to month,
       date '2011-11-11' + interval '-2' month,
       - interval '2-2' year to month + date '2012-01-01',
       interval '-2' day + date '2011-11-11'
-- !query schema
struct<DATE '2012-01-01' - INTERVAL '2-2' YEAR TO MONTH:date,date_add(DATE '2011-11-11', (- extractansiintervaldays(INTERVAL '2' DAY))):date,DATE '2012-01-01' + INTERVAL '-2-2' YEAR TO MONTH:date,DATE '2011-11-11' + INTERVAL '-2' MONTH:date,DATE '2012-01-01' + (- INTERVAL '2-2' YEAR TO MONTH):date,date_add(DATE '2011-11-11', extractansiintervaldays(INTERVAL '-2' DAY)):date>
-- !query output
2009-11-01	2011-11-09	2009-11-01	2011-09-11	2009-11-01	2011-11-09


-- !query
select to_date('26/October/2015', 'dd/MMMMM/yyyy')
-- !query schema
struct<>
-- !query output
org.apache.spark.SparkUpgradeException
[INCONSISTENT_BEHAVIOR_CROSS_VERSION.DATETIME_PATTERN_RECOGNITION] You may get a different result due to the upgrading to Spark >= 3.0:
Fail to recognize 'dd/MMMMM/yyyy' pattern in the DateTimeFormatter. 1) You can set "spark.sql.legacy.timeParserPolicy" to "LEGACY" to restore the behavior before Spark 3.0. 2) You can form a valid datetime pattern with the guide from https://spark.apache.org/docs/latest/sql-ref-datetime-pattern.html


-- !query
select from_json('{"d":"26/October/2015"}', 'd Date', map('dateFormat', 'dd/MMMMM/yyyy'))
-- !query schema
struct<>
-- !query output
org.apache.spark.SparkUpgradeException
[INCONSISTENT_BEHAVIOR_CROSS_VERSION.DATETIME_PATTERN_RECOGNITION] You may get a different result due to the upgrading to Spark >= 3.0:
Fail to recognize 'dd/MMMMM/yyyy' pattern in the DateTimeFormatter. 1) You can set "spark.sql.legacy.timeParserPolicy" to "LEGACY" to restore the behavior before Spark 3.0. 2) You can form a valid datetime pattern with the guide from https://spark.apache.org/docs/latest/sql-ref-datetime-pattern.html


-- !query
select from_csv('26/October/2015', 'd Date', map('dateFormat', 'dd/MMMMM/yyyy'))
-- !query schema
struct<>
-- !query output
org.apache.spark.SparkUpgradeException
[INCONSISTENT_BEHAVIOR_CROSS_VERSION.DATETIME_PATTERN_RECOGNITION] You may get a different result due to the upgrading to Spark >= 3.0:
Fail to recognize 'dd/MMMMM/yyyy' pattern in the DateTimeFormatter. 1) You can set "spark.sql.legacy.timeParserPolicy" to "LEGACY" to restore the behavior before Spark 3.0. 2) You can form a valid datetime pattern with the guide from https://spark.apache.org/docs/latest/sql-ref-datetime-pattern.html


-- !query
select dateadd(MICROSECOND, 1001, timestamp'2022-02-25 01:02:03.123')
-- !query schema
struct<timestampadd(MICROSECOND, 1001, TIMESTAMP '2022-02-25 01:02:03.123'):timestamp>
-- !query output
2022-02-25 01:02:03.124001


-- !query
select dateadd(MILLISECOND, -1, timestamp'2022-02-25 01:02:03.456')
-- !query schema
struct<timestampadd(MILLISECOND, -1, TIMESTAMP '2022-02-25 01:02:03.456'):timestamp>
-- !query output
2022-02-25 01:02:03.455


-- !query
select dateadd(SECOND, 58, timestamp'2022-02-25 01:02:03')
-- !query schema
struct<timestampadd(SECOND, 58, TIMESTAMP '2022-02-25 01:02:03'):timestamp>
-- !query output
2022-02-25 01:03:01


-- !query
select dateadd(MINUTE, -100, date'2022-02-25')
-- !query schema
struct<timestampadd(MINUTE, -100, DATE '2022-02-25'):timestamp>
-- !query output
2022-02-24 22:20:00


-- !query
select dateadd(HOUR, -1, timestamp'2022-02-25 01:02:03')
-- !query schema
struct<timestampadd(HOUR, -1, TIMESTAMP '2022-02-25 01:02:03'):timestamp>
-- !query output
2022-02-25 00:02:03


-- !query
select dateadd(DAY, 367, date'2022-02-25')
-- !query schema
struct<timestampadd(DAY, 367, DATE '2022-02-25'):timestamp>
-- !query output
2023-02-27 00:00:00


-- !query
select dateadd(WEEK, -4, timestamp'2022-02-25 01:02:03')
-- !query schema
struct<timestampadd(WEEK, -4, TIMESTAMP '2022-02-25 01:02:03'):timestamp>
-- !query output
2022-01-28 01:02:03


-- !query
select dateadd(MONTH, -1, timestamp'2022-02-25 01:02:03')
-- !query schema
struct<timestampadd(MONTH, -1, TIMESTAMP '2022-02-25 01:02:03'):timestamp>
-- !query output
2022-01-25 01:02:03


-- !query
select dateadd(QUARTER, 5, date'2022-02-25')
-- !query schema
struct<timestampadd(QUARTER, 5, DATE '2022-02-25'):timestamp>
-- !query output
2023-05-25 00:00:00


-- !query
select dateadd(YEAR, 1, date'2022-02-25')
-- !query schema
struct<timestampadd(YEAR, 1, DATE '2022-02-25'):timestamp>
-- !query output
2023-02-25 00:00:00


-- !query
select datediff(MICROSECOND, timestamp'2022-02-25 01:02:03.123', timestamp'2022-02-25 01:02:03.124001')
-- !query schema
struct<timestampdiff(MICROSECOND, TIMESTAMP '2022-02-25 01:02:03.123', TIMESTAMP '2022-02-25 01:02:03.124001'):bigint>
-- !query output
1001


-- !query
select datediff(MILLISECOND, timestamp'2022-02-25 01:02:03.456', timestamp'2022-02-25 01:02:03.455')
-- !query schema
struct<timestampdiff(MILLISECOND, TIMESTAMP '2022-02-25 01:02:03.456', TIMESTAMP '2022-02-25 01:02:03.455'):bigint>
-- !query output
-1


-- !query
select datediff(SECOND, timestamp'2022-02-25 01:02:03', timestamp'2022-02-25 01:03:01')
-- !query schema
struct<timestampdiff(SECOND, TIMESTAMP '2022-02-25 01:02:03', TIMESTAMP '2022-02-25 01:03:01'):bigint>
-- !query output
58


-- !query
select datediff(MINUTE, date'2022-02-25', timestamp'2022-02-24 22:20:00')
-- !query schema
struct<timestampdiff(MINUTE, DATE '2022-02-25', TIMESTAMP '2022-02-24 22:20:00'):bigint>
-- !query output
-100


-- !query
select datediff(HOUR, timestamp'2022-02-25 01:02:03', timestamp'2022-02-25 00:02:03')
-- !query schema
struct<timestampdiff(HOUR, TIMESTAMP '2022-02-25 01:02:03', TIMESTAMP '2022-02-25 00:02:03'):bigint>
-- !query output
-1


-- !query
select datediff(DAY, date'2022-02-25', timestamp'2023-02-27 00:00:00')
-- !query schema
struct<timestampdiff(DAY, DATE '2022-02-25', TIMESTAMP '2023-02-27 00:00:00'):bigint>
-- !query output
367


-- !query
select datediff(WEEK, timestamp'2022-02-25 01:02:03', timestamp'2022-01-28 01:02:03')
-- !query schema
struct<timestampdiff(WEEK, TIMESTAMP '2022-02-25 01:02:03', TIMESTAMP '2022-01-28 01:02:03'):bigint>
-- !query output
-4


-- !query
select datediff(MONTH, timestamp'2022-02-25 01:02:03', timestamp'2022-01-25 01:02:03')
-- !query schema
struct<timestampdiff(MONTH, TIMESTAMP '2022-02-25 01:02:03', TIMESTAMP '2022-01-25 01:02:03'):bigint>
-- !query output
-1


-- !query
select datediff(QUARTER, date'2022-02-25', date'2023-05-25')
-- !query schema
struct<timestampdiff(QUARTER, DATE '2022-02-25', DATE '2023-05-25'):bigint>
-- !query output
5


-- !query
select datediff(YEAR, date'2022-02-25', date'2023-02-25')
-- !query schema
struct<timestampdiff(YEAR, DATE '2022-02-25', DATE '2023-02-25'):bigint>
-- !query output
1
