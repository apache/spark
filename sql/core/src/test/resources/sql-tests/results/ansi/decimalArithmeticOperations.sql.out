-- Automatically generated by SQLQueryTestSuite
-- Number of queries: 16


-- !query
create table decimals_test(id int, a decimal(38,18), b decimal(38,18)) using parquet
-- !query schema
struct<>
-- !query output



-- !query
insert into decimals_test values(1, 100.0, 999.0), (2, 12345.123, 12345.123),
  (3, 0.1234567891011, 1234.1), (4, 123456789123456789.0, 1.123456789123456789)
-- !query schema
struct<>
-- !query output



-- !query
select id, a*10, b/10 from decimals_test order by id
-- !query schema
struct<id:int,(a * 10):decimal(38,15),(b / 10):decimal(38,18)>
-- !query output
1	1000.000000000000000	99.900000000000000000
2	123451.230000000000000	1234.512300000000000000
3	1.234567891011000	123.410000000000000000
4	1234567891234567890.000000000000000	0.112345678912345679


-- !query
select 10.3 * 3.0
-- !query schema
struct<(10.3 * 3.0):decimal(6,2)>
-- !query output
30.90


-- !query
select 10.3000 * 3.0
-- !query schema
struct<(10.3000 * 3.0):decimal(9,5)>
-- !query output
30.90000


-- !query
select 10.30000 * 30.0
-- !query schema
struct<(10.30000 * 30.0):decimal(11,6)>
-- !query output
309.000000


-- !query
select 10.300000000000000000 * 3.000000000000000000
-- !query schema
struct<(10.300000000000000000 * 3.000000000000000000):decimal(38,34)>
-- !query output
30.9000000000000000000000000000000000


-- !query
select 10.300000000000000000 * 3.0000000000000000000
-- !query schema
struct<(10.300000000000000000 * 3.0000000000000000000):decimal(38,34)>
-- !query output
30.9000000000000000000000000000000000


-- !query
select (5e36BD + 0.1) + 5e36BD
-- !query schema
struct<>
-- !query output
org.apache.spark.SparkArithmeticException
[CANNOT_CHANGE_DECIMAL_PRECISION] Decimal(expanded,10000000000000000000000000000000000000.1,39,1}) cannot be represented as Decimal(38, 1). If necessary set "spark.sql.ansi.enabled" to false to bypass this error.
== SQL(line 1, position 7) ==
select (5e36BD + 0.1) + 5e36BD
       ^^^^^^^^^^^^^^^^^^^^^^^


-- !query
select (-4e36BD - 0.1) - 7e36BD
-- !query schema
struct<>
-- !query output
org.apache.spark.SparkArithmeticException
[CANNOT_CHANGE_DECIMAL_PRECISION] Decimal(expanded,-11000000000000000000000000000000000000.1,39,1}) cannot be represented as Decimal(38, 1). If necessary set "spark.sql.ansi.enabled" to false to bypass this error.
== SQL(line 1, position 7) ==
select (-4e36BD - 0.1) - 7e36BD
       ^^^^^^^^^^^^^^^^^^^^^^^^


-- !query
select 12345678901234567890.0 * 12345678901234567890.0
-- !query schema
struct<>
-- !query output
org.apache.spark.SparkArithmeticException
[CANNOT_CHANGE_DECIMAL_PRECISION] Decimal(expanded,152415787532388367501905199875019052100,39,0}) cannot be represented as Decimal(38, 2). If necessary set "spark.sql.ansi.enabled" to false to bypass this error.
== SQL(line 1, position 7) ==
select 12345678901234567890.0 * 12345678901234567890.0
       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^


-- !query
select 1e35BD / 0.1
-- !query schema
struct<>
-- !query output
org.apache.spark.SparkArithmeticException
[CANNOT_CHANGE_DECIMAL_PRECISION] Decimal(expanded,1000000000000000000000000000000000000,37,0}) cannot be represented as Decimal(38, 6). If necessary set "spark.sql.ansi.enabled" to false to bypass this error.
== SQL(line 1, position 7) ==
select 1e35BD / 0.1
       ^^^^^^^^^^^^


-- !query
select 123456789123456789.1234567890 * 1.123456789123456789
-- !query schema
struct<(123456789123456789.1234567890 * 1.123456789123456789):decimal(38,18)>
-- !query output
138698367904130467.654320988515622621


-- !query
select 123456789123456789.1234567890 * 1.123456789123456789
-- !query schema
struct<(123456789123456789.1234567890 * 1.123456789123456789):decimal(38,18)>
-- !query output
138698367904130467.654320988515622621


-- !query
select 12345678912345.123456789123 / 0.000000012345678
-- !query schema
struct<(12345678912345.123456789123 / 1.2345678E-8):decimal(38,9)>
-- !query output
1000000073899961059796.725866332


-- !query
drop table decimals_test
-- !query schema
struct<>
-- !query output

