-- Automatically generated by SQLQueryTestSuite
-- Number of queries: 7


-- !query
CREATE TABLE explain_temp1(a INT, b INT) USING PARQUET
-- !query schema
struct<>
-- !query output



-- !query
CREATE TABLE explain_temp2(c INT, d INT) USING PARQUET
-- !query schema
struct<>
-- !query output



-- !query
ANALYZE TABLE explain_temp1 COMPUTE STATISTICS FOR ALL COLUMNS
-- !query schema
struct<>
-- !query output



-- !query
ANALYZE TABLE explain_temp2 COMPUTE STATISTICS FOR ALL COLUMNS
-- !query schema
struct<>
-- !query output



-- !query
EXPLAIN COST WITH max_store_sales AS
(
  SELECT max(csales) tpcds_cmax
  FROM (
    SELECT sum(b) csales
    FROM explain_temp1 WHERE a < 100
  ) x
),
best_ss_customer AS
(
  SELECT c
  FROM explain_temp2
  WHERE d > (SELECT * FROM max_store_sales)
)
SELECT c FROM best_ss_customer
-- !query schema
struct<plan:string>
-- !query output
== Optimized Logical Plan ==
Project [c#x], Statistics(sizeInBytes=1.0 B, rowCount=0)
+- Filter ((cast(d#x as bigint) > scalar-subquery#x []) AND isnotnull(d#x)), Statistics(sizeInBytes=1.0 B, rowCount=0)
   :  +- Aggregate [max(csales#xL) AS tpcds_cmax#xL], Statistics(sizeInBytes=16.0 B, rowCount=1)
   :     +- Aggregate [sum(b#x) AS csales#xL], Statistics(sizeInBytes=16.0 B, rowCount=1)
   :        +- Project [b#x], Statistics(sizeInBytes=1.0 B, rowCount=0)
   :           +- Filter ((a#x < 100) AND isnotnull(a#x)), Statistics(sizeInBytes=1.0 B, rowCount=0)
   :              +- Relation default.explain_temp1[a#x,b#x] parquet, Statistics(sizeInBytes=1.0 B, rowCount=0)
   +- Relation default.explain_temp2[c#x,d#x] parquet, Statistics(sizeInBytes=1.0 B, rowCount=0)

== Physical Plan ==
AdaptiveSparkPlan isFinalPlan=false
+- Project [c#x]
   +- Filter ((cast(d#x as bigint) > Subquery subquery#x, [id=#x]) AND isnotnull(d#x))
      :  +- Subquery subquery#x, [id=#x]
      :     +- AdaptiveSparkPlan isFinalPlan=false
      :        +- HashAggregate(keys=[], functions=[max(csales#xL)], output=[tpcds_cmax#xL])
      :           +- HashAggregate(keys=[], functions=[partial_max(csales#xL)], output=[max#xL])
      :              +- HashAggregate(keys=[], functions=[sum(b#x)], output=[csales#xL])
      :                 +- Exchange SinglePartition, ENSURE_REQUIREMENTS, [id=#x]
      :                    +- HashAggregate(keys=[], functions=[partial_sum(b#x)], output=[sum#xL])
      :                       +- Project [b#x]
      :                          +- Filter ((a#x < 100) AND isnotnull(a#x))
      :                             +- FileScan parquet default.explain_temp1[a#x,b#x] Batched: true, DataFilters: [(a#x < 100), isnotnull(a#x)], Format: Parquet, Location [not included in comparison]/{warehouse_dir}/explain_temp1], PartitionFilters: [], PushedFilters: [LessThan(a,100), IsNotNull(a)], ReadSchema: struct<a:int,b:int>
      +- FileScan parquet default.explain_temp2[c#x,d#x] Batched: true, DataFilters: [isnotnull(d#x)], Format: Parquet, Location [not included in comparison]/{warehouse_dir}/explain_temp2], PartitionFilters: [], PushedFilters: [IsNotNull(d)], ReadSchema: struct<c:int,d:int>


-- !query
DROP TABLE explain_temp1
-- !query schema
struct<>
-- !query output



-- !query
DROP TABLE explain_temp2
-- !query schema
struct<>
-- !query output

