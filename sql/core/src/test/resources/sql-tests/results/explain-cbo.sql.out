-- Automatically generated by SQLQueryTestSuite
-- !query
CREATE TABLE explain_temp1(a INT, b INT) USING PARQUET
-- !query schema
struct<>
-- !query output



-- !query
CREATE TABLE explain_temp2(c INT, d INT) USING PARQUET
-- !query schema
struct<>
-- !query output



-- !query
ANALYZE TABLE explain_temp1 COMPUTE STATISTICS FOR ALL COLUMNS
-- !query schema
struct<>
-- !query output



-- !query
ANALYZE TABLE explain_temp2 COMPUTE STATISTICS FOR ALL COLUMNS
-- !query schema
struct<>
-- !query output



-- !query
EXPLAIN COST WITH max_store_sales AS
(
  SELECT max(csales) tpcds_cmax
  FROM (
    SELECT sum(b) csales
    FROM explain_temp1 WHERE a < 100
  ) x
),
best_ss_customer AS
(
  SELECT c
  FROM explain_temp2
  WHERE d > (SELECT * FROM max_store_sales)
)
SELECT c FROM best_ss_customer
-- !query schema
struct<plan:string>
-- !query output
== Optimized Logical Plan ==
Project [c#x], Statistics(sizeInBytes=1.0 B, rowCount=0)
+- Filter (isnotnull(d#x) AND (cast(d#x as bigint) > scalar-subquery#x [])), Statistics(sizeInBytes=1.0 B, rowCount=0)
   :  +- Aggregate [max(csales#xL) AS tpcds_cmax#xL], Statistics(sizeInBytes=16.0 B, rowCount=1)
   :     +- Aggregate [sum(b#x) AS csales#xL], Statistics(sizeInBytes=16.0 B, rowCount=1)
   :        +- Project [b#x], Statistics(sizeInBytes=1.0 B, rowCount=0)
   :           +- Filter (isnotnull(a#x) AND (a#x < 100)), Statistics(sizeInBytes=1.0 B, rowCount=0)
   :              +- Relation spark_catalog.default.explain_temp1[a#x,b#x] parquet, Statistics(sizeInBytes=1.0 B, rowCount=0)
   +- Relation spark_catalog.default.explain_temp2[c#x,d#x] parquet, Statistics(sizeInBytes=1.0 B, rowCount=0)

== Physical Plan ==
AdaptiveSparkPlan isFinalPlan=false
+- Project [c#x]
   +- Filter (isnotnull(d#x) AND (cast(d#x as bigint) > Subquery subquery#x, [id=#x]))
      :  +- Subquery subquery#x, [id=#x]
      :     +- AdaptiveSparkPlan isFinalPlan=false
      :        +- HashAggregate(keys=[], functions=[max(csales#xL)], output=[tpcds_cmax#xL])
      :           +- HashAggregate(keys=[], functions=[partial_max(csales#xL)], output=[max#xL])
      :              +- HashAggregate(keys=[], functions=[sum(b#x)], output=[csales#xL])
      :                 +- Exchange SinglePartition, ENSURE_REQUIREMENTS, [plan_id=x]
      :                    +- HashAggregate(keys=[], functions=[partial_sum(b#x)], output=[sum#xL])
      :                       +- Project [b#x]
      :                          +- Filter (isnotnull(a#x) AND (a#x < 100))
      :                             +- FileScan parquet spark_catalog.default.explain_temp1[a#x,b#x] Batched: true, DataFilters: [isnotnull(a#x), (a#x < 100)], Format: Parquet, Location [not included in comparison]/{warehouse_dir}/explain_temp1], PartitionFilters: [], PushedFilters: [IsNotNull(a), LessThan(a,100)], ReadSchema: struct<a:int,b:int>
      +- FileScan parquet spark_catalog.default.explain_temp2[c#x,d#x] Batched: true, DataFilters: [isnotnull(d#x)], Format: Parquet, Location [not included in comparison]/{warehouse_dir}/explain_temp2], PartitionFilters: [], PushedFilters: [IsNotNull(d)], ReadSchema: struct<c:int,d:int>


-- !query
DROP TABLE explain_temp1
-- !query schema
struct<>
-- !query output



-- !query
DROP TABLE explain_temp2
-- !query schema
struct<>
-- !query output

