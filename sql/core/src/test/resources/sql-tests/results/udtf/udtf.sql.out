-- Automatically generated by SQLQueryTestSuite
-- !query
DROP VIEW IF EXISTS t1
-- !query schema
struct<>
-- !query output



-- !query
DROP VIEW IF EXISTS t2
-- !query schema
struct<>
-- !query output



-- !query
CREATE OR REPLACE TEMPORARY VIEW t1 AS VALUES (0, 1), (1, 2) t(c1, c2)
-- !query schema
struct<>
-- !query output



-- !query
CREATE OR REPLACE TEMPORARY VIEW t2 AS VALUES (0, 1), (1, 2), (1, 3) t(partition_col, input)
-- !query schema
struct<>
-- !query output



-- !query
SELECT * FROM udtf(1, 2)
-- !query schema
struct<x:int,y:int>
-- !query output
1	-1
2	1


-- !query
SELECT * FROM udtf(-1, 0)
-- !query schema
struct<x:int,y:int>
-- !query output



-- !query
SELECT * FROM udtf(0, -1)
-- !query schema
struct<x:int,y:int>
-- !query output



-- !query
SELECT * FROM udtf(0, 0)
-- !query schema
struct<x:int,y:int>
-- !query output
0	0


-- !query
SELECT a, b FROM udtf(1, 2) t(a, b)
-- !query schema
struct<a:int,b:int>
-- !query output
1	-1
2	1


-- !query
SELECT * FROM t1, LATERAL udtf(c1, c2)
-- !query schema
struct<c1:int,c2:int,x:int,y:int>
-- !query output
1	2	1	-1
1	2	2	1


-- !query
SELECT * FROM t1 LEFT JOIN LATERAL udtf(c1, c2)
-- !query schema
struct<c1:int,c2:int,x:int,y:int>
-- !query output
1	2	1	-1
1	2	2	1


-- !query
SELECT * FROM udtf(1, 2) t(c1, c2), LATERAL udtf(c1, c2)
-- !query schema
struct<c1:int,c2:int,x:int,y:int>
-- !query output
2	1	1	-1
2	1	2	1


-- !query
SELECT * FROM udtf(cast(rand(0) AS int) + 1, 1)
-- !query schema
struct<x:int,y:int>
-- !query output
1	0
1	0


-- !query
SELECT * FROM UDTFCountSumLast(TABLE(t2) WITH SINGLE PARTITION)
-- !query schema
struct<count:int,total:int,last:int>
-- !query output
3	6	3


-- !query
SELECT * FROM UDTFCountSumLast(TABLE(t2) PARTITION BY partition_col ORDER BY input)
-- !query schema
struct<count:int,total:int,last:int>
-- !query output
1	1	1
2	5	3


-- !query
SELECT * FROM UDTFCountSumLast(TABLE(t2) PARTITION BY partition_col ORDER BY input DESC)
-- !query schema
struct<count:int,total:int,last:int>
-- !query output
1	1	1
2	5	2


-- !query
SELECT * FROM
    VALUES (0), (1) AS t(col)
    JOIN LATERAL
    UDTFCountSumLast(TABLE(t2) PARTITION BY partition_col ORDER BY input DESC)
-- !query schema
struct<>
-- !query output
org.apache.spark.sql.catalyst.ExtendedAnalysisException
{
  "condition" : "UNSUPPORTED_SUBQUERY_EXPRESSION_CATEGORY.NON_DETERMINISTIC_LATERAL_SUBQUERIES",
  "sqlState" : "0A000",
  "messageParameters" : {
    "treeNode" : "LateralJoin lateral-subquery#x [], Inner\n:  +- Project [count#x, total#x, last#x]\n:     +- LateralJoin lateral-subquery#x [c#x], Inner\n:        :  +- SubqueryAlias __auto_generated_subquery_name_1\n:        :     +- Generate UDTFCountSumLast(outer(c#x))#x, false, [count#x, total#x, last#x]\n:        :        +- OneRowRelation\n:        +- SubqueryAlias __auto_generated_subquery_name_0\n:           +- Project [named_struct(partition_col, partition_col#x, input, input#x, partition_by_0, partition_by_0#x) AS c#x]\n:              +- Sort [partition_by_0#x ASC NULLS FIRST, input#x DESC NULLS LAST], false\n:                 +- RepartitionByExpression [partition_by_0#x]\n:                    +- Project [partition_col#x, input#x, partition_col#x AS partition_by_0#x]\n:                       +- SubqueryAlias t2\n:                          +- View (`t2`, [partition_col#x, input#x])\n:                             +- Project [cast(partition_col#x as int) AS partition_col#x, cast(input#x as int) AS input#x]\n:                                +- SubqueryAlias t\n:                                   +- LocalRelation [partition_col#x, input#x]\n+- SubqueryAlias t\n   +- LocalRelation [col#x]\n"
  },
  "queryContext" : [ {
    "objectType" : "",
    "objectName" : "",
    "startIndex" : 49,
    "stopIndex" : 139,
    "fragment" : "JOIN LATERAL\n    UDTFCountSumLast(TABLE(t2) PARTITION BY partition_col ORDER BY input DESC)"
  } ]
}


-- !query
SELECT * FROM UDTFWithSinglePartition(0, TABLE(t2))
-- !query schema
struct<count:int,total:int,last:int>
-- !query output
3	6	3


-- !query
SELECT * FROM UDTFWithSinglePartition(1, TABLE(t2))
-- !query schema
struct<count:int,total:int,last:int>
-- !query output
3	6	3


-- !query
SELECT * FROM UDTFWithSinglePartition(0, TABLE(t2) WITH SINGLE PARTITION)
-- !query schema
struct<>
-- !query output
org.apache.spark.sql.AnalysisException
{
  "condition" : "TABLE_VALUED_FUNCTION_REQUIRED_METADATA_INCOMPATIBLE_WITH_CALL",
  "sqlState" : "22023",
  "messageParameters" : {
    "functionName" : "UDTFWithSinglePartition",
    "invalidFunctionCallProperty" : "specified the WITH SINGLE PARTITION or PARTITION BY clause; please remove these clauses and retry the query again.",
    "requestedMetadata" : "specified its own required partitioning of the input table"
  },
  "queryContext" : [ {
    "objectType" : "",
    "objectName" : "",
    "startIndex" : 15,
    "stopIndex" : 73,
    "fragment" : "UDTFWithSinglePartition(0, TABLE(t2) WITH SINGLE PARTITION)"
  } ]
}


-- !query
SELECT * FROM UDTFWithSinglePartition(0, TABLE(t2) PARTITION BY partition_col)
-- !query schema
struct<>
-- !query output
org.apache.spark.sql.AnalysisException
{
  "condition" : "TABLE_VALUED_FUNCTION_REQUIRED_METADATA_INCOMPATIBLE_WITH_CALL",
  "sqlState" : "22023",
  "messageParameters" : {
    "functionName" : "UDTFWithSinglePartition",
    "invalidFunctionCallProperty" : "specified the WITH SINGLE PARTITION or PARTITION BY clause; please remove these clauses and retry the query again.",
    "requestedMetadata" : "specified its own required partitioning of the input table"
  },
  "queryContext" : [ {
    "objectType" : "",
    "objectName" : "",
    "startIndex" : 15,
    "stopIndex" : 78,
    "fragment" : "UDTFWithSinglePartition(0, TABLE(t2) PARTITION BY partition_col)"
  } ]
}


-- !query
SELECT * FROM
    VALUES (0), (1) AS t(col)
    JOIN LATERAL
    UDTFWithSinglePartition(0, TABLE(t2) PARTITION BY partition_col)
-- !query schema
struct<>
-- !query output
org.apache.spark.sql.AnalysisException
{
  "condition" : "TABLE_VALUED_FUNCTION_REQUIRED_METADATA_INCOMPATIBLE_WITH_CALL",
  "sqlState" : "22023",
  "messageParameters" : {
    "functionName" : "UDTFWithSinglePartition",
    "invalidFunctionCallProperty" : "specified the WITH SINGLE PARTITION or PARTITION BY clause; please remove these clauses and retry the query again.",
    "requestedMetadata" : "specified its own required partitioning of the input table"
  },
  "queryContext" : [ {
    "objectType" : "",
    "objectName" : "",
    "startIndex" : 66,
    "stopIndex" : 129,
    "fragment" : "UDTFWithSinglePartition(0, TABLE(t2) PARTITION BY partition_col)"
  } ]
}


-- !query
SELECT * FROM UDTFPartitionByOrderBy(TABLE(t2))
-- !query schema
struct<partition_col:int,count:int,total:int,last:int>
-- !query output
0	1	1	1
1	2	5	3


-- !query
SELECT * FROM UDTFPartitionByOrderBy(TABLE(t2) WITH SINGLE PARTITION)
-- !query schema
struct<>
-- !query output
org.apache.spark.sql.AnalysisException
{
  "condition" : "TABLE_VALUED_FUNCTION_REQUIRED_METADATA_INCOMPATIBLE_WITH_CALL",
  "sqlState" : "22023",
  "messageParameters" : {
    "functionName" : "UDTFPartitionByOrderBy",
    "invalidFunctionCallProperty" : "specified the WITH SINGLE PARTITION or PARTITION BY clause; please remove these clauses and retry the query again.",
    "requestedMetadata" : "specified its own required partitioning of the input table"
  },
  "queryContext" : [ {
    "objectType" : "",
    "objectName" : "",
    "startIndex" : 15,
    "stopIndex" : 69,
    "fragment" : "UDTFPartitionByOrderBy(TABLE(t2) WITH SINGLE PARTITION)"
  } ]
}


-- !query
SELECT * FROM UDTFPartitionByOrderBy(TABLE(t2) PARTITION BY partition_col)
-- !query schema
struct<>
-- !query output
org.apache.spark.sql.AnalysisException
{
  "condition" : "TABLE_VALUED_FUNCTION_REQUIRED_METADATA_INCOMPATIBLE_WITH_CALL",
  "sqlState" : "22023",
  "messageParameters" : {
    "functionName" : "UDTFPartitionByOrderBy",
    "invalidFunctionCallProperty" : "specified the WITH SINGLE PARTITION or PARTITION BY clause; please remove these clauses and retry the query again.",
    "requestedMetadata" : "specified its own required partitioning of the input table"
  },
  "queryContext" : [ {
    "objectType" : "",
    "objectName" : "",
    "startIndex" : 15,
    "stopIndex" : 74,
    "fragment" : "UDTFPartitionByOrderBy(TABLE(t2) PARTITION BY partition_col)"
  } ]
}


-- !query
SELECT * FROM
    VALUES (0), (1) AS t(col)
    JOIN LATERAL
    UDTFPartitionByOrderBy(TABLE(t2) PARTITION BY partition_col)
-- !query schema
struct<>
-- !query output
org.apache.spark.sql.AnalysisException
{
  "condition" : "TABLE_VALUED_FUNCTION_REQUIRED_METADATA_INCOMPATIBLE_WITH_CALL",
  "sqlState" : "22023",
  "messageParameters" : {
    "functionName" : "UDTFPartitionByOrderBy",
    "invalidFunctionCallProperty" : "specified the WITH SINGLE PARTITION or PARTITION BY clause; please remove these clauses and retry the query again.",
    "requestedMetadata" : "specified its own required partitioning of the input table"
  },
  "queryContext" : [ {
    "objectType" : "",
    "objectName" : "",
    "startIndex" : 66,
    "stopIndex" : 125,
    "fragment" : "UDTFPartitionByOrderBy(TABLE(t2) PARTITION BY partition_col)"
  } ]
}


-- !query
SELECT * FROM UDTFPartitionByOrderByComplexExpr(TABLE(t2))
-- !query schema
struct<partition_col:int,count:int,total:int,last:int>
-- !query output
0	1	1	1
1	2	5	3


-- !query
SELECT * FROM UDTFPartitionByOrderBySelectExpr(TABLE(t2))
-- !query schema
struct<partition_col:int,count:int,total:int,last:int>
-- !query output
0	1	1	1
1	2	5	3


-- !query
SELECT * FROM UDTFPartitionByOrderBySelectComplexExpr(TABLE(t2))
-- !query schema
struct<partition_col:int,count:int,total:int,last:int>
-- !query output
0	1	2	2
1	2	7	4


-- !query
SELECT * FROM UDTFPartitionByOrderBySelectExprOnlyPartitionColumn(TABLE(t2))
-- !query schema
struct<>
-- !query output
org.apache.spark.api.python.PythonException
ValueError: 'input' is not in list

During handling of the above exception, another exception occurred:

pyspark.errors.exceptions.base.PySparkValueError: input


-- !query
SELECT * FROM UDTFInvalidSelectExprParseError(TABLE(t2))
-- !query schema
struct<>
-- !query output
org.apache.spark.sql.catalyst.ExtendedAnalysisException
{
  "condition" : "UNRESOLVED_COLUMN.WITH_SUGGESTION",
  "sqlState" : "42703",
  "messageParameters" : {
    "objectName" : "`unparsable`",
    "proposal" : "`t2`.`input`, `partition_by_0`, `t2`.`partition_col`"
  },
  "queryContext" : [ {
    "objectType" : "",
    "objectName" : "",
    "startIndex" : 1,
    "stopIndex" : 10,
    "fragment" : "unparsable"
  } ]
}


-- !query
SELECT * FROM UDTFInvalidSelectExprStringValue(TABLE(t2))
-- !query schema
struct<>
-- !query output
org.apache.spark.sql.AnalysisException
{
  "condition" : "TABLE_VALUED_FUNCTION_FAILED_TO_ANALYZE_IN_PYTHON",
  "sqlState" : "38000",
  "messageParameters" : {
    "msg" : "Failed to evaluate the user-defined table function 'UDTFInvalidSelectExprStringValue' because the static 'analyze' method returned an 'AnalyzeResult' object with the 'select' field set to a value besides a list or tuple of 'SelectedColumn' objects. Please update the table function and then try the query again."
  },
  "queryContext" : [ {
    "objectType" : "",
    "objectName" : "",
    "startIndex" : 15,
    "stopIndex" : 57,
    "fragment" : "UDTFInvalidSelectExprStringValue(TABLE(t2))"
  } ]
}


-- !query
SELECT * FROM UDTFInvalidComplexSelectExprMissingAlias(TABLE(t2))
-- !query schema
struct<>
-- !query output
org.apache.spark.sql.AnalysisException
{
  "condition" : "UDTF_INVALID_REQUESTED_SELECTED_EXPRESSION_FROM_ANALYZE_METHOD_REQUIRES_ALIAS",
  "sqlState" : "42802",
  "messageParameters" : {
    "expression" : "(input + 1)"
  },
  "queryContext" : [ {
    "objectType" : "",
    "objectName" : "",
    "startIndex" : 15,
    "stopIndex" : 65,
    "fragment" : "UDTFInvalidComplexSelectExprMissingAlias(TABLE(t2))"
  } ]
}


-- !query
SELECT * FROM UDTFInvalidOrderByAscKeyword(TABLE(t2))
-- !query schema
struct<>
-- !query output
org.apache.spark.sql.AnalysisException
{
  "condition" : "UDTF_INVALID_ALIAS_IN_REQUESTED_ORDERING_STRING_FROM_ANALYZE_METHOD",
  "sqlState" : "42802",
  "messageParameters" : {
    "aliasName" : "ASC"
  },
  "queryContext" : [ {
    "objectType" : "",
    "objectName" : "",
    "startIndex" : 15,
    "stopIndex" : 53,
    "fragment" : "UDTFInvalidOrderByAscKeyword(TABLE(t2))"
  } ]
}


-- !query
SELECT * FROM UDTFInvalidOrderByStringList(TABLE(t2))
-- !query schema
struct<>
-- !query output
org.apache.spark.sql.AnalysisException
{
  "condition" : "TABLE_VALUED_FUNCTION_FAILED_TO_ANALYZE_IN_PYTHON",
  "sqlState" : "38000",
  "messageParameters" : {
    "msg" : "Failed to evaluate the user-defined table function 'UDTFInvalidOrderByStringList' because the static 'analyze' method returned an 'AnalyzeResult' object with the 'orderBy' field set to a value besides a list or tuple of 'OrderingColumn' objects. Please update the table function and then try the query again."
  },
  "queryContext" : [ {
    "objectType" : "",
    "objectName" : "",
    "startIndex" : 15,
    "stopIndex" : 53,
    "fragment" : "UDTFInvalidOrderByStringList(TABLE(t2))"
  } ]
}


-- !query
SELECT * FROM UDTFInvalidPartitionByAndWithSinglePartition(TABLE(t2))
-- !query schema
struct<>
-- !query output
org.apache.spark.sql.AnalysisException
{
  "condition" : "TABLE_VALUED_FUNCTION_REQUIRED_METADATA_INVALID",
  "sqlState" : "22023",
  "messageParameters" : {
    "functionName" : "UDTFInvalidPartitionByAndWithSinglePartition",
    "reason" : "the 'with_single_partition' field cannot be assigned to true if the 'partition_by' list is non-empty"
  },
  "queryContext" : [ {
    "objectType" : "",
    "objectName" : "",
    "startIndex" : 15,
    "stopIndex" : 69,
    "fragment" : "UDTFInvalidPartitionByAndWithSinglePartition(TABLE(t2))"
  } ]
}


-- !query
SELECT * FROM UDTFInvalidPartitionByAndWithSinglePartition(TABLE(t2) WITH SINGLE PARTITION)
-- !query schema
struct<>
-- !query output
org.apache.spark.sql.AnalysisException
{
  "condition" : "TABLE_VALUED_FUNCTION_REQUIRED_METADATA_INVALID",
  "sqlState" : "22023",
  "messageParameters" : {
    "functionName" : "UDTFInvalidPartitionByAndWithSinglePartition",
    "reason" : "the 'with_single_partition' field cannot be assigned to true if the 'partition_by' list is non-empty"
  },
  "queryContext" : [ {
    "objectType" : "",
    "objectName" : "",
    "startIndex" : 15,
    "stopIndex" : 91,
    "fragment" : "UDTFInvalidPartitionByAndWithSinglePartition(TABLE(t2) WITH SINGLE PARTITION)"
  } ]
}


-- !query
SELECT * FROM UDTFInvalidPartitionByAndWithSinglePartition(TABLE(t2) PARTITION BY partition_col)
-- !query schema
struct<>
-- !query output
org.apache.spark.sql.AnalysisException
{
  "condition" : "TABLE_VALUED_FUNCTION_REQUIRED_METADATA_INVALID",
  "sqlState" : "22023",
  "messageParameters" : {
    "functionName" : "UDTFInvalidPartitionByAndWithSinglePartition",
    "reason" : "the 'with_single_partition' field cannot be assigned to true if the 'partition_by' list is non-empty"
  },
  "queryContext" : [ {
    "objectType" : "",
    "objectName" : "",
    "startIndex" : 15,
    "stopIndex" : 96,
    "fragment" : "UDTFInvalidPartitionByAndWithSinglePartition(TABLE(t2) PARTITION BY partition_col)"
  } ]
}


-- !query
SELECT * FROM
    VALUES (0), (1) AS t(col)
    JOIN LATERAL
    UDTFInvalidPartitionByAndWithSinglePartition(TABLE(t2) PARTITION BY partition_col)
-- !query schema
struct<>
-- !query output
org.apache.spark.sql.AnalysisException
{
  "condition" : "TABLE_VALUED_FUNCTION_REQUIRED_METADATA_INVALID",
  "sqlState" : "22023",
  "messageParameters" : {
    "functionName" : "UDTFInvalidPartitionByAndWithSinglePartition",
    "reason" : "the 'with_single_partition' field cannot be assigned to true if the 'partition_by' list is non-empty"
  },
  "queryContext" : [ {
    "objectType" : "",
    "objectName" : "",
    "startIndex" : 66,
    "stopIndex" : 147,
    "fragment" : "UDTFInvalidPartitionByAndWithSinglePartition(TABLE(t2) PARTITION BY partition_col)"
  } ]
}


-- !query
SELECT * FROM UDTFInvalidOrderByWithoutPartitionBy(TABLE(t2))
-- !query schema
struct<>
-- !query output
org.apache.spark.sql.AnalysisException
{
  "condition" : "TABLE_VALUED_FUNCTION_REQUIRED_METADATA_INVALID",
  "sqlState" : "22023",
  "messageParameters" : {
    "functionName" : "UDTFInvalidOrderByWithoutPartitionBy",
    "reason" : "the 'order_by' field cannot be non-empty unless the 'with_single_partition' field is set to true or the 'partition_by' list is non-empty"
  },
  "queryContext" : [ {
    "objectType" : "",
    "objectName" : "",
    "startIndex" : 15,
    "stopIndex" : 61,
    "fragment" : "UDTFInvalidOrderByWithoutPartitionBy(TABLE(t2))"
  } ]
}


-- !query
SELECT * FROM UDTFInvalidOrderByWithoutPartitionBy(TABLE(t2) WITH SINGLE PARTITION)
-- !query schema
struct<>
-- !query output
org.apache.spark.sql.AnalysisException
{
  "condition" : "TABLE_VALUED_FUNCTION_REQUIRED_METADATA_INVALID",
  "sqlState" : "22023",
  "messageParameters" : {
    "functionName" : "UDTFInvalidOrderByWithoutPartitionBy",
    "reason" : "the 'order_by' field cannot be non-empty unless the 'with_single_partition' field is set to true or the 'partition_by' list is non-empty"
  },
  "queryContext" : [ {
    "objectType" : "",
    "objectName" : "",
    "startIndex" : 15,
    "stopIndex" : 83,
    "fragment" : "UDTFInvalidOrderByWithoutPartitionBy(TABLE(t2) WITH SINGLE PARTITION)"
  } ]
}


-- !query
SELECT * FROM UDTFInvalidOrderByWithoutPartitionBy(TABLE(t2) PARTITION BY partition_col)
-- !query schema
struct<>
-- !query output
org.apache.spark.sql.AnalysisException
{
  "condition" : "TABLE_VALUED_FUNCTION_REQUIRED_METADATA_INVALID",
  "sqlState" : "22023",
  "messageParameters" : {
    "functionName" : "UDTFInvalidOrderByWithoutPartitionBy",
    "reason" : "the 'order_by' field cannot be non-empty unless the 'with_single_partition' field is set to true or the 'partition_by' list is non-empty"
  },
  "queryContext" : [ {
    "objectType" : "",
    "objectName" : "",
    "startIndex" : 15,
    "stopIndex" : 88,
    "fragment" : "UDTFInvalidOrderByWithoutPartitionBy(TABLE(t2) PARTITION BY partition_col)"
  } ]
}


-- !query
SELECT * FROM
    VALUES (0), (1) AS t(col)
    JOIN LATERAL
    UDTFInvalidOrderByWithoutPartitionBy(TABLE(t2) PARTITION BY partition_col)
-- !query schema
struct<>
-- !query output
org.apache.spark.sql.AnalysisException
{
  "condition" : "TABLE_VALUED_FUNCTION_REQUIRED_METADATA_INVALID",
  "sqlState" : "22023",
  "messageParameters" : {
    "functionName" : "UDTFInvalidOrderByWithoutPartitionBy",
    "reason" : "the 'order_by' field cannot be non-empty unless the 'with_single_partition' field is set to true or the 'partition_by' list is non-empty"
  },
  "queryContext" : [ {
    "objectType" : "",
    "objectName" : "",
    "startIndex" : 66,
    "stopIndex" : 139,
    "fragment" : "UDTFInvalidOrderByWithoutPartitionBy(TABLE(t2) PARTITION BY partition_col)"
  } ]
}


-- !query
SELECT * FROM InvalidEvalReturnsNoneToNonNullableColumnScalarType(TABLE(t2))
-- !query schema
struct<>
-- !query output
org.apache.spark.api.python.PythonException
pyspark.errors.exceptions.base.PySparkRuntimeError: [UDTF_EXEC_ERROR] User defined table function encountered an error in the 'eval' or 'terminate' method: Column 0 within a returned row had a value of None, either directly or within array/struct/map subfields, but the corresponding column type was declared as non-nullable; please update the UDTF to return a non-None value at this location or otherwise declare the column type as nullable.


-- !query
SELECT * FROM InvalidEvalReturnsNoneToNonNullableColumnArrayType(TABLE(t2))
-- !query schema
struct<>
-- !query output
org.apache.spark.api.python.PythonException
pyspark.errors.exceptions.base.PySparkRuntimeError: [UDTF_EXEC_ERROR] User defined table function encountered an error in the 'eval' or 'terminate' method: Column 0 within a returned row had a value of None, either directly or within array/struct/map subfields, but the corresponding column type was declared as non-nullable; please update the UDTF to return a non-None value at this location or otherwise declare the column type as nullable.


-- !query
SELECT * FROM InvalidEvalReturnsNoneToNonNullableColumnArrayElementType(TABLE(t2))
-- !query schema
struct<>
-- !query output
org.apache.spark.api.python.PythonException
pyspark.errors.exceptions.base.PySparkRuntimeError: [UDTF_EXEC_ERROR] User defined table function encountered an error in the 'eval' or 'terminate' method: Column 0 within a returned row had a value of None, either directly or within array/struct/map subfields, but the corresponding column type was declared as non-nullable; please update the UDTF to return a non-None value at this location or otherwise declare the column type as nullable.


-- !query
SELECT * FROM InvalidEvalReturnsNoneToNonNullableColumnStructType(TABLE(t2))
-- !query schema
struct<>
-- !query output
org.apache.spark.api.python.PythonException
pyspark.errors.exceptions.base.PySparkRuntimeError: [UDTF_EXEC_ERROR] User defined table function encountered an error in the 'eval' or 'terminate' method: Column 0 within a returned row had a value of None, either directly or within array/struct/map subfields, but the corresponding column type was declared as non-nullable; please update the UDTF to return a non-None value at this location or otherwise declare the column type as nullable.


-- !query
SELECT * FROM InvalidEvalReturnsNoneToNonNullableColumnMapType(TABLE(t2))
-- !query schema
struct<>
-- !query output
org.apache.spark.api.python.PythonException
pyspark.errors.exceptions.base.PySparkRuntimeError: [UDTF_EXEC_ERROR] User defined table function encountered an error in the 'eval' or 'terminate' method: Column 0 within a returned row had a value of None, either directly or within array/struct/map subfields, but the corresponding column type was declared as non-nullable; please update the UDTF to return a non-None value at this location or otherwise declare the column type as nullable.


-- !query
SELECT * FROM InvalidTerminateReturnsNoneToNonNullableColumnScalarType(TABLE(t2))
-- !query schema
struct<>
-- !query output
org.apache.spark.api.python.PythonException
pyspark.errors.exceptions.base.PySparkRuntimeError: [UDTF_EXEC_ERROR] User defined table function encountered an error in the 'eval' or 'terminate' method: Column 0 within a returned row had a value of None, either directly or within array/struct/map subfields, but the corresponding column type was declared as non-nullable; please update the UDTF to return a non-None value at this location or otherwise declare the column type as nullable.


-- !query
SELECT * FROM InvalidTerminateReturnsNoneToNonNullableColumnArrayType(TABLE(t2))
-- !query schema
struct<>
-- !query output
org.apache.spark.api.python.PythonException
pyspark.errors.exceptions.base.PySparkRuntimeError: [UDTF_EXEC_ERROR] User defined table function encountered an error in the 'eval' or 'terminate' method: Column 0 within a returned row had a value of None, either directly or within array/struct/map subfields, but the corresponding column type was declared as non-nullable; please update the UDTF to return a non-None value at this location or otherwise declare the column type as nullable.


-- !query
SELECT * FROM InvalidTerminateReturnsNoneToNonNullableColumnArrayElementType(TABLE(t2))
-- !query schema
struct<>
-- !query output
org.apache.spark.api.python.PythonException
pyspark.errors.exceptions.base.PySparkRuntimeError: [UDTF_EXEC_ERROR] User defined table function encountered an error in the 'eval' or 'terminate' method: Column 0 within a returned row had a value of None, either directly or within array/struct/map subfields, but the corresponding column type was declared as non-nullable; please update the UDTF to return a non-None value at this location or otherwise declare the column type as nullable.


-- !query
SELECT * FROM InvalidTerminateReturnsNoneToNonNullableColumnStructType(TABLE(t2))
-- !query schema
struct<>
-- !query output
org.apache.spark.api.python.PythonException
pyspark.errors.exceptions.base.PySparkRuntimeError: [UDTF_EXEC_ERROR] User defined table function encountered an error in the 'eval' or 'terminate' method: Column 0 within a returned row had a value of None, either directly or within array/struct/map subfields, but the corresponding column type was declared as non-nullable; please update the UDTF to return a non-None value at this location or otherwise declare the column type as nullable.


-- !query
SELECT * FROM InvalidTerminateReturnsNoneToNonNullableColumnMapType(TABLE(t2))
-- !query schema
struct<>
-- !query output
org.apache.spark.api.python.PythonException
pyspark.errors.exceptions.base.PySparkRuntimeError: [UDTF_EXEC_ERROR] User defined table function encountered an error in the 'eval' or 'terminate' method: Column 0 within a returned row had a value of None, either directly or within array/struct/map subfields, but the corresponding column type was declared as non-nullable; please update the UDTF to return a non-None value at this location or otherwise declare the column type as nullable.


-- !query
SELECT * FROM UDTFForwardStateFromAnalyzeWithKwargs()
-- !query schema
struct<>
-- !query output
org.apache.spark.api.python.PythonException
pyspark.errors.exceptions.base.PySparkRuntimeError: [UDTF_EVAL_METHOD_ARGUMENTS_DO_NOT_MATCH_SIGNATURE] Failed to evaluate the user-defined table function 'UDTFForwardStateFromAnalyzeWithKwargs' because the function arguments did not match the expected signature of the 'eval' method (missing a required argument: 'argument'). Please update the query so that this table function call provides arguments matching the expected signature, or else update the table function so that its 'eval' method accepts the provided arguments, and then try the query again.


-- !query
SELECT * FROM UDTFForwardStateFromAnalyzeWithKwargs(1, 2)
-- !query schema
struct<>
-- !query output
org.apache.spark.sql.AnalysisException
{
  "condition" : "TABLE_VALUED_FUNCTION_FAILED_TO_ANALYZE_IN_PYTHON",
  "sqlState" : "38000",
  "messageParameters" : {
    "msg" : "Failed to evaluate the user-defined table function 'UDTFForwardStateFromAnalyzeWithKwargs' because the function arguments did not match the expected signature of the static 'analyze' method (too many positional arguments). Please update the query so that this table function call provides arguments matching the expected signature, or else update the table function so that its static 'analyze' method accepts the provided arguments, and then try the query again."
  },
  "queryContext" : [ {
    "objectType" : "",
    "objectName" : "",
    "startIndex" : 15,
    "stopIndex" : 57,
    "fragment" : "UDTFForwardStateFromAnalyzeWithKwargs(1, 2)"
  } ]
}


-- !query
SELECT * FROM UDTFForwardStateFromAnalyzeWithKwargs(invalid => 2)
-- !query schema
struct<>
-- !query output
org.apache.spark.api.python.PythonException
pyspark.errors.exceptions.base.PySparkRuntimeError: [UDTF_EVAL_METHOD_ARGUMENTS_DO_NOT_MATCH_SIGNATURE] Failed to evaluate the user-defined table function 'UDTFForwardStateFromAnalyzeWithKwargs' because the function arguments did not match the expected signature of the 'eval' method (missing a required argument: 'argument'). Please update the query so that this table function call provides arguments matching the expected signature, or else update the table function so that its 'eval' method accepts the provided arguments, and then try the query again.


-- !query
SELECT * FROM UDTFForwardStateFromAnalyzeWithKwargs(argument => 1, argument => 2)
-- !query schema
struct<>
-- !query output
org.apache.spark.sql.AnalysisException
{
  "condition" : "DUPLICATE_ROUTINE_PARAMETER_ASSIGNMENT.DOUBLE_NAMED_ARGUMENT_REFERENCE",
  "sqlState" : "4274K",
  "messageParameters" : {
    "parameterName" : "`argument`",
    "routineName" : "`UDTFForwardStateFromAnalyzeWithKwargs`"
  },
  "queryContext" : [ {
    "objectType" : "",
    "objectName" : "",
    "startIndex" : 15,
    "stopIndex" : 81,
    "fragment" : "UDTFForwardStateFromAnalyzeWithKwargs(argument => 1, argument => 2)"
  } ]
}


-- !query
SELECT * FROM InvalidAnalyzeMethodWithSinglePartitionNoInputTable(argument => 1)
-- !query schema
struct<>
-- !query output
org.apache.spark.sql.AnalysisException
{
  "condition" : "TABLE_VALUED_FUNCTION_FAILED_TO_ANALYZE_IN_PYTHON",
  "sqlState" : "38000",
  "messageParameters" : {
    "msg" : "Failed to evaluate the user-defined table function 'InvalidAnalyzeMethodWithSinglePartitionNoInputTable' because the static 'analyze' method returned an 'AnalyzeResult' object with the 'withSinglePartition' field set to 'true', but the function call did not provide any table argument. Please update the query so that it provides a table argument, or else update the table function so that its 'analyze' method returns an 'AnalyzeResult' object with the 'withSinglePartition' field set to 'false', and then try the query again."
  },
  "queryContext" : [ {
    "objectType" : "",
    "objectName" : "",
    "startIndex" : 15,
    "stopIndex" : 80,
    "fragment" : "InvalidAnalyzeMethodWithSinglePartitionNoInputTable(argument => 1)"
  } ]
}


-- !query
SELECT * FROM InvalidAnalyzeMethodWithPartitionByNoInputTable(argument => 1)
-- !query schema
struct<>
-- !query output
org.apache.spark.sql.AnalysisException
{
  "condition" : "TABLE_VALUED_FUNCTION_FAILED_TO_ANALYZE_IN_PYTHON",
  "sqlState" : "38000",
  "messageParameters" : {
    "msg" : "Failed to evaluate the user-defined table function 'InvalidAnalyzeMethodWithPartitionByNoInputTable' because the static 'analyze' method returned an 'AnalyzeResult' object with the 'partitionBy' list set to non-empty, but the function call did not provide any table argument. Please update the query so that it provides a table argument, or else update the table function so that its 'analyze' method returns an 'AnalyzeResult' object with the 'partitionBy' list set to empty, and then try the query again."
  },
  "queryContext" : [ {
    "objectType" : "",
    "objectName" : "",
    "startIndex" : 15,
    "stopIndex" : 76,
    "fragment" : "InvalidAnalyzeMethodWithPartitionByNoInputTable(argument => 1)"
  } ]
}


-- !query
SELECT * FROM InvalidAnalyzeMethodReturnsNonStructTypeSchema(TABLE(t2))
-- !query schema
struct<>
-- !query output
org.apache.spark.sql.AnalysisException
{
  "condition" : "TABLE_VALUED_FUNCTION_FAILED_TO_ANALYZE_IN_PYTHON",
  "sqlState" : "38000",
  "messageParameters" : {
    "msg" : "Failed to evaluate the user-defined table function 'InvalidAnalyzeMethodReturnsNonStructTypeSchema' because the static 'analyze' method expects a result of type pyspark.sql.udtf.AnalyzeResult with a 'schema' field comprising a StructType, but the 'schema' field had the wrong type: <class 'int'>"
  },
  "queryContext" : [ {
    "objectType" : "",
    "objectName" : "",
    "startIndex" : 15,
    "stopIndex" : 71,
    "fragment" : "InvalidAnalyzeMethodReturnsNonStructTypeSchema(TABLE(t2))"
  } ]
}


-- !query
SELECT * FROM InvalidAnalyzeMethodWithPartitionByListOfStrings(argument => TABLE(t2))
-- !query schema
struct<>
-- !query output
org.apache.spark.sql.AnalysisException
{
  "condition" : "TABLE_VALUED_FUNCTION_FAILED_TO_ANALYZE_IN_PYTHON",
  "sqlState" : "38000",
  "messageParameters" : {
    "msg" : "Failed to evaluate the user-defined table function 'InvalidAnalyzeMethodWithPartitionByListOfStrings' because the static 'analyze' method returned an 'AnalyzeResult' object with the 'partitionBy' field set to a value besides a list or tuple of 'PartitioningColumn' objects. Please update the table function and then try the query again."
  },
  "queryContext" : [ {
    "objectType" : "",
    "objectName" : "",
    "startIndex" : 15,
    "stopIndex" : 85,
    "fragment" : "InvalidAnalyzeMethodWithPartitionByListOfStrings(argument => TABLE(t2))"
  } ]
}


-- !query
SELECT * FROM InvalidForwardStateFromAnalyzeTooManyInitArgs(TABLE(t2))
-- !query schema
struct<>
-- !query output
org.apache.spark.api.python.PythonException
pyspark.errors.exceptions.base.PySparkRuntimeError: [UDTF_CONSTRUCTOR_INVALID_IMPLEMENTS_ANALYZE_METHOD] Failed to evaluate the user-defined table function 'InvalidForwardStateFromAnalyzeTooManyInitArgs' because its constructor is invalid: the function implements the 'analyze' method, but its constructor has more than two arguments (including the 'self' reference). Please update the table function so that its constructor accepts exactly one 'self' argument, or one 'self' argument plus another argument for the result of the 'analyze' method, and try the query again.


-- !query
SELECT * FROM InvalidNotForwardStateFromAnalyzeTooManyInitArgs(TABLE(t2))
-- !query schema
struct<>
-- !query output
org.apache.spark.api.python.PythonException
pyspark.errors.exceptions.base.PySparkRuntimeError: [UDTF_CONSTRUCTOR_INVALID_NO_ANALYZE_METHOD] Failed to evaluate the user-defined table function 'InvalidNotForwardStateFromAnalyzeTooManyInitArgs' because its constructor is invalid: the function does not implement the 'analyze' method, and its constructor has more than one argument (including the 'self' reference). Please update the table function so that its constructor accepts exactly one 'self' argument, and try the query again.


-- !query
SELECT * FROM UDTFWithSinglePartition(1)
-- !query schema
struct<>
-- !query output
org.apache.spark.sql.AnalysisException
{
  "condition" : "TABLE_VALUED_FUNCTION_FAILED_TO_ANALYZE_IN_PYTHON",
  "sqlState" : "38000",
  "messageParameters" : {
    "msg" : "Failed to evaluate the user-defined table function 'UDTFWithSinglePartition' because the function arguments did not match the expected signature of the static 'analyze' method (missing a required argument: 'input_table'). Please update the query so that this table function call provides arguments matching the expected signature, or else update the table function so that its static 'analyze' method accepts the provided arguments, and then try the query again."
  },
  "queryContext" : [ {
    "objectType" : "",
    "objectName" : "",
    "startIndex" : 15,
    "stopIndex" : 40,
    "fragment" : "UDTFWithSinglePartition(1)"
  } ]
}


-- !query
SELECT * FROM UDTFWithSinglePartition(1, 2, 3)
-- !query schema
struct<>
-- !query output
org.apache.spark.sql.AnalysisException
{
  "condition" : "TABLE_VALUED_FUNCTION_FAILED_TO_ANALYZE_IN_PYTHON",
  "sqlState" : "38000",
  "messageParameters" : {
    "msg" : "Failed to evaluate the user-defined table function 'UDTFWithSinglePartition' because the function arguments did not match the expected signature of the static 'analyze' method (too many positional arguments). Please update the query so that this table function call provides arguments matching the expected signature, or else update the table function so that its static 'analyze' method accepts the provided arguments, and then try the query again."
  },
  "queryContext" : [ {
    "objectType" : "",
    "objectName" : "",
    "startIndex" : 15,
    "stopIndex" : 46,
    "fragment" : "UDTFWithSinglePartition(1, 2, 3)"
  } ]
}


-- !query
SELECT * FROM UDTFWithSinglePartition(1, invalid_arg_name => 2)
-- !query schema
struct<>
-- !query output
org.apache.spark.sql.AnalysisException
{
  "condition" : "TABLE_VALUED_FUNCTION_FAILED_TO_ANALYZE_IN_PYTHON",
  "sqlState" : "38000",
  "messageParameters" : {
    "msg" : "Failed to evaluate the user-defined table function 'UDTFWithSinglePartition' because the function arguments did not match the expected signature of the static 'analyze' method (missing a required argument: 'input_table'). Please update the query so that this table function call provides arguments matching the expected signature, or else update the table function so that its static 'analyze' method accepts the provided arguments, and then try the query again."
  },
  "queryContext" : [ {
    "objectType" : "",
    "objectName" : "",
    "startIndex" : 15,
    "stopIndex" : 63,
    "fragment" : "UDTFWithSinglePartition(1, invalid_arg_name => 2)"
  } ]
}


-- !query
SELECT * FROM UDTFWithSinglePartition(1, initial_count => 2)
-- !query schema
struct<>
-- !query output
org.apache.spark.sql.AnalysisException
{
  "condition" : "TABLE_VALUED_FUNCTION_FAILED_TO_ANALYZE_IN_PYTHON",
  "sqlState" : "38000",
  "messageParameters" : {
    "msg" : "Failed to evaluate the user-defined table function 'UDTFWithSinglePartition' because the function arguments did not match the expected signature of the static 'analyze' method (multiple values for argument 'initial_count'). Please update the query so that this table function call provides arguments matching the expected signature, or else update the table function so that its static 'analyze' method accepts the provided arguments, and then try the query again."
  },
  "queryContext" : [ {
    "objectType" : "",
    "objectName" : "",
    "startIndex" : 15,
    "stopIndex" : 60,
    "fragment" : "UDTFWithSinglePartition(1, initial_count => 2)"
  } ]
}


-- !query
SELECT * FROM UDTFWithSinglePartition(initial_count => 1, initial_count => 2)
-- !query schema
struct<>
-- !query output
org.apache.spark.sql.AnalysisException
{
  "condition" : "DUPLICATE_ROUTINE_PARAMETER_ASSIGNMENT.DOUBLE_NAMED_ARGUMENT_REFERENCE",
  "sqlState" : "4274K",
  "messageParameters" : {
    "parameterName" : "`initial_count`",
    "routineName" : "`UDTFWithSinglePartition`"
  },
  "queryContext" : [ {
    "objectType" : "",
    "objectName" : "",
    "startIndex" : 15,
    "stopIndex" : 77,
    "fragment" : "UDTFWithSinglePartition(initial_count => 1, initial_count => 2)"
  } ]
}


-- !query
SELECT * FROM UDTFInvalidPartitionByOrderByParseError(TABLE(t2))
-- !query schema
struct<>
-- !query output
org.apache.spark.sql.catalyst.ExtendedAnalysisException
{
  "condition" : "UNRESOLVED_COLUMN.WITH_SUGGESTION",
  "sqlState" : "42703",
  "messageParameters" : {
    "objectName" : "`unparsable`",
    "proposal" : "`input`, `partition_col`"
  },
  "queryContext" : [ {
    "objectType" : "",
    "objectName" : "",
    "startIndex" : 1,
    "stopIndex" : 10,
    "fragment" : "unparsable"
  } ]
}


-- !query
SELECT * FROM UDTFPartitionByIndexingBug(
    TABLE(
        SELECT
            5 AS unused_col,
            'hi' AS partition_col,
            1.0 AS double_col

        UNION ALL

        SELECT
            4 AS unused_col,
            'hi' AS partition_col,
            1.0 AS double_col
    )
)
-- !query schema
struct<partition_col:string,double_col:double>
-- !query output
NULL	1.0
NULL	1.0
NULL	1.0
NULL	1.0
NULL	1.0


-- !query
SELECT * FROM
    InvalidEvalReturnsNoneToNonNullableColumnScalarType(TABLE(SELECT 1 AS X), unresolved_column)
-- !query schema
struct<>
-- !query output
org.apache.spark.sql.catalyst.ExtendedAnalysisException
{
  "condition" : "UNRESOLVED_COLUMN.WITHOUT_SUGGESTION",
  "sqlState" : "42703",
  "messageParameters" : {
    "objectName" : "`unresolved_column`"
  },
  "queryContext" : [ {
    "objectType" : "",
    "objectName" : "",
    "startIndex" : 93,
    "stopIndex" : 109,
    "fragment" : "unresolved_column"
  } ]
}


-- !query
DROP VIEW t1
-- !query schema
struct<>
-- !query output



-- !query
DROP VIEW t2
-- !query schema
struct<>
-- !query output

