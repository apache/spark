-- Automatically generated by SQLQueryTestSuite
-- !query
CREATE table  explain_temp1 (key int, val int) USING PARQUET
-- !query schema
struct<>
-- !query output



-- !query
CREATE table  explain_temp2 (key int, val int) USING PARQUET
-- !query schema
struct<>
-- !query output



-- !query
CREATE table  explain_temp3 (key int, val int) USING PARQUET
-- !query schema
struct<>
-- !query output



-- !query
CREATE table  explain_temp4 (key int, val string) USING PARQUET
-- !query schema
struct<>
-- !query output



-- !query
CREATE table  explain_temp5 (key int) USING PARQUET PARTITIONED BY(val string)
-- !query schema
struct<>
-- !query output



-- !query
SET spark.sql.codegen.wholeStage = true
-- !query schema
struct<key:string,value:string>
-- !query output
spark.sql.codegen.wholeStage	true


-- !query
EXPLAIN EXTENDED
  SELECT sum(distinct val)
  FROM explain_temp1
-- !query schema
struct<plan:string>
-- !query output
== Parsed Logical Plan ==
'Project [unresolvedalias('sum(distinct 'val))]
+- 'UnresolvedRelation [explain_temp1], [], false

== Analyzed Logical Plan ==
sum(DISTINCT val): bigint
Aggregate [sum(distinct val#x) AS sum(DISTINCT val)#xL]
+- SubqueryAlias spark_catalog.default.explain_temp1
   +- Relation spark_catalog.default.explain_temp1[key#x,val#x] parquet

== Optimized Logical Plan ==
Aggregate [sum(distinct val#x) AS sum(DISTINCT val)#xL]
+- Project [val#x]
   +- Relation spark_catalog.default.explain_temp1[key#x,val#x] parquet

== Physical Plan ==
AdaptiveSparkPlan isFinalPlan=false
+- HashAggregate(keys=[], functions=[sum(distinct val#x)], output=[sum(DISTINCT val)#xL])
   +- Exchange SinglePartition, ENSURE_REQUIREMENTS, [plan_id=x]
      +- HashAggregate(keys=[], functions=[partial_sum(distinct val#x)], output=[sum#xL])
         +- HashAggregate(keys=[val#x], functions=[], output=[val#x])
            +- Exchange hashpartitioning(val#x, 4), ENSURE_REQUIREMENTS, [plan_id=x]
               +- HashAggregate(keys=[val#x], functions=[], output=[val#x])
                  +- FileScan parquet spark_catalog.default.explain_temp1[val#x] Batched: true, DataFilters: [], Format: Parquet, Location [not included in comparison]/{warehouse_dir}/explain_temp1], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<val:int>


-- !query
EXPLAIN FORMATTED
  SELECT key, max(val) 
  FROM   explain_temp1 
  WHERE  key > 0 
  GROUP  BY key 
  ORDER  BY key
-- !query schema
struct<plan:string>
-- !query output
== Physical Plan ==
AdaptiveSparkPlan (8)
+- Sort (7)
   +- Exchange (6)
      +- HashAggregate (5)
         +- Exchange (4)
            +- HashAggregate (3)
               +- Filter (2)
                  +- Scan parquet spark_catalog.default.explain_temp1 (1)


(1) Scan parquet spark_catalog.default.explain_temp1
Output [2]: [key#x, val#x]
Batched: true
Location [not included in comparison]/{warehouse_dir}/explain_temp1]
PushedFilters: [IsNotNull(key), GreaterThan(key,0)]
ReadSchema: struct<key:int,val:int>

(2) Filter
Input [2]: [key#x, val#x]
Condition : (isnotnull(key#x) AND (key#x > 0))

(3) HashAggregate
Input [2]: [key#x, val#x]
Keys [1]: [key#x]
Functions [1]: [partial_max(val#x)]
Aggregate Attributes [1]: [max#x]
Results [2]: [key#x, max#x]

(4) Exchange
Input [2]: [key#x, max#x]
Arguments: hashpartitioning(key#x, 4), ENSURE_REQUIREMENTS, [plan_id=x]

(5) HashAggregate
Input [2]: [key#x, max#x]
Keys [1]: [key#x]
Functions [1]: [max(val#x)]
Aggregate Attributes [1]: [max(val#x)#x]
Results [2]: [key#x, max(val#x)#x AS max(val)#x]

(6) Exchange
Input [2]: [key#x, max(val)#x]
Arguments: rangepartitioning(key#x ASC NULLS FIRST, 4), ENSURE_REQUIREMENTS, [plan_id=x]

(7) Sort
Input [2]: [key#x, max(val)#x]
Arguments: [key#x ASC NULLS FIRST], true, 0

(8) AdaptiveSparkPlan
Output [2]: [key#x, max(val)#x]
Arguments: isFinalPlan=false


-- !query
EXPLAIN FORMATTED
  SELECT key, max(val)
  FROM explain_temp1
  WHERE key > 0
  GROUP BY key
  HAVING max(val) > 0
-- !query schema
struct<plan:string>
-- !query output
== Physical Plan ==
AdaptiveSparkPlan (7)
+- Filter (6)
   +- HashAggregate (5)
      +- Exchange (4)
         +- HashAggregate (3)
            +- Filter (2)
               +- Scan parquet spark_catalog.default.explain_temp1 (1)


(1) Scan parquet spark_catalog.default.explain_temp1
Output [2]: [key#x, val#x]
Batched: true
Location [not included in comparison]/{warehouse_dir}/explain_temp1]
PushedFilters: [IsNotNull(key), GreaterThan(key,0)]
ReadSchema: struct<key:int,val:int>

(2) Filter
Input [2]: [key#x, val#x]
Condition : (isnotnull(key#x) AND (key#x > 0))

(3) HashAggregate
Input [2]: [key#x, val#x]
Keys [1]: [key#x]
Functions [1]: [partial_max(val#x)]
Aggregate Attributes [1]: [max#x]
Results [2]: [key#x, max#x]

(4) Exchange
Input [2]: [key#x, max#x]
Arguments: hashpartitioning(key#x, 4), ENSURE_REQUIREMENTS, [plan_id=x]

(5) HashAggregate
Input [2]: [key#x, max#x]
Keys [1]: [key#x]
Functions [1]: [max(val#x)]
Aggregate Attributes [1]: [max(val#x)#x]
Results [2]: [key#x, max(val#x)#x AS max(val)#x]

(6) Filter
Input [2]: [key#x, max(val)#x]
Condition : (isnotnull(max(val)#x) AND (max(val)#x > 0))

(7) AdaptiveSparkPlan
Output [2]: [key#x, max(val)#x]
Arguments: isFinalPlan=false


-- !query
EXPLAIN FORMATTED
  SELECT key, val FROM explain_temp1 WHERE key > 0
  UNION 
  SELECT key, val FROM explain_temp1 WHERE key > 1
-- !query schema
struct<plan:string>
-- !query output
== Physical Plan ==
AdaptiveSparkPlan (9)
+- HashAggregate (8)
   +- Exchange (7)
      +- HashAggregate (6)
         +- Union (5)
            :- Filter (2)
            :  +- Scan parquet spark_catalog.default.explain_temp1 (1)
            +- Filter (4)
               +- Scan parquet spark_catalog.default.explain_temp1 (3)


(1) Scan parquet spark_catalog.default.explain_temp1
Output [2]: [key#x, val#x]
Batched: true
Location [not included in comparison]/{warehouse_dir}/explain_temp1]
PushedFilters: [IsNotNull(key), GreaterThan(key,0)]
ReadSchema: struct<key:int,val:int>

(2) Filter
Input [2]: [key#x, val#x]
Condition : (isnotnull(key#x) AND (key#x > 0))

(3) Scan parquet spark_catalog.default.explain_temp1
Output [2]: [key#x, val#x]
Batched: true
Location [not included in comparison]/{warehouse_dir}/explain_temp1]
PushedFilters: [IsNotNull(key), GreaterThan(key,1)]
ReadSchema: struct<key:int,val:int>

(4) Filter
Input [2]: [key#x, val#x]
Condition : (isnotnull(key#x) AND (key#x > 1))

(5) Union

(6) HashAggregate
Input [2]: [key#x, val#x]
Keys [2]: [key#x, val#x]
Functions: []
Aggregate Attributes: []
Results [2]: [key#x, val#x]

(7) Exchange
Input [2]: [key#x, val#x]
Arguments: hashpartitioning(key#x, val#x, 4), ENSURE_REQUIREMENTS, [plan_id=x]

(8) HashAggregate
Input [2]: [key#x, val#x]
Keys [2]: [key#x, val#x]
Functions: []
Aggregate Attributes: []
Results [2]: [key#x, val#x]

(9) AdaptiveSparkPlan
Output [2]: [key#x, val#x]
Arguments: isFinalPlan=false


-- !query
EXPLAIN FORMATTED
  SELECT * 
  FROM   explain_temp1 a, 
         explain_temp2 b 
  WHERE  a.key = b.key
-- !query schema
struct<plan:string>
-- !query output
== Physical Plan ==
AdaptiveSparkPlan (7)
+- BroadcastHashJoin Inner BuildRight (6)
   :- Filter (2)
   :  +- Scan parquet spark_catalog.default.explain_temp1 (1)
   +- BroadcastExchange (5)
      +- Filter (4)
         +- Scan parquet spark_catalog.default.explain_temp2 (3)


(1) Scan parquet spark_catalog.default.explain_temp1
Output [2]: [key#x, val#x]
Batched: true
Location [not included in comparison]/{warehouse_dir}/explain_temp1]
PushedFilters: [IsNotNull(key)]
ReadSchema: struct<key:int,val:int>

(2) Filter
Input [2]: [key#x, val#x]
Condition : isnotnull(key#x)

(3) Scan parquet spark_catalog.default.explain_temp2
Output [2]: [key#x, val#x]
Batched: true
Location [not included in comparison]/{warehouse_dir}/explain_temp2]
PushedFilters: [IsNotNull(key)]
ReadSchema: struct<key:int,val:int>

(4) Filter
Input [2]: [key#x, val#x]
Condition : isnotnull(key#x)

(5) BroadcastExchange
Input [2]: [key#x, val#x]
Arguments: HashedRelationBroadcastMode(List(cast(input[0, int, false] as bigint)),false), [plan_id=x]

(6) BroadcastHashJoin
Left keys [1]: [key#x]
Right keys [1]: [key#x]
Join type: Inner
Join condition: None

(7) AdaptiveSparkPlan
Output [4]: [key#x, val#x, key#x, val#x]
Arguments: isFinalPlan=false


-- !query
EXPLAIN FORMATTED
  SELECT * 
  FROM   explain_temp1 a 
         LEFT OUTER JOIN explain_temp2 b 
                      ON a.key = b.key
-- !query schema
struct<plan:string>
-- !query output
== Physical Plan ==
AdaptiveSparkPlan (6)
+- BroadcastHashJoin LeftOuter BuildRight (5)
   :- Scan parquet spark_catalog.default.explain_temp1 (1)
   +- BroadcastExchange (4)
      +- Filter (3)
         +- Scan parquet spark_catalog.default.explain_temp2 (2)


(1) Scan parquet spark_catalog.default.explain_temp1
Output [2]: [key#x, val#x]
Batched: true
Location [not included in comparison]/{warehouse_dir}/explain_temp1]
ReadSchema: struct<key:int,val:int>

(2) Scan parquet spark_catalog.default.explain_temp2
Output [2]: [key#x, val#x]
Batched: true
Location [not included in comparison]/{warehouse_dir}/explain_temp2]
PushedFilters: [IsNotNull(key)]
ReadSchema: struct<key:int,val:int>

(3) Filter
Input [2]: [key#x, val#x]
Condition : isnotnull(key#x)

(4) BroadcastExchange
Input [2]: [key#x, val#x]
Arguments: HashedRelationBroadcastMode(List(cast(input[0, int, false] as bigint)),false), [plan_id=x]

(5) BroadcastHashJoin
Left keys [1]: [key#x]
Right keys [1]: [key#x]
Join type: LeftOuter
Join condition: None

(6) AdaptiveSparkPlan
Output [4]: [key#x, val#x, key#x, val#x]
Arguments: isFinalPlan=false


-- !query
EXPLAIN FORMATTED
  SELECT * 
  FROM   explain_temp1 
  WHERE  key = (SELECT max(key) 
                FROM   explain_temp2 
                WHERE  key = (SELECT max(key) 
                              FROM   explain_temp3 
                              WHERE  val > 0) 
                       AND val = 2) 
         AND val > 3
-- !query schema
struct<plan:string>
-- !query output
== Physical Plan ==
AdaptiveSparkPlan (3)
+- Filter (2)
   +- Scan parquet spark_catalog.default.explain_temp1 (1)


(1) Scan parquet spark_catalog.default.explain_temp1
Output [2]: [key#x, val#x]
Batched: true
Location [not included in comparison]/{warehouse_dir}/explain_temp1]
PushedFilters: [IsNotNull(key), IsNotNull(val), EqualTo(key,ScalarSubquery#x), GreaterThan(val,3)]
ReadSchema: struct<key:int,val:int>

(2) Filter
Input [2]: [key#x, val#x]
Condition : (((isnotnull(key#x) AND isnotnull(val#x)) AND (key#x = Subquery subquery#x, [id=#x])) AND (val#x > 3))

(3) AdaptiveSparkPlan
Output [2]: [key#x, val#x]
Arguments: isFinalPlan=false

===== Subqueries =====

Subquery:1 Hosting operator id = 2 Hosting Expression = Subquery subquery#x, [id=#x]
AdaptiveSparkPlan (10)
+- HashAggregate (9)
   +- Exchange (8)
      +- HashAggregate (7)
         +- Project (6)
            +- Filter (5)
               +- Scan parquet spark_catalog.default.explain_temp2 (4)


(4) Scan parquet spark_catalog.default.explain_temp2
Output [2]: [key#x, val#x]
Batched: true
Location [not included in comparison]/{warehouse_dir}/explain_temp2]
PushedFilters: [IsNotNull(key), IsNotNull(val), EqualTo(key,ScalarSubquery#x), EqualTo(val,2)]
ReadSchema: struct<key:int,val:int>

(5) Filter
Input [2]: [key#x, val#x]
Condition : (((isnotnull(key#x) AND isnotnull(val#x)) AND (key#x = Subquery subquery#x, [id=#x])) AND (val#x = 2))

(6) Project
Output [1]: [key#x]
Input [2]: [key#x, val#x]

(7) HashAggregate
Input [1]: [key#x]
Keys: []
Functions [1]: [partial_max(key#x)]
Aggregate Attributes [1]: [max#x]
Results [1]: [max#x]

(8) Exchange
Input [1]: [max#x]
Arguments: SinglePartition, ENSURE_REQUIREMENTS, [plan_id=x]

(9) HashAggregate
Input [1]: [max#x]
Keys: []
Functions [1]: [max(key#x)]
Aggregate Attributes [1]: [max(key#x)#x]
Results [1]: [max(key#x)#x AS max(key)#x]

(10) AdaptiveSparkPlan
Output [1]: [max(key)#x]
Arguments: isFinalPlan=false

Subquery:2 Hosting operator id = 5 Hosting Expression = Subquery subquery#x, [id=#x]
AdaptiveSparkPlan (17)
+- HashAggregate (16)
   +- Exchange (15)
      +- HashAggregate (14)
         +- Project (13)
            +- Filter (12)
               +- Scan parquet spark_catalog.default.explain_temp3 (11)


(11) Scan parquet spark_catalog.default.explain_temp3
Output [2]: [key#x, val#x]
Batched: true
Location [not included in comparison]/{warehouse_dir}/explain_temp3]
PushedFilters: [IsNotNull(val), GreaterThan(val,0)]
ReadSchema: struct<key:int,val:int>

(12) Filter
Input [2]: [key#x, val#x]
Condition : (isnotnull(val#x) AND (val#x > 0))

(13) Project
Output [1]: [key#x]
Input [2]: [key#x, val#x]

(14) HashAggregate
Input [1]: [key#x]
Keys: []
Functions [1]: [partial_max(key#x)]
Aggregate Attributes [1]: [max#x]
Results [1]: [max#x]

(15) Exchange
Input [1]: [max#x]
Arguments: SinglePartition, ENSURE_REQUIREMENTS, [plan_id=x]

(16) HashAggregate
Input [1]: [max#x]
Keys: []
Functions [1]: [max(key#x)]
Aggregate Attributes [1]: [max(key#x)#x]
Results [1]: [max(key#x)#x AS max(key)#x]

(17) AdaptiveSparkPlan
Output [1]: [max(key)#x]
Arguments: isFinalPlan=false

Subquery:3 Hosting operator id = 4 Hosting Expression = Subquery subquery#x, [id=#x]
AdaptiveSparkPlan (17)
+- HashAggregate (16)
   +- Exchange (15)
      +- HashAggregate (14)
         +- Project (13)
            +- Filter (12)
               +- Scan parquet spark_catalog.default.explain_temp3 (11)


Subquery:4 Hosting operator id = 1 Hosting Expression = Subquery subquery#x, [id=#x]
AdaptiveSparkPlan (10)
+- HashAggregate (9)
   +- Exchange (8)
      +- HashAggregate (7)
         +- Project (6)
            +- Filter (5)
               +- Scan parquet spark_catalog.default.explain_temp2 (4)


Subquery:5 Hosting operator id = 5 Hosting Expression = Subquery subquery#x, [id=#x]
AdaptiveSparkPlan (17)
+- HashAggregate (16)
   +- Exchange (15)
      +- HashAggregate (14)
         +- Project (13)
            +- Filter (12)
               +- Scan parquet spark_catalog.default.explain_temp3 (11)


Subquery:6 Hosting operator id = 4 Hosting Expression = Subquery subquery#x, [id=#x]
AdaptiveSparkPlan (17)
+- HashAggregate (16)
   +- Exchange (15)
      +- HashAggregate (14)
         +- Project (13)
            +- Filter (12)
               +- Scan parquet spark_catalog.default.explain_temp3 (11)


-- !query
EXPLAIN FORMATTED
  SELECT * 
  FROM   explain_temp1 
  WHERE  key = (SELECT max(key) 
                FROM   explain_temp2 
                WHERE  val > 0) 
         OR
         key = (SELECT avg(key)
                FROM   explain_temp3
                WHERE  val > 0)
-- !query schema
struct<plan:string>
-- !query output
== Physical Plan ==
AdaptiveSparkPlan (3)
+- Filter (2)
   +- Scan parquet spark_catalog.default.explain_temp1 (1)


(1) Scan parquet spark_catalog.default.explain_temp1
Output [2]: [key#x, val#x]
Batched: true
Location [not included in comparison]/{warehouse_dir}/explain_temp1]
ReadSchema: struct<key:int,val:int>

(2) Filter
Input [2]: [key#x, val#x]
Condition : ((key#x = Subquery subquery#x, [id=#x]) OR (cast(key#x as double) = Subquery subquery#x, [id=#x]))

(3) AdaptiveSparkPlan
Output [2]: [key#x, val#x]
Arguments: isFinalPlan=false

===== Subqueries =====

Subquery:1 Hosting operator id = 2 Hosting Expression = Subquery subquery#x, [id=#x]
AdaptiveSparkPlan (10)
+- HashAggregate (9)
   +- Exchange (8)
      +- HashAggregate (7)
         +- Project (6)
            +- Filter (5)
               +- Scan parquet spark_catalog.default.explain_temp2 (4)


(4) Scan parquet spark_catalog.default.explain_temp2
Output [2]: [key#x, val#x]
Batched: true
Location [not included in comparison]/{warehouse_dir}/explain_temp2]
PushedFilters: [IsNotNull(val), GreaterThan(val,0)]
ReadSchema: struct<key:int,val:int>

(5) Filter
Input [2]: [key#x, val#x]
Condition : (isnotnull(val#x) AND (val#x > 0))

(6) Project
Output [1]: [key#x]
Input [2]: [key#x, val#x]

(7) HashAggregate
Input [1]: [key#x]
Keys: []
Functions [1]: [partial_max(key#x)]
Aggregate Attributes [1]: [max#x]
Results [1]: [max#x]

(8) Exchange
Input [1]: [max#x]
Arguments: SinglePartition, ENSURE_REQUIREMENTS, [plan_id=x]

(9) HashAggregate
Input [1]: [max#x]
Keys: []
Functions [1]: [max(key#x)]
Aggregate Attributes [1]: [max(key#x)#x]
Results [1]: [max(key#x)#x AS max(key)#x]

(10) AdaptiveSparkPlan
Output [1]: [max(key)#x]
Arguments: isFinalPlan=false

Subquery:2 Hosting operator id = 2 Hosting Expression = Subquery subquery#x, [id=#x]
AdaptiveSparkPlan (17)
+- HashAggregate (16)
   +- Exchange (15)
      +- HashAggregate (14)
         +- Project (13)
            +- Filter (12)
               +- Scan parquet spark_catalog.default.explain_temp3 (11)


(11) Scan parquet spark_catalog.default.explain_temp3
Output [2]: [key#x, val#x]
Batched: true
Location [not included in comparison]/{warehouse_dir}/explain_temp3]
PushedFilters: [IsNotNull(val), GreaterThan(val,0)]
ReadSchema: struct<key:int,val:int>

(12) Filter
Input [2]: [key#x, val#x]
Condition : (isnotnull(val#x) AND (val#x > 0))

(13) Project
Output [1]: [key#x]
Input [2]: [key#x, val#x]

(14) HashAggregate
Input [1]: [key#x]
Keys: []
Functions [1]: [partial_avg(key#x)]
Aggregate Attributes [2]: [sum#x, count#xL]
Results [2]: [sum#x, count#xL]

(15) Exchange
Input [2]: [sum#x, count#xL]
Arguments: SinglePartition, ENSURE_REQUIREMENTS, [plan_id=x]

(16) HashAggregate
Input [2]: [sum#x, count#xL]
Keys: []
Functions [1]: [avg(key#x)]
Aggregate Attributes [1]: [avg(key#x)#x]
Results [1]: [avg(key#x)#x AS avg(key)#x]

(17) AdaptiveSparkPlan
Output [1]: [avg(key)#x]
Arguments: isFinalPlan=false

Subquery:3 Hosting operator id = 1 Hosting Expression = Subquery subquery#x, [id=#x]
AdaptiveSparkPlan (10)
+- HashAggregate (9)
   +- Exchange (8)
      +- HashAggregate (7)
         +- Project (6)
            +- Filter (5)
               +- Scan parquet spark_catalog.default.explain_temp2 (4)


Subquery:4 Hosting operator id = 1 Hosting Expression = Subquery subquery#x, [id=#x]
AdaptiveSparkPlan (17)
+- HashAggregate (16)
   +- Exchange (15)
      +- HashAggregate (14)
         +- Project (13)
            +- Filter (12)
               +- Scan parquet spark_catalog.default.explain_temp3 (11)


-- !query
EXPLAIN FORMATTED
  SELECT (SELECT Avg(key) FROM explain_temp1) + (SELECT Avg(key) FROM explain_temp1)
  FROM explain_temp1
-- !query schema
struct<plan:string>
-- !query output
== Physical Plan ==
AdaptiveSparkPlan (3)
+- Project (2)
   +- Scan parquet spark_catalog.default.explain_temp1 (1)


(1) Scan parquet spark_catalog.default.explain_temp1
Output: []
Batched: true
Location [not included in comparison]/{warehouse_dir}/explain_temp1]
ReadSchema: struct<>

(2) Project
Output [1]: [(Subquery subquery#x, [id=#x] + Subquery subquery#x, [id=#x]) AS (scalarsubquery() + scalarsubquery())#x]
Input: []

(3) AdaptiveSparkPlan
Output [1]: [(scalarsubquery() + scalarsubquery())#x]
Arguments: isFinalPlan=false

===== Subqueries =====

Subquery:1 Hosting operator id = 2 Hosting Expression = Subquery subquery#x, [id=#x]
AdaptiveSparkPlan (8)
+- HashAggregate (7)
   +- Exchange (6)
      +- HashAggregate (5)
         +- Scan parquet spark_catalog.default.explain_temp1 (4)


(4) Scan parquet spark_catalog.default.explain_temp1
Output [1]: [key#x]
Batched: true
Location [not included in comparison]/{warehouse_dir}/explain_temp1]
ReadSchema: struct<key:int>

(5) HashAggregate
Input [1]: [key#x]
Keys: []
Functions [1]: [partial_avg(key#x)]
Aggregate Attributes [2]: [sum#x, count#xL]
Results [2]: [sum#x, count#xL]

(6) Exchange
Input [2]: [sum#x, count#xL]
Arguments: SinglePartition, ENSURE_REQUIREMENTS, [plan_id=x]

(7) HashAggregate
Input [2]: [sum#x, count#xL]
Keys: []
Functions [1]: [avg(key#x)]
Aggregate Attributes [1]: [avg(key#x)#x]
Results [1]: [avg(key#x)#x AS avg(key)#x]

(8) AdaptiveSparkPlan
Output [1]: [avg(key)#x]
Arguments: isFinalPlan=false

Subquery:2 Hosting operator id = 2 Hosting Expression = Subquery subquery#x, [id=#x]
AdaptiveSparkPlan (13)
+- HashAggregate (12)
   +- Exchange (11)
      +- HashAggregate (10)
         +- Scan parquet spark_catalog.default.explain_temp1 (9)


(9) Scan parquet spark_catalog.default.explain_temp1
Output [1]: [key#x]
Batched: true
Location [not included in comparison]/{warehouse_dir}/explain_temp1]
ReadSchema: struct<key:int>

(10) HashAggregate
Input [1]: [key#x]
Keys: []
Functions [1]: [partial_avg(key#x)]
Aggregate Attributes [2]: [sum#x, count#xL]
Results [2]: [sum#x, count#xL]

(11) Exchange
Input [2]: [sum#x, count#xL]
Arguments: SinglePartition, ENSURE_REQUIREMENTS, [plan_id=x]

(12) HashAggregate
Input [2]: [sum#x, count#xL]
Keys: []
Functions [1]: [avg(key#x)]
Aggregate Attributes [1]: [avg(key#x)#x]
Results [1]: [avg(key#x)#x AS avg(key)#x]

(13) AdaptiveSparkPlan
Output [1]: [avg(key)#x]
Arguments: isFinalPlan=false


-- !query
EXPLAIN FORMATTED
  WITH cte1 AS (
    SELECT *
    FROM explain_temp1 
    WHERE key > 10
  )
  SELECT * FROM cte1 a, cte1 b WHERE a.key = b.key
-- !query schema
struct<plan:string>
-- !query output
== Physical Plan ==
AdaptiveSparkPlan (7)
+- BroadcastHashJoin Inner BuildRight (6)
   :- Filter (2)
   :  +- Scan parquet spark_catalog.default.explain_temp1 (1)
   +- BroadcastExchange (5)
      +- Filter (4)
         +- Scan parquet spark_catalog.default.explain_temp1 (3)


(1) Scan parquet spark_catalog.default.explain_temp1
Output [2]: [key#x, val#x]
Batched: true
Location [not included in comparison]/{warehouse_dir}/explain_temp1]
PushedFilters: [IsNotNull(key), GreaterThan(key,10)]
ReadSchema: struct<key:int,val:int>

(2) Filter
Input [2]: [key#x, val#x]
Condition : (isnotnull(key#x) AND (key#x > 10))

(3) Scan parquet spark_catalog.default.explain_temp1
Output [2]: [key#x, val#x]
Batched: true
Location [not included in comparison]/{warehouse_dir}/explain_temp1]
PushedFilters: [IsNotNull(key), GreaterThan(key,10)]
ReadSchema: struct<key:int,val:int>

(4) Filter
Input [2]: [key#x, val#x]
Condition : (isnotnull(key#x) AND (key#x > 10))

(5) BroadcastExchange
Input [2]: [key#x, val#x]
Arguments: HashedRelationBroadcastMode(List(cast(input[0, int, false] as bigint)),false), [plan_id=x]

(6) BroadcastHashJoin
Left keys [1]: [key#x]
Right keys [1]: [key#x]
Join type: Inner
Join condition: None

(7) AdaptiveSparkPlan
Output [4]: [key#x, val#x, key#x, val#x]
Arguments: isFinalPlan=false


-- !query
EXPLAIN FORMATTED
  WITH cte1 AS (
    SELECT key, max(val)
    FROM explain_temp1 
    WHERE key > 10
    GROUP BY key
  )
  SELECT * FROM cte1 a, cte1 b WHERE a.key = b.key
-- !query schema
struct<plan:string>
-- !query output
== Physical Plan ==
AdaptiveSparkPlan (13)
+- BroadcastHashJoin Inner BuildRight (12)
   :- HashAggregate (5)
   :  +- Exchange (4)
   :     +- HashAggregate (3)
   :        +- Filter (2)
   :           +- Scan parquet spark_catalog.default.explain_temp1 (1)
   +- BroadcastExchange (11)
      +- HashAggregate (10)
         +- Exchange (9)
            +- HashAggregate (8)
               +- Filter (7)
                  +- Scan parquet spark_catalog.default.explain_temp1 (6)


(1) Scan parquet spark_catalog.default.explain_temp1
Output [2]: [key#x, val#x]
Batched: true
Location [not included in comparison]/{warehouse_dir}/explain_temp1]
PushedFilters: [IsNotNull(key), GreaterThan(key,10)]
ReadSchema: struct<key:int,val:int>

(2) Filter
Input [2]: [key#x, val#x]
Condition : (isnotnull(key#x) AND (key#x > 10))

(3) HashAggregate
Input [2]: [key#x, val#x]
Keys [1]: [key#x]
Functions [1]: [partial_max(val#x)]
Aggregate Attributes [1]: [max#x]
Results [2]: [key#x, max#x]

(4) Exchange
Input [2]: [key#x, max#x]
Arguments: hashpartitioning(key#x, 4), ENSURE_REQUIREMENTS, [plan_id=x]

(5) HashAggregate
Input [2]: [key#x, max#x]
Keys [1]: [key#x]
Functions [1]: [max(val#x)]
Aggregate Attributes [1]: [max(val#x)#x]
Results [2]: [key#x, max(val#x)#x AS max(val)#x]

(6) Scan parquet spark_catalog.default.explain_temp1
Output [2]: [key#x, val#x]
Batched: true
Location [not included in comparison]/{warehouse_dir}/explain_temp1]
PushedFilters: [IsNotNull(key), GreaterThan(key,10)]
ReadSchema: struct<key:int,val:int>

(7) Filter
Input [2]: [key#x, val#x]
Condition : (isnotnull(key#x) AND (key#x > 10))

(8) HashAggregate
Input [2]: [key#x, val#x]
Keys [1]: [key#x]
Functions [1]: [partial_max(val#x)]
Aggregate Attributes [1]: [max#x]
Results [2]: [key#x, max#x]

(9) Exchange
Input [2]: [key#x, max#x]
Arguments: hashpartitioning(key#x, 4), ENSURE_REQUIREMENTS, [plan_id=x]

(10) HashAggregate
Input [2]: [key#x, max#x]
Keys [1]: [key#x]
Functions [1]: [max(val#x)]
Aggregate Attributes [1]: [max(val#x)#x]
Results [2]: [key#x, max(val#x)#x AS max(val)#x]

(11) BroadcastExchange
Input [2]: [key#x, max(val)#x]
Arguments: HashedRelationBroadcastMode(List(cast(input[0, int, true] as bigint)),false), [plan_id=x]

(12) BroadcastHashJoin
Left keys [1]: [key#x]
Right keys [1]: [key#x]
Join type: Inner
Join condition: None

(13) AdaptiveSparkPlan
Output [4]: [key#x, max(val)#x, key#x, max(val)#x]
Arguments: isFinalPlan=false


-- !query
EXPLAIN FORMATTED
  CREATE VIEW explain_view AS
    SELECT key, val FROM explain_temp1
-- !query schema
struct<plan:string>
-- !query output
== Physical Plan ==
Execute CreateViewCommand (1)
   +- CreateViewCommand (2)
         +- Project (5)
            +- SubqueryAlias (4)
               +- LogicalRelation (3)


(1) Execute CreateViewCommand
Output: []

(2) CreateViewCommand
Arguments: `spark_catalog`.`default`.`explain_view`, SELECT key, val FROM explain_temp1, false, false, PersistedView, COMPENSATION, true

(3) LogicalRelation
Arguments: parquet, [key#x, val#x], `spark_catalog`.`default`.`explain_temp1`, false

(4) SubqueryAlias
Arguments: spark_catalog.default.explain_temp1

(5) Project
Arguments: [key#x, val#x]


-- !query
EXPLAIN FORMATTED
  SELECT
    COUNT(val) + SUM(key) as TOTAL,
    COUNT(key) FILTER (WHERE val > 1)
  FROM explain_temp1
-- !query schema
struct<plan:string>
-- !query output
== Physical Plan ==
AdaptiveSparkPlan (5)
+- HashAggregate (4)
   +- Exchange (3)
      +- HashAggregate (2)
         +- Scan parquet spark_catalog.default.explain_temp1 (1)


(1) Scan parquet spark_catalog.default.explain_temp1
Output [2]: [key#x, val#x]
Batched: true
Location [not included in comparison]/{warehouse_dir}/explain_temp1]
ReadSchema: struct<key:int,val:int>

(2) HashAggregate
Input [2]: [key#x, val#x]
Keys: []
Functions [3]: [partial_count(val#x), partial_sum(key#x), partial_count(key#x) FILTER (WHERE (val#x > 1))]
Aggregate Attributes [3]: [count#xL, sum#xL, count#xL]
Results [3]: [count#xL, sum#xL, count#xL]

(3) Exchange
Input [3]: [count#xL, sum#xL, count#xL]
Arguments: SinglePartition, ENSURE_REQUIREMENTS, [plan_id=x]

(4) HashAggregate
Input [3]: [count#xL, sum#xL, count#xL]
Keys: []
Functions [3]: [count(val#x), sum(key#x), count(key#x)]
Aggregate Attributes [3]: [count(val#x)#xL, sum(key#x)#xL, count(key#x)#xL]
Results [2]: [(count(val#x)#xL + sum(key#x)#xL) AS TOTAL#xL, count(key#x)#xL AS count(key) FILTER (WHERE (val > 1))#xL]

(5) AdaptiveSparkPlan
Output [2]: [TOTAL#xL, count(key) FILTER (WHERE (val > 1))#xL]
Arguments: isFinalPlan=false


-- !query
EXPLAIN FORMATTED
  SELECT key, sort_array(collect_set(val))[0]
  FROM explain_temp4
  GROUP BY key
-- !query schema
struct<plan:string>
-- !query output
== Physical Plan ==
AdaptiveSparkPlan (5)
+- ObjectHashAggregate (4)
   +- Exchange (3)
      +- ObjectHashAggregate (2)
         +- Scan parquet spark_catalog.default.explain_temp4 (1)


(1) Scan parquet spark_catalog.default.explain_temp4
Output [2]: [key#x, val#x]
Batched: true
Location [not included in comparison]/{warehouse_dir}/explain_temp4]
ReadSchema: struct<key:int,val:string>

(2) ObjectHashAggregate
Input [2]: [key#x, val#x]
Keys [1]: [key#x]
Functions [1]: [partial_collect_set(val#x, 0, 0)]
Aggregate Attributes [1]: [buf#x]
Results [2]: [key#x, buf#x]

(3) Exchange
Input [2]: [key#x, buf#x]
Arguments: hashpartitioning(key#x, 4), ENSURE_REQUIREMENTS, [plan_id=x]

(4) ObjectHashAggregate
Input [2]: [key#x, buf#x]
Keys [1]: [key#x]
Functions [1]: [collect_set(val#x, 0, 0)]
Aggregate Attributes [1]: [collect_set(val#x, 0, 0)#x]
Results [2]: [key#x, sort_array(collect_set(val#x, 0, 0)#x, true)[0] AS sort_array(collect_set(val), true)[0]#x]

(5) AdaptiveSparkPlan
Output [2]: [key#x, sort_array(collect_set(val), true)[0]#x]
Arguments: isFinalPlan=false


-- !query
EXPLAIN FORMATTED
  SELECT key, MIN(val)
  FROM explain_temp4
  GROUP BY key
-- !query schema
struct<plan:string>
-- !query output
== Physical Plan ==
AdaptiveSparkPlan (7)
+- SortAggregate (6)
   +- Sort (5)
      +- Exchange (4)
         +- SortAggregate (3)
            +- Sort (2)
               +- Scan parquet spark_catalog.default.explain_temp4 (1)


(1) Scan parquet spark_catalog.default.explain_temp4
Output [2]: [key#x, val#x]
Batched: true
Location [not included in comparison]/{warehouse_dir}/explain_temp4]
ReadSchema: struct<key:int,val:string>

(2) Sort
Input [2]: [key#x, val#x]
Arguments: [key#x ASC NULLS FIRST], false, 0

(3) SortAggregate
Input [2]: [key#x, val#x]
Keys [1]: [key#x]
Functions [1]: [partial_min(val#x)]
Aggregate Attributes [1]: [min#x]
Results [2]: [key#x, min#x]

(4) Exchange
Input [2]: [key#x, min#x]
Arguments: hashpartitioning(key#x, 4), ENSURE_REQUIREMENTS, [plan_id=x]

(5) Sort
Input [2]: [key#x, min#x]
Arguments: [key#x ASC NULLS FIRST], false, 0

(6) SortAggregate
Input [2]: [key#x, min#x]
Keys [1]: [key#x]
Functions [1]: [min(val#x)]
Aggregate Attributes [1]: [min(val#x)#x]
Results [2]: [key#x, min(val#x)#x AS min(val)#x]

(7) AdaptiveSparkPlan
Output [2]: [key#x, min(val)#x]
Arguments: isFinalPlan=false


-- !query
EXPLAIN EXTENDED INSERT INTO TABLE explain_temp5 SELECT * FROM explain_temp4
-- !query schema
struct<plan:string>
-- !query output
== Parsed Logical Plan ==
'InsertIntoStatement 'UnresolvedRelation [explain_temp5], [__required_write_privileges__=INSERT], false, false, false, false
+- 'Project [*]
   +- 'UnresolvedRelation [explain_temp4], [], false

== Analyzed Logical Plan ==
InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/explain_temp5, false, [val#x], Parquet, [path=file:[not included in comparison]/{warehouse_dir}/explain_temp5], Append, `spark_catalog`.`default`.`explain_temp5`, org.apache.spark.sql.execution.datasources.CatalogFileIndex(file:[not included in comparison]/{warehouse_dir}/explain_temp5), [key, val]
+- Project [key#x, val#x]
   +- SubqueryAlias spark_catalog.default.explain_temp4
      +- Relation spark_catalog.default.explain_temp4[key#x,val#x] parquet

== Optimized Logical Plan ==
InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/explain_temp5, false, [val#x], Parquet, [path=file:[not included in comparison]/{warehouse_dir}/explain_temp5], Append, `spark_catalog`.`default`.`explain_temp5`, org.apache.spark.sql.execution.datasources.CatalogFileIndex(file:[not included in comparison]/{warehouse_dir}/explain_temp5), [key, val]
+- WriteFiles
   +- Sort [val#x ASC NULLS FIRST], false
      +- Project [key#x, empty2null(val#x) AS val#x]
         +- Relation spark_catalog.default.explain_temp4[key#x,val#x] parquet

== Physical Plan ==
Execute InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/explain_temp5, false, [val#x], Parquet, [path=file:[not included in comparison]/{warehouse_dir}/explain_temp5], Append, `spark_catalog`.`default`.`explain_temp5`, org.apache.spark.sql.execution.datasources.CatalogFileIndex(file:[not included in comparison]/{warehouse_dir}/explain_temp5), [key, val]
+- WriteFiles
   +- *Sort [val#x ASC NULLS FIRST], false, 0
      +- *Project [key#x, empty2null(val#x) AS val#x]
         +- *ColumnarToRow
            +- FileScan parquet spark_catalog.default.explain_temp4[key#x,val#x] Batched: true, DataFilters: [], Format: Parquet, Location [not included in comparison]/{warehouse_dir}/explain_temp4], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<key:int,val:string>


-- !query
DROP TABLE explain_temp1
-- !query schema
struct<>
-- !query output



-- !query
DROP TABLE explain_temp2
-- !query schema
struct<>
-- !query output



-- !query
DROP TABLE explain_temp3
-- !query schema
struct<>
-- !query output



-- !query
DROP TABLE explain_temp4
-- !query schema
struct<>
-- !query output



-- !query
DROP TABLE explain_temp5
-- !query schema
struct<>
-- !query output



-- !query
CREATE table  t(v array<string>) USING PARQUET
-- !query schema
struct<>
-- !query output



-- !query
EXPLAIN SELECT * FROM t  WHERE v IN (array('a'), null)
-- !query schema
struct<plan:string>
-- !query output
== Physical Plan ==
*Filter v#x IN ([a],null)
+- *ColumnarToRow
   +- FileScan parquet spark_catalog.default.t[v#x] Batched: true, DataFilters: [v#x IN ([a],null)], Format: Parquet, Location [not included in comparison]/{warehouse_dir}/t], PartitionFilters: [], PushedFilters: [In(v, [[a],null])], ReadSchema: struct<v:array<string>>


-- !query
DROP TABLE t
-- !query schema
struct<>
-- !query output

