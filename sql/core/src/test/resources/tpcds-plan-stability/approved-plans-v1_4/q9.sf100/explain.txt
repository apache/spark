== Physical Plan ==
* Project (4)
+- * Filter (3)
   +- * ColumnarToRow (2)
      +- Scan parquet spark_catalog.default.reason (1)


(1) Scan parquet spark_catalog.default.reason
Output [1]: [r_reason_sk#1]
Batched: true
Location [not included in comparison]/{warehouse_dir}/reason]
PushedFilters: [IsNotNull(r_reason_sk), EqualTo(r_reason_sk,1)]
ReadSchema: struct<r_reason_sk:int>

(2) ColumnarToRow [codegen id : 1]
Input [1]: [r_reason_sk#1]

(3) Filter [codegen id : 1]
Input [1]: [r_reason_sk#1]
Condition : (isnotnull(r_reason_sk#1) AND (r_reason_sk#1 = 1))

(4) Project [codegen id : 1]
Output [5]: [CASE WHEN (Subquery scalar-subquery#2, [id=#3].count(1) > 62316685) THEN ReusedSubquery Subquery scalar-subquery#2, [id=#3].avg(ss_ext_discount_amt) ELSE ReusedSubquery Subquery scalar-subquery#2, [id=#3].avg(ss_net_paid) END AS bucket1#4, CASE WHEN (Subquery scalar-subquery#5, [id=#6].count(1) > 19045798) THEN ReusedSubquery Subquery scalar-subquery#5, [id=#6].avg(ss_ext_discount_amt) ELSE ReusedSubquery Subquery scalar-subquery#5, [id=#6].avg(ss_net_paid) END AS bucket2#7, CASE WHEN (Subquery scalar-subquery#8, [id=#9].count(1) > 365541424) THEN ReusedSubquery Subquery scalar-subquery#8, [id=#9].avg(ss_ext_discount_amt) ELSE ReusedSubquery Subquery scalar-subquery#8, [id=#9].avg(ss_net_paid) END AS bucket3#10, CASE WHEN (Subquery scalar-subquery#11, [id=#12].count(1) > 216357808) THEN ReusedSubquery Subquery scalar-subquery#11, [id=#12].avg(ss_ext_discount_amt) ELSE ReusedSubquery Subquery scalar-subquery#11, [id=#12].avg(ss_net_paid) END AS bucket4#13, CASE WHEN (Subquery scalar-subquery#14, [id=#15].count(1) > 184483884) THEN ReusedSubquery Subquery scalar-subquery#14, [id=#15].avg(ss_ext_discount_amt) ELSE ReusedSubquery Subquery scalar-subquery#14, [id=#15].avg(ss_net_paid) END AS bucket5#16]
Input [1]: [r_reason_sk#1]

===== Subqueries =====

Subquery:1 Hosting operator id = 4 Hosting Expression = Subquery scalar-subquery#2, [id=#3]
* HashAggregate (11)
+- Exchange (10)
   +- * HashAggregate (9)
      +- * Project (8)
         +- * Filter (7)
            +- * ColumnarToRow (6)
               +- Scan parquet spark_catalog.default.store_sales (5)


(5) Scan parquet spark_catalog.default.store_sales
Output [4]: [ss_quantity#17, ss_ext_discount_amt#18, ss_net_paid#19, ss_sold_date_sk#20]
Batched: true
Location [not included in comparison]/{warehouse_dir}/store_sales]
PushedFilters: [IsNotNull(ss_quantity), GreaterThanOrEqual(ss_quantity,1), LessThanOrEqual(ss_quantity,20)]
ReadSchema: struct<ss_quantity:int,ss_ext_discount_amt:decimal(7,2),ss_net_paid:decimal(7,2)>

(6) ColumnarToRow [codegen id : 1]
Input [4]: [ss_quantity#17, ss_ext_discount_amt#18, ss_net_paid#19, ss_sold_date_sk#20]

(7) Filter [codegen id : 1]
Input [4]: [ss_quantity#17, ss_ext_discount_amt#18, ss_net_paid#19, ss_sold_date_sk#20]
Condition : ((isnotnull(ss_quantity#17) AND (ss_quantity#17 >= 1)) AND (ss_quantity#17 <= 20))

(8) Project [codegen id : 1]
Output [2]: [ss_ext_discount_amt#18, ss_net_paid#19]
Input [4]: [ss_quantity#17, ss_ext_discount_amt#18, ss_net_paid#19, ss_sold_date_sk#20]

(9) HashAggregate [codegen id : 1]
Input [2]: [ss_ext_discount_amt#18, ss_net_paid#19]
Keys: []
Functions [3]: [partial_count(1), partial_avg(UnscaledValue(ss_ext_discount_amt#18)), partial_avg(UnscaledValue(ss_net_paid#19))]
Aggregate Attributes [5]: [count#21, sum#22, count#23, sum#24, count#25]
Results [5]: [count#26, sum#27, count#28, sum#29, count#30]

(10) Exchange
Input [5]: [count#26, sum#27, count#28, sum#29, count#30]
Arguments: SinglePartition, ENSURE_REQUIREMENTS, [plan_id=1]

(11) HashAggregate [codegen id : 2]
Input [5]: [count#26, sum#27, count#28, sum#29, count#30]
Keys: []
Functions [3]: [count(1), avg(UnscaledValue(ss_ext_discount_amt#18)), avg(UnscaledValue(ss_net_paid#19))]
Aggregate Attributes [3]: [count(1)#31, avg(UnscaledValue(ss_ext_discount_amt#18))#32, avg(UnscaledValue(ss_net_paid#19))#33]
Results [1]: [named_struct(count(1), count(1)#31, avg(ss_ext_discount_amt), cast((avg(UnscaledValue(ss_ext_discount_amt#18))#32 / 100.0) as decimal(11,6)), avg(ss_net_paid), cast((avg(UnscaledValue(ss_net_paid#19))#33 / 100.0) as decimal(11,6))) AS mergedValue#34]

Subquery:2 Hosting operator id = 4 Hosting Expression = ReusedSubquery Subquery scalar-subquery#2, [id=#3]

Subquery:3 Hosting operator id = 4 Hosting Expression = ReusedSubquery Subquery scalar-subquery#2, [id=#3]

Subquery:4 Hosting operator id = 4 Hosting Expression = Subquery scalar-subquery#5, [id=#6]
* HashAggregate (18)
+- Exchange (17)
   +- * HashAggregate (16)
      +- * Project (15)
         +- * Filter (14)
            +- * ColumnarToRow (13)
               +- Scan parquet spark_catalog.default.store_sales (12)


(12) Scan parquet spark_catalog.default.store_sales
Output [4]: [ss_quantity#35, ss_ext_discount_amt#36, ss_net_paid#37, ss_sold_date_sk#38]
Batched: true
Location [not included in comparison]/{warehouse_dir}/store_sales]
PushedFilters: [IsNotNull(ss_quantity), GreaterThanOrEqual(ss_quantity,21), LessThanOrEqual(ss_quantity,40)]
ReadSchema: struct<ss_quantity:int,ss_ext_discount_amt:decimal(7,2),ss_net_paid:decimal(7,2)>

(13) ColumnarToRow [codegen id : 1]
Input [4]: [ss_quantity#35, ss_ext_discount_amt#36, ss_net_paid#37, ss_sold_date_sk#38]

(14) Filter [codegen id : 1]
Input [4]: [ss_quantity#35, ss_ext_discount_amt#36, ss_net_paid#37, ss_sold_date_sk#38]
Condition : ((isnotnull(ss_quantity#35) AND (ss_quantity#35 >= 21)) AND (ss_quantity#35 <= 40))

(15) Project [codegen id : 1]
Output [2]: [ss_ext_discount_amt#36, ss_net_paid#37]
Input [4]: [ss_quantity#35, ss_ext_discount_amt#36, ss_net_paid#37, ss_sold_date_sk#38]

(16) HashAggregate [codegen id : 1]
Input [2]: [ss_ext_discount_amt#36, ss_net_paid#37]
Keys: []
Functions [3]: [partial_count(1), partial_avg(UnscaledValue(ss_ext_discount_amt#36)), partial_avg(UnscaledValue(ss_net_paid#37))]
Aggregate Attributes [5]: [count#39, sum#40, count#41, sum#42, count#43]
Results [5]: [count#44, sum#45, count#46, sum#47, count#48]

(17) Exchange
Input [5]: [count#44, sum#45, count#46, sum#47, count#48]
Arguments: SinglePartition, ENSURE_REQUIREMENTS, [plan_id=2]

(18) HashAggregate [codegen id : 2]
Input [5]: [count#44, sum#45, count#46, sum#47, count#48]
Keys: []
Functions [3]: [count(1), avg(UnscaledValue(ss_ext_discount_amt#36)), avg(UnscaledValue(ss_net_paid#37))]
Aggregate Attributes [3]: [count(1)#49, avg(UnscaledValue(ss_ext_discount_amt#36))#50, avg(UnscaledValue(ss_net_paid#37))#51]
Results [1]: [named_struct(count(1), count(1)#49, avg(ss_ext_discount_amt), cast((avg(UnscaledValue(ss_ext_discount_amt#36))#50 / 100.0) as decimal(11,6)), avg(ss_net_paid), cast((avg(UnscaledValue(ss_net_paid#37))#51 / 100.0) as decimal(11,6))) AS mergedValue#52]

Subquery:5 Hosting operator id = 4 Hosting Expression = ReusedSubquery Subquery scalar-subquery#5, [id=#6]

Subquery:6 Hosting operator id = 4 Hosting Expression = ReusedSubquery Subquery scalar-subquery#5, [id=#6]

Subquery:7 Hosting operator id = 4 Hosting Expression = Subquery scalar-subquery#8, [id=#9]
* HashAggregate (25)
+- Exchange (24)
   +- * HashAggregate (23)
      +- * Project (22)
         +- * Filter (21)
            +- * ColumnarToRow (20)
               +- Scan parquet spark_catalog.default.store_sales (19)


(19) Scan parquet spark_catalog.default.store_sales
Output [4]: [ss_quantity#53, ss_ext_discount_amt#54, ss_net_paid#55, ss_sold_date_sk#56]
Batched: true
Location [not included in comparison]/{warehouse_dir}/store_sales]
PushedFilters: [IsNotNull(ss_quantity), GreaterThanOrEqual(ss_quantity,41), LessThanOrEqual(ss_quantity,60)]
ReadSchema: struct<ss_quantity:int,ss_ext_discount_amt:decimal(7,2),ss_net_paid:decimal(7,2)>

(20) ColumnarToRow [codegen id : 1]
Input [4]: [ss_quantity#53, ss_ext_discount_amt#54, ss_net_paid#55, ss_sold_date_sk#56]

(21) Filter [codegen id : 1]
Input [4]: [ss_quantity#53, ss_ext_discount_amt#54, ss_net_paid#55, ss_sold_date_sk#56]
Condition : ((isnotnull(ss_quantity#53) AND (ss_quantity#53 >= 41)) AND (ss_quantity#53 <= 60))

(22) Project [codegen id : 1]
Output [2]: [ss_ext_discount_amt#54, ss_net_paid#55]
Input [4]: [ss_quantity#53, ss_ext_discount_amt#54, ss_net_paid#55, ss_sold_date_sk#56]

(23) HashAggregate [codegen id : 1]
Input [2]: [ss_ext_discount_amt#54, ss_net_paid#55]
Keys: []
Functions [3]: [partial_count(1), partial_avg(UnscaledValue(ss_ext_discount_amt#54)), partial_avg(UnscaledValue(ss_net_paid#55))]
Aggregate Attributes [5]: [count#57, sum#58, count#59, sum#60, count#61]
Results [5]: [count#62, sum#63, count#64, sum#65, count#66]

(24) Exchange
Input [5]: [count#62, sum#63, count#64, sum#65, count#66]
Arguments: SinglePartition, ENSURE_REQUIREMENTS, [plan_id=3]

(25) HashAggregate [codegen id : 2]
Input [5]: [count#62, sum#63, count#64, sum#65, count#66]
Keys: []
Functions [3]: [count(1), avg(UnscaledValue(ss_ext_discount_amt#54)), avg(UnscaledValue(ss_net_paid#55))]
Aggregate Attributes [3]: [count(1)#67, avg(UnscaledValue(ss_ext_discount_amt#54))#68, avg(UnscaledValue(ss_net_paid#55))#69]
Results [1]: [named_struct(count(1), count(1)#67, avg(ss_ext_discount_amt), cast((avg(UnscaledValue(ss_ext_discount_amt#54))#68 / 100.0) as decimal(11,6)), avg(ss_net_paid), cast((avg(UnscaledValue(ss_net_paid#55))#69 / 100.0) as decimal(11,6))) AS mergedValue#70]

Subquery:8 Hosting operator id = 4 Hosting Expression = ReusedSubquery Subquery scalar-subquery#8, [id=#9]

Subquery:9 Hosting operator id = 4 Hosting Expression = ReusedSubquery Subquery scalar-subquery#8, [id=#9]

Subquery:10 Hosting operator id = 4 Hosting Expression = Subquery scalar-subquery#11, [id=#12]
* HashAggregate (32)
+- Exchange (31)
   +- * HashAggregate (30)
      +- * Project (29)
         +- * Filter (28)
            +- * ColumnarToRow (27)
               +- Scan parquet spark_catalog.default.store_sales (26)


(26) Scan parquet spark_catalog.default.store_sales
Output [4]: [ss_quantity#71, ss_ext_discount_amt#72, ss_net_paid#73, ss_sold_date_sk#74]
Batched: true
Location [not included in comparison]/{warehouse_dir}/store_sales]
PushedFilters: [IsNotNull(ss_quantity), GreaterThanOrEqual(ss_quantity,61), LessThanOrEqual(ss_quantity,80)]
ReadSchema: struct<ss_quantity:int,ss_ext_discount_amt:decimal(7,2),ss_net_paid:decimal(7,2)>

(27) ColumnarToRow [codegen id : 1]
Input [4]: [ss_quantity#71, ss_ext_discount_amt#72, ss_net_paid#73, ss_sold_date_sk#74]

(28) Filter [codegen id : 1]
Input [4]: [ss_quantity#71, ss_ext_discount_amt#72, ss_net_paid#73, ss_sold_date_sk#74]
Condition : ((isnotnull(ss_quantity#71) AND (ss_quantity#71 >= 61)) AND (ss_quantity#71 <= 80))

(29) Project [codegen id : 1]
Output [2]: [ss_ext_discount_amt#72, ss_net_paid#73]
Input [4]: [ss_quantity#71, ss_ext_discount_amt#72, ss_net_paid#73, ss_sold_date_sk#74]

(30) HashAggregate [codegen id : 1]
Input [2]: [ss_ext_discount_amt#72, ss_net_paid#73]
Keys: []
Functions [3]: [partial_count(1), partial_avg(UnscaledValue(ss_ext_discount_amt#72)), partial_avg(UnscaledValue(ss_net_paid#73))]
Aggregate Attributes [5]: [count#75, sum#76, count#77, sum#78, count#79]
Results [5]: [count#80, sum#81, count#82, sum#83, count#84]

(31) Exchange
Input [5]: [count#80, sum#81, count#82, sum#83, count#84]
Arguments: SinglePartition, ENSURE_REQUIREMENTS, [plan_id=4]

(32) HashAggregate [codegen id : 2]
Input [5]: [count#80, sum#81, count#82, sum#83, count#84]
Keys: []
Functions [3]: [count(1), avg(UnscaledValue(ss_ext_discount_amt#72)), avg(UnscaledValue(ss_net_paid#73))]
Aggregate Attributes [3]: [count(1)#85, avg(UnscaledValue(ss_ext_discount_amt#72))#86, avg(UnscaledValue(ss_net_paid#73))#87]
Results [1]: [named_struct(count(1), count(1)#85, avg(ss_ext_discount_amt), cast((avg(UnscaledValue(ss_ext_discount_amt#72))#86 / 100.0) as decimal(11,6)), avg(ss_net_paid), cast((avg(UnscaledValue(ss_net_paid#73))#87 / 100.0) as decimal(11,6))) AS mergedValue#88]

Subquery:11 Hosting operator id = 4 Hosting Expression = ReusedSubquery Subquery scalar-subquery#11, [id=#12]

Subquery:12 Hosting operator id = 4 Hosting Expression = ReusedSubquery Subquery scalar-subquery#11, [id=#12]

Subquery:13 Hosting operator id = 4 Hosting Expression = Subquery scalar-subquery#14, [id=#15]
* HashAggregate (39)
+- Exchange (38)
   +- * HashAggregate (37)
      +- * Project (36)
         +- * Filter (35)
            +- * ColumnarToRow (34)
               +- Scan parquet spark_catalog.default.store_sales (33)


(33) Scan parquet spark_catalog.default.store_sales
Output [4]: [ss_quantity#89, ss_ext_discount_amt#90, ss_net_paid#91, ss_sold_date_sk#92]
Batched: true
Location [not included in comparison]/{warehouse_dir}/store_sales]
PushedFilters: [IsNotNull(ss_quantity), GreaterThanOrEqual(ss_quantity,81), LessThanOrEqual(ss_quantity,100)]
ReadSchema: struct<ss_quantity:int,ss_ext_discount_amt:decimal(7,2),ss_net_paid:decimal(7,2)>

(34) ColumnarToRow [codegen id : 1]
Input [4]: [ss_quantity#89, ss_ext_discount_amt#90, ss_net_paid#91, ss_sold_date_sk#92]

(35) Filter [codegen id : 1]
Input [4]: [ss_quantity#89, ss_ext_discount_amt#90, ss_net_paid#91, ss_sold_date_sk#92]
Condition : ((isnotnull(ss_quantity#89) AND (ss_quantity#89 >= 81)) AND (ss_quantity#89 <= 100))

(36) Project [codegen id : 1]
Output [2]: [ss_ext_discount_amt#90, ss_net_paid#91]
Input [4]: [ss_quantity#89, ss_ext_discount_amt#90, ss_net_paid#91, ss_sold_date_sk#92]

(37) HashAggregate [codegen id : 1]
Input [2]: [ss_ext_discount_amt#90, ss_net_paid#91]
Keys: []
Functions [3]: [partial_count(1), partial_avg(UnscaledValue(ss_ext_discount_amt#90)), partial_avg(UnscaledValue(ss_net_paid#91))]
Aggregate Attributes [5]: [count#93, sum#94, count#95, sum#96, count#97]
Results [5]: [count#98, sum#99, count#100, sum#101, count#102]

(38) Exchange
Input [5]: [count#98, sum#99, count#100, sum#101, count#102]
Arguments: SinglePartition, ENSURE_REQUIREMENTS, [plan_id=5]

(39) HashAggregate [codegen id : 2]
Input [5]: [count#98, sum#99, count#100, sum#101, count#102]
Keys: []
Functions [3]: [count(1), avg(UnscaledValue(ss_ext_discount_amt#90)), avg(UnscaledValue(ss_net_paid#91))]
Aggregate Attributes [3]: [count(1)#103, avg(UnscaledValue(ss_ext_discount_amt#90))#104, avg(UnscaledValue(ss_net_paid#91))#105]
Results [1]: [named_struct(count(1), count(1)#103, avg(ss_ext_discount_amt), cast((avg(UnscaledValue(ss_ext_discount_amt#90))#104 / 100.0) as decimal(11,6)), avg(ss_net_paid), cast((avg(UnscaledValue(ss_net_paid#91))#105 / 100.0) as decimal(11,6))) AS mergedValue#106]

Subquery:14 Hosting operator id = 4 Hosting Expression = ReusedSubquery Subquery scalar-subquery#14, [id=#15]

Subquery:15 Hosting operator id = 4 Hosting Expression = ReusedSubquery Subquery scalar-subquery#14, [id=#15]


