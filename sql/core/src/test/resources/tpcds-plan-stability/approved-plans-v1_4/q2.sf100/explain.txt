== Physical Plan ==
* Sort (51)
+- Exchange (50)
   +- * Project (49)
      +- * BroadcastHashJoin Inner BuildRight (48)
         :- * Project (23)
         :  +- * BroadcastHashJoin Inner BuildRight (22)
         :     :- * HashAggregate (16)
         :     :  +- Exchange (15)
         :     :     +- * HashAggregate (14)
         :     :        +- * Project (13)
         :     :           +- * BroadcastHashJoin Inner BuildRight (12)
         :     :              :- Union (7)
         :     :              :  :- * Project (3)
         :     :              :  :  +- * ColumnarToRow (2)
         :     :              :  :     +- Scan parquet spark_catalog.default.web_sales (1)
         :     :              :  +- * Project (6)
         :     :              :     +- * ColumnarToRow (5)
         :     :              :        +- Scan parquet spark_catalog.default.catalog_sales (4)
         :     :              +- BroadcastExchange (11)
         :     :                 +- * Filter (10)
         :     :                    +- * ColumnarToRow (9)
         :     :                       +- Scan parquet spark_catalog.default.date_dim (8)
         :     +- BroadcastExchange (21)
         :        +- * Project (20)
         :           +- * Filter (19)
         :              +- * ColumnarToRow (18)
         :                 +- Scan parquet spark_catalog.default.date_dim (17)
         +- BroadcastExchange (47)
            +- * Project (46)
               +- * BroadcastHashJoin Inner BuildRight (45)
                  :- * HashAggregate (39)
                  :  +- Exchange (38)
                  :     +- * HashAggregate (37)
                  :        +- * Project (36)
                  :           +- * BroadcastHashJoin Inner BuildRight (35)
                  :              :- Union (30)
                  :              :  :- * Project (26)
                  :              :  :  +- * ColumnarToRow (25)
                  :              :  :     +- Scan parquet spark_catalog.default.web_sales (24)
                  :              :  +- * Project (29)
                  :              :     +- * ColumnarToRow (28)
                  :              :        +- Scan parquet spark_catalog.default.catalog_sales (27)
                  :              +- BroadcastExchange (34)
                  :                 +- * Filter (33)
                  :                    +- * ColumnarToRow (32)
                  :                       +- Scan parquet spark_catalog.default.date_dim (31)
                  +- BroadcastExchange (44)
                     +- * Project (43)
                        +- * Filter (42)
                           +- * ColumnarToRow (41)
                              +- Scan parquet spark_catalog.default.date_dim (40)


(1) Scan parquet spark_catalog.default.web_sales
Output [2]: [ws_ext_sales_price#1, ws_sold_date_sk#2]
Batched: true
Location: InMemoryFileIndex []
PartitionFilters: [isnotnull(ws_sold_date_sk#2)]
ReadSchema: struct<ws_ext_sales_price:decimal(7,2)>

(2) ColumnarToRow [codegen id : 1]
Input [2]: [ws_ext_sales_price#1, ws_sold_date_sk#2]

(3) Project [codegen id : 1]
Output [2]: [ws_sold_date_sk#2 AS sold_date_sk#3, ws_ext_sales_price#1 AS sales_price#4]
Input [2]: [ws_ext_sales_price#1, ws_sold_date_sk#2]

(4) Scan parquet spark_catalog.default.catalog_sales
Output [2]: [cs_ext_sales_price#5, cs_sold_date_sk#6]
Batched: true
Location: InMemoryFileIndex []
PartitionFilters: [isnotnull(cs_sold_date_sk#6)]
ReadSchema: struct<cs_ext_sales_price:decimal(7,2)>

(5) ColumnarToRow [codegen id : 2]
Input [2]: [cs_ext_sales_price#5, cs_sold_date_sk#6]

(6) Project [codegen id : 2]
Output [2]: [cs_sold_date_sk#6 AS sold_date_sk#7, cs_ext_sales_price#5 AS sales_price#8]
Input [2]: [cs_ext_sales_price#5, cs_sold_date_sk#6]

(7) Union

(8) Scan parquet spark_catalog.default.date_dim
Output [3]: [d_date_sk#9, d_week_seq#10, d_day_name#11]
Batched: true
Location [not included in comparison]/{warehouse_dir}/date_dim]
PushedFilters: [IsNotNull(d_date_sk), IsNotNull(d_week_seq)]
ReadSchema: struct<d_date_sk:int,d_week_seq:int,d_day_name:string>

(9) ColumnarToRow [codegen id : 3]
Input [3]: [d_date_sk#9, d_week_seq#10, d_day_name#11]

(10) Filter [codegen id : 3]
Input [3]: [d_date_sk#9, d_week_seq#10, d_day_name#11]
Condition : ((isnotnull(d_date_sk#9) AND isnotnull(d_week_seq#10)) AND might_contain(Subquery scalar-subquery#12, [id=#13], xxhash64(d_week_seq#10, 42)))

(11) BroadcastExchange
Input [3]: [d_date_sk#9, d_week_seq#10, d_day_name#11]
Arguments: HashedRelationBroadcastMode(List(cast(input[0, int, false] as bigint)),false), [plan_id=1]

(12) BroadcastHashJoin [codegen id : 4]
Left keys [1]: [sold_date_sk#3]
Right keys [1]: [d_date_sk#9]
Join type: Inner
Join condition: None

(13) Project [codegen id : 4]
Output [3]: [sales_price#4, d_week_seq#10, d_day_name#11]
Input [5]: [sold_date_sk#3, sales_price#4, d_date_sk#9, d_week_seq#10, d_day_name#11]

(14) HashAggregate [codegen id : 4]
Input [3]: [sales_price#4, d_week_seq#10, d_day_name#11]
Keys [1]: [d_week_seq#10]
Functions [7]: [partial_sum(UnscaledValue(CASE WHEN (d_day_name#11 = Sunday   ) THEN sales_price#4 END)), partial_sum(UnscaledValue(CASE WHEN (d_day_name#11 = Monday   ) THEN sales_price#4 END)), partial_sum(UnscaledValue(CASE WHEN (d_day_name#11 = Tuesday  ) THEN sales_price#4 END)), partial_sum(UnscaledValue(CASE WHEN (d_day_name#11 = Wednesday) THEN sales_price#4 END)), partial_sum(UnscaledValue(CASE WHEN (d_day_name#11 = Thursday ) THEN sales_price#4 END)), partial_sum(UnscaledValue(CASE WHEN (d_day_name#11 = Friday   ) THEN sales_price#4 END)), partial_sum(UnscaledValue(CASE WHEN (d_day_name#11 = Saturday ) THEN sales_price#4 END))]
Aggregate Attributes [7]: [sum#14, sum#15, sum#16, sum#17, sum#18, sum#19, sum#20]
Results [8]: [d_week_seq#10, sum#21, sum#22, sum#23, sum#24, sum#25, sum#26, sum#27]

(15) Exchange
Input [8]: [d_week_seq#10, sum#21, sum#22, sum#23, sum#24, sum#25, sum#26, sum#27]
Arguments: hashpartitioning(d_week_seq#10, 5), ENSURE_REQUIREMENTS, [plan_id=2]

(16) HashAggregate [codegen id : 12]
Input [8]: [d_week_seq#10, sum#21, sum#22, sum#23, sum#24, sum#25, sum#26, sum#27]
Keys [1]: [d_week_seq#10]
Functions [7]: [sum(UnscaledValue(CASE WHEN (d_day_name#11 = Sunday   ) THEN sales_price#4 END)), sum(UnscaledValue(CASE WHEN (d_day_name#11 = Monday   ) THEN sales_price#4 END)), sum(UnscaledValue(CASE WHEN (d_day_name#11 = Tuesday  ) THEN sales_price#4 END)), sum(UnscaledValue(CASE WHEN (d_day_name#11 = Wednesday) THEN sales_price#4 END)), sum(UnscaledValue(CASE WHEN (d_day_name#11 = Thursday ) THEN sales_price#4 END)), sum(UnscaledValue(CASE WHEN (d_day_name#11 = Friday   ) THEN sales_price#4 END)), sum(UnscaledValue(CASE WHEN (d_day_name#11 = Saturday ) THEN sales_price#4 END))]
Aggregate Attributes [7]: [sum(UnscaledValue(CASE WHEN (d_day_name#11 = Sunday   ) THEN sales_price#4 END))#28, sum(UnscaledValue(CASE WHEN (d_day_name#11 = Monday   ) THEN sales_price#4 END))#29, sum(UnscaledValue(CASE WHEN (d_day_name#11 = Tuesday  ) THEN sales_price#4 END))#30, sum(UnscaledValue(CASE WHEN (d_day_name#11 = Wednesday) THEN sales_price#4 END))#31, sum(UnscaledValue(CASE WHEN (d_day_name#11 = Thursday ) THEN sales_price#4 END))#32, sum(UnscaledValue(CASE WHEN (d_day_name#11 = Friday   ) THEN sales_price#4 END))#33, sum(UnscaledValue(CASE WHEN (d_day_name#11 = Saturday ) THEN sales_price#4 END))#34]
Results [8]: [d_week_seq#10, MakeDecimal(sum(UnscaledValue(CASE WHEN (d_day_name#11 = Sunday   ) THEN sales_price#4 END))#28,17,2) AS sun_sales#35, MakeDecimal(sum(UnscaledValue(CASE WHEN (d_day_name#11 = Monday   ) THEN sales_price#4 END))#29,17,2) AS mon_sales#36, MakeDecimal(sum(UnscaledValue(CASE WHEN (d_day_name#11 = Tuesday  ) THEN sales_price#4 END))#30,17,2) AS tue_sales#37, MakeDecimal(sum(UnscaledValue(CASE WHEN (d_day_name#11 = Wednesday) THEN sales_price#4 END))#31,17,2) AS wed_sales#38, MakeDecimal(sum(UnscaledValue(CASE WHEN (d_day_name#11 = Thursday ) THEN sales_price#4 END))#32,17,2) AS thu_sales#39, MakeDecimal(sum(UnscaledValue(CASE WHEN (d_day_name#11 = Friday   ) THEN sales_price#4 END))#33,17,2) AS fri_sales#40, MakeDecimal(sum(UnscaledValue(CASE WHEN (d_day_name#11 = Saturday ) THEN sales_price#4 END))#34,17,2) AS sat_sales#41]

(17) Scan parquet spark_catalog.default.date_dim
Output [2]: [d_week_seq#42, d_year#43]
Batched: true
Location [not included in comparison]/{warehouse_dir}/date_dim]
PushedFilters: [IsNotNull(d_year), EqualTo(d_year,2001), IsNotNull(d_week_seq)]
ReadSchema: struct<d_week_seq:int,d_year:int>

(18) ColumnarToRow [codegen id : 5]
Input [2]: [d_week_seq#42, d_year#43]

(19) Filter [codegen id : 5]
Input [2]: [d_week_seq#42, d_year#43]
Condition : ((isnotnull(d_year#43) AND (d_year#43 = 2001)) AND isnotnull(d_week_seq#42))

(20) Project [codegen id : 5]
Output [1]: [d_week_seq#42]
Input [2]: [d_week_seq#42, d_year#43]

(21) BroadcastExchange
Input [1]: [d_week_seq#42]
Arguments: HashedRelationBroadcastMode(List(cast(input[0, int, true] as bigint)),false), [plan_id=3]

(22) BroadcastHashJoin [codegen id : 12]
Left keys [1]: [d_week_seq#10]
Right keys [1]: [d_week_seq#42]
Join type: Inner
Join condition: None

(23) Project [codegen id : 12]
Output [8]: [d_week_seq#10 AS d_week_seq1#44, sun_sales#35 AS sun_sales1#45, mon_sales#36 AS mon_sales1#46, tue_sales#37 AS tue_sales1#47, wed_sales#38 AS wed_sales1#48, thu_sales#39 AS thu_sales1#49, fri_sales#40 AS fri_sales1#50, sat_sales#41 AS sat_sales1#51]
Input [9]: [d_week_seq#10, sun_sales#35, mon_sales#36, tue_sales#37, wed_sales#38, thu_sales#39, fri_sales#40, sat_sales#41, d_week_seq#42]

(24) Scan parquet spark_catalog.default.web_sales
Output [2]: [ws_ext_sales_price#1, ws_sold_date_sk#2]
Batched: true
Location: InMemoryFileIndex []
PartitionFilters: [isnotnull(ws_sold_date_sk#2)]
ReadSchema: struct<ws_ext_sales_price:decimal(7,2)>

(25) ColumnarToRow [codegen id : 6]
Input [2]: [ws_ext_sales_price#1, ws_sold_date_sk#2]

(26) Project [codegen id : 6]
Output [2]: [ws_sold_date_sk#2 AS sold_date_sk#3, ws_ext_sales_price#1 AS sales_price#4]
Input [2]: [ws_ext_sales_price#1, ws_sold_date_sk#2]

(27) Scan parquet spark_catalog.default.catalog_sales
Output [2]: [cs_ext_sales_price#5, cs_sold_date_sk#6]
Batched: true
Location: InMemoryFileIndex []
PartitionFilters: [isnotnull(cs_sold_date_sk#6)]
ReadSchema: struct<cs_ext_sales_price:decimal(7,2)>

(28) ColumnarToRow [codegen id : 7]
Input [2]: [cs_ext_sales_price#5, cs_sold_date_sk#6]

(29) Project [codegen id : 7]
Output [2]: [cs_sold_date_sk#6 AS sold_date_sk#7, cs_ext_sales_price#5 AS sales_price#8]
Input [2]: [cs_ext_sales_price#5, cs_sold_date_sk#6]

(30) Union

(31) Scan parquet spark_catalog.default.date_dim
Output [3]: [d_date_sk#9, d_week_seq#10, d_day_name#11]
Batched: true
Location [not included in comparison]/{warehouse_dir}/date_dim]
PushedFilters: [IsNotNull(d_date_sk), IsNotNull(d_week_seq)]
ReadSchema: struct<d_date_sk:int,d_week_seq:int,d_day_name:string>

(32) ColumnarToRow [codegen id : 8]
Input [3]: [d_date_sk#9, d_week_seq#10, d_day_name#11]

(33) Filter [codegen id : 8]
Input [3]: [d_date_sk#9, d_week_seq#10, d_day_name#11]
Condition : ((isnotnull(d_date_sk#9) AND isnotnull(d_week_seq#10)) AND might_contain(Subquery scalar-subquery#52, [id=#53], xxhash64(d_week_seq#10, 42)))

(34) BroadcastExchange
Input [3]: [d_date_sk#9, d_week_seq#10, d_day_name#11]
Arguments: HashedRelationBroadcastMode(List(cast(input[0, int, false] as bigint)),false), [plan_id=4]

(35) BroadcastHashJoin [codegen id : 9]
Left keys [1]: [sold_date_sk#3]
Right keys [1]: [d_date_sk#9]
Join type: Inner
Join condition: None

(36) Project [codegen id : 9]
Output [3]: [sales_price#4, d_week_seq#10, d_day_name#11]
Input [5]: [sold_date_sk#3, sales_price#4, d_date_sk#9, d_week_seq#10, d_day_name#11]

(37) HashAggregate [codegen id : 9]
Input [3]: [sales_price#4, d_week_seq#10, d_day_name#11]
Keys [1]: [d_week_seq#10]
Functions [7]: [partial_sum(UnscaledValue(CASE WHEN (d_day_name#11 = Sunday   ) THEN sales_price#4 END)), partial_sum(UnscaledValue(CASE WHEN (d_day_name#11 = Monday   ) THEN sales_price#4 END)), partial_sum(UnscaledValue(CASE WHEN (d_day_name#11 = Tuesday  ) THEN sales_price#4 END)), partial_sum(UnscaledValue(CASE WHEN (d_day_name#11 = Wednesday) THEN sales_price#4 END)), partial_sum(UnscaledValue(CASE WHEN (d_day_name#11 = Thursday ) THEN sales_price#4 END)), partial_sum(UnscaledValue(CASE WHEN (d_day_name#11 = Friday   ) THEN sales_price#4 END)), partial_sum(UnscaledValue(CASE WHEN (d_day_name#11 = Saturday ) THEN sales_price#4 END))]
Aggregate Attributes [7]: [sum#54, sum#55, sum#56, sum#57, sum#58, sum#59, sum#60]
Results [8]: [d_week_seq#10, sum#61, sum#62, sum#63, sum#64, sum#65, sum#66, sum#67]

(38) Exchange
Input [8]: [d_week_seq#10, sum#61, sum#62, sum#63, sum#64, sum#65, sum#66, sum#67]
Arguments: hashpartitioning(d_week_seq#10, 5), ENSURE_REQUIREMENTS, [plan_id=5]

(39) HashAggregate [codegen id : 11]
Input [8]: [d_week_seq#10, sum#61, sum#62, sum#63, sum#64, sum#65, sum#66, sum#67]
Keys [1]: [d_week_seq#10]
Functions [7]: [sum(UnscaledValue(CASE WHEN (d_day_name#11 = Sunday   ) THEN sales_price#4 END)), sum(UnscaledValue(CASE WHEN (d_day_name#11 = Monday   ) THEN sales_price#4 END)), sum(UnscaledValue(CASE WHEN (d_day_name#11 = Tuesday  ) THEN sales_price#4 END)), sum(UnscaledValue(CASE WHEN (d_day_name#11 = Wednesday) THEN sales_price#4 END)), sum(UnscaledValue(CASE WHEN (d_day_name#11 = Thursday ) THEN sales_price#4 END)), sum(UnscaledValue(CASE WHEN (d_day_name#11 = Friday   ) THEN sales_price#4 END)), sum(UnscaledValue(CASE WHEN (d_day_name#11 = Saturday ) THEN sales_price#4 END))]
Aggregate Attributes [7]: [sum(UnscaledValue(CASE WHEN (d_day_name#11 = Sunday   ) THEN sales_price#4 END))#28, sum(UnscaledValue(CASE WHEN (d_day_name#11 = Monday   ) THEN sales_price#4 END))#29, sum(UnscaledValue(CASE WHEN (d_day_name#11 = Tuesday  ) THEN sales_price#4 END))#30, sum(UnscaledValue(CASE WHEN (d_day_name#11 = Wednesday) THEN sales_price#4 END))#31, sum(UnscaledValue(CASE WHEN (d_day_name#11 = Thursday ) THEN sales_price#4 END))#32, sum(UnscaledValue(CASE WHEN (d_day_name#11 = Friday   ) THEN sales_price#4 END))#33, sum(UnscaledValue(CASE WHEN (d_day_name#11 = Saturday ) THEN sales_price#4 END))#34]
Results [8]: [d_week_seq#10, MakeDecimal(sum(UnscaledValue(CASE WHEN (d_day_name#11 = Sunday   ) THEN sales_price#4 END))#28,17,2) AS sun_sales#35, MakeDecimal(sum(UnscaledValue(CASE WHEN (d_day_name#11 = Monday   ) THEN sales_price#4 END))#29,17,2) AS mon_sales#36, MakeDecimal(sum(UnscaledValue(CASE WHEN (d_day_name#11 = Tuesday  ) THEN sales_price#4 END))#30,17,2) AS tue_sales#37, MakeDecimal(sum(UnscaledValue(CASE WHEN (d_day_name#11 = Wednesday) THEN sales_price#4 END))#31,17,2) AS wed_sales#38, MakeDecimal(sum(UnscaledValue(CASE WHEN (d_day_name#11 = Thursday ) THEN sales_price#4 END))#32,17,2) AS thu_sales#39, MakeDecimal(sum(UnscaledValue(CASE WHEN (d_day_name#11 = Friday   ) THEN sales_price#4 END))#33,17,2) AS fri_sales#40, MakeDecimal(sum(UnscaledValue(CASE WHEN (d_day_name#11 = Saturday ) THEN sales_price#4 END))#34,17,2) AS sat_sales#41]

(40) Scan parquet spark_catalog.default.date_dim
Output [2]: [d_week_seq#68, d_year#69]
Batched: true
Location [not included in comparison]/{warehouse_dir}/date_dim]
PushedFilters: [IsNotNull(d_year), EqualTo(d_year,2002), IsNotNull(d_week_seq)]
ReadSchema: struct<d_week_seq:int,d_year:int>

(41) ColumnarToRow [codegen id : 10]
Input [2]: [d_week_seq#68, d_year#69]

(42) Filter [codegen id : 10]
Input [2]: [d_week_seq#68, d_year#69]
Condition : ((isnotnull(d_year#69) AND (d_year#69 = 2002)) AND isnotnull(d_week_seq#68))

(43) Project [codegen id : 10]
Output [1]: [d_week_seq#68]
Input [2]: [d_week_seq#68, d_year#69]

(44) BroadcastExchange
Input [1]: [d_week_seq#68]
Arguments: HashedRelationBroadcastMode(List(cast(input[0, int, true] as bigint)),false), [plan_id=6]

(45) BroadcastHashJoin [codegen id : 11]
Left keys [1]: [d_week_seq#10]
Right keys [1]: [d_week_seq#68]
Join type: Inner
Join condition: None

(46) Project [codegen id : 11]
Output [8]: [d_week_seq#10 AS d_week_seq2#70, sun_sales#35 AS sun_sales2#71, mon_sales#36 AS mon_sales2#72, tue_sales#37 AS tue_sales2#73, wed_sales#38 AS wed_sales2#74, thu_sales#39 AS thu_sales2#75, fri_sales#40 AS fri_sales2#76, sat_sales#41 AS sat_sales2#77]
Input [9]: [d_week_seq#10, sun_sales#35, mon_sales#36, tue_sales#37, wed_sales#38, thu_sales#39, fri_sales#40, sat_sales#41, d_week_seq#68]

(47) BroadcastExchange
Input [8]: [d_week_seq2#70, sun_sales2#71, mon_sales2#72, tue_sales2#73, wed_sales2#74, thu_sales2#75, fri_sales2#76, sat_sales2#77]
Arguments: HashedRelationBroadcastMode(List(cast((input[0, int, true] - 53) as bigint)),false), [plan_id=7]

(48) BroadcastHashJoin [codegen id : 12]
Left keys [1]: [d_week_seq1#44]
Right keys [1]: [(d_week_seq2#70 - 53)]
Join type: Inner
Join condition: None

(49) Project [codegen id : 12]
Output [8]: [d_week_seq1#44, round((sun_sales1#45 / sun_sales2#71), 2) AS round((sun_sales1 / sun_sales2), 2)#78, round((mon_sales1#46 / mon_sales2#72), 2) AS round((mon_sales1 / mon_sales2), 2)#79, round((tue_sales1#47 / tue_sales2#73), 2) AS round((tue_sales1 / tue_sales2), 2)#80, round((wed_sales1#48 / wed_sales2#74), 2) AS round((wed_sales1 / wed_sales2), 2)#81, round((thu_sales1#49 / thu_sales2#75), 2) AS round((thu_sales1 / thu_sales2), 2)#82, round((fri_sales1#50 / fri_sales2#76), 2) AS round((fri_sales1 / fri_sales2), 2)#83, round((sat_sales1#51 / sat_sales2#77), 2) AS round((sat_sales1 / sat_sales2), 2)#84]
Input [16]: [d_week_seq1#44, sun_sales1#45, mon_sales1#46, tue_sales1#47, wed_sales1#48, thu_sales1#49, fri_sales1#50, sat_sales1#51, d_week_seq2#70, sun_sales2#71, mon_sales2#72, tue_sales2#73, wed_sales2#74, thu_sales2#75, fri_sales2#76, sat_sales2#77]

(50) Exchange
Input [8]: [d_week_seq1#44, round((sun_sales1 / sun_sales2), 2)#78, round((mon_sales1 / mon_sales2), 2)#79, round((tue_sales1 / tue_sales2), 2)#80, round((wed_sales1 / wed_sales2), 2)#81, round((thu_sales1 / thu_sales2), 2)#82, round((fri_sales1 / fri_sales2), 2)#83, round((sat_sales1 / sat_sales2), 2)#84]
Arguments: rangepartitioning(d_week_seq1#44 ASC NULLS FIRST, 5), ENSURE_REQUIREMENTS, [plan_id=8]

(51) Sort [codegen id : 13]
Input [8]: [d_week_seq1#44, round((sun_sales1 / sun_sales2), 2)#78, round((mon_sales1 / mon_sales2), 2)#79, round((tue_sales1 / tue_sales2), 2)#80, round((wed_sales1 / wed_sales2), 2)#81, round((thu_sales1 / thu_sales2), 2)#82, round((fri_sales1 / fri_sales2), 2)#83, round((sat_sales1 / sat_sales2), 2)#84]
Arguments: [d_week_seq1#44 ASC NULLS FIRST], true, 0

===== Subqueries =====

Subquery:1 Hosting operator id = 10 Hosting Expression = Subquery scalar-subquery#12, [id=#13]
ObjectHashAggregate (58)
+- Exchange (57)
   +- ObjectHashAggregate (56)
      +- * Project (55)
         +- * Filter (54)
            +- * ColumnarToRow (53)
               +- Scan parquet spark_catalog.default.date_dim (52)


(52) Scan parquet spark_catalog.default.date_dim
Output [2]: [d_week_seq#42, d_year#43]
Batched: true
Location [not included in comparison]/{warehouse_dir}/date_dim]
PushedFilters: [IsNotNull(d_year), EqualTo(d_year,2001), IsNotNull(d_week_seq)]
ReadSchema: struct<d_week_seq:int,d_year:int>

(53) ColumnarToRow [codegen id : 1]
Input [2]: [d_week_seq#42, d_year#43]

(54) Filter [codegen id : 1]
Input [2]: [d_week_seq#42, d_year#43]
Condition : ((isnotnull(d_year#43) AND (d_year#43 = 2001)) AND isnotnull(d_week_seq#42))

(55) Project [codegen id : 1]
Output [1]: [d_week_seq#42]
Input [2]: [d_week_seq#42, d_year#43]

(56) ObjectHashAggregate
Input [1]: [d_week_seq#42]
Keys: []
Functions [1]: [partial_bloom_filter_agg(xxhash64(d_week_seq#42, 42), 362, 2896, 0, 0)]
Aggregate Attributes [1]: [buf#85]
Results [1]: [buf#86]

(57) Exchange
Input [1]: [buf#86]
Arguments: SinglePartition, ENSURE_REQUIREMENTS, [plan_id=9]

(58) ObjectHashAggregate
Input [1]: [buf#86]
Keys: []
Functions [1]: [bloom_filter_agg(xxhash64(d_week_seq#42, 42), 362, 2896, 0, 0)]
Aggregate Attributes [1]: [bloom_filter_agg(xxhash64(d_week_seq#42, 42), 362, 2896, 0, 0)#87]
Results [1]: [bloom_filter_agg(xxhash64(d_week_seq#42, 42), 362, 2896, 0, 0)#87 AS bloomFilter#88]

Subquery:2 Hosting operator id = 33 Hosting Expression = Subquery scalar-subquery#52, [id=#53]
ObjectHashAggregate (65)
+- Exchange (64)
   +- ObjectHashAggregate (63)
      +- * Project (62)
         +- * Filter (61)
            +- * ColumnarToRow (60)
               +- Scan parquet spark_catalog.default.date_dim (59)


(59) Scan parquet spark_catalog.default.date_dim
Output [2]: [d_week_seq#68, d_year#69]
Batched: true
Location [not included in comparison]/{warehouse_dir}/date_dim]
PushedFilters: [IsNotNull(d_year), EqualTo(d_year,2002), IsNotNull(d_week_seq)]
ReadSchema: struct<d_week_seq:int,d_year:int>

(60) ColumnarToRow [codegen id : 1]
Input [2]: [d_week_seq#68, d_year#69]

(61) Filter [codegen id : 1]
Input [2]: [d_week_seq#68, d_year#69]
Condition : ((isnotnull(d_year#69) AND (d_year#69 = 2002)) AND isnotnull(d_week_seq#68))

(62) Project [codegen id : 1]
Output [1]: [d_week_seq#68]
Input [2]: [d_week_seq#68, d_year#69]

(63) ObjectHashAggregate
Input [1]: [d_week_seq#68]
Keys: []
Functions [1]: [partial_bloom_filter_agg(xxhash64(d_week_seq#68, 42), 362, 2896, 0, 0)]
Aggregate Attributes [1]: [buf#89]
Results [1]: [buf#90]

(64) Exchange
Input [1]: [buf#90]
Arguments: SinglePartition, ENSURE_REQUIREMENTS, [plan_id=10]

(65) ObjectHashAggregate
Input [1]: [buf#90]
Keys: []
Functions [1]: [bloom_filter_agg(xxhash64(d_week_seq#68, 42), 362, 2896, 0, 0)]
Aggregate Attributes [1]: [bloom_filter_agg(xxhash64(d_week_seq#68, 42), 362, 2896, 0, 0)#91]
Results [1]: [bloom_filter_agg(xxhash64(d_week_seq#68, 42), 362, 2896, 0, 0)#91 AS bloomFilter#92]


