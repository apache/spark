== Physical Plan ==
TakeOrderedAndProject (77)
+- * HashAggregate (76)
   +- Exchange (75)
      +- * HashAggregate (74)
         +- * Expand (73)
            +- Union (72)
               :- * HashAggregate (21)
               :  +- Exchange (20)
               :     +- * HashAggregate (19)
               :        +- * Project (18)
               :           +- * BroadcastHashJoin Inner BuildRight (17)
               :              :- * Project (15)
               :              :  +- * BroadcastHashJoin Inner BuildRight (14)
               :              :     :- Union (9)
               :              :     :  :- * Project (4)
               :              :     :  :  +- * Filter (3)
               :              :     :  :     +- * ColumnarToRow (2)
               :              :     :  :        +- Scan parquet spark_catalog.default.store_sales (1)
               :              :     :  +- * Project (8)
               :              :     :     +- * Filter (7)
               :              :     :        +- * ColumnarToRow (6)
               :              :     :           +- Scan parquet spark_catalog.default.store_returns (5)
               :              :     +- BroadcastExchange (13)
               :              :        +- * Filter (12)
               :              :           +- * ColumnarToRow (11)
               :              :              +- Scan parquet spark_catalog.default.store (10)
               :              +- ReusedExchange (16)
               :- * HashAggregate (42)
               :  +- Exchange (41)
               :     +- * HashAggregate (40)
               :        +- * Project (39)
               :           +- * BroadcastHashJoin Inner BuildRight (38)
               :              :- * Project (36)
               :              :  +- * BroadcastHashJoin Inner BuildRight (35)
               :              :     :- Union (30)
               :              :     :  :- * Project (25)
               :              :     :  :  +- * Filter (24)
               :              :     :  :     +- * ColumnarToRow (23)
               :              :     :  :        +- Scan parquet spark_catalog.default.catalog_sales (22)
               :              :     :  +- * Project (29)
               :              :     :     +- * Filter (28)
               :              :     :        +- * ColumnarToRow (27)
               :              :     :           +- Scan parquet spark_catalog.default.catalog_returns (26)
               :              :     +- BroadcastExchange (34)
               :              :        +- * Filter (33)
               :              :           +- * ColumnarToRow (32)
               :              :              +- Scan parquet spark_catalog.default.catalog_page (31)
               :              +- ReusedExchange (37)
               +- * HashAggregate (71)
                  +- Exchange (70)
                     +- * HashAggregate (69)
                        +- * Project (68)
                           +- * BroadcastHashJoin Inner BuildRight (67)
                              :- * Project (65)
                              :  +- * BroadcastHashJoin Inner BuildRight (64)
                              :     :- Union (59)
                              :     :  :- * Project (46)
                              :     :  :  +- * Filter (45)
                              :     :  :     +- * ColumnarToRow (44)
                              :     :  :        +- Scan parquet spark_catalog.default.web_sales (43)
                              :     :  +- * Project (58)
                              :     :     +- * SortMergeJoin Inner (57)
                              :     :        :- * Sort (50)
                              :     :        :  +- Exchange (49)
                              :     :        :     +- * ColumnarToRow (48)
                              :     :        :        +- Scan parquet spark_catalog.default.web_returns (47)
                              :     :        +- * Sort (56)
                              :     :           +- Exchange (55)
                              :     :              +- * Project (54)
                              :     :                 +- * Filter (53)
                              :     :                    +- * ColumnarToRow (52)
                              :     :                       +- Scan parquet spark_catalog.default.web_sales (51)
                              :     +- BroadcastExchange (63)
                              :        +- * Filter (62)
                              :           +- * ColumnarToRow (61)
                              :              +- Scan parquet spark_catalog.default.web_site (60)
                              +- ReusedExchange (66)


(1) Scan parquet spark_catalog.default.store_sales
Output [4]: [ss_store_sk#1, ss_ext_sales_price#2, ss_net_profit#3, ss_sold_date_sk#4]
Batched: true
Location: InMemoryFileIndex []
PartitionFilters: [isnotnull(ss_sold_date_sk#4), dynamicpruningexpression(ss_sold_date_sk#4 IN dynamicpruning#5)]
PushedFilters: [IsNotNull(ss_store_sk)]
ReadSchema: struct<ss_store_sk:int,ss_ext_sales_price:decimal(7,2),ss_net_profit:decimal(7,2)>

(2) ColumnarToRow [codegen id : 1]
Input [4]: [ss_store_sk#1, ss_ext_sales_price#2, ss_net_profit#3, ss_sold_date_sk#4]

(3) Filter [codegen id : 1]
Input [4]: [ss_store_sk#1, ss_ext_sales_price#2, ss_net_profit#3, ss_sold_date_sk#4]
Condition : isnotnull(ss_store_sk#1)

(4) Project [codegen id : 1]
Output [6]: [ss_store_sk#1 AS store_sk#6, ss_sold_date_sk#4 AS date_sk#7, ss_ext_sales_price#2 AS sales_price#8, ss_net_profit#3 AS profit#9, 0.00 AS return_amt#10, 0.00 AS net_loss#11]
Input [4]: [ss_store_sk#1, ss_ext_sales_price#2, ss_net_profit#3, ss_sold_date_sk#4]

(5) Scan parquet spark_catalog.default.store_returns
Output [4]: [sr_store_sk#12, sr_return_amt#13, sr_net_loss#14, sr_returned_date_sk#15]
Batched: true
Location: InMemoryFileIndex []
PartitionFilters: [isnotnull(sr_returned_date_sk#15), dynamicpruningexpression(sr_returned_date_sk#15 IN dynamicpruning#5)]
PushedFilters: [IsNotNull(sr_store_sk)]
ReadSchema: struct<sr_store_sk:int,sr_return_amt:decimal(7,2),sr_net_loss:decimal(7,2)>

(6) ColumnarToRow [codegen id : 2]
Input [4]: [sr_store_sk#12, sr_return_amt#13, sr_net_loss#14, sr_returned_date_sk#15]

(7) Filter [codegen id : 2]
Input [4]: [sr_store_sk#12, sr_return_amt#13, sr_net_loss#14, sr_returned_date_sk#15]
Condition : isnotnull(sr_store_sk#12)

(8) Project [codegen id : 2]
Output [6]: [sr_store_sk#12 AS store_sk#16, sr_returned_date_sk#15 AS date_sk#17, 0.00 AS sales_price#18, 0.00 AS profit#19, sr_return_amt#13 AS return_amt#20, sr_net_loss#14 AS net_loss#21]
Input [4]: [sr_store_sk#12, sr_return_amt#13, sr_net_loss#14, sr_returned_date_sk#15]

(9) Union

(10) Scan parquet spark_catalog.default.store
Output [2]: [s_store_sk#22, s_store_id#23]
Batched: true
Location [not included in comparison]/{warehouse_dir}/store]
PushedFilters: [IsNotNull(s_store_sk)]
ReadSchema: struct<s_store_sk:int,s_store_id:string>

(11) ColumnarToRow [codegen id : 3]
Input [2]: [s_store_sk#22, s_store_id#23]

(12) Filter [codegen id : 3]
Input [2]: [s_store_sk#22, s_store_id#23]
Condition : isnotnull(s_store_sk#22)

(13) BroadcastExchange
Input [2]: [s_store_sk#22, s_store_id#23]
Arguments: HashedRelationBroadcastMode(List(cast(input[0, int, false] as bigint)),false), [plan_id=1]

(14) BroadcastHashJoin [codegen id : 5]
Left keys [1]: [store_sk#6]
Right keys [1]: [s_store_sk#22]
Join type: Inner
Join condition: None

(15) Project [codegen id : 5]
Output [6]: [date_sk#7, sales_price#8, profit#9, return_amt#10, net_loss#11, s_store_id#23]
Input [8]: [store_sk#6, date_sk#7, sales_price#8, profit#9, return_amt#10, net_loss#11, s_store_sk#22, s_store_id#23]

(16) ReusedExchange [Reuses operator id: 82]
Output [1]: [d_date_sk#24]

(17) BroadcastHashJoin [codegen id : 5]
Left keys [1]: [date_sk#7]
Right keys [1]: [d_date_sk#24]
Join type: Inner
Join condition: None

(18) Project [codegen id : 5]
Output [5]: [sales_price#8, profit#9, return_amt#10, net_loss#11, s_store_id#23]
Input [7]: [date_sk#7, sales_price#8, profit#9, return_amt#10, net_loss#11, s_store_id#23, d_date_sk#24]

(19) HashAggregate [codegen id : 5]
Input [5]: [sales_price#8, profit#9, return_amt#10, net_loss#11, s_store_id#23]
Keys [1]: [s_store_id#23]
Functions [4]: [partial_sum(UnscaledValue(sales_price#8)), partial_sum(UnscaledValue(return_amt#10)), partial_sum(UnscaledValue(profit#9)), partial_sum(UnscaledValue(net_loss#11))]
Aggregate Attributes [4]: [sum#25, sum#26, sum#27, sum#28]
Results [5]: [s_store_id#23, sum#29, sum#30, sum#31, sum#32]

(20) Exchange
Input [5]: [s_store_id#23, sum#29, sum#30, sum#31, sum#32]
Arguments: hashpartitioning(s_store_id#23, 5), ENSURE_REQUIREMENTS, [plan_id=2]

(21) HashAggregate [codegen id : 6]
Input [5]: [s_store_id#23, sum#29, sum#30, sum#31, sum#32]
Keys [1]: [s_store_id#23]
Functions [4]: [sum(UnscaledValue(sales_price#8)), sum(UnscaledValue(return_amt#10)), sum(UnscaledValue(profit#9)), sum(UnscaledValue(net_loss#11))]
Aggregate Attributes [4]: [sum(UnscaledValue(sales_price#8))#33, sum(UnscaledValue(return_amt#10))#34, sum(UnscaledValue(profit#9))#35, sum(UnscaledValue(net_loss#11))#36]
Results [5]: [MakeDecimal(sum(UnscaledValue(sales_price#8))#33,17,2) AS sales#37, MakeDecimal(sum(UnscaledValue(return_amt#10))#34,17,2) AS returns#38, (MakeDecimal(sum(UnscaledValue(profit#9))#35,17,2) - MakeDecimal(sum(UnscaledValue(net_loss#11))#36,17,2)) AS profit#39, store channel AS channel#40, concat(store, s_store_id#23) AS id#41]

(22) Scan parquet spark_catalog.default.catalog_sales
Output [4]: [cs_catalog_page_sk#42, cs_ext_sales_price#43, cs_net_profit#44, cs_sold_date_sk#45]
Batched: true
Location: InMemoryFileIndex []
PartitionFilters: [isnotnull(cs_sold_date_sk#45), dynamicpruningexpression(cs_sold_date_sk#45 IN dynamicpruning#5)]
PushedFilters: [IsNotNull(cs_catalog_page_sk)]
ReadSchema: struct<cs_catalog_page_sk:int,cs_ext_sales_price:decimal(7,2),cs_net_profit:decimal(7,2)>

(23) ColumnarToRow [codegen id : 7]
Input [4]: [cs_catalog_page_sk#42, cs_ext_sales_price#43, cs_net_profit#44, cs_sold_date_sk#45]

(24) Filter [codegen id : 7]
Input [4]: [cs_catalog_page_sk#42, cs_ext_sales_price#43, cs_net_profit#44, cs_sold_date_sk#45]
Condition : isnotnull(cs_catalog_page_sk#42)

(25) Project [codegen id : 7]
Output [6]: [cs_catalog_page_sk#42 AS page_sk#46, cs_sold_date_sk#45 AS date_sk#47, cs_ext_sales_price#43 AS sales_price#48, cs_net_profit#44 AS profit#49, 0.00 AS return_amt#50, 0.00 AS net_loss#51]
Input [4]: [cs_catalog_page_sk#42, cs_ext_sales_price#43, cs_net_profit#44, cs_sold_date_sk#45]

(26) Scan parquet spark_catalog.default.catalog_returns
Output [4]: [cr_catalog_page_sk#52, cr_return_amount#53, cr_net_loss#54, cr_returned_date_sk#55]
Batched: true
Location: InMemoryFileIndex []
PartitionFilters: [isnotnull(cr_returned_date_sk#55), dynamicpruningexpression(cr_returned_date_sk#55 IN dynamicpruning#5)]
PushedFilters: [IsNotNull(cr_catalog_page_sk)]
ReadSchema: struct<cr_catalog_page_sk:int,cr_return_amount:decimal(7,2),cr_net_loss:decimal(7,2)>

(27) ColumnarToRow [codegen id : 8]
Input [4]: [cr_catalog_page_sk#52, cr_return_amount#53, cr_net_loss#54, cr_returned_date_sk#55]

(28) Filter [codegen id : 8]
Input [4]: [cr_catalog_page_sk#52, cr_return_amount#53, cr_net_loss#54, cr_returned_date_sk#55]
Condition : isnotnull(cr_catalog_page_sk#52)

(29) Project [codegen id : 8]
Output [6]: [cr_catalog_page_sk#52 AS page_sk#56, cr_returned_date_sk#55 AS date_sk#57, 0.00 AS sales_price#58, 0.00 AS profit#59, cr_return_amount#53 AS return_amt#60, cr_net_loss#54 AS net_loss#61]
Input [4]: [cr_catalog_page_sk#52, cr_return_amount#53, cr_net_loss#54, cr_returned_date_sk#55]

(30) Union

(31) Scan parquet spark_catalog.default.catalog_page
Output [2]: [cp_catalog_page_sk#62, cp_catalog_page_id#63]
Batched: true
Location [not included in comparison]/{warehouse_dir}/catalog_page]
PushedFilters: [IsNotNull(cp_catalog_page_sk)]
ReadSchema: struct<cp_catalog_page_sk:int,cp_catalog_page_id:string>

(32) ColumnarToRow [codegen id : 9]
Input [2]: [cp_catalog_page_sk#62, cp_catalog_page_id#63]

(33) Filter [codegen id : 9]
Input [2]: [cp_catalog_page_sk#62, cp_catalog_page_id#63]
Condition : isnotnull(cp_catalog_page_sk#62)

(34) BroadcastExchange
Input [2]: [cp_catalog_page_sk#62, cp_catalog_page_id#63]
Arguments: HashedRelationBroadcastMode(List(cast(input[0, int, false] as bigint)),false), [plan_id=3]

(35) BroadcastHashJoin [codegen id : 11]
Left keys [1]: [page_sk#46]
Right keys [1]: [cp_catalog_page_sk#62]
Join type: Inner
Join condition: None

(36) Project [codegen id : 11]
Output [6]: [date_sk#47, sales_price#48, profit#49, return_amt#50, net_loss#51, cp_catalog_page_id#63]
Input [8]: [page_sk#46, date_sk#47, sales_price#48, profit#49, return_amt#50, net_loss#51, cp_catalog_page_sk#62, cp_catalog_page_id#63]

(37) ReusedExchange [Reuses operator id: 82]
Output [1]: [d_date_sk#64]

(38) BroadcastHashJoin [codegen id : 11]
Left keys [1]: [date_sk#47]
Right keys [1]: [d_date_sk#64]
Join type: Inner
Join condition: None

(39) Project [codegen id : 11]
Output [5]: [sales_price#48, profit#49, return_amt#50, net_loss#51, cp_catalog_page_id#63]
Input [7]: [date_sk#47, sales_price#48, profit#49, return_amt#50, net_loss#51, cp_catalog_page_id#63, d_date_sk#64]

(40) HashAggregate [codegen id : 11]
Input [5]: [sales_price#48, profit#49, return_amt#50, net_loss#51, cp_catalog_page_id#63]
Keys [1]: [cp_catalog_page_id#63]
Functions [4]: [partial_sum(UnscaledValue(sales_price#48)), partial_sum(UnscaledValue(return_amt#50)), partial_sum(UnscaledValue(profit#49)), partial_sum(UnscaledValue(net_loss#51))]
Aggregate Attributes [4]: [sum#65, sum#66, sum#67, sum#68]
Results [5]: [cp_catalog_page_id#63, sum#69, sum#70, sum#71, sum#72]

(41) Exchange
Input [5]: [cp_catalog_page_id#63, sum#69, sum#70, sum#71, sum#72]
Arguments: hashpartitioning(cp_catalog_page_id#63, 5), ENSURE_REQUIREMENTS, [plan_id=4]

(42) HashAggregate [codegen id : 12]
Input [5]: [cp_catalog_page_id#63, sum#69, sum#70, sum#71, sum#72]
Keys [1]: [cp_catalog_page_id#63]
Functions [4]: [sum(UnscaledValue(sales_price#48)), sum(UnscaledValue(return_amt#50)), sum(UnscaledValue(profit#49)), sum(UnscaledValue(net_loss#51))]
Aggregate Attributes [4]: [sum(UnscaledValue(sales_price#48))#73, sum(UnscaledValue(return_amt#50))#74, sum(UnscaledValue(profit#49))#75, sum(UnscaledValue(net_loss#51))#76]
Results [5]: [MakeDecimal(sum(UnscaledValue(sales_price#48))#73,17,2) AS sales#77, MakeDecimal(sum(UnscaledValue(return_amt#50))#74,17,2) AS returns#78, (MakeDecimal(sum(UnscaledValue(profit#49))#75,17,2) - MakeDecimal(sum(UnscaledValue(net_loss#51))#76,17,2)) AS profit#79, catalog channel AS channel#80, concat(catalog_page, cp_catalog_page_id#63) AS id#81]

(43) Scan parquet spark_catalog.default.web_sales
Output [4]: [ws_web_site_sk#82, ws_ext_sales_price#83, ws_net_profit#84, ws_sold_date_sk#85]
Batched: true
Location: InMemoryFileIndex []
PartitionFilters: [isnotnull(ws_sold_date_sk#85), dynamicpruningexpression(ws_sold_date_sk#85 IN dynamicpruning#5)]
PushedFilters: [IsNotNull(ws_web_site_sk)]
ReadSchema: struct<ws_web_site_sk:int,ws_ext_sales_price:decimal(7,2),ws_net_profit:decimal(7,2)>

(44) ColumnarToRow [codegen id : 13]
Input [4]: [ws_web_site_sk#82, ws_ext_sales_price#83, ws_net_profit#84, ws_sold_date_sk#85]

(45) Filter [codegen id : 13]
Input [4]: [ws_web_site_sk#82, ws_ext_sales_price#83, ws_net_profit#84, ws_sold_date_sk#85]
Condition : isnotnull(ws_web_site_sk#82)

(46) Project [codegen id : 13]
Output [6]: [ws_web_site_sk#82 AS wsr_web_site_sk#86, ws_sold_date_sk#85 AS date_sk#87, ws_ext_sales_price#83 AS sales_price#88, ws_net_profit#84 AS profit#89, 0.00 AS return_amt#90, 0.00 AS net_loss#91]
Input [4]: [ws_web_site_sk#82, ws_ext_sales_price#83, ws_net_profit#84, ws_sold_date_sk#85]

(47) Scan parquet spark_catalog.default.web_returns
Output [5]: [wr_item_sk#92, wr_order_number#93, wr_return_amt#94, wr_net_loss#95, wr_returned_date_sk#96]
Batched: true
Location: InMemoryFileIndex []
PartitionFilters: [isnotnull(wr_returned_date_sk#96), dynamicpruningexpression(wr_returned_date_sk#96 IN dynamicpruning#5)]
ReadSchema: struct<wr_item_sk:int,wr_order_number:int,wr_return_amt:decimal(7,2),wr_net_loss:decimal(7,2)>

(48) ColumnarToRow [codegen id : 14]
Input [5]: [wr_item_sk#92, wr_order_number#93, wr_return_amt#94, wr_net_loss#95, wr_returned_date_sk#96]

(49) Exchange
Input [5]: [wr_item_sk#92, wr_order_number#93, wr_return_amt#94, wr_net_loss#95, wr_returned_date_sk#96]
Arguments: hashpartitioning(wr_item_sk#92, wr_order_number#93, 5), ENSURE_REQUIREMENTS, [plan_id=5]

(50) Sort [codegen id : 15]
Input [5]: [wr_item_sk#92, wr_order_number#93, wr_return_amt#94, wr_net_loss#95, wr_returned_date_sk#96]
Arguments: [wr_item_sk#92 ASC NULLS FIRST, wr_order_number#93 ASC NULLS FIRST], false, 0

(51) Scan parquet spark_catalog.default.web_sales
Output [4]: [ws_item_sk#97, ws_web_site_sk#98, ws_order_number#99, ws_sold_date_sk#100]
Batched: true
Location [not included in comparison]/{warehouse_dir}/web_sales]
PushedFilters: [IsNotNull(ws_item_sk), IsNotNull(ws_order_number), IsNotNull(ws_web_site_sk)]
ReadSchema: struct<ws_item_sk:int,ws_web_site_sk:int,ws_order_number:int>

(52) ColumnarToRow [codegen id : 16]
Input [4]: [ws_item_sk#97, ws_web_site_sk#98, ws_order_number#99, ws_sold_date_sk#100]

(53) Filter [codegen id : 16]
Input [4]: [ws_item_sk#97, ws_web_site_sk#98, ws_order_number#99, ws_sold_date_sk#100]
Condition : ((isnotnull(ws_item_sk#97) AND isnotnull(ws_order_number#99)) AND isnotnull(ws_web_site_sk#98))

(54) Project [codegen id : 16]
Output [3]: [ws_item_sk#97, ws_web_site_sk#98, ws_order_number#99]
Input [4]: [ws_item_sk#97, ws_web_site_sk#98, ws_order_number#99, ws_sold_date_sk#100]

(55) Exchange
Input [3]: [ws_item_sk#97, ws_web_site_sk#98, ws_order_number#99]
Arguments: hashpartitioning(ws_item_sk#97, ws_order_number#99, 5), ENSURE_REQUIREMENTS, [plan_id=6]

(56) Sort [codegen id : 17]
Input [3]: [ws_item_sk#97, ws_web_site_sk#98, ws_order_number#99]
Arguments: [ws_item_sk#97 ASC NULLS FIRST, ws_order_number#99 ASC NULLS FIRST], false, 0

(57) SortMergeJoin [codegen id : 18]
Left keys [2]: [wr_item_sk#92, wr_order_number#93]
Right keys [2]: [ws_item_sk#97, ws_order_number#99]
Join type: Inner
Join condition: None

(58) Project [codegen id : 18]
Output [6]: [ws_web_site_sk#98 AS wsr_web_site_sk#101, wr_returned_date_sk#96 AS date_sk#102, 0.00 AS sales_price#103, 0.00 AS profit#104, wr_return_amt#94 AS return_amt#105, wr_net_loss#95 AS net_loss#106]
Input [8]: [wr_item_sk#92, wr_order_number#93, wr_return_amt#94, wr_net_loss#95, wr_returned_date_sk#96, ws_item_sk#97, ws_web_site_sk#98, ws_order_number#99]

(59) Union

(60) Scan parquet spark_catalog.default.web_site
Output [2]: [web_site_sk#107, web_site_id#108]
Batched: true
Location [not included in comparison]/{warehouse_dir}/web_site]
PushedFilters: [IsNotNull(web_site_sk)]
ReadSchema: struct<web_site_sk:int,web_site_id:string>

(61) ColumnarToRow [codegen id : 19]
Input [2]: [web_site_sk#107, web_site_id#108]

(62) Filter [codegen id : 19]
Input [2]: [web_site_sk#107, web_site_id#108]
Condition : isnotnull(web_site_sk#107)

(63) BroadcastExchange
Input [2]: [web_site_sk#107, web_site_id#108]
Arguments: HashedRelationBroadcastMode(List(cast(input[0, int, false] as bigint)),false), [plan_id=7]

(64) BroadcastHashJoin [codegen id : 21]
Left keys [1]: [wsr_web_site_sk#86]
Right keys [1]: [web_site_sk#107]
Join type: Inner
Join condition: None

(65) Project [codegen id : 21]
Output [6]: [date_sk#87, sales_price#88, profit#89, return_amt#90, net_loss#91, web_site_id#108]
Input [8]: [wsr_web_site_sk#86, date_sk#87, sales_price#88, profit#89, return_amt#90, net_loss#91, web_site_sk#107, web_site_id#108]

(66) ReusedExchange [Reuses operator id: 82]
Output [1]: [d_date_sk#109]

(67) BroadcastHashJoin [codegen id : 21]
Left keys [1]: [date_sk#87]
Right keys [1]: [d_date_sk#109]
Join type: Inner
Join condition: None

(68) Project [codegen id : 21]
Output [5]: [sales_price#88, profit#89, return_amt#90, net_loss#91, web_site_id#108]
Input [7]: [date_sk#87, sales_price#88, profit#89, return_amt#90, net_loss#91, web_site_id#108, d_date_sk#109]

(69) HashAggregate [codegen id : 21]
Input [5]: [sales_price#88, profit#89, return_amt#90, net_loss#91, web_site_id#108]
Keys [1]: [web_site_id#108]
Functions [4]: [partial_sum(UnscaledValue(sales_price#88)), partial_sum(UnscaledValue(return_amt#90)), partial_sum(UnscaledValue(profit#89)), partial_sum(UnscaledValue(net_loss#91))]
Aggregate Attributes [4]: [sum#110, sum#111, sum#112, sum#113]
Results [5]: [web_site_id#108, sum#114, sum#115, sum#116, sum#117]

(70) Exchange
Input [5]: [web_site_id#108, sum#114, sum#115, sum#116, sum#117]
Arguments: hashpartitioning(web_site_id#108, 5), ENSURE_REQUIREMENTS, [plan_id=8]

(71) HashAggregate [codegen id : 22]
Input [5]: [web_site_id#108, sum#114, sum#115, sum#116, sum#117]
Keys [1]: [web_site_id#108]
Functions [4]: [sum(UnscaledValue(sales_price#88)), sum(UnscaledValue(return_amt#90)), sum(UnscaledValue(profit#89)), sum(UnscaledValue(net_loss#91))]
Aggregate Attributes [4]: [sum(UnscaledValue(sales_price#88))#118, sum(UnscaledValue(return_amt#90))#119, sum(UnscaledValue(profit#89))#120, sum(UnscaledValue(net_loss#91))#121]
Results [5]: [MakeDecimal(sum(UnscaledValue(sales_price#88))#118,17,2) AS sales#122, MakeDecimal(sum(UnscaledValue(return_amt#90))#119,17,2) AS returns#123, (MakeDecimal(sum(UnscaledValue(profit#89))#120,17,2) - MakeDecimal(sum(UnscaledValue(net_loss#91))#121,17,2)) AS profit#124, web channel AS channel#125, concat(web_site, web_site_id#108) AS id#126]

(72) Union

(73) Expand [codegen id : 23]
Input [5]: [sales#37, returns#38, profit#39, channel#40, id#41]
Arguments: [[sales#37, returns#38, profit#39, channel#40, id#41, 0], [sales#37, returns#38, profit#39, channel#40, null, 1], [sales#37, returns#38, profit#39, null, null, 3]], [sales#37, returns#38, profit#39, channel#127, id#128, spark_grouping_id#129]

(74) HashAggregate [codegen id : 23]
Input [6]: [sales#37, returns#38, profit#39, channel#127, id#128, spark_grouping_id#129]
Keys [3]: [channel#127, id#128, spark_grouping_id#129]
Functions [3]: [partial_sum(sales#37), partial_sum(returns#38), partial_sum(profit#39)]
Aggregate Attributes [6]: [sum#130, isEmpty#131, sum#132, isEmpty#133, sum#134, isEmpty#135]
Results [9]: [channel#127, id#128, spark_grouping_id#129, sum#136, isEmpty#137, sum#138, isEmpty#139, sum#140, isEmpty#141]

(75) Exchange
Input [9]: [channel#127, id#128, spark_grouping_id#129, sum#136, isEmpty#137, sum#138, isEmpty#139, sum#140, isEmpty#141]
Arguments: hashpartitioning(channel#127, id#128, spark_grouping_id#129, 5), ENSURE_REQUIREMENTS, [plan_id=9]

(76) HashAggregate [codegen id : 24]
Input [9]: [channel#127, id#128, spark_grouping_id#129, sum#136, isEmpty#137, sum#138, isEmpty#139, sum#140, isEmpty#141]
Keys [3]: [channel#127, id#128, spark_grouping_id#129]
Functions [3]: [sum(sales#37), sum(returns#38), sum(profit#39)]
Aggregate Attributes [3]: [sum(sales#37)#142, sum(returns#38)#143, sum(profit#39)#144]
Results [5]: [channel#127, id#128, sum(sales#37)#142 AS sales#145, sum(returns#38)#143 AS returns#146, sum(profit#39)#144 AS profit#147]

(77) TakeOrderedAndProject
Input [5]: [channel#127, id#128, sales#145, returns#146, profit#147]
Arguments: 100, [channel#127 ASC NULLS FIRST, id#128 ASC NULLS FIRST], [channel#127, id#128, sales#145, returns#146, profit#147]

===== Subqueries =====

Subquery:1 Hosting operator id = 1 Hosting Expression = ss_sold_date_sk#4 IN dynamicpruning#5
BroadcastExchange (82)
+- * Project (81)
   +- * Filter (80)
      +- * ColumnarToRow (79)
         +- Scan parquet spark_catalog.default.date_dim (78)


(78) Scan parquet spark_catalog.default.date_dim
Output [2]: [d_date_sk#24, d_date#148]
Batched: true
Location [not included in comparison]/{warehouse_dir}/date_dim]
PushedFilters: [IsNotNull(d_date), GreaterThanOrEqual(d_date,2000-08-23), LessThanOrEqual(d_date,2000-09-06), IsNotNull(d_date_sk)]
ReadSchema: struct<d_date_sk:int,d_date:date>

(79) ColumnarToRow [codegen id : 1]
Input [2]: [d_date_sk#24, d_date#148]

(80) Filter [codegen id : 1]
Input [2]: [d_date_sk#24, d_date#148]
Condition : (((isnotnull(d_date#148) AND (d_date#148 >= 2000-08-23)) AND (d_date#148 <= 2000-09-06)) AND isnotnull(d_date_sk#24))

(81) Project [codegen id : 1]
Output [1]: [d_date_sk#24]
Input [2]: [d_date_sk#24, d_date#148]

(82) BroadcastExchange
Input [1]: [d_date_sk#24]
Arguments: HashedRelationBroadcastMode(List(cast(input[0, int, true] as bigint)),false), [plan_id=10]

Subquery:2 Hosting operator id = 5 Hosting Expression = sr_returned_date_sk#15 IN dynamicpruning#5

Subquery:3 Hosting operator id = 22 Hosting Expression = cs_sold_date_sk#45 IN dynamicpruning#5

Subquery:4 Hosting operator id = 26 Hosting Expression = cr_returned_date_sk#55 IN dynamicpruning#5

Subquery:5 Hosting operator id = 43 Hosting Expression = ws_sold_date_sk#85 IN dynamicpruning#5

Subquery:6 Hosting operator id = 47 Hosting Expression = wr_returned_date_sk#96 IN dynamicpruning#5


