== Physical Plan ==
TakeOrderedAndProject (64)
+- * HashAggregate (63)
   +- Exchange (62)
      +- * HashAggregate (61)
         +- * Project (60)
            +- * BroadcastHashJoin LeftOuter BuildRight (59)
               :- * Project (54)
               :  +- * BroadcastHashJoin LeftOuter BuildRight (53)
               :     :- * Project (48)
               :     :  +- * BroadcastHashJoin Inner BuildRight (47)
               :     :     :- * Project (42)
               :     :     :  +- * BroadcastHashJoin Inner BuildRight (41)
               :     :     :     :- * Project (36)
               :     :     :     :  +- * BroadcastHashJoin Inner BuildRight (35)
               :     :     :     :     :- * Project (33)
               :     :     :     :     :  +- * BroadcastHashJoin Inner BuildRight (32)
               :     :     :     :     :     :- * Project (27)
               :     :     :     :     :     :  +- * BroadcastHashJoin Inner BuildRight (26)
               :     :     :     :     :     :     :- * Project (21)
               :     :     :     :     :     :     :  +- * BroadcastHashJoin Inner BuildRight (20)
               :     :     :     :     :     :     :     :- * Project (15)
               :     :     :     :     :     :     :     :  +- * BroadcastHashJoin Inner BuildRight (14)
               :     :     :     :     :     :     :     :     :- * Project (9)
               :     :     :     :     :     :     :     :     :  +- * BroadcastHashJoin Inner BuildRight (8)
               :     :     :     :     :     :     :     :     :     :- * Project (3)
               :     :     :     :     :     :     :     :     :     :  +- * Filter (2)
               :     :     :     :     :     :     :     :     :     :     +- BatchScan spark_catalog.default.catalog_sales (1)
               :     :     :     :     :     :     :     :     :     +- BroadcastExchange (7)
               :     :     :     :     :     :     :     :     :        +- * Project (6)
               :     :     :     :     :     :     :     :     :           +- * Filter (5)
               :     :     :     :     :     :     :     :     :              +- BatchScan spark_catalog.default.inventory (4)
               :     :     :     :     :     :     :     :     +- BroadcastExchange (13)
               :     :     :     :     :     :     :     :        +- * Project (12)
               :     :     :     :     :     :     :     :           +- * Filter (11)
               :     :     :     :     :     :     :     :              +- BatchScan spark_catalog.default.warehouse (10)
               :     :     :     :     :     :     :     +- BroadcastExchange (19)
               :     :     :     :     :     :     :        +- * Project (18)
               :     :     :     :     :     :     :           +- * Filter (17)
               :     :     :     :     :     :     :              +- BatchScan spark_catalog.default.item (16)
               :     :     :     :     :     :     +- BroadcastExchange (25)
               :     :     :     :     :     :        +- * Project (24)
               :     :     :     :     :     :           +- * Filter (23)
               :     :     :     :     :     :              +- BatchScan spark_catalog.default.customer_demographics (22)
               :     :     :     :     :     +- BroadcastExchange (31)
               :     :     :     :     :        +- * Project (30)
               :     :     :     :     :           +- * Filter (29)
               :     :     :     :     :              +- BatchScan spark_catalog.default.household_demographics (28)
               :     :     :     :     +- ReusedExchange (34)
               :     :     :     +- BroadcastExchange (40)
               :     :     :        +- * Project (39)
               :     :     :           +- * Filter (38)
               :     :     :              +- BatchScan spark_catalog.default.date_dim (37)
               :     :     +- BroadcastExchange (46)
               :     :        +- * Project (45)
               :     :           +- * Filter (44)
               :     :              +- BatchScan spark_catalog.default.date_dim (43)
               :     +- BroadcastExchange (52)
               :        +- * Project (51)
               :           +- * Filter (50)
               :              +- BatchScan spark_catalog.default.promotion (49)
               +- BroadcastExchange (58)
                  +- * Project (57)
                     +- * Filter (56)
                        +- BatchScan spark_catalog.default.catalog_returns (55)


(1) BatchScan spark_catalog.default.catalog_sales
Output [8]: [cs_sold_date_sk#1, cs_ship_date_sk#2, cs_bill_cdemo_sk#3, cs_bill_hdemo_sk#4, cs_item_sk#5, cs_promo_sk#6, cs_order_number#7, cs_quantity#8]
spark_catalog.default.catalog_sales [scan class = SparkBatchQueryScan] [filters=cs_quantity IS NOT NULL, cs_item_sk IS NOT NULL, cs_bill_cdemo_sk IS NOT NULL, cs_bill_hdemo_sk IS NOT NULL, cs_sold_date_sk IS NOT NULL, cs_ship_date_sk IS NOT NULL], [runtimeFilters=[]], caseSensitive=false,[ Broadcast Var UNUSED =]

(2) Filter [codegen id : 11]
Input [8]: [cs_sold_date_sk#1, cs_ship_date_sk#2, cs_bill_cdemo_sk#3, cs_bill_hdemo_sk#4, cs_item_sk#5, cs_promo_sk#6, cs_order_number#7, cs_quantity#8]
Condition : (((((isnotnull(cs_quantity#8) AND isnotnull(cs_item_sk#5)) AND isnotnull(cs_bill_cdemo_sk#3)) AND isnotnull(cs_bill_hdemo_sk#4)) AND isnotnull(cs_sold_date_sk#1)) AND isnotnull(cs_ship_date_sk#2))

(3) Project [codegen id : 11]
Output [8]: [cs_sold_date_sk#1, cs_ship_date_sk#2, cs_bill_cdemo_sk#3, cs_bill_hdemo_sk#4, cs_item_sk#5, cs_promo_sk#6, cs_order_number#7, cs_quantity#8]
Input [8]: [cs_sold_date_sk#1, cs_ship_date_sk#2, cs_bill_cdemo_sk#3, cs_bill_hdemo_sk#4, cs_item_sk#5, cs_promo_sk#6, cs_order_number#7, cs_quantity#8]

(4) BatchScan spark_catalog.default.inventory
Output [4]: [inv_date_sk#9, inv_item_sk#10, inv_warehouse_sk#11, inv_quantity_on_hand#12]
spark_catalog.default.inventory [scan class = SparkBatchQueryScan] [filters=inv_quantity_on_hand IS NOT NULL, inv_item_sk IS NOT NULL, inv_warehouse_sk IS NOT NULL, inv_date_sk IS NOT NULL], [runtimeFilters=[]], caseSensitive=false,[ Broadcast Var UNUSED =]

(5) Filter [codegen id : 1]
Input [4]: [inv_date_sk#9, inv_item_sk#10, inv_warehouse_sk#11, inv_quantity_on_hand#12]
Condition : (((isnotnull(inv_quantity_on_hand#12) AND isnotnull(inv_item_sk#10)) AND isnotnull(inv_warehouse_sk#11)) AND isnotnull(inv_date_sk#9))

(6) Project [codegen id : 1]
Output [4]: [inv_date_sk#9, inv_item_sk#10, inv_warehouse_sk#11, inv_quantity_on_hand#12]
Input [4]: [inv_date_sk#9, inv_item_sk#10, inv_warehouse_sk#11, inv_quantity_on_hand#12]

(7) BroadcastExchange
Input [4]: [inv_date_sk#9, inv_item_sk#10, inv_warehouse_sk#11, inv_quantity_on_hand#12]
Arguments: HashedRelationBroadcastMode(List(cast(input[1, int, true] as bigint)),false), [plan_id=1]

(8) BroadcastHashJoin [codegen id : 11]
Left keys [1]: [cs_item_sk#5]
Right keys [1]: [inv_item_sk#10]
Join condition: (inv_quantity_on_hand#12 < cs_quantity#8)

(9) Project [codegen id : 11]
Output [9]: [cs_sold_date_sk#1, cs_ship_date_sk#2, cs_bill_cdemo_sk#3, cs_bill_hdemo_sk#4, cs_item_sk#5, cs_promo_sk#6, cs_order_number#7, inv_date_sk#9, inv_warehouse_sk#11]
Input [12]: [cs_sold_date_sk#1, cs_ship_date_sk#2, cs_bill_cdemo_sk#3, cs_bill_hdemo_sk#4, cs_item_sk#5, cs_promo_sk#6, cs_order_number#7, cs_quantity#8, inv_date_sk#9, inv_item_sk#10, inv_warehouse_sk#11, inv_quantity_on_hand#12]

(10) BatchScan spark_catalog.default.warehouse
Output [2]: [w_warehouse_sk#13, w_warehouse_name#14]
spark_catalog.default.warehouse [scan class = SparkBatchQueryScan] [filters=w_warehouse_sk IS NOT NULL], [runtimeFilters=[]], caseSensitive=false,[ Broadcast Var UNUSED =]

(11) Filter [codegen id : 2]
Input [2]: [w_warehouse_sk#13, w_warehouse_name#14]
Condition : isnotnull(w_warehouse_sk#13)

(12) Project [codegen id : 2]
Output [2]: [w_warehouse_sk#13, w_warehouse_name#14]
Input [2]: [w_warehouse_sk#13, w_warehouse_name#14]

(13) BroadcastExchange
Input [2]: [w_warehouse_sk#13, w_warehouse_name#14]
Arguments: HashedRelationBroadcastMode(List(cast(input[0, int, true] as bigint)),false), [plan_id=2]

(14) BroadcastHashJoin [codegen id : 11]
Left keys [1]: [inv_warehouse_sk#11]
Right keys [1]: [w_warehouse_sk#13]
Join condition: None

(15) Project [codegen id : 11]
Output [9]: [cs_sold_date_sk#1, cs_ship_date_sk#2, cs_bill_cdemo_sk#3, cs_bill_hdemo_sk#4, cs_item_sk#5, cs_promo_sk#6, cs_order_number#7, inv_date_sk#9, w_warehouse_name#14]
Input [11]: [cs_sold_date_sk#1, cs_ship_date_sk#2, cs_bill_cdemo_sk#3, cs_bill_hdemo_sk#4, cs_item_sk#5, cs_promo_sk#6, cs_order_number#7, inv_date_sk#9, inv_warehouse_sk#11, w_warehouse_sk#13, w_warehouse_name#14]

(16) BatchScan spark_catalog.default.item
Output [2]: [i_item_sk#15, i_item_desc#16]
spark_catalog.default.item [scan class = SparkBatchQueryScan] [filters=i_item_sk IS NOT NULL], [runtimeFilters=[]], caseSensitive=false,[ Broadcast Var UNUSED =]

(17) Filter [codegen id : 3]
Input [2]: [i_item_sk#15, i_item_desc#16]
Condition : isnotnull(i_item_sk#15)

(18) Project [codegen id : 3]
Output [2]: [i_item_sk#15, i_item_desc#16]
Input [2]: [i_item_sk#15, i_item_desc#16]

(19) BroadcastExchange
Input [2]: [i_item_sk#15, i_item_desc#16]
Arguments: HashedRelationBroadcastMode(List(cast(input[0, int, true] as bigint)),false), [plan_id=3]

(20) BroadcastHashJoin [codegen id : 11]
Left keys [1]: [cs_item_sk#5]
Right keys [1]: [i_item_sk#15]
Join condition: None

(21) Project [codegen id : 11]
Output [10]: [cs_sold_date_sk#1, cs_ship_date_sk#2, cs_bill_cdemo_sk#3, cs_bill_hdemo_sk#4, cs_item_sk#5, cs_promo_sk#6, cs_order_number#7, inv_date_sk#9, w_warehouse_name#14, i_item_desc#16]
Input [11]: [cs_sold_date_sk#1, cs_ship_date_sk#2, cs_bill_cdemo_sk#3, cs_bill_hdemo_sk#4, cs_item_sk#5, cs_promo_sk#6, cs_order_number#7, inv_date_sk#9, w_warehouse_name#14, i_item_sk#15, i_item_desc#16]

(22) BatchScan spark_catalog.default.customer_demographics
Output [2]: [cd_demo_sk#17, cd_marital_status#18]
spark_catalog.default.customer_demographics [scan class = SparkBatchQueryScan] [filters=cd_marital_status IS NOT NULL, cd_marital_status = 'D', cd_demo_sk IS NOT NULL], [runtimeFilters=[]], caseSensitive=false,[ Broadcast Var UNUSED =]

(23) Filter [codegen id : 4]
Input [2]: [cd_demo_sk#17, cd_marital_status#18]
Condition : ((isnotnull(cd_marital_status#18) AND (cd_marital_status#18 = D)) AND isnotnull(cd_demo_sk#17))

(24) Project [codegen id : 4]
Output [1]: [cd_demo_sk#17]
Input [2]: [cd_demo_sk#17, cd_marital_status#18]

(25) BroadcastExchange
Input [1]: [cd_demo_sk#17]
Arguments: HashedRelationBroadcastMode(List(cast(input[0, int, true] as bigint)),false), [plan_id=4]

(26) BroadcastHashJoin [codegen id : 11]
Left keys [1]: [cs_bill_cdemo_sk#3]
Right keys [1]: [cd_demo_sk#17]
Join condition: None

(27) Project [codegen id : 11]
Output [9]: [cs_sold_date_sk#1, cs_ship_date_sk#2, cs_bill_hdemo_sk#4, cs_item_sk#5, cs_promo_sk#6, cs_order_number#7, inv_date_sk#9, w_warehouse_name#14, i_item_desc#16]
Input [11]: [cs_sold_date_sk#1, cs_ship_date_sk#2, cs_bill_cdemo_sk#3, cs_bill_hdemo_sk#4, cs_item_sk#5, cs_promo_sk#6, cs_order_number#7, inv_date_sk#9, w_warehouse_name#14, i_item_desc#16, cd_demo_sk#17]

(28) BatchScan spark_catalog.default.household_demographics
Output [2]: [hd_demo_sk#19, hd_buy_potential#20]
spark_catalog.default.household_demographics [scan class = SparkBatchQueryScan] [filters=hd_buy_potential IS NOT NULL, hd_buy_potential = '>10000', hd_demo_sk IS NOT NULL], [runtimeFilters=[]], caseSensitive=false,[ Broadcast Var UNUSED =]

(29) Filter [codegen id : 5]
Input [2]: [hd_demo_sk#19, hd_buy_potential#20]
Condition : ((isnotnull(hd_buy_potential#20) AND (hd_buy_potential#20 = >10000)) AND isnotnull(hd_demo_sk#19))

(30) Project [codegen id : 5]
Output [1]: [hd_demo_sk#19]
Input [2]: [hd_demo_sk#19, hd_buy_potential#20]

(31) BroadcastExchange
Input [1]: [hd_demo_sk#19]
Arguments: HashedRelationBroadcastMode(List(cast(input[0, int, true] as bigint)),false), [plan_id=5]

(32) BroadcastHashJoin [codegen id : 11]
Left keys [1]: [cs_bill_hdemo_sk#4]
Right keys [1]: [hd_demo_sk#19]
Join condition: None

(33) Project [codegen id : 11]
Output [8]: [cs_sold_date_sk#1, cs_ship_date_sk#2, cs_item_sk#5, cs_promo_sk#6, cs_order_number#7, inv_date_sk#9, w_warehouse_name#14, i_item_desc#16]
Input [10]: [cs_sold_date_sk#1, cs_ship_date_sk#2, cs_bill_hdemo_sk#4, cs_item_sk#5, cs_promo_sk#6, cs_order_number#7, inv_date_sk#9, w_warehouse_name#14, i_item_desc#16, hd_demo_sk#19]

(34) ReusedExchange [Reuses operator id: 68]
Output [3]: [d_date_sk#21, d_date#22, d_week_seq#23]

(35) BroadcastHashJoin [codegen id : 11]
Left keys [1]: [cs_sold_date_sk#1]
Right keys [1]: [d_date_sk#21]
Join condition: None

(36) Project [codegen id : 11]
Output [9]: [cs_ship_date_sk#2, cs_item_sk#5, cs_promo_sk#6, cs_order_number#7, inv_date_sk#9, w_warehouse_name#14, i_item_desc#16, d_date#22, d_week_seq#23]
Input [11]: [cs_sold_date_sk#1, cs_ship_date_sk#2, cs_item_sk#5, cs_promo_sk#6, cs_order_number#7, inv_date_sk#9, w_warehouse_name#14, i_item_desc#16, d_date_sk#21, d_date#22, d_week_seq#23]

(37) BatchScan spark_catalog.default.date_dim
Output [2]: [d_date_sk#24, d_week_seq#25]
spark_catalog.default.date_dim [scan class = SparkBatchQueryScan] [filters=d_week_seq IS NOT NULL, d_date_sk IS NOT NULL], [runtimeFilters=[]], caseSensitive=false,[ Broadcast Var UNUSED =]

(38) Filter [codegen id : 7]
Input [2]: [d_date_sk#24, d_week_seq#25]
Condition : (isnotnull(d_week_seq#25) AND isnotnull(d_date_sk#24))

(39) Project [codegen id : 7]
Output [2]: [d_date_sk#24, d_week_seq#25]
Input [2]: [d_date_sk#24, d_week_seq#25]

(40) BroadcastExchange
Input [2]: [d_date_sk#24, d_week_seq#25]
Arguments: HashedRelationBroadcastMode(List((shiftleft(cast(input[1, int, true] as bigint), 32) | (cast(input[0, int, true] as bigint) & 4294967295))),false), [plan_id=6]

(41) BroadcastHashJoin [codegen id : 11]
Left keys [2]: [d_week_seq#23, inv_date_sk#9]
Right keys [2]: [d_week_seq#25, d_date_sk#24]
Join condition: None

(42) Project [codegen id : 11]
Output [8]: [cs_ship_date_sk#2, cs_item_sk#5, cs_promo_sk#6, cs_order_number#7, w_warehouse_name#14, i_item_desc#16, d_date#22, d_week_seq#23]
Input [11]: [cs_ship_date_sk#2, cs_item_sk#5, cs_promo_sk#6, cs_order_number#7, inv_date_sk#9, w_warehouse_name#14, i_item_desc#16, d_date#22, d_week_seq#23, d_date_sk#24, d_week_seq#25]

(43) BatchScan spark_catalog.default.date_dim
Output [2]: [d_date_sk#26, d_date#27]
spark_catalog.default.date_dim [scan class = SparkBatchQueryScan] [filters=d_date IS NOT NULL, d_date_sk IS NOT NULL], [runtimeFilters=[]], caseSensitive=false,[ Broadcast Var UNUSED =]

(44) Filter [codegen id : 8]
Input [2]: [d_date_sk#26, d_date#27]
Condition : (isnotnull(d_date#27) AND isnotnull(d_date_sk#26))

(45) Project [codegen id : 8]
Output [2]: [d_date_sk#26, d_date#27]
Input [2]: [d_date_sk#26, d_date#27]

(46) BroadcastExchange
Input [2]: [d_date_sk#26, d_date#27]
Arguments: HashedRelationBroadcastMode(List(cast(input[0, int, true] as bigint)),false), [plan_id=7]

(47) BroadcastHashJoin [codegen id : 11]
Left keys [1]: [cs_ship_date_sk#2]
Right keys [1]: [d_date_sk#26]
Join condition: (d_date#27 > date_add(d_date#22, 5))

(48) Project [codegen id : 11]
Output [6]: [cs_item_sk#5, cs_promo_sk#6, cs_order_number#7, w_warehouse_name#14, i_item_desc#16, d_week_seq#23]
Input [10]: [cs_ship_date_sk#2, cs_item_sk#5, cs_promo_sk#6, cs_order_number#7, w_warehouse_name#14, i_item_desc#16, d_date#22, d_week_seq#23, d_date_sk#26, d_date#27]

(49) BatchScan spark_catalog.default.promotion
Output [1]: [p_promo_sk#28]
spark_catalog.default.promotion [scan class = SparkBatchQueryScan] [filters=p_promo_sk IS NOT NULL], [runtimeFilters=[]], caseSensitive=false,[ Broadcast Var UNUSED =]

(50) Filter [codegen id : 9]
Input [1]: [p_promo_sk#28]
Condition : isnotnull(p_promo_sk#28)

(51) Project [codegen id : 9]
Output [1]: [p_promo_sk#28]
Input [1]: [p_promo_sk#28]

(52) BroadcastExchange
Input [1]: [p_promo_sk#28]
Arguments: HashedRelationBroadcastMode(List(cast(input[0, int, true] as bigint)),false), [plan_id=8]

(53) BroadcastHashJoin [codegen id : 11]
Left keys [1]: [cs_promo_sk#6]
Right keys [1]: [p_promo_sk#28]
Join condition: None

(54) Project [codegen id : 11]
Output [5]: [cs_item_sk#5, cs_order_number#7, w_warehouse_name#14, i_item_desc#16, d_week_seq#23]
Input [7]: [cs_item_sk#5, cs_promo_sk#6, cs_order_number#7, w_warehouse_name#14, i_item_desc#16, d_week_seq#23, p_promo_sk#28]

(55) BatchScan spark_catalog.default.catalog_returns
Output [2]: [cr_item_sk#29, cr_order_number#30]
spark_catalog.default.catalog_returns [scan class = SparkBatchQueryScan] [filters=cr_item_sk IS NOT NULL, cr_order_number IS NOT NULL], [runtimeFilters=[]], caseSensitive=false,[ Broadcast Var UNUSED =]

(56) Filter [codegen id : 10]
Input [2]: [cr_item_sk#29, cr_order_number#30]
Condition : (isnotnull(cr_item_sk#29) AND isnotnull(cr_order_number#30))

(57) Project [codegen id : 10]
Output [2]: [cr_item_sk#29, cr_order_number#30]
Input [2]: [cr_item_sk#29, cr_order_number#30]

(58) BroadcastExchange
Input [2]: [cr_item_sk#29, cr_order_number#30]
Arguments: HashedRelationBroadcastMode(List((shiftleft(cast(input[0, int, true] as bigint), 32) | (cast(input[1, int, true] as bigint) & 4294967295))),false), [plan_id=9]

(59) BroadcastHashJoin [codegen id : 11]
Left keys [2]: [cs_item_sk#5, cs_order_number#7]
Right keys [2]: [cr_item_sk#29, cr_order_number#30]
Join condition: None

(60) Project [codegen id : 11]
Output [3]: [w_warehouse_name#14, i_item_desc#16, d_week_seq#23]
Input [7]: [cs_item_sk#5, cs_order_number#7, w_warehouse_name#14, i_item_desc#16, d_week_seq#23, cr_item_sk#29, cr_order_number#30]

(61) HashAggregate [codegen id : 11]
Input [3]: [w_warehouse_name#14, i_item_desc#16, d_week_seq#23]
Keys [3]: [i_item_desc#16, w_warehouse_name#14, d_week_seq#23]
Functions [1]: [partial_count(1)]
Aggregate Attributes [1]: [count#31]
Results [4]: [i_item_desc#16, w_warehouse_name#14, d_week_seq#23, count#32]

(62) Exchange
Input [4]: [i_item_desc#16, w_warehouse_name#14, d_week_seq#23, count#32]
Arguments: hashpartitioning(i_item_desc#16, w_warehouse_name#14, d_week_seq#23, 1), ENSURE_REQUIREMENTS, [plan_id=10]

(63) HashAggregate [codegen id : 12]
Input [4]: [i_item_desc#16, w_warehouse_name#14, d_week_seq#23, count#32]
Keys [3]: [i_item_desc#16, w_warehouse_name#14, d_week_seq#23]
Functions [1]: [count(1)]
Aggregate Attributes [1]: [count(1)#33]
Results [6]: [i_item_desc#16, w_warehouse_name#14, d_week_seq#23, count(1)#33 AS no_promo#34, count(1)#33 AS promo#35, count(1)#33 AS total_cnt#36]

(64) TakeOrderedAndProject
Input [6]: [i_item_desc#16, w_warehouse_name#14, d_week_seq#23, no_promo#34, promo#35, total_cnt#36]
Arguments: 100, [total_cnt#36 DESC NULLS LAST, i_item_desc#16 ASC NULLS FIRST, w_warehouse_name#14 ASC NULLS FIRST, d_week_seq#23 ASC NULLS FIRST], [i_item_desc#16, w_warehouse_name#14, d_week_seq#23, no_promo#34, promo#35, total_cnt#36]

===== Subqueries =====

Subquery:1 Hosting operator id = 1 Hosting Expression = cs_sold_date_sk#1 IN dynamicpruning#37
BroadcastExchange (68)
+- * Project (67)
   +- * Filter (66)
      +- BatchScan spark_catalog.default.date_dim (65)


(65) BatchScan spark_catalog.default.date_dim
Output [4]: [d_date_sk#21, d_date#22, d_week_seq#23, d_year#38]
spark_catalog.default.date_dim [scan class = SparkBatchQueryScan] [filters=d_year IS NOT NULL, d_year = 1999, d_date_sk IS NOT NULL, d_week_seq IS NOT NULL, d_date IS NOT NULL], [runtimeFilters=[]], caseSensitive=false,[ Broadcast Var UNUSED =]

(66) Filter [codegen id : 1]
Input [4]: [d_date_sk#21, d_date#22, d_week_seq#23, d_year#38]
Condition : ((((isnotnull(d_year#38) AND (d_year#38 = 1999)) AND isnotnull(d_date_sk#21)) AND isnotnull(d_week_seq#23)) AND isnotnull(d_date#22))

(67) Project [codegen id : 1]
Output [3]: [d_date_sk#21, d_date#22, d_week_seq#23]
Input [4]: [d_date_sk#21, d_date#22, d_week_seq#23, d_year#38]

(68) BroadcastExchange
Input [3]: [d_date_sk#21, d_date#22, d_week_seq#23]
Arguments: HashedRelationBroadcastMode(List(cast(input[0, int, true] as bigint)),false), [plan_id=11]


