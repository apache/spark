== Physical Plan ==
TakeOrderedAndProject (21)
+- * Project (20)
   +- Window (19)
      +- * Sort (18)
         +- Exchange (17)
            +- * HashAggregate (16)
               +- Exchange (15)
                  +- * HashAggregate (14)
                     +- * Expand (13)
                        +- * Project (12)
                           +- * BroadcastHashJoin Inner BuildRight (11)
                              :- * Project (6)
                              :  +- * BroadcastHashJoin Inner BuildRight (5)
                              :     :- * Project (3)
                              :     :  +- * Filter (2)
                              :     :     +- BatchScan spark_catalog.default.web_sales (1)
                              :     +- ReusedExchange (4)
                              +- BroadcastExchange (10)
                                 +- * Project (9)
                                    +- * Filter (8)
                                       +- BatchScan spark_catalog.default.item (7)


(1) BatchScan spark_catalog.default.web_sales
Output [3]: [ws_sold_date_sk#1, ws_item_sk#2, ws_net_paid#3]
spark_catalog.default.web_sales [scan class = SparkBatchQueryScan] [filters=ws_sold_date_sk IS NOT NULL, ws_item_sk IS NOT NULL], [runtimeFilters=[]], caseSensitive=false,[ Broadcast Var UNUSED =]

(2) Filter [codegen id : 3]
Input [3]: [ws_sold_date_sk#1, ws_item_sk#2, ws_net_paid#3]
Condition : (isnotnull(ws_sold_date_sk#1) AND isnotnull(ws_item_sk#2))

(3) Project [codegen id : 3]
Output [3]: [ws_sold_date_sk#1, ws_item_sk#2, ws_net_paid#3]
Input [3]: [ws_sold_date_sk#1, ws_item_sk#2, ws_net_paid#3]

(4) ReusedExchange [Reuses operator id: 25]
Output [1]: [d_date_sk#4]

(5) BroadcastHashJoin [codegen id : 3]
Left keys [1]: [ws_sold_date_sk#1]
Right keys [1]: [d_date_sk#4]
Join condition: None

(6) Project [codegen id : 3]
Output [2]: [ws_item_sk#2, ws_net_paid#3]
Input [4]: [ws_sold_date_sk#1, ws_item_sk#2, ws_net_paid#3, d_date_sk#4]

(7) BatchScan spark_catalog.default.item
Output [3]: [i_item_sk#5, i_class#6, i_category#7]
spark_catalog.default.item [scan class = SparkBatchQueryScan] [filters=i_item_sk IS NOT NULL], [runtimeFilters=[]], caseSensitive=false,[ Broadcast Var UNUSED =]

(8) Filter [codegen id : 2]
Input [3]: [i_item_sk#5, i_class#6, i_category#7]
Condition : isnotnull(i_item_sk#5)

(9) Project [codegen id : 2]
Output [3]: [i_item_sk#5, i_class#6, i_category#7]
Input [3]: [i_item_sk#5, i_class#6, i_category#7]

(10) BroadcastExchange
Input [3]: [i_item_sk#5, i_class#6, i_category#7]
Arguments: HashedRelationBroadcastMode(List(cast(input[0, int, true] as bigint)),false), [plan_id=1]

(11) BroadcastHashJoin [codegen id : 3]
Left keys [1]: [ws_item_sk#2]
Right keys [1]: [i_item_sk#5]
Join condition: None

(12) Project [codegen id : 3]
Output [3]: [ws_net_paid#3, i_category#7, i_class#6]
Input [5]: [ws_item_sk#2, ws_net_paid#3, i_item_sk#5, i_class#6, i_category#7]

(13) Expand [codegen id : 3]
Input [3]: [ws_net_paid#3, i_category#7, i_class#6]
Arguments: [[ws_net_paid#3, i_category#7, i_class#6, 0], [ws_net_paid#3, i_category#7, null, 1], [ws_net_paid#3, null, null, 3]], [ws_net_paid#3, i_category#8, i_class#9, spark_grouping_id#10]

(14) HashAggregate [codegen id : 3]
Input [4]: [ws_net_paid#3, i_category#8, i_class#9, spark_grouping_id#10]
Keys [3]: [i_category#8, i_class#9, spark_grouping_id#10]
Functions [1]: [partial_sum(UnscaledValue(ws_net_paid#3))]
Aggregate Attributes [1]: [sum#11]
Results [4]: [i_category#8, i_class#9, spark_grouping_id#10, sum#12]

(15) Exchange
Input [4]: [i_category#8, i_class#9, spark_grouping_id#10, sum#12]
Arguments: hashpartitioning(i_category#8, i_class#9, spark_grouping_id#10, 1), ENSURE_REQUIREMENTS, [plan_id=2]

(16) HashAggregate [codegen id : 4]
Input [4]: [i_category#8, i_class#9, spark_grouping_id#10, sum#12]
Keys [3]: [i_category#8, i_class#9, spark_grouping_id#10]
Functions [1]: [sum(UnscaledValue(ws_net_paid#3))]
Aggregate Attributes [1]: [sum(UnscaledValue(ws_net_paid#3))#13]
Results [7]: [MakeDecimal(sum(UnscaledValue(ws_net_paid#3))#13,17,2) AS total_sum#14, i_category#8, i_class#9, (cast((shiftright(spark_grouping_id#10, 1) & 1) as tinyint) + cast((shiftright(spark_grouping_id#10, 0) & 1) as tinyint)) AS lochierarchy#15, (cast((shiftright(spark_grouping_id#10, 1) & 1) as tinyint) + cast((shiftright(spark_grouping_id#10, 0) & 1) as tinyint)) AS _w1#16, CASE WHEN (cast((shiftright(spark_grouping_id#10, 0) & 1) as tinyint) = 0) THEN i_category#8 END AS _w2#17, MakeDecimal(sum(UnscaledValue(ws_net_paid#3))#13,17,2) AS _w3#18]

(17) Exchange
Input [7]: [total_sum#14, i_category#8, i_class#9, lochierarchy#15, _w1#16, _w2#17, _w3#18]
Arguments: hashpartitioning(_w1#16, _w2#17, 1), ENSURE_REQUIREMENTS, [plan_id=3]

(18) Sort [codegen id : 5]
Input [7]: [total_sum#14, i_category#8, i_class#9, lochierarchy#15, _w1#16, _w2#17, _w3#18]
Arguments: [_w1#16 ASC NULLS FIRST, _w2#17 ASC NULLS FIRST, _w3#18 DESC NULLS LAST], false, 0

(19) Window
Input [7]: [total_sum#14, i_category#8, i_class#9, lochierarchy#15, _w1#16, _w2#17, _w3#18]
Arguments: [rank(_w3#18) windowspecdefinition(_w1#16, _w2#17, _w3#18 DESC NULLS LAST, specifiedwindowframe(RowFrame, unboundedpreceding$(), currentrow$())) AS rank_within_parent#19], [_w1#16, _w2#17], [_w3#18 DESC NULLS LAST]

(20) Project [codegen id : 6]
Output [5]: [total_sum#14, i_category#8, i_class#9, lochierarchy#15, rank_within_parent#19]
Input [8]: [total_sum#14, i_category#8, i_class#9, lochierarchy#15, _w1#16, _w2#17, _w3#18, rank_within_parent#19]

(21) TakeOrderedAndProject
Input [5]: [total_sum#14, i_category#8, i_class#9, lochierarchy#15, rank_within_parent#19]
Arguments: 100, [lochierarchy#15 DESC NULLS LAST, CASE WHEN (lochierarchy#15 = 0) THEN i_category#8 END ASC NULLS FIRST, rank_within_parent#19 ASC NULLS FIRST], [total_sum#14, i_category#8, i_class#9, lochierarchy#15, rank_within_parent#19]

===== Subqueries =====

Subquery:1 Hosting operator id = 1 Hosting Expression = ws_sold_date_sk#1 IN dynamicpruning#20
BroadcastExchange (25)
+- * Project (24)
   +- * Filter (23)
      +- BatchScan spark_catalog.default.date_dim (22)


(22) BatchScan spark_catalog.default.date_dim
Output [2]: [d_date_sk#4, d_month_seq#21]
spark_catalog.default.date_dim [scan class = SparkBatchQueryScan] [filters=d_month_seq IS NOT NULL, d_month_seq >= 1200, d_month_seq <= 1211, d_date_sk IS NOT NULL], [runtimeFilters=[]], caseSensitive=false,[ Broadcast Var UNUSED =]

(23) Filter [codegen id : 1]
Input [2]: [d_date_sk#4, d_month_seq#21]
Condition : (((isnotnull(d_month_seq#21) AND (d_month_seq#21 >= 1200)) AND (d_month_seq#21 <= 1211)) AND isnotnull(d_date_sk#4))

(24) Project [codegen id : 1]
Output [1]: [d_date_sk#4]
Input [2]: [d_date_sk#4, d_month_seq#21]

(25) BroadcastExchange
Input [1]: [d_date_sk#4]
Arguments: HashedRelationBroadcastMode(List(cast(input[0, int, true] as bigint)),false), [plan_id=4]


