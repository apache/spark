== Physical Plan ==
* Sort (30)
+- Exchange (29)
   +- * Project (28)
      +- * BroadcastHashJoin Inner BuildRight (27)
         :- * Filter (22)
         :  +- * HashAggregate (21)
         :     +- Exchange (20)
         :        +- * HashAggregate (19)
         :           +- * Project (18)
         :              +- * BroadcastHashJoin Inner BuildRight (17)
         :                 :- * Project (12)
         :                 :  +- * BroadcastHashJoin Inner BuildRight (11)
         :                 :     :- * Project (6)
         :                 :     :  +- * BroadcastHashJoin Inner BuildRight (5)
         :                 :     :     :- * Project (3)
         :                 :     :     :  +- * Filter (2)
         :                 :     :     :     +- BatchScan spark_catalog.default.store_sales (1)
         :                 :     :     +- ReusedExchange (4)
         :                 :     +- BroadcastExchange (10)
         :                 :        +- * Project (9)
         :                 :           +- * Filter (8)
         :                 :              +- BatchScan spark_catalog.default.store (7)
         :                 +- BroadcastExchange (16)
         :                    +- * Project (15)
         :                       +- * Filter (14)
         :                          +- BatchScan spark_catalog.default.household_demographics (13)
         +- BroadcastExchange (26)
            +- * Project (25)
               +- * Filter (24)
                  +- BatchScan spark_catalog.default.customer (23)


(1) BatchScan spark_catalog.default.store_sales
Output [5]: [ss_sold_date_sk#1, ss_customer_sk#2, ss_hdemo_sk#3, ss_store_sk#4, ss_ticket_number#5]
spark_catalog.default.store_sales [scan class = SparkBatchQueryScan] [filters=ss_sold_date_sk IS NOT NULL, ss_sold_date_sk >= 2450816, ss_sold_date_sk <= 2451910, ss_store_sk IS NOT NULL, ss_hdemo_sk IS NOT NULL, ss_customer_sk IS NOT NULL], [runtimeFilters=[]], caseSensitive=false,[ Broadcast Var UNUSED =]

(2) Filter [codegen id : 4]
Input [5]: [ss_sold_date_sk#1, ss_customer_sk#2, ss_hdemo_sk#3, ss_store_sk#4, ss_ticket_number#5]
Condition : (((((isnotnull(ss_sold_date_sk#1) AND (ss_sold_date_sk#1 >= 2450816)) AND (ss_sold_date_sk#1 <= 2451910)) AND isnotnull(ss_store_sk#4)) AND isnotnull(ss_hdemo_sk#3)) AND isnotnull(ss_customer_sk#2))

(3) Project [codegen id : 4]
Output [5]: [ss_sold_date_sk#1, ss_customer_sk#2, ss_hdemo_sk#3, ss_store_sk#4, ss_ticket_number#5]
Input [5]: [ss_sold_date_sk#1, ss_customer_sk#2, ss_hdemo_sk#3, ss_store_sk#4, ss_ticket_number#5]

(4) ReusedExchange [Reuses operator id: 34]
Output [1]: [d_date_sk#6]

(5) BroadcastHashJoin [codegen id : 4]
Left keys [1]: [ss_sold_date_sk#1]
Right keys [1]: [d_date_sk#6]
Join condition: None

(6) Project [codegen id : 4]
Output [4]: [ss_customer_sk#2, ss_hdemo_sk#3, ss_store_sk#4, ss_ticket_number#5]
Input [6]: [ss_sold_date_sk#1, ss_customer_sk#2, ss_hdemo_sk#3, ss_store_sk#4, ss_ticket_number#5, d_date_sk#6]

(7) BatchScan spark_catalog.default.store
Output [2]: [s_store_sk#7, s_county#8]
spark_catalog.default.store [scan class = SparkBatchQueryScan] [filters=s_county IN ('Saginaw County', 'Sumner County', 'Appanoose County', 'Daviess County', 'Fairfield County', 'Raleigh County', 'Ziebach County', 'Williamson County'), s_store_sk IS NOT NULL], [runtimeFilters=[]], caseSensitive=false,[ Broadcast Var UNUSED =]

(8) Filter [codegen id : 2]
Input [2]: [s_store_sk#7, s_county#8]
Condition : (s_county#8 IN (Saginaw County,Sumner County,Appanoose County,Daviess County,Fairfield County,Raleigh County,Ziebach County,Williamson County) AND isnotnull(s_store_sk#7))

(9) Project [codegen id : 2]
Output [1]: [s_store_sk#7]
Input [2]: [s_store_sk#7, s_county#8]

(10) BroadcastExchange
Input [1]: [s_store_sk#7]
Arguments: HashedRelationBroadcastMode(List(cast(input[0, int, true] as bigint)),false), [plan_id=1]

(11) BroadcastHashJoin [codegen id : 4]
Left keys [1]: [ss_store_sk#4]
Right keys [1]: [s_store_sk#7]
Join condition: None

(12) Project [codegen id : 4]
Output [3]: [ss_customer_sk#2, ss_hdemo_sk#3, ss_ticket_number#5]
Input [5]: [ss_customer_sk#2, ss_hdemo_sk#3, ss_store_sk#4, ss_ticket_number#5, s_store_sk#7]

(13) BatchScan spark_catalog.default.household_demographics
Output [4]: [hd_demo_sk#9, hd_buy_potential#10, hd_dep_count#11, hd_vehicle_count#12]
spark_catalog.default.household_demographics [scan class = SparkBatchQueryScan] [filters=hd_vehicle_count IS NOT NULL, (hd_buy_potential = '>10000' OR hd_buy_potential = 'Unknown'), hd_vehicle_count > 0, hd_demo_sk IS NOT NULL], [runtimeFilters=[]], caseSensitive=false,[ Broadcast Var UNUSED =]

(14) Filter [codegen id : 3]
Input [4]: [hd_demo_sk#9, hd_buy_potential#10, hd_dep_count#11, hd_vehicle_count#12]
Condition : ((((CASE WHEN (hd_vehicle_count#12 > 0) THEN ((cast(hd_dep_count#11 as double) / cast(hd_vehicle_count#12 as double)) > 1.2) END AND isnotnull(hd_vehicle_count#12)) AND ((hd_buy_potential#10 = >10000) OR (hd_buy_potential#10 = Unknown))) AND (hd_vehicle_count#12 > 0)) AND isnotnull(hd_demo_sk#9))

(15) Project [codegen id : 3]
Output [1]: [hd_demo_sk#9]
Input [4]: [hd_demo_sk#9, hd_buy_potential#10, hd_dep_count#11, hd_vehicle_count#12]

(16) BroadcastExchange
Input [1]: [hd_demo_sk#9]
Arguments: HashedRelationBroadcastMode(List(cast(input[0, int, true] as bigint)),false), [plan_id=2]

(17) BroadcastHashJoin [codegen id : 4]
Left keys [1]: [ss_hdemo_sk#3]
Right keys [1]: [hd_demo_sk#9]
Join condition: None

(18) Project [codegen id : 4]
Output [2]: [ss_customer_sk#2, ss_ticket_number#5]
Input [4]: [ss_customer_sk#2, ss_hdemo_sk#3, ss_ticket_number#5, hd_demo_sk#9]

(19) HashAggregate [codegen id : 4]
Input [2]: [ss_customer_sk#2, ss_ticket_number#5]
Keys [2]: [ss_ticket_number#5, ss_customer_sk#2]
Functions [1]: [partial_count(1)]
Aggregate Attributes [1]: [count#13]
Results [3]: [ss_ticket_number#5, ss_customer_sk#2, count#14]

(20) Exchange
Input [3]: [ss_ticket_number#5, ss_customer_sk#2, count#14]
Arguments: hashpartitioning(ss_ticket_number#5, ss_customer_sk#2, 1), ENSURE_REQUIREMENTS, [plan_id=3]

(21) HashAggregate [codegen id : 6]
Input [3]: [ss_ticket_number#5, ss_customer_sk#2, count#14]
Keys [2]: [ss_ticket_number#5, ss_customer_sk#2]
Functions [1]: [count(1)]
Aggregate Attributes [1]: [count(1)#15]
Results [3]: [ss_ticket_number#5, ss_customer_sk#2, count(1)#15 AS cnt#16]

(22) Filter [codegen id : 6]
Input [3]: [ss_ticket_number#5, ss_customer_sk#2, cnt#16]
Condition : ((cnt#16 >= 15) AND (cnt#16 <= 20))

(23) BatchScan spark_catalog.default.customer
Output [5]: [c_customer_sk#17, c_salutation#18, c_first_name#19, c_last_name#20, c_preferred_cust_flag#21]
spark_catalog.default.customer [scan class = SparkBatchQueryScan] [filters=c_customer_sk IS NOT NULL], [runtimeFilters=[]], caseSensitive=false,[ Broadcast Var UNUSED =]

(24) Filter [codegen id : 5]
Input [5]: [c_customer_sk#17, c_salutation#18, c_first_name#19, c_last_name#20, c_preferred_cust_flag#21]
Condition : isnotnull(c_customer_sk#17)

(25) Project [codegen id : 5]
Output [5]: [c_customer_sk#17, c_salutation#18, c_first_name#19, c_last_name#20, c_preferred_cust_flag#21]
Input [5]: [c_customer_sk#17, c_salutation#18, c_first_name#19, c_last_name#20, c_preferred_cust_flag#21]

(26) BroadcastExchange
Input [5]: [c_customer_sk#17, c_salutation#18, c_first_name#19, c_last_name#20, c_preferred_cust_flag#21]
Arguments: HashedRelationBroadcastMode(List(cast(input[0, int, true] as bigint)),false), [plan_id=4]

(27) BroadcastHashJoin [codegen id : 6]
Left keys [1]: [ss_customer_sk#2]
Right keys [1]: [c_customer_sk#17]
Join condition: None

(28) Project [codegen id : 6]
Output [6]: [c_last_name#20, c_first_name#19, c_salutation#18, c_preferred_cust_flag#21, ss_ticket_number#5, cnt#16]
Input [8]: [ss_ticket_number#5, ss_customer_sk#2, cnt#16, c_customer_sk#17, c_salutation#18, c_first_name#19, c_last_name#20, c_preferred_cust_flag#21]

(29) Exchange
Input [6]: [c_last_name#20, c_first_name#19, c_salutation#18, c_preferred_cust_flag#21, ss_ticket_number#5, cnt#16]
Arguments: rangepartitioning(c_last_name#20 ASC NULLS FIRST, c_first_name#19 ASC NULLS FIRST, c_salutation#18 ASC NULLS FIRST, c_preferred_cust_flag#21 DESC NULLS LAST, 1), ENSURE_REQUIREMENTS, [plan_id=5]

(30) Sort [codegen id : 7]
Input [6]: [c_last_name#20, c_first_name#19, c_salutation#18, c_preferred_cust_flag#21, ss_ticket_number#5, cnt#16]
Arguments: [c_last_name#20 ASC NULLS FIRST, c_first_name#19 ASC NULLS FIRST, c_salutation#18 ASC NULLS FIRST, c_preferred_cust_flag#21 DESC NULLS LAST], true, 0

===== Subqueries =====

Subquery:1 Hosting operator id = 1 Hosting Expression = ss_sold_date_sk#1 IN dynamicpruning#22
BroadcastExchange (34)
+- * Project (33)
   +- * Filter (32)
      +- BatchScan spark_catalog.default.date_dim (31)


(31) BatchScan spark_catalog.default.date_dim
Output [3]: [d_date_sk#6, d_year#23, d_dom#24]
spark_catalog.default.date_dim [scan class = SparkBatchQueryScan] [filters=((d_dom >= 1 AND d_dom <= 3) OR (d_dom >= 25 AND d_dom <= 28)), d_year IN (1998, 1999, 2000), d_date_sk >= 2450816, d_date_sk <= 2451910, d_date_sk IS NOT NULL], [runtimeFilters=[]], caseSensitive=false,[ Broadcast Var UNUSED =]

(32) Filter [codegen id : 1]
Input [3]: [d_date_sk#6, d_year#23, d_dom#24]
Condition : (((((((d_dom#24 >= 1) AND (d_dom#24 <= 3)) OR ((d_dom#24 >= 25) AND (d_dom#24 <= 28))) AND d_year#23 IN (1998,1999,2000)) AND (d_date_sk#6 >= 2450816)) AND (d_date_sk#6 <= 2451910)) AND isnotnull(d_date_sk#6))

(33) Project [codegen id : 1]
Output [1]: [d_date_sk#6]
Input [3]: [d_date_sk#6, d_year#23, d_dom#24]

(34) BroadcastExchange
Input [1]: [d_date_sk#6]
Arguments: HashedRelationBroadcastMode(List(cast(input[0, int, true] as bigint)),false), [plan_id=6]


