== Physical Plan ==
TakeOrderedAndProject (27)
+- * Project (26)
   +- * Filter (25)
      +- Window (24)
         +- * Sort (23)
            +- Exchange (22)
               +- * HashAggregate (21)
                  +- Exchange (20)
                     +- * HashAggregate (19)
                        +- * Project (18)
                           +- * BroadcastHashJoin Inner BuildRight (17)
                              :- * Project (12)
                              :  +- * BroadcastHashJoin Inner BuildRight (11)
                              :     :- * Project (9)
                              :     :  +- * BroadcastHashJoin Inner BuildRight (8)
                              :     :     :- * Project (3)
                              :     :     :  +- * Filter (2)
                              :     :     :     +- BatchScan spark_catalog.default.item (1)
                              :     :     +- BroadcastExchange (7)
                              :     :        +- * Project (6)
                              :     :           +- * Filter (5)
                              :     :              +- BatchScan spark_catalog.default.store_sales (4)
                              :     +- ReusedExchange (10)
                              +- BroadcastExchange (16)
                                 +- * Project (15)
                                    +- * Filter (14)
                                       +- BatchScan spark_catalog.default.store (13)


(1) BatchScan spark_catalog.default.item
Output [4]: [i_item_sk#1, i_brand#2, i_class#3, i_category#4]
spark_catalog.default.item [scan class = SparkBatchQueryScan] [filters=((i_category IN ('Home', 'Books', 'Electronics') AND i_class IN ('wallpaper', 'parenting', 'musical')) OR (i_category IN ('Shoes', 'Jewelry', 'Men') AND i_class IN ('womens', 'birdal', 'pants'))), i_item_sk IS NOT NULL], [runtimeFilters=[]], caseSensitive=false,[ Broadcast Var UNUSED =]

(2) Filter [codegen id : 4]
Input [4]: [i_item_sk#1, i_brand#2, i_class#3, i_category#4]
Condition : (((i_category#4 IN (Home,Books,Electronics) AND i_class#3 IN (wallpaper,parenting,musical)) OR (i_category#4 IN (Shoes,Jewelry,Men) AND i_class#3 IN (womens,birdal,pants))) AND isnotnull(i_item_sk#1))

(3) Project [codegen id : 4]
Output [4]: [i_item_sk#1, i_brand#2, i_class#3, i_category#4]
Input [4]: [i_item_sk#1, i_brand#2, i_class#3, i_category#4]

(4) BatchScan spark_catalog.default.store_sales
Output [4]: [ss_sold_date_sk#5, ss_item_sk#6, ss_store_sk#7, ss_sales_price#8]
spark_catalog.default.store_sales [scan class = SparkBatchQueryScan] [filters=ss_sold_date_sk IS NOT NULL, ss_sold_date_sk >= 2451545, ss_sold_date_sk <= 2451910, ss_item_sk IS NOT NULL, ss_store_sk IS NOT NULL], [runtimeFilters=[]], caseSensitive=false,[ Broadcast Var UNUSED =]

(5) Filter [codegen id : 1]
Input [4]: [ss_sold_date_sk#5, ss_item_sk#6, ss_store_sk#7, ss_sales_price#8]
Condition : ((((isnotnull(ss_sold_date_sk#5) AND (ss_sold_date_sk#5 >= 2451545)) AND (ss_sold_date_sk#5 <= 2451910)) AND isnotnull(ss_item_sk#6)) AND isnotnull(ss_store_sk#7))

(6) Project [codegen id : 1]
Output [4]: [ss_sold_date_sk#5, ss_item_sk#6, ss_store_sk#7, ss_sales_price#8]
Input [4]: [ss_sold_date_sk#5, ss_item_sk#6, ss_store_sk#7, ss_sales_price#8]

(7) BroadcastExchange
Input [4]: [ss_sold_date_sk#5, ss_item_sk#6, ss_store_sk#7, ss_sales_price#8]
Arguments: HashedRelationBroadcastMode(List(cast(input[1, int, true] as bigint)),false), [plan_id=1]

(8) BroadcastHashJoin [codegen id : 4]
Left keys [1]: [i_item_sk#1]
Right keys [1]: [ss_item_sk#6]
Join condition: None

(9) Project [codegen id : 4]
Output [6]: [i_brand#2, i_class#3, i_category#4, ss_sold_date_sk#5, ss_store_sk#7, ss_sales_price#8]
Input [8]: [i_item_sk#1, i_brand#2, i_class#3, i_category#4, ss_sold_date_sk#5, ss_item_sk#6, ss_store_sk#7, ss_sales_price#8]

(10) ReusedExchange [Reuses operator id: 31]
Output [2]: [d_date_sk#9, d_moy#10]

(11) BroadcastHashJoin [codegen id : 4]
Left keys [1]: [ss_sold_date_sk#5]
Right keys [1]: [d_date_sk#9]
Join condition: None

(12) Project [codegen id : 4]
Output [6]: [i_brand#2, i_class#3, i_category#4, ss_store_sk#7, ss_sales_price#8, d_moy#10]
Input [8]: [i_brand#2, i_class#3, i_category#4, ss_sold_date_sk#5, ss_store_sk#7, ss_sales_price#8, d_date_sk#9, d_moy#10]

(13) BatchScan spark_catalog.default.store
Output [3]: [s_store_sk#11, s_store_name#12, s_company_name#13]
spark_catalog.default.store [scan class = SparkBatchQueryScan] [filters=s_store_sk IS NOT NULL], [runtimeFilters=[]], caseSensitive=false,[ Broadcast Var UNUSED =]

(14) Filter [codegen id : 3]
Input [3]: [s_store_sk#11, s_store_name#12, s_company_name#13]
Condition : isnotnull(s_store_sk#11)

(15) Project [codegen id : 3]
Output [3]: [s_store_sk#11, s_store_name#12, s_company_name#13]
Input [3]: [s_store_sk#11, s_store_name#12, s_company_name#13]

(16) BroadcastExchange
Input [3]: [s_store_sk#11, s_store_name#12, s_company_name#13]
Arguments: HashedRelationBroadcastMode(List(cast(input[0, int, true] as bigint)),false), [plan_id=2]

(17) BroadcastHashJoin [codegen id : 4]
Left keys [1]: [ss_store_sk#7]
Right keys [1]: [s_store_sk#11]
Join condition: None

(18) Project [codegen id : 4]
Output [7]: [i_brand#2, i_class#3, i_category#4, ss_sales_price#8, d_moy#10, s_store_name#12, s_company_name#13]
Input [9]: [i_brand#2, i_class#3, i_category#4, ss_store_sk#7, ss_sales_price#8, d_moy#10, s_store_sk#11, s_store_name#12, s_company_name#13]

(19) HashAggregate [codegen id : 4]
Input [7]: [i_brand#2, i_class#3, i_category#4, ss_sales_price#8, d_moy#10, s_store_name#12, s_company_name#13]
Keys [6]: [i_category#4, i_class#3, i_brand#2, s_store_name#12, s_company_name#13, d_moy#10]
Functions [1]: [partial_sum(UnscaledValue(ss_sales_price#8))]
Aggregate Attributes [1]: [sum#14]
Results [7]: [i_category#4, i_class#3, i_brand#2, s_store_name#12, s_company_name#13, d_moy#10, sum#15]

(20) Exchange
Input [7]: [i_category#4, i_class#3, i_brand#2, s_store_name#12, s_company_name#13, d_moy#10, sum#15]
Arguments: hashpartitioning(i_category#4, i_class#3, i_brand#2, s_store_name#12, s_company_name#13, d_moy#10, 1), ENSURE_REQUIREMENTS, [plan_id=3]

(21) HashAggregate [codegen id : 5]
Input [7]: [i_category#4, i_class#3, i_brand#2, s_store_name#12, s_company_name#13, d_moy#10, sum#15]
Keys [6]: [i_category#4, i_class#3, i_brand#2, s_store_name#12, s_company_name#13, d_moy#10]
Functions [1]: [sum(UnscaledValue(ss_sales_price#8))]
Aggregate Attributes [1]: [sum(UnscaledValue(ss_sales_price#8))#16]
Results [8]: [i_category#4, i_class#3, i_brand#2, s_store_name#12, s_company_name#13, d_moy#10, MakeDecimal(sum(UnscaledValue(ss_sales_price#8))#16,17,2) AS sum_sales#17, MakeDecimal(sum(UnscaledValue(ss_sales_price#8))#16,17,2) AS _w0#18]

(22) Exchange
Input [8]: [i_category#4, i_class#3, i_brand#2, s_store_name#12, s_company_name#13, d_moy#10, sum_sales#17, _w0#18]
Arguments: hashpartitioning(i_category#4, i_brand#2, s_store_name#12, s_company_name#13, 1), ENSURE_REQUIREMENTS, [plan_id=4]

(23) Sort [codegen id : 6]
Input [8]: [i_category#4, i_class#3, i_brand#2, s_store_name#12, s_company_name#13, d_moy#10, sum_sales#17, _w0#18]
Arguments: [i_category#4 ASC NULLS FIRST, i_brand#2 ASC NULLS FIRST, s_store_name#12 ASC NULLS FIRST, s_company_name#13 ASC NULLS FIRST], false, 0

(24) Window
Input [8]: [i_category#4, i_class#3, i_brand#2, s_store_name#12, s_company_name#13, d_moy#10, sum_sales#17, _w0#18]
Arguments: [avg(_w0#18) windowspecdefinition(i_category#4, i_brand#2, s_store_name#12, s_company_name#13, specifiedwindowframe(RowFrame, unboundedpreceding$(), unboundedfollowing$())) AS avg_monthly_sales#19], [i_category#4, i_brand#2, s_store_name#12, s_company_name#13]

(25) Filter [codegen id : 7]
Input [9]: [i_category#4, i_class#3, i_brand#2, s_store_name#12, s_company_name#13, d_moy#10, sum_sales#17, _w0#18, avg_monthly_sales#19]
Condition : CASE WHEN NOT (avg_monthly_sales#19 = 0.000000) THEN (CheckOverflow((promote_precision(abs(CheckOverflow((promote_precision(cast(sum_sales#17 as decimal(22,6))) - promote_precision(cast(avg_monthly_sales#19 as decimal(22,6)))), DecimalType(22,6)))) / promote_precision(cast(avg_monthly_sales#19 as decimal(22,6)))), DecimalType(38,16)) > 0.1000000000000000) END

(26) Project [codegen id : 7]
Output [8]: [i_category#4, i_class#3, i_brand#2, s_store_name#12, s_company_name#13, d_moy#10, sum_sales#17, avg_monthly_sales#19]
Input [9]: [i_category#4, i_class#3, i_brand#2, s_store_name#12, s_company_name#13, d_moy#10, sum_sales#17, _w0#18, avg_monthly_sales#19]

(27) TakeOrderedAndProject
Input [8]: [i_category#4, i_class#3, i_brand#2, s_store_name#12, s_company_name#13, d_moy#10, sum_sales#17, avg_monthly_sales#19]
Arguments: 100, [CheckOverflow((promote_precision(cast(sum_sales#17 as decimal(22,6))) - promote_precision(cast(avg_monthly_sales#19 as decimal(22,6)))), DecimalType(22,6)) ASC NULLS FIRST, s_store_name#12 ASC NULLS FIRST], [i_category#4, i_class#3, i_brand#2, s_store_name#12, s_company_name#13, d_moy#10, sum_sales#17, avg_monthly_sales#19]

===== Subqueries =====

Subquery:1 Hosting operator id = 4 Hosting Expression = ss_sold_date_sk#5 IN dynamicpruning#20
BroadcastExchange (31)
+- * Project (30)
   +- * Filter (29)
      +- BatchScan spark_catalog.default.date_dim (28)


(28) BatchScan spark_catalog.default.date_dim
Output [3]: [d_date_sk#9, d_year#21, d_moy#10]
spark_catalog.default.date_dim [scan class = SparkBatchQueryScan] [filters=d_year IS NOT NULL, d_year = 2000, d_date_sk >= 2451545, d_date_sk <= 2451910, d_date_sk IS NOT NULL], [runtimeFilters=[]], caseSensitive=false,[ Broadcast Var UNUSED =]

(29) Filter [codegen id : 1]
Input [3]: [d_date_sk#9, d_year#21, d_moy#10]
Condition : ((((isnotnull(d_year#21) AND (d_year#21 = 2000)) AND (d_date_sk#9 >= 2451545)) AND (d_date_sk#9 <= 2451910)) AND isnotnull(d_date_sk#9))

(30) Project [codegen id : 1]
Output [2]: [d_date_sk#9, d_moy#10]
Input [3]: [d_date_sk#9, d_year#21, d_moy#10]

(31) BroadcastExchange
Input [2]: [d_date_sk#9, d_moy#10]
Arguments: HashedRelationBroadcastMode(List(cast(input[0, int, true] as bigint)),false), [plan_id=5]


