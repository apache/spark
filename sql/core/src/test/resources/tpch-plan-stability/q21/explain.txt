== Physical Plan ==
TakeOrderedAndProject (38)
+- * HashAggregate (37)
   +- Exchange (36)
      +- * HashAggregate (35)
         +- * Project (34)
            +- * BroadcastHashJoin Inner BuildRight (33)
               :- * Project (27)
               :  +- * BroadcastHashJoin Inner BuildRight (26)
               :     :- * Project (20)
               :     :  +- * BroadcastHashJoin Inner BuildRight (19)
               :     :     :- * Filter (3)
               :     :     :  +- * ColumnarToRow (2)
               :     :     :     +- Scan parquet spark_catalog.default.supplier (1)
               :     :     +- BroadcastExchange (18)
               :     :        +- * BroadcastHashJoin LeftAnti BuildRight (17)
               :     :           :- * BroadcastHashJoin LeftSemi BuildRight (11)
               :     :           :  :- * Project (7)
               :     :           :  :  +- * Filter (6)
               :     :           :  :     +- * ColumnarToRow (5)
               :     :           :  :        +- Scan parquet spark_catalog.default.lineitem (4)
               :     :           :  +- BroadcastExchange (10)
               :     :           :     +- * ColumnarToRow (9)
               :     :           :        +- Scan parquet spark_catalog.default.lineitem (8)
               :     :           +- BroadcastExchange (16)
               :     :              +- * Project (15)
               :     :                 +- * Filter (14)
               :     :                    +- * ColumnarToRow (13)
               :     :                       +- Scan parquet spark_catalog.default.lineitem (12)
               :     +- BroadcastExchange (25)
               :        +- * Project (24)
               :           +- * Filter (23)
               :              +- * ColumnarToRow (22)
               :                 +- Scan parquet spark_catalog.default.orders (21)
               +- BroadcastExchange (32)
                  +- * Project (31)
                     +- * Filter (30)
                        +- * ColumnarToRow (29)
                           +- Scan parquet spark_catalog.default.nation (28)


(1) Scan parquet spark_catalog.default.supplier
Output [3]: [s_suppkey#1, s_name#2, s_nationkey#3]
Batched: true
Location [not included in comparison]/{warehouse_dir}/supplier]
PushedFilters: [IsNotNull(s_suppkey), IsNotNull(s_nationkey)]
ReadSchema: struct<s_suppkey:bigint,s_name:string,s_nationkey:bigint>

(2) ColumnarToRow [codegen id : 6]
Input [3]: [s_suppkey#1, s_name#2, s_nationkey#3]

(3) Filter [codegen id : 6]
Input [3]: [s_suppkey#1, s_name#2, s_nationkey#3]
Condition : (isnotnull(s_suppkey#1) AND isnotnull(s_nationkey#3))

(4) Scan parquet spark_catalog.default.lineitem
Output [4]: [l_orderkey#4, l_suppkey#5, l_commitdate#6, l_receiptdate#7]
Batched: true
Location [not included in comparison]/{warehouse_dir}/lineitem]
PushedFilters: [IsNotNull(l_receiptdate), IsNotNull(l_commitdate), IsNotNull(l_suppkey), IsNotNull(l_orderkey)]
ReadSchema: struct<l_orderkey:bigint,l_suppkey:bigint,l_commitdate:date,l_receiptdate:date>

(5) ColumnarToRow [codegen id : 3]
Input [4]: [l_orderkey#4, l_suppkey#5, l_commitdate#6, l_receiptdate#7]

(6) Filter [codegen id : 3]
Input [4]: [l_orderkey#4, l_suppkey#5, l_commitdate#6, l_receiptdate#7]
Condition : ((((isnotnull(l_receiptdate#7) AND isnotnull(l_commitdate#6)) AND (l_receiptdate#7 > l_commitdate#6)) AND isnotnull(l_suppkey#5)) AND isnotnull(l_orderkey#4))

(7) Project [codegen id : 3]
Output [2]: [l_orderkey#4, l_suppkey#5]
Input [4]: [l_orderkey#4, l_suppkey#5, l_commitdate#6, l_receiptdate#7]

(8) Scan parquet spark_catalog.default.lineitem
Output [2]: [l_orderkey#8, l_suppkey#9]
Batched: true
Location [not included in comparison]/{warehouse_dir}/lineitem]
ReadSchema: struct<l_orderkey:bigint,l_suppkey:bigint>

(9) ColumnarToRow [codegen id : 1]
Input [2]: [l_orderkey#8, l_suppkey#9]

(10) BroadcastExchange
Input [2]: [l_orderkey#8, l_suppkey#9]
Arguments: HashedRelationBroadcastMode(List(input[0, bigint, true]),false), [plan_id=1]

(11) BroadcastHashJoin [codegen id : 3]
Left keys [1]: [l_orderkey#4]
Right keys [1]: [l_orderkey#8]
Join type: LeftSemi
Join condition: NOT (l_suppkey#9 = l_suppkey#5)

(12) Scan parquet spark_catalog.default.lineitem
Output [4]: [l_orderkey#10, l_suppkey#11, l_commitdate#12, l_receiptdate#13]
Batched: true
Location [not included in comparison]/{warehouse_dir}/lineitem]
PushedFilters: [IsNotNull(l_receiptdate), IsNotNull(l_commitdate)]
ReadSchema: struct<l_orderkey:bigint,l_suppkey:bigint,l_commitdate:date,l_receiptdate:date>

(13) ColumnarToRow [codegen id : 2]
Input [4]: [l_orderkey#10, l_suppkey#11, l_commitdate#12, l_receiptdate#13]

(14) Filter [codegen id : 2]
Input [4]: [l_orderkey#10, l_suppkey#11, l_commitdate#12, l_receiptdate#13]
Condition : ((isnotnull(l_receiptdate#13) AND isnotnull(l_commitdate#12)) AND (l_receiptdate#13 > l_commitdate#12))

(15) Project [codegen id : 2]
Output [2]: [l_orderkey#10, l_suppkey#11]
Input [4]: [l_orderkey#10, l_suppkey#11, l_commitdate#12, l_receiptdate#13]

(16) BroadcastExchange
Input [2]: [l_orderkey#10, l_suppkey#11]
Arguments: HashedRelationBroadcastMode(List(input[0, bigint, true]),false), [plan_id=2]

(17) BroadcastHashJoin [codegen id : 3]
Left keys [1]: [l_orderkey#4]
Right keys [1]: [l_orderkey#10]
Join type: LeftAnti
Join condition: NOT (l_suppkey#11 = l_suppkey#5)

(18) BroadcastExchange
Input [2]: [l_orderkey#4, l_suppkey#5]
Arguments: HashedRelationBroadcastMode(List(input[1, bigint, true]),false), [plan_id=3]

(19) BroadcastHashJoin [codegen id : 6]
Left keys [1]: [s_suppkey#1]
Right keys [1]: [l_suppkey#5]
Join type: Inner
Join condition: None

(20) Project [codegen id : 6]
Output [3]: [s_name#2, s_nationkey#3, l_orderkey#4]
Input [5]: [s_suppkey#1, s_name#2, s_nationkey#3, l_orderkey#4, l_suppkey#5]

(21) Scan parquet spark_catalog.default.orders
Output [2]: [o_orderkey#14, o_orderstatus#15]
Batched: true
Location [not included in comparison]/{warehouse_dir}/orders]
PushedFilters: [IsNotNull(o_orderstatus), EqualTo(o_orderstatus,F), IsNotNull(o_orderkey)]
ReadSchema: struct<o_orderkey:bigint,o_orderstatus:string>

(22) ColumnarToRow [codegen id : 4]
Input [2]: [o_orderkey#14, o_orderstatus#15]

(23) Filter [codegen id : 4]
Input [2]: [o_orderkey#14, o_orderstatus#15]
Condition : ((isnotnull(o_orderstatus#15) AND (o_orderstatus#15 = F)) AND isnotnull(o_orderkey#14))

(24) Project [codegen id : 4]
Output [1]: [o_orderkey#14]
Input [2]: [o_orderkey#14, o_orderstatus#15]

(25) BroadcastExchange
Input [1]: [o_orderkey#14]
Arguments: HashedRelationBroadcastMode(List(input[0, bigint, true]),false), [plan_id=4]

(26) BroadcastHashJoin [codegen id : 6]
Left keys [1]: [l_orderkey#4]
Right keys [1]: [o_orderkey#14]
Join type: Inner
Join condition: None

(27) Project [codegen id : 6]
Output [2]: [s_name#2, s_nationkey#3]
Input [4]: [s_name#2, s_nationkey#3, l_orderkey#4, o_orderkey#14]

(28) Scan parquet spark_catalog.default.nation
Output [2]: [n_nationkey#16, n_name#17]
Batched: true
Location [not included in comparison]/{warehouse_dir}/nation]
PushedFilters: [IsNotNull(n_name), EqualTo(n_name,SAUDI ARABIA), IsNotNull(n_nationkey)]
ReadSchema: struct<n_nationkey:bigint,n_name:string>

(29) ColumnarToRow [codegen id : 5]
Input [2]: [n_nationkey#16, n_name#17]

(30) Filter [codegen id : 5]
Input [2]: [n_nationkey#16, n_name#17]
Condition : ((isnotnull(n_name#17) AND (n_name#17 = SAUDI ARABIA)) AND isnotnull(n_nationkey#16))

(31) Project [codegen id : 5]
Output [1]: [n_nationkey#16]
Input [2]: [n_nationkey#16, n_name#17]

(32) BroadcastExchange
Input [1]: [n_nationkey#16]
Arguments: HashedRelationBroadcastMode(List(input[0, bigint, true]),false), [plan_id=5]

(33) BroadcastHashJoin [codegen id : 6]
Left keys [1]: [s_nationkey#3]
Right keys [1]: [n_nationkey#16]
Join type: Inner
Join condition: None

(34) Project [codegen id : 6]
Output [1]: [s_name#2]
Input [3]: [s_name#2, s_nationkey#3, n_nationkey#16]

(35) HashAggregate [codegen id : 6]
Input [1]: [s_name#2]
Keys [1]: [s_name#2]
Functions [1]: [partial_count(1)]
Aggregate Attributes [1]: [count#18]
Results [2]: [s_name#2, count#19]

(36) Exchange
Input [2]: [s_name#2, count#19]
Arguments: hashpartitioning(s_name#2, 5), ENSURE_REQUIREMENTS, [plan_id=6]

(37) HashAggregate [codegen id : 7]
Input [2]: [s_name#2, count#19]
Keys [1]: [s_name#2]
Functions [1]: [count(1)]
Aggregate Attributes [1]: [count(1)#20]
Results [2]: [s_name#2, count(1)#20 AS numwait#21]

(38) TakeOrderedAndProject
Input [2]: [s_name#2, numwait#21]
Arguments: 100, [numwait#21 DESC NULLS LAST, s_name#2 ASC NULLS FIRST], [s_name#2, numwait#21]

