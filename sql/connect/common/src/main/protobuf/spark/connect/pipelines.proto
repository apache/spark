/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *    http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

syntax = "proto3";

package spark.connect;

import "google/protobuf/timestamp.proto";
import "spark/connect/common.proto";
import "spark/connect/relations.proto";
import "spark/connect/types.proto";

option java_multiple_files = true;
option java_package = "org.apache.spark.connect.proto";
option go_package = "internal/generated";

// Dispatch object for pipelines commands. See each individual command for documentation.
message PipelineCommand {
  oneof command_type {
    CreateDataflowGraph create_dataflow_graph = 1;
    DefineDataset define_dataset = 2;
    DefineFlow define_flow = 3;
    DropDataflowGraph drop_dataflow_graph = 4;
    StartRun start_run = 5;
    DefineSqlGraphElements define_sql_graph_elements = 6;
    GetQueryFunctionExecutionSignalStream get_query_function_execution_signal_stream = 7;
    DefineFlowQueryFunctionResult define_flow_query_function_result = 8;
  }

  // Request to create a new dataflow graph.
  message CreateDataflowGraph {
    // The default catalog.
    optional string default_catalog = 1;

    // The default database.
    optional string default_database = 2;

    // SQL configurations for all flows in this graph.
    map<string, string> sql_conf = 5;
  }

  // Drops the graph and stops any running attached flows.
  message DropDataflowGraph {
    // The graph to drop.
    optional string dataflow_graph_id = 1;
  }

  // Request to define a dataset: a table, a materialized view, or a temporary view.
  message DefineDataset {
    // The graph to attach this dataset to.
    optional string dataflow_graph_id = 1;

    // Name of the dataset. Can be partially or fully qualified.
    optional string dataset_name = 2;

    // The type of the dataset.
    optional DatasetType dataset_type = 3;

    // Optional comment for the dataset.
    optional string comment = 4;

    // Optional table properties. Only applies to dataset_type == TABLE and dataset_type == MATERIALIZED_VIEW.
    map<string, string> table_properties = 5;

    // Optional partition columns for the dataset. Only applies to dataset_type == TABLE and
    // dataset_type == MATERIALIZED_VIEW.
    repeated string partition_cols = 6;

    // Schema for the dataset. If unset, this will be inferred from incoming flows.
    oneof schema {
      spark.connect.DataType schema_data_type = 7;
      string schema_string = 8;
    }

    // The output table format of the dataset. Only applies to dataset_type == TABLE and
    // dataset_type == MATERIALIZED_VIEW.
    optional string format = 9;

    // The location in source code that this dataset was defined.
    optional SourceCodeLocation source_code_location = 10;
  }

  // Request to define a flow targeting a dataset.
  message DefineFlow {
    // The graph to attach this flow to.
    optional string dataflow_graph_id = 1;

    // Name of the flow. For standalone flows, this must be a single-part name.
    optional string flow_name = 2;

    // Name of the dataset this flow writes to. Can be partially or fully qualified.
    optional string target_dataset_name = 3;

    // An unresolved relation that defines the dataset's flow. Empty if the query function
    // that defines the flow cannot be analyzed at the time of flow definition.
    optional spark.connect.Relation relation = 4;

    // SQL configurations set when running this flow.
    map<string, string> sql_conf = 5;

    // Identifier for the client making the request. The server uses this to determine what flow
    // evaluation request stream to dispatch evaluation requests to for this flow.
    optional string client_id = 6;

    // The location in source code that this flow was defined.
    optional SourceCodeLocation source_code_location = 7;

    message Response {
      // Fully qualified flow name that uniquely identify a flow in the Dataflow graph.
      optional string flow_name = 1;
    }
  }

  // Resolves all datasets and flows and start a pipeline update. Should be called after all
  // graph elements are registered.
  message StartRun {
    // The graph to start.
    optional string dataflow_graph_id = 1;

    // List of dataset to reset and recompute.
    repeated string full_refresh_selection = 2;

    // Perform a full graph reset and recompute.
    optional bool full_refresh_all = 3;

    // List of dataset to update.
    repeated string refresh_selection = 4;

    // If true, the run will not actually execute any flows, but will only validate the graph and
    // check for any errors. This is useful for testing and validation purposes.
    optional bool dry = 5;

    // storage location for pipeline checkpoints and metadata.
    optional string storage = 6;
  }

  // Parses the SQL file and registers all datasets and flows.
  message DefineSqlGraphElements {
    // The graph to attach this dataset to.
    optional string dataflow_graph_id = 1;

    // The full path to the SQL file. Can be relative or absolute.
    optional string sql_file_path = 2;

    // The contents of the SQL file.
    optional string sql_text = 3;
  }

  // Request to get the stream of query function execution signals for a graph. Responses should
  // be a stream of PipelineQueryFunctionExecutionSignal messages.
  message GetQueryFunctionExecutionSignalStream {
    // The graph to get the query function execution signal stream for.
    optional string dataflow_graph_id = 1;

    // Identifier for the client that is requesting the stream.
    optional string client_id = 2;
  }

  // Request from the client to update the flow function evaluation result
  // for a previously un-analyzed flow.
  message DefineFlowQueryFunctionResult {
    // The fully qualified name of the flow being updated.
    optional string flow_name = 1;

    // The ID of the graph this flow belongs to.
    optional string dataflow_graph_id = 2;

    // An unresolved relation that defines the dataset's flow.
    optional spark.connect.Relation relation = 3;
  }
}

// Dispatch object for pipelines command results.
message PipelineCommandResult {
  oneof result_type {
    CreateDataflowGraphResult create_dataflow_graph_result = 1;
    DefineDatasetResult define_dataset_result = 2;
    DefineFlowResult define_flow_result = 3;
  }
  message CreateDataflowGraphResult {
    // The ID of the created graph.
    optional string dataflow_graph_id = 1;
  }
  message DefineDatasetResult {
    // Resolved identifier of the dataset
    optional ResolvedIdentifier resolved_identifier = 1;
  }
  message DefineFlowResult {
    // Resolved identifier of the flow
    optional ResolvedIdentifier resolved_identifier = 1;
  }
}

// The type of dataset.
enum DatasetType {
  // Safe default value. Should not be used.
  DATASET_TYPE_UNSPECIFIED = 0;
  // A materialized view dataset which is published to the catalog
  MATERIALIZED_VIEW = 1;
  // A table which is published to the catalog
  TABLE = 2;
  // A view which is not published to the catalog
  TEMPORARY_VIEW = 3;
}

// A response containing an event emitted during the run of a pipeline.
message PipelineEventResult {
  PipelineEvent event = 1;
}

message PipelineEvent {
  // The timestamp corresponding to when the event occurred.
  google.protobuf.Timestamp timestamp = 1;
  // The message that should be displayed to users.
  optional string message = 2;
}

// Source code location information associated with a particular dataset or flow.
message SourceCodeLocation {
  // The file that this pipeline source code was defined in.
  optional string file_name = 1;
  // The specific line number that this pipeline source code is located at, if applicable.
  optional int32 line_number = 2;
}

// A signal from the server to the client to execute the query function for one or more flows, and
// to register their results with the server.
message PipelineQueryFunctionExecutionSignal {
  repeated string flow_names = 1;
}
