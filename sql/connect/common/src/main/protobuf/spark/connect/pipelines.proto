/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *    http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

syntax = "proto3";

package spark.connect;

import "google/protobuf/timestamp.proto";
import "spark/connect/relations.proto";
import "spark/connect/types.proto";

option java_multiple_files = true;
option java_package = "org.apache.spark.connect.proto";
option go_package = "internal/generated";

// Dispatch object for pipelines commands. See each individual command for documentation.
message PipelineCommand {
  oneof command_type {
    CreateDataflowGraph create_dataflow_graph = 1;
    DefineDataset define_dataset = 2;
    DefineFlow define_flow = 3;
    DropDataflowGraph drop_dataflow_graph = 4;
    StartRun start_run = 5;
    DefineSqlGraphElements define_sql_graph_elements = 6;
  }

  // Request to create a new dataflow graph.
  message CreateDataflowGraph {
    // The default catalog.
    optional string default_catalog = 1;

    // The default database.
    optional string default_database = 2;

    // SQL configurations for all flows in this graph.
    map<string, string> sql_conf = 5;

    message Response {
      // The ID of the created graph.
      optional string dataflow_graph_id = 1;
    }
  }

  // Drops the graph and stops any running attached flows.
  message DropDataflowGraph {
    // The graph to drop.
    optional string dataflow_graph_id = 1;
  }

  // Request to define a dataset: a table, a materialized view, or a temporary view.
  message DefineDataset {
    // The graph to attach this dataset to.
    optional string dataflow_graph_id = 1;

    // Name of the dataset. Can be partially or fully qualified.
    optional string dataset_name = 2;

    // The type of the dataset.
    optional DatasetType dataset_type = 3;

    // Optional comment for the dataset.
    optional string comment = 4;

    // Optional table properties. Only applies to dataset_type == TABLE and dataset_type == MATERIALIZED_VIEW.
    map<string, string> table_properties = 5;

    // Optional partition columns for the dataset. Only applies to dataset_type == TABLE and
    // dataset_type == MATERIALIZED_VIEW.
    repeated string partition_cols = 6;

    // Schema for the dataset. If unset, this will be inferred from incoming flows.
    optional spark.connect.DataType schema = 7;

    // The output table format of the dataset. Only applies to dataset_type == TABLE and
    // dataset_type == MATERIALIZED_VIEW.
    optional string format = 8;
  }

  // Request to define a flow targeting a dataset.
  message DefineFlow {
    // The graph to attach this flow to.
    optional string dataflow_graph_id = 1;

    // Name of the flow. For standalone flows, this must be a single-part name.
    optional string flow_name = 2;

    // Name of the dataset this flow writes to. Can be partially or fully qualified.
    optional string target_dataset_name = 3;

    // An unresolved relation that defines the dataset's flow.
    optional spark.connect.Relation relation = 4;

    // SQL configurations set when running this flow.
    map<string, string> sql_conf = 5;

    // If true, this flow will only be run once per full refresh.
    optional bool once = 6;
  }

  // Resolves all datasets and flows and start a pipeline update. Should be called after all
  // graph elements are registered.
  message StartRun {
    // The graph to start.
    optional string dataflow_graph_id = 1;

    // List of dataset to reset and recompute.
    repeated string full_refresh_selection = 2;
    
    // Perform a full graph reset and recompute.
    optional bool full_refresh_all = 3;
    
    // List of dataset to update.
    repeated string refresh_selection = 4;

    // If true, the run will not actually execute any flows, but will only validate the graph and
    // check for any errors. This is useful for testing and validation purposes.
    optional bool dry = 5;
  }

  // Parses the SQL file and registers all datasets and flows.
  message DefineSqlGraphElements {
    // The graph to attach this dataset to.
    optional string dataflow_graph_id = 1;

    // The full path to the SQL file. Can be relative or absolute.
    optional string sql_file_path = 2;

    // The contents of the SQL file.
    optional string sql_text = 3;
  }
}


// Dispatch object for pipelines command results.
message PipelineCommandResult {
  oneof result_type {
    CreateDataflowGraphResult create_dataflow_graph_result = 1;
  }
  message CreateDataflowGraphResult {
    // The ID of the created graph.
    optional string dataflow_graph_id = 1;
  }
}

// The type of dataset.
enum DatasetType {
  // Safe default value. Should not be used.
  DATASET_TYPE_UNSPECIFIED = 0;
  // A materialized view dataset which is published to the catalog
  MATERIALIZED_VIEW = 1;
  // A table which is published to the catalog
  TABLE = 2;
  // A view which is not published to the catalog
  TEMPORARY_VIEW = 3;
}

// A response containing an event emitted during the run of a pipeline.
message PipelineEventResult {
  PipelineEvent event = 1;
}

message PipelineEvent {
  // The timestamp corresponding to when the event occurred.
  google.protobuf.Timestamp timestamp = 1;
  // The message that should be displayed to users.
  optional string message = 2;
}
