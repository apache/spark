/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *    http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

package org.apache.spark.ml.ensemble

import scala.util.Random

import org.apache.spark.Logging
import org.apache.spark.annotation.Experimental
import org.apache.spark.ml._
import org.apache.spark.ml.param._
import org.apache.spark.ml.param.shared.HasSeed
import org.apache.spark.ml.util.Identifiable
import org.apache.spark.sql.{DataFrame, Row}
import org.apache.spark.sql.functions.{col, udf}
import org.apache.spark.sql.types.{LongType, StructField, StructType}

/**
 * Params for [[Bagging]] and [[BaggingModel]].
 */
private[ml] trait BaggingParams[FeatureType, M <: PredictionModel[FeatureType, M]]
  extends PredictorParams with HasSeed {
  /**
   * Param for indicating whether bagged model is a classifier (true) or regressor (false).
   * This parameter affects how models are aggregated: voting is used for classification (with ties
   * broken arbitrarily) and averaging is used for regression.
   * Default: true (classification)
   * @group param
   */
  val isClassifier: BooleanParam = new BooleanParam(this, "isClassification",
    "indicates if bagged model is a classifier or regressor")

  /** @group getParam */
  def getIsClassifier: Boolean = $(isClassifier)

  /**
   * Param for number of bootstraped models.
   * Default: 3
   * @group param
   */
  val numModels: IntParam = new IntParam(this, "numModels",
    "number of models to train on bootstrapped samples (>=1)", ParamValidators.gtEq(1))

  /** @group getParam */
  def getNumModels: Int = $(numModels)

  setDefault(numModels-> 3, isClassifier->true)
}

/**
 * :: Experimental ::
 * Trains an ensemble of models using bootstrap aggregation. Given a dataset with N points,
 * the traditional bootstrap sample consists of N points sampled with replacement from the original
 * dataset. This class generates `numModels` bootstrap samples and uses `estimator` to train a model
 * on each sample. The predictions generated by the trained models are then aggregated to generate
 * the ensemble prediction.
 */
@Experimental
class Bagging[
    FeatureType,
    Learner <: Predictor[FeatureType, Learner, M],
    M <: PredictionModel[FeatureType, M]](override val uid: String)
  extends Estimator[BaggingModel[FeatureType, M]]
  with BaggingParams[FeatureType, M] with Logging {

  def this() = this(Identifiable.randomUID("bagging"))

  /** @group setParam */
  def setSeed(value: Long): this.type = set(seed, value)

  /** @group setParam */
  def setNumModels(value: Int): this.type = set(numModels, value)

  /** @group setParam */
  def setIsClassifier(value: Boolean): this.type = set(isClassifier, value)

  /** @group setParam */
  def setFeaturesCol(value: String): this.type = set(featuresCol, value)

  /** @group setParam */
  def setLabelCol(value: String): this.type = set(labelCol, value)

  /** @group setParam */
  def setPredictionCol(value: String): this.type = set(predictionCol, value)

  /**
   * Param for the [[predictor]] to be validated.
   * @group param
   */
  val predictor: Param[Predictor[FeatureType, Learner, M]] =
    new Param(this, "estimator", "estimator for bagging")

  /** @group getParam */
  def getPredictor: Predictor[FeatureType, Learner, M] = $(predictor)

  /** @group setParam */
  def setPredictor(value: Predictor[FeatureType, Learner, M]): this.type = set(predictor, value)

  override def fit(dataset: DataFrame): BaggingModel[FeatureType, M] = {
    Random.setSeed($(seed))
    val models = (0 until $(numModels)).map { _ =>
      val bootstrapSample = dataset.sample(true, 1.0, Random.nextLong())
      val x = $(predictor)
      $(predictor)
        .setFeaturesCol($(featuresCol))
        .setPredictionCol($(predictionCol))
        .setLabelCol($(labelCol))
        .fit(bootstrapSample)
    }
    copyValues(new BaggingModel[FeatureType, M](uid, models).setParent(this))
  }

  override def transformSchema(schema: StructType): StructType = {
    $(predictor).transformSchema(schema)
  }

  override def copy(extra: ParamMap): Bagging[FeatureType, Learner, M] = {
    val copied = defaultCopy(extra).asInstanceOf[Bagging[FeatureType, Learner, M]]
    if (copied.isDefined(predictor)) {
      copied.setPredictor(copied.getPredictor.copy(extra))
    }
    copied
  }
}

/**
 * :: Experimental ::
 * Model from bootstrap aggregating (bagging).
 *
 * TODO: type-safe way to ensure models has at least one
 */
@Experimental
class BaggingModel[FeatureType, M <: PredictionModel[FeatureType, M]] private[ml] (
    override val uid: String,
    val models: Seq[M])
  extends Model[BaggingModel[FeatureType, M]]
  with BaggingParams[FeatureType, M] {

  assert(models.size > 0,
    s"BaggingModel requires > 0 models to aggregate over, got ${models.size}")

  /** @group setParam */
  def setFeaturesCol(value: String): this.type = set(featuresCol, value)

  /** @group setParam */
  def setPredictionCol(value: String): this.type = set(predictionCol, value)

  override def transform(dataset: DataFrame): DataFrame = {
    transformSchema(dataset.schema, logging = true)

    // these are constant across models since the estimator unchanged
    val predictionColName = $(predictionCol)
    val instanceIdColName = "instanceId"
    val modelIdColName = "modelId"
    val predictionCountsColName = "predCounts"

    val numberedDataset = Bagging.dfZipWithIndex(dataset, colName = instanceIdColName)
    val predictions = models.zipWithIndex.map { case (model, modelId: Int) =>
      val toModelId = udf { (x: Any) => modelId }
      // cast is required because of erasure
      model.asInstanceOf[M]
        .setFeaturesCol($(featuresCol))
        .setPredictionCol($(predictionCol))
        .transform(numberedDataset)
        .withColumn(modelIdColName, toModelId(col(instanceIdColName)))
    }.reduce[DataFrame] { case (a: DataFrame, b: DataFrame) =>
      a.unionAll(b)
    }
    val aggregatedPrediction = if (this.getIsClassifier) { // aggregate by voting
      // counts number of models voting for each (instance, prediction) pair
      val predictionCounts = predictions
        .groupBy(instanceIdColName, predictionColName)
        .agg(modelIdColName -> "count")
        .withColumnRenamed("count(" + modelIdColName + ")", predictionCountsColName)

      // gets the counts for the most predicted prediction
      val maxPredictionCounts = predictionCounts
        .groupBy(instanceIdColName)
        .agg(predictionCountsColName -> "max")
        .withColumnRenamed("max(" + predictionCountsColName + ")", predictionCountsColName)

      // join and project to recover actual prediction
      maxPredictionCounts
        .join(predictionCounts, Seq(instanceIdColName, predictionCountsColName))
        .drop(predictionCountsColName)
    } else { // aggregate by averaging
      predictions.groupBy(instanceIdColName)
        .agg(predictionColName -> "avg")
        .withColumnRenamed("avg(" + predictionColName + ")", predictionColName)
    }

    numberedDataset.join(aggregatedPrediction, instanceIdColName).drop(instanceIdColName)
  }

  override def transformSchema(schema: StructType): StructType = {
    models.head.transformSchema(schema)
  }

  override def copy(extra: ParamMap): BaggingModel[FeatureType, M] = {
    val copied = new BaggingModel[FeatureType, M](
      uid,
      models.map(_.copy(extra)))
    copyValues(copied, extra).setParent(parent)
  }
}

private object Bagging {
  def dfZipWithIndex(
      df: DataFrame,
      offset: Int = 1,
      colName: String = "id",
      inFront: Boolean = true) : DataFrame = {
    df.sqlContext.createDataFrame(
      df.rdd.zipWithIndex.map(ln =>
        Row.fromSeq(
          (if (inFront) Seq(ln._2 + offset) else Seq())
            ++ ln._1.toSeq ++
            (if (inFront) Seq() else Seq(ln._2 + offset))
        )
      ),
      StructType(
        (if (inFront) Array(StructField(colName, LongType, false)) else Array[StructField]()) ++
          df.schema.fields ++
          (if (inFront) Array[StructField]() else Array(StructField(colName, LongType, false)))
      )
    )
  }
}
