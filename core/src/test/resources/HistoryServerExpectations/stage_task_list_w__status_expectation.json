[ {
  "taskId" : 1,
  "index" : 1,
  "attempt" : 0,
  "partitionId" : -1,
  "launchTime" : "2016-11-15T23:20:44.052GMT",
  "duration" : 675,
  "executorId" : "0",
  "host" : "172.22.0.111",
  "status" : "FAILED",
  "taskLocality" : "PROCESS_LOCAL",
  "speculative" : false,
  "accumulatorUpdates" : [ ],
  "errorMessage" : "java.lang.RuntimeException: bad exec\n\tat $line16.$read$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$anonfun$1.apply$mcII$sp(<console>:26)\n\tat $line16.$read$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$anonfun$1.apply(<console>:26)\n\tat $line16.$read$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$anonfun$1.apply(<console>:26)\n\tat scala.collection.Iterator$$anon$11.next(Iterator.scala:409)\n\tat org.apache.spark.util.Utils$.getIteratorSize(Utils.scala:1757)\n\tat org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1135)\n\tat org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1135)\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1927)\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1927)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:99)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:282)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n\tat java.lang.Thread.run(Thread.java:745)\n",
  "taskMetrics" : {
    "executorDeserializeTime" : 0,
    "executorDeserializeCpuTime" : 0,
    "executorRunTime" : 494,
    "executorCpuTime" : 0,
    "resultSize" : 0,
    "jvmGcTime" : 30,
    "resultSerializationTime" : 0,
    "memoryBytesSpilled" : 0,
    "diskBytesSpilled" : 0,
    "peakExecutionMemory" : 0,
    "inputMetrics" : {
      "bytesRead" : 0,
      "recordsRead" : 0
    },
    "outputMetrics" : {
      "bytesWritten" : 0,
      "recordsWritten" : 0
    },
    "shuffleReadMetrics" : {
      "remoteBlocksFetched" : 0,
      "localBlocksFetched" : 0,
      "fetchWaitTime" : 0,
      "remoteBytesRead" : 0,
      "remoteBytesReadToDisk" : 0,
      "localBytesRead" : 0,
      "recordsRead" : 0,
      "remoteReqsDuration" : 0,
      "shufflePushReadMetrics" : {
        "corruptMergedBlockChunks": 0,
        "mergedFetchFallbackCount": 0,
        "remoteMergedBlocksFetched" : 0,
        "localMergedBlocksFetched" :  0,
        "remoteMergedChunksFetched": 0,
        "localMergedChunksFetched": 0,
        "remoteMergedBytesRead" : 0,
        "localMergedBytesRead" : 0,
        "remoteMergedReqsDuration" : 0
      }
    },
    "shuffleWriteMetrics" : {
      "bytesWritten" : 0,
      "writeTime" : 0,
      "recordsWritten" : 0
    }
  },
  "executorLogs" : {
    "stdout" : "http://172.22.0.111:64517/logPage/?appId=app-20161115172038-0000&executorId=0&logType=stdout",
    "stderr" : "http://172.22.0.111:64517/logPage/?appId=app-20161115172038-0000&executorId=0&logType=stderr"
  },
  "schedulerDelay" : 181,
  "gettingResultTime" : 0
}, {
  "taskId" : 3,
  "index" : 3,
  "attempt" : 0,
  "partitionId" : -1,
  "launchTime" : "2016-11-15T23:20:44.053GMT",
  "duration" : 725,
  "executorId" : "2",
  "host" : "172.22.0.111",
  "status" : "FAILED",
  "taskLocality" : "PROCESS_LOCAL",
  "speculative" : false,
  "accumulatorUpdates" : [ ],
  "errorMessage" : "java.lang.RuntimeException: bad exec\n\tat $line16.$read$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$anonfun$1.apply$mcII$sp(<console>:26)\n\tat $line16.$read$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$anonfun$1.apply(<console>:26)\n\tat $line16.$read$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$anonfun$1.apply(<console>:26)\n\tat scala.collection.Iterator$$anon$11.next(Iterator.scala:409)\n\tat org.apache.spark.util.Utils$.getIteratorSize(Utils.scala:1757)\n\tat org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1135)\n\tat org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1135)\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1927)\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1927)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:99)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:282)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n\tat java.lang.Thread.run(Thread.java:745)\n",
  "taskMetrics" : {
    "executorDeserializeTime" : 0,
    "executorDeserializeCpuTime" : 0,
    "executorRunTime" : 456,
    "executorCpuTime" : 0,
    "resultSize" : 0,
    "jvmGcTime" : 32,
    "resultSerializationTime" : 0,
    "memoryBytesSpilled" : 0,
    "diskBytesSpilled" : 0,
    "peakExecutionMemory" : 0,
    "inputMetrics" : {
      "bytesRead" : 0,
      "recordsRead" : 0
    },
    "outputMetrics" : {
      "bytesWritten" : 0,
      "recordsWritten" : 0
    },
    "shuffleReadMetrics" : {
      "remoteBlocksFetched" : 0,
      "localBlocksFetched" : 0,
      "fetchWaitTime" : 0,
      "remoteBytesRead" : 0,
      "remoteBytesReadToDisk" : 0,
      "localBytesRead" : 0,
      "recordsRead" : 0,
      "remoteReqsDuration" : 0,
      "shufflePushReadMetrics" : {
        "corruptMergedBlockChunks": 0,
        "mergedFetchFallbackCount": 0,
        "remoteMergedBlocksFetched" : 0,
        "localMergedBlocksFetched" :  0,
        "remoteMergedChunksFetched": 0,
        "localMergedChunksFetched": 0,
        "remoteMergedBytesRead" : 0,
        "localMergedBytesRead" : 0,
        "remoteMergedReqsDuration" : 0
      }
    },
    "shuffleWriteMetrics" : {
      "bytesWritten" : 0,
      "writeTime" : 0,
      "recordsWritten" : 0
    }
  },
  "executorLogs" : {
    "stdout" : "http://172.22.0.111:64519/logPage/?appId=app-20161115172038-0000&executorId=2&logType=stdout",
    "stderr" : "http://172.22.0.111:64519/logPage/?appId=app-20161115172038-0000&executorId=2&logType=stderr"
  },
  "schedulerDelay" : 269,
  "gettingResultTime" : 0
}, {
  "taskId" : 5,
  "index" : 5,
  "attempt" : 0,
  "partitionId" : -1,
  "launchTime" : "2016-11-15T23:20:44.055GMT",
  "duration" : 665,
  "executorId" : "0",
  "host" : "172.22.0.111",
  "status" : "FAILED",
  "taskLocality" : "PROCESS_LOCAL",
  "speculative" : false,
  "accumulatorUpdates" : [ ],
  "errorMessage" : "java.lang.RuntimeException: bad exec\n\tat $line16.$read$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$anonfun$1.apply$mcII$sp(<console>:26)\n\tat $line16.$read$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$anonfun$1.apply(<console>:26)\n\tat $line16.$read$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$anonfun$1.apply(<console>:26)\n\tat scala.collection.Iterator$$anon$11.next(Iterator.scala:409)\n\tat org.apache.spark.util.Utils$.getIteratorSize(Utils.scala:1757)\n\tat org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1135)\n\tat org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1135)\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1927)\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1927)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:99)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:282)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n\tat java.lang.Thread.run(Thread.java:745)\n",
  "taskMetrics" : {
    "executorDeserializeTime" : 0,
    "executorDeserializeCpuTime" : 0,
    "executorRunTime" : 495,
    "executorCpuTime" : 0,
    "resultSize" : 0,
    "jvmGcTime" : 30,
    "resultSerializationTime" : 0,
    "memoryBytesSpilled" : 0,
    "diskBytesSpilled" : 0,
    "peakExecutionMemory" : 0,
    "inputMetrics" : {
      "bytesRead" : 0,
      "recordsRead" : 0
    },
    "outputMetrics" : {
      "bytesWritten" : 0,
      "recordsWritten" : 0
    },
    "shuffleReadMetrics" : {
      "remoteBlocksFetched" : 0,
      "localBlocksFetched" : 0,
      "fetchWaitTime" : 0,
      "remoteBytesRead" : 0,
      "remoteBytesReadToDisk" : 0,
      "localBytesRead" : 0,
      "recordsRead" : 0,
      "remoteReqsDuration" : 0,
      "shufflePushReadMetrics" : {
        "corruptMergedBlockChunks": 0,
        "mergedFetchFallbackCount": 0,
        "remoteMergedBlocksFetched" : 0,
        "localMergedBlocksFetched" :  0,
        "remoteMergedChunksFetched": 0,
        "localMergedChunksFetched": 0,
        "remoteMergedBytesRead" : 0,
        "localMergedBytesRead" : 0,
        "remoteMergedReqsDuration" : 0
      }
    },
    "shuffleWriteMetrics" : {
      "bytesWritten" : 0,
      "writeTime" : 0,
      "recordsWritten" : 0
    }
  },
  "executorLogs" : {
    "stdout" : "http://172.22.0.111:64517/logPage/?appId=app-20161115172038-0000&executorId=0&logType=stdout",
    "stderr" : "http://172.22.0.111:64517/logPage/?appId=app-20161115172038-0000&executorId=0&logType=stderr"
  },
  "schedulerDelay" : 170,
  "gettingResultTime" : 0
}, {
  "taskId" : 7,
  "index" : 7,
  "attempt" : 0,
  "partitionId" : -1,
  "launchTime" : "2016-11-15T23:20:44.056GMT",
  "duration" : 685,
  "executorId" : "2",
  "host" : "172.22.0.111",
  "status" : "FAILED",
  "taskLocality" : "PROCESS_LOCAL",
  "speculative" : false,
  "accumulatorUpdates" : [ ],
  "errorMessage" : "java.lang.RuntimeException: bad exec\n\tat $line16.$read$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$anonfun$1.apply$mcII$sp(<console>:26)\n\tat $line16.$read$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$anonfun$1.apply(<console>:26)\n\tat $line16.$read$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$anonfun$1.apply(<console>:26)\n\tat scala.collection.Iterator$$anon$11.next(Iterator.scala:409)\n\tat org.apache.spark.util.Utils$.getIteratorSize(Utils.scala:1757)\n\tat org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1135)\n\tat org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1135)\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1927)\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1927)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:99)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:282)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n\tat java.lang.Thread.run(Thread.java:745)\n",
  "taskMetrics" : {
    "executorDeserializeTime" : 0,
    "executorDeserializeCpuTime" : 0,
    "executorRunTime" : 448,
    "executorCpuTime" : 0,
    "resultSize" : 0,
    "jvmGcTime" : 32,
    "resultSerializationTime" : 0,
    "memoryBytesSpilled" : 0,
    "diskBytesSpilled" : 0,
    "peakExecutionMemory" : 0,
    "inputMetrics" : {
      "bytesRead" : 0,
      "recordsRead" : 0
    },
    "outputMetrics" : {
      "bytesWritten" : 0,
      "recordsWritten" : 0
    },
    "shuffleReadMetrics" : {
      "remoteBlocksFetched" : 0,
      "localBlocksFetched" : 0,
      "fetchWaitTime" : 0,
      "remoteBytesRead" : 0,
      "remoteBytesReadToDisk" : 0,
      "localBytesRead" : 0,
      "recordsRead" : 0,
      "remoteReqsDuration" : 0,
      "shufflePushReadMetrics" : {
        "corruptMergedBlockChunks": 0,
        "mergedFetchFallbackCount": 0,
        "remoteMergedBlocksFetched" : 0,
        "localMergedBlocksFetched" :  0,
        "remoteMergedChunksFetched": 0,
        "localMergedChunksFetched": 0,
        "remoteMergedBytesRead" : 0,
        "localMergedBytesRead" : 0,
        "remoteMergedReqsDuration" : 0
      }
    },
    "shuffleWriteMetrics" : {
      "bytesWritten" : 0,
      "writeTime" : 0,
      "recordsWritten" : 0
    }
  },
  "executorLogs" : {
    "stdout" : "http://172.22.0.111:64519/logPage/?appId=app-20161115172038-0000&executorId=2&logType=stdout",
    "stderr" : "http://172.22.0.111:64519/logPage/?appId=app-20161115172038-0000&executorId=2&logType=stderr"
  },
  "schedulerDelay" : 237,
  "gettingResultTime" : 0
}, {
  "taskId" : 9,
  "index" : 9,
  "attempt" : 0,
  "partitionId" : -1,
  "launchTime" : "2016-11-15T23:20:44.057GMT",
  "duration" : 732,
  "executorId" : "0",
  "host" : "172.22.0.111",
  "status" : "FAILED",
  "taskLocality" : "PROCESS_LOCAL",
  "speculative" : false,
  "accumulatorUpdates" : [ ],
  "errorMessage" : "java.lang.RuntimeException: bad exec\n\tat $line16.$read$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$anonfun$1.apply$mcII$sp(<console>:26)\n\tat $line16.$read$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$anonfun$1.apply(<console>:26)\n\tat $line16.$read$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$anonfun$1.apply(<console>:26)\n\tat scala.collection.Iterator$$anon$11.next(Iterator.scala:409)\n\tat org.apache.spark.util.Utils$.getIteratorSize(Utils.scala:1757)\n\tat org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1135)\n\tat org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1135)\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1927)\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1927)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:99)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:282)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n\tat java.lang.Thread.run(Thread.java:745)\n",
  "taskMetrics" : {
    "executorDeserializeTime" : 0,
    "executorDeserializeCpuTime" : 0,
    "executorRunTime" : 503,
    "executorCpuTime" : 0,
    "resultSize" : 0,
    "jvmGcTime" : 30,
    "resultSerializationTime" : 0,
    "memoryBytesSpilled" : 0,
    "diskBytesSpilled" : 0,
    "peakExecutionMemory" : 0,
    "inputMetrics" : {
      "bytesRead" : 0,
      "recordsRead" : 0
    },
    "outputMetrics" : {
      "bytesWritten" : 0,
      "recordsWritten" : 0
    },
    "shuffleReadMetrics" : {
      "remoteBlocksFetched" : 0,
      "localBlocksFetched" : 0,
      "fetchWaitTime" : 0,
      "remoteBytesRead" : 0,
      "remoteBytesReadToDisk" : 0,
      "localBytesRead" : 0,
      "recordsRead" : 0,
      "remoteReqsDuration" : 0,
      "shufflePushReadMetrics" : {
        "corruptMergedBlockChunks": 0,
        "mergedFetchFallbackCount": 0,
        "remoteMergedBlocksFetched" : 0,
        "localMergedBlocksFetched" :  0,
        "remoteMergedChunksFetched": 0,
        "localMergedChunksFetched": 0,
        "remoteMergedBytesRead" : 0,
        "localMergedBytesRead" : 0,
        "remoteMergedReqsDuration" : 0
      }
    },
    "shuffleWriteMetrics" : {
      "bytesWritten" : 0,
      "writeTime" : 0,
      "recordsWritten" : 0
    }
  },
  "executorLogs" : {
    "stdout" : "http://172.22.0.111:64517/logPage/?appId=app-20161115172038-0000&executorId=0&logType=stdout",
    "stderr" : "http://172.22.0.111:64517/logPage/?appId=app-20161115172038-0000&executorId=0&logType=stderr"
  },
  "schedulerDelay" : 229,
  "gettingResultTime" : 0
}, {
  "taskId" : 11,
  "index" : 11,
  "attempt" : 0,
  "partitionId" : -1,
  "launchTime" : "2016-11-15T23:20:44.058GMT",
  "duration" : 678,
  "executorId" : "2",
  "host" : "172.22.0.111",
  "status" : "FAILED",
  "taskLocality" : "PROCESS_LOCAL",
  "speculative" : false,
  "accumulatorUpdates" : [ ],
  "errorMessage" : "java.lang.RuntimeException: bad exec\n\tat $line16.$read$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$anonfun$1.apply$mcII$sp(<console>:26)\n\tat $line16.$read$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$anonfun$1.apply(<console>:26)\n\tat $line16.$read$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$anonfun$1.apply(<console>:26)\n\tat scala.collection.Iterator$$anon$11.next(Iterator.scala:409)\n\tat org.apache.spark.util.Utils$.getIteratorSize(Utils.scala:1757)\n\tat org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1135)\n\tat org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1135)\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1927)\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1927)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:99)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:282)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n\tat java.lang.Thread.run(Thread.java:745)\n",
  "taskMetrics" : {
    "executorDeserializeTime" : 0,
    "executorDeserializeCpuTime" : 0,
    "executorRunTime" : 451,
    "executorCpuTime" : 0,
    "resultSize" : 0,
    "jvmGcTime" : 32,
    "resultSerializationTime" : 0,
    "memoryBytesSpilled" : 0,
    "diskBytesSpilled" : 0,
    "peakExecutionMemory" : 0,
    "inputMetrics" : {
      "bytesRead" : 0,
      "recordsRead" : 0
    },
    "outputMetrics" : {
      "bytesWritten" : 0,
      "recordsWritten" : 0
    },
    "shuffleReadMetrics" : {
      "remoteBlocksFetched" : 0,
      "localBlocksFetched" : 0,
      "fetchWaitTime" : 0,
      "remoteBytesRead" : 0,
      "remoteBytesReadToDisk" : 0,
      "localBytesRead" : 0,
      "recordsRead" : 0,
      "remoteReqsDuration" : 0,
      "shufflePushReadMetrics" : {
        "corruptMergedBlockChunks": 0,
        "mergedFetchFallbackCount": 0,
        "remoteMergedBlocksFetched" : 0,
        "localMergedBlocksFetched" :  0,
        "remoteMergedChunksFetched": 0,
        "localMergedChunksFetched": 0,
        "remoteMergedBytesRead" : 0,
        "localMergedBytesRead" : 0,
        "remoteMergedReqsDuration" : 0
      }
    },
    "shuffleWriteMetrics" : {
      "bytesWritten" : 0,
      "writeTime" : 0,
      "recordsWritten" : 0
    }
  },
  "executorLogs" : {
    "stdout" : "http://172.22.0.111:64519/logPage/?appId=app-20161115172038-0000&executorId=2&logType=stdout",
    "stderr" : "http://172.22.0.111:64519/logPage/?appId=app-20161115172038-0000&executorId=2&logType=stderr"
  },
  "schedulerDelay" : 227,
  "gettingResultTime" : 0
}, {
  "taskId" : 13,
  "index" : 13,
  "attempt" : 0,
  "partitionId" : -1,
  "launchTime" : "2016-11-15T23:20:44.060GMT",
  "duration" : 669,
  "executorId" : "0",
  "host" : "172.22.0.111",
  "status" : "FAILED",
  "taskLocality" : "PROCESS_LOCAL",
  "speculative" : false,
  "accumulatorUpdates" : [ ],
  "errorMessage" : "java.lang.RuntimeException: bad exec\n\tat $line16.$read$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$anonfun$1.apply$mcII$sp(<console>:26)\n\tat $line16.$read$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$anonfun$1.apply(<console>:26)\n\tat $line16.$read$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$anonfun$1.apply(<console>:26)\n\tat scala.collection.Iterator$$anon$11.next(Iterator.scala:409)\n\tat org.apache.spark.util.Utils$.getIteratorSize(Utils.scala:1757)\n\tat org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1135)\n\tat org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1135)\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1927)\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1927)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:99)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:282)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n\tat java.lang.Thread.run(Thread.java:745)\n",
  "taskMetrics" : {
    "executorDeserializeTime" : 0,
    "executorDeserializeCpuTime" : 0,
    "executorRunTime" : 494,
    "executorCpuTime" : 0,
    "resultSize" : 0,
    "jvmGcTime" : 30,
    "resultSerializationTime" : 0,
    "memoryBytesSpilled" : 0,
    "diskBytesSpilled" : 0,
    "peakExecutionMemory" : 0,
    "inputMetrics" : {
      "bytesRead" : 0,
      "recordsRead" : 0
    },
    "outputMetrics" : {
      "bytesWritten" : 0,
      "recordsWritten" : 0
    },
    "shuffleReadMetrics" : {
      "remoteBlocksFetched" : 0,
      "localBlocksFetched" : 0,
      "fetchWaitTime" : 0,
      "remoteBytesRead" : 0,
      "remoteBytesReadToDisk" : 0,
      "localBytesRead" : 0,
      "recordsRead" : 0,
      "remoteReqsDuration" : 0,
      "shufflePushReadMetrics" : {
        "corruptMergedBlockChunks": 0,
        "mergedFetchFallbackCount": 0,
        "remoteMergedBlocksFetched" : 0,
        "localMergedBlocksFetched" :  0,
        "remoteMergedChunksFetched": 0,
        "localMergedChunksFetched": 0,
        "remoteMergedBytesRead" : 0,
        "localMergedBytesRead" : 0,
        "remoteMergedReqsDuration" : 0
      }
    },
    "shuffleWriteMetrics" : {
      "bytesWritten" : 0,
      "writeTime" : 0,
      "recordsWritten" : 0
    }
  },
  "executorLogs" : {
    "stdout" : "http://172.22.0.111:64517/logPage/?appId=app-20161115172038-0000&executorId=0&logType=stdout",
    "stderr" : "http://172.22.0.111:64517/logPage/?appId=app-20161115172038-0000&executorId=0&logType=stderr"
  },
  "schedulerDelay" : 175,
  "gettingResultTime" : 0
}, {
  "taskId" : 15,
  "index" : 15,
  "attempt" : 0,
  "partitionId" : -1,
  "launchTime" : "2016-11-15T23:20:44.065GMT",
  "duration" : 672,
  "executorId" : "2",
  "host" : "172.22.0.111",
  "status" : "FAILED",
  "taskLocality" : "PROCESS_LOCAL",
  "speculative" : false,
  "accumulatorUpdates" : [ ],
  "errorMessage" : "java.lang.RuntimeException: bad exec\n\tat $line16.$read$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$anonfun$1.apply$mcII$sp(<console>:26)\n\tat $line16.$read$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$anonfun$1.apply(<console>:26)\n\tat $line16.$read$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$anonfun$1.apply(<console>:26)\n\tat scala.collection.Iterator$$anon$11.next(Iterator.scala:409)\n\tat org.apache.spark.util.Utils$.getIteratorSize(Utils.scala:1757)\n\tat org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1135)\n\tat org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1135)\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1927)\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1927)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:99)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:282)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n\tat java.lang.Thread.run(Thread.java:745)\n",
  "taskMetrics" : {
    "executorDeserializeTime" : 0,
    "executorDeserializeCpuTime" : 0,
    "executorRunTime" : 446,
    "executorCpuTime" : 0,
    "resultSize" : 0,
    "jvmGcTime" : 32,
    "resultSerializationTime" : 0,
    "memoryBytesSpilled" : 0,
    "diskBytesSpilled" : 0,
    "peakExecutionMemory" : 0,
    "inputMetrics" : {
      "bytesRead" : 0,
      "recordsRead" : 0
    },
    "outputMetrics" : {
      "bytesWritten" : 0,
      "recordsWritten" : 0
    },
    "shuffleReadMetrics" : {
      "remoteBlocksFetched" : 0,
      "localBlocksFetched" : 0,
      "fetchWaitTime" : 0,
      "remoteBytesRead" : 0,
      "remoteBytesReadToDisk" : 0,
      "localBytesRead" : 0,
      "recordsRead" : 0,
      "remoteReqsDuration" : 0,
      "shufflePushReadMetrics" : {
        "corruptMergedBlockChunks": 0,
        "mergedFetchFallbackCount": 0,
        "remoteMergedBlocksFetched" : 0,
        "localMergedBlocksFetched" :  0,
        "remoteMergedChunksFetched": 0,
        "localMergedChunksFetched": 0,
        "remoteMergedBytesRead" : 0,
        "localMergedBytesRead" : 0,
        "remoteMergedReqsDuration" : 0
      }
    },
    "shuffleWriteMetrics" : {
      "bytesWritten" : 0,
      "writeTime" : 0,
      "recordsWritten" : 0
    }
  },
  "executorLogs" : {
    "stdout" : "http://172.22.0.111:64519/logPage/?appId=app-20161115172038-0000&executorId=2&logType=stdout",
    "stderr" : "http://172.22.0.111:64519/logPage/?appId=app-20161115172038-0000&executorId=2&logType=stderr"
  },
  "schedulerDelay" : 226,
  "gettingResultTime" : 0
}, {
  "taskId" : 19,
  "index" : 11,
  "attempt" : 1,
  "partitionId": -1,
  "launchTime" : "2016-11-15T23:20:44.736GMT",
  "duration" : 13,
  "executorId" : "2",
  "host" : "172.22.0.111",
  "status" : "FAILED",
  "taskLocality" : "PROCESS_LOCAL",
  "speculative" : false,
  "accumulatorUpdates" : [ ],
  "errorMessage" : "java.lang.RuntimeException: bad exec\n\tat $line16.$read$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$anonfun$1.apply$mcII$sp(<console>:26)\n\tat $line16.$read$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$anonfun$1.apply(<console>:26)\n\tat $line16.$read$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$anonfun$1.apply(<console>:26)\n\tat scala.collection.Iterator$$anon$11.next(Iterator.scala:409)\n\tat org.apache.spark.util.Utils$.getIteratorSize(Utils.scala:1757)\n\tat org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1135)\n\tat org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1135)\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1927)\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1927)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:99)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:282)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n\tat java.lang.Thread.run(Thread.java:745)\n",
  "taskMetrics" : {
    "executorDeserializeTime" : 0,
    "executorDeserializeCpuTime" : 0,
    "executorRunTime" : 2,
    "executorCpuTime" : 0,
    "resultSize" : 0,
    "jvmGcTime" : 0,
    "resultSerializationTime" : 0,
    "memoryBytesSpilled" : 0,
    "diskBytesSpilled" : 0,
    "peakExecutionMemory" : 0,
    "inputMetrics" : {
      "bytesRead" : 0,
      "recordsRead" : 0
    },
    "outputMetrics" : {
      "bytesWritten" : 0,
      "recordsWritten" : 0
    },
    "shuffleReadMetrics" : {
      "remoteBlocksFetched" : 0,
      "localBlocksFetched" : 0,
      "fetchWaitTime" : 0,
      "remoteBytesRead" : 0,
      "remoteBytesReadToDisk" : 0,
      "localBytesRead" : 0,
      "recordsRead" : 0,
      "remoteReqsDuration" : 0,
      "shufflePushReadMetrics" : {
        "corruptMergedBlockChunks": 0,
        "mergedFetchFallbackCount": 0,
        "remoteMergedBlocksFetched" : 0,
        "localMergedBlocksFetched" :  0,
        "remoteMergedChunksFetched": 0,
        "localMergedChunksFetched": 0,
        "remoteMergedBytesRead" : 0,
        "localMergedBytesRead" : 0,
        "remoteMergedReqsDuration" : 0
      }
    },
    "shuffleWriteMetrics" : {
      "bytesWritten" : 0,
      "writeTime" : 0,
      "recordsWritten" : 0
    }
  },
  "executorLogs" : {
    "stdout" : "http://172.22.0.111:64519/logPage/?appId=app-20161115172038-0000&executorId=2&logType=stdout",
    "stderr" : "http://172.22.0.111:64519/logPage/?appId=app-20161115172038-0000&executorId=2&logType=stderr"
  },
  "schedulerDelay" : 11,
  "gettingResultTime" : 0
}, {
  "taskId" : 20,
  "index" : 15,
  "attempt" : 1,
  "partitionId": -1,
  "launchTime" : "2016-11-15T23:20:44.737GMT",
  "duration" : 19,
  "executorId" : "2",
  "host" : "172.22.0.111",
  "status" : "FAILED",
  "taskLocality" : "PROCESS_LOCAL",
  "speculative" : false,
  "accumulatorUpdates" : [ ],
  "errorMessage" : "java.lang.RuntimeException: bad exec\n\tat $line16.$read$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$anonfun$1.apply$mcII$sp(<console>:26)\n\tat $line16.$read$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$anonfun$1.apply(<console>:26)\n\tat $line16.$read$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$anonfun$1.apply(<console>:26)\n\tat scala.collection.Iterator$$anon$11.next(Iterator.scala:409)\n\tat org.apache.spark.util.Utils$.getIteratorSize(Utils.scala:1757)\n\tat org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1135)\n\tat org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1135)\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1927)\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1927)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:99)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:282)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n\tat java.lang.Thread.run(Thread.java:745)\n",
  "taskMetrics" : {
    "executorDeserializeTime" : 0,
    "executorDeserializeCpuTime" : 0,
    "executorRunTime" : 10,
    "executorCpuTime" : 0,
    "resultSize" : 0,
    "jvmGcTime" : 0,
    "resultSerializationTime" : 0,
    "memoryBytesSpilled" : 0,
    "diskBytesSpilled" : 0,
    "peakExecutionMemory" : 0,
    "inputMetrics" : {
      "bytesRead" : 0,
      "recordsRead" : 0
    },
    "outputMetrics" : {
      "bytesWritten" : 0,
      "recordsWritten" : 0
    },
    "shuffleReadMetrics" : {
      "remoteBlocksFetched" : 0,
      "localBlocksFetched" : 0,
      "fetchWaitTime" : 0,
      "remoteBytesRead" : 0,
      "remoteBytesReadToDisk" : 0,
      "localBytesRead" : 0,
      "recordsRead" : 0,
      "remoteReqsDuration" : 0,
      "shufflePushReadMetrics" : {
        "corruptMergedBlockChunks": 0,
        "mergedFetchFallbackCount": 0,
        "remoteMergedBlocksFetched" : 0,
        "localMergedBlocksFetched" :  0,
        "remoteMergedChunksFetched": 0,
        "localMergedChunksFetched": 0,
        "remoteMergedBytesRead" : 0,
        "localMergedBytesRead" : 0,
        "remoteMergedReqsDuration" : 0
      }
    },
    "shuffleWriteMetrics" : {
      "bytesWritten" : 0,
      "writeTime" : 0,
      "recordsWritten" : 0
    }
  },
  "executorLogs" : {
    "stdout" : "http://172.22.0.111:64519/logPage/?appId=app-20161115172038-0000&executorId=2&logType=stdout",
    "stderr" : "http://172.22.0.111:64519/logPage/?appId=app-20161115172038-0000&executorId=2&logType=stderr"
  },
  "schedulerDelay" : 9,
  "gettingResultTime" : 0
} ]
