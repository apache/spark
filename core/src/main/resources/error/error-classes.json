{
  "AMBIGUOUS_FIELD_NAME" : {
    "message" : [ "Field name %s is ambiguous and has %s matching fields in the struct." ],
    "sqlState" : "42000"
  },
  "CANNOT_CAST_DATATYPE" : {
    "message" : [ "Cannot cast %s to %s." ],
    "sqlState" : "22005"
  },
  "CANNOT_CHANGE_DECIMAL_PRECISION" : {
    "message" : [ "%s cannot be represented as Decimal(%s, %s)." ],
    "sqlState" : "22005"
  },
  "CANNOT_CLEAR_DIRECTORY" : {
    "message" : [ "Unable to clear %s directory %s prior to writing to it" ]
  },
  "CANNOT_DROP_NONEMPTY_NAMESPACE" : {
    "message" : [ "Cannot drop a non-empty namespace: %s. Use CASCADE option to drop a non-empty namespace." ],
    "sqlState" : "42000"
  },
  "CANNOT_FIND_CLASS_IN_SPARK2" : {
    "message" : [ "%s was removed in Spark 2.0. Please check if your library is compatible with Spark 2.0" ],
    "sqlState" : "0A000"
  },
  "CANNOT_PARSE_DECIMAL" : {
    "message" : [ "Cannot parse decimal" ],
    "sqlState" : "42000"
  },
  "CANNOT_READ_CURRENT_FILE" : {
    "message" : [ "%s", "It is possible the underlying files have been updated. You can explicitly invalidate the cache in Spark by running 'REFRESH TABLE tableName' command in SQL or by recreating the Dataset/DataFrame involved." ]
  },
  "CAST_CAUSES_OVERFLOW" : {
    "message" : [ "Casting %s to %s causes overflow" ],
    "sqlState" : "22005"
  },
  "CONCURRENT_QUERY" : {
    "message" : [ "Another instance of this query was just started by a concurrent session." ]
  },
  "DIVIDE_BY_ZERO" : {
    "message" : [ "divide by zero" ],
    "sqlState" : "22012"
  },
  "DUPLICATE_KEY" : {
    "message" : [ "Found duplicate keys '%s'" ],
    "sqlState" : "23000"
  },
  "END_OF_STREAM" : {
    "message" : [ "End of stream" ]
  },
  "FAILED_CAST_VALUE_TO_DATATYPE_FOR_PARTITION_COLUMN" : {
    "message" : [ "Failed to cast value `%s` to `%s` for partition column `%s`" ],
    "sqlState" : "22005"
  },
  "FAILED_EXECUTE_UDF" : {
    "message" : [ "Failed to execute user defined function (%s: (%s) => %s)" ]
  },
  "FAILED_FIND_DATA_SOURCE" : {
    "message" : [ "Failed to find data source: %s. Please find packages at http://spark.apache.org/third-party-projects.html" ],
    "sqlState" : "22023"
  },
  "FAILED_FORMAT_DATE_TIME_AFTER_UPGRADE" : {
    "message" : [ "Fail to format it to '%s' in the new formatter. You can set %s to LEGACY to restore the behavior before Spark 3.0, or set to CORRECTED and treat it as an invalid datetime string." ]
  },
  "FAILED_PARSE_DATE_TIME_AFTER_UPGRADE" : {
    "message" : [ "Fail to parse '%s' in the new parser. You can set %s to LEGACY to restore the behavior before Spark 3.0, or set to CORRECTED and treat it as an invalid datetime string." ]
  },
  "FAILED_RECOGNIZE_DATE_TIME_PATTERN_AFTER_UPGRADE" : {
    "message" : [ "Fail to recognize '%s' pattern in the DateTimeFormatter. 1) You can set %s to LEGACY to restore the behavior before Spark 3.0. 2) You can form a valid datetime pattern with the guide from https://spark.apache.org/docs/latest/sql-ref-datetime-pattern.html" ]
  },
  "FAILED_RENAME_PATH" : {
    "message" : [ "Failed to rename %s to %s as destination already exists" ],
    "sqlState" : "22023"
  },
  "FAILED_SET_ORIGINAL_PERMISSION_BACK" : {
    "message" : [ "Failed to set original permission %s back to the created path: %s. Exception: %s" ]
  },
  "FAILED_TASK_WHILE_WRITING_ROWS" : {
    "message" : [ "Task failed while writing rows." ]
  },
  "GROUPING_COLUMN_MISMATCH" : {
    "message" : [ "Column of grouping (%s) can't be found in grouping columns %s" ],
    "sqlState" : "42000"
  },
  "GROUPING_ID_COLUMN_MISMATCH" : {
    "message" : [ "Columns of grouping_id (%s) does not match grouping columns (%s)" ],
    "sqlState" : "42000"
  },
  "GROUPING_SIZE_LIMIT_EXCEEDED" : {
    "message" : [ "Grouping sets size cannot be greater than %s" ]
  },
  "IF_PARTITION_NOT_EXISTS_UNSUPPORTED" : {
    "message" : [ "Cannot write, IF NOT EXISTS is not supported for table: %s" ]
  },
  "INCOMPARABLE_PIVOT_COLUMN" : {
    "message" : [ "Invalid pivot column '%s'. Pivot columns must be comparable." ],
    "sqlState" : "42000"
  },
  "INCOMPATIBLE_DATASOURCE_REGISTER" : {
    "message" : [ "Detected an incompatible DataSourceRegister. Please remove the incompatible library from classpath or upgrade it. Error: %s" ]
  },
  "INDEX_OUT_OF_BOUNDS" : {
    "message" : [ "Index %s must be between 0 and the length of the ArrayData." ],
    "sqlState" : "22023"
  },
  "INTERNAL_ERROR" : {
    "message" : [ "%s" ]
  },
  "INVALID_ARRAY_INDEX" : {
    "message" : [ "Invalid index: %s, numElements: %s" ]
  },
  "INVALID_FIELD_NAME" : {
    "message" : [ "Field name %s is invalid: %s is not a struct." ],
    "sqlState" : "42000"
  },
  "INVALID_FRACTION_OF_SECOND" : {
    "message" : [ "The fraction of sec must be zero. Valid range is [0, 60]." ],
    "sqlState" : "22023"
  },
  "INVALID_INPUT_SYNTAX_FOR_NUMERIC_TYPE" : {
    "message" : [ "invalid input syntax for type numeric: %s" ],
    "sqlState" : "42000"
  },
  "INVALID_JSON_SCHEMA_MAPTYPE" : {
    "message" : [ "Input schema %s can only contain StringType as a key type for a MapType." ]
  },
  "JOB_ABORTED" : {
    "message" : [ "Job aborted." ]
  },
  "MAP_KEY_DOES_NOT_EXIST" : {
    "message" : [ "Key %s does not exist." ]
  },
  "MISSING_COLUMN" : {
    "message" : [ "Column '%s' does not exist. Did you mean one of the following? [%s]" ],
    "sqlState" : "42000"
  },
  "MISSING_STATIC_PARTITION_COLUMN" : {
    "message" : [ "Unknown static partition column: %s" ],
    "sqlState" : "42000"
  },
  "MISSING_STREAMING_SOURCE_SCHEMA" : {
    "message" : [ "Schema must be specified when creating a streaming source DataFrame. If some files already exist in the directory, then depending on the file format you may be able to create a static DataFrame on that directory with 'spark.read.load(directory)' and infer schema from it." ],
    "sqlState" : "22023"
  },
  "NON_LITERAL_PIVOT_VALUES" : {
    "message" : [ "Literal expressions required for pivot values, found '%s'" ],
    "sqlState" : "42000"
  },
  "NON_PARTITION_COLUMN" : {
    "message" : [ "PARTITION clause cannot contain a non-partition column name: %s" ],
    "sqlState" : "42000"
  },
  "PIVOT_VALUE_DATA_TYPE_MISMATCH" : {
    "message" : [ "Invalid pivot value '%s': value data type %s does not match pivot column data type %s" ],
    "sqlState" : "42000"
  },
  "READING_AMBIGUOUS_DATES_AFTER_UPGRADE" : {
    "message" : [ "reading dates before 1582-10-15 or timestamps before 1900-01-01T00:00:00Z from %s files can be ambiguous, as the files may be written by Spark 2.x or legacy versions of Hive, which uses a legacy hybrid calendar that is different from Spark 3.0+'s Proleptic Gregorian calendar. See more details in SPARK-31404. You can set the SQL config '%s' or the datasource option '%s' to 'LEGACY' to rebase the datetime values w.r.t. the calendar difference during reading. To read the datetime values as it is, set the SQL config '%s' or the datasource option '%s' to 'CORRECTED'." ]
  },
  "RENAME_SRC_PATH_NOT_FOUND" : {
    "message" : [ "Failed to rename as %s was not found" ],
    "sqlState" : "22023"
  },
  "ROW_FROM_CSV_PARSER_NOT_EXPECTED" : {
    "message" : [ "Expected one row from CSV parser." ],
    "sqlState" : "42000"
  },
  "SECOND_FUNCTION_ARGUMENT_NOT_INTEGER" : {
    "message" : [ "The second argument of '%s' function needs to be an integer." ],
    "sqlState" : "22023"
  },
  "SPECIFIED_MULTIPLE_PATHS" : {
    "message" : [ "Expected exactly one path to be specified, but got: %s" ],
    "sqlState" : "42000"
  },
  "UNABLE_TO_ACQUIRE_MEMORY" : {
    "message" : [ "Unable to acquire %s bytes of memory, got %s" ]
  },
  "UNRECOGNIZED_FORMAT" : {
    "message" : [ "unrecognized format %s" ],
    "sqlState" : "22023"
  },
  "UNRECOGNIZED_SQL_TYPE" : {
    "message" : [ "Unrecognized SQL type %s" ],
    "sqlState" : "42000"
  },
  "UNSUPPORTED_BUILD_READER_FOR_FILE_FORMAT" : {
    "message" : [ "buildReader is not supported for %s" ],
    "sqlState" : "0A000"
  },
  "UNSUPPORTED_CHANGE_COLUMN" : {
    "message" : [ "Please add an implementation for a column change here" ],
    "sqlState" : "0A000"
  },
  "UNSUPPORTED_DATATYPE" : {
    "message" : [ "Unsupported data type %s" ],
    "sqlState" : "0A000"
  },
  "UNSUPPORTED_LITERAL_TYPE" : {
    "message" : [ "Unsupported literal type %s %s" ],
    "sqlState" : "0A000"
  },
  "UNSUPPORTED_SAVE_MODE" : {
    "message" : [ "unsupported save mode %s (%s)" ],
    "sqlState" : "0A000"
  },
  "UNSUPPORTED_SIMPLE_STRING_WITH_NODE_ID" : {
    "message" : [ "%s does not implement simpleStringWithNodeId" ]
  },
  "UNSUPPORTED_STREAMED_OPERATOR_BY_DATA_SOURCE" : {
    "message" : [ "Data source %s does not support streamed %s" ],
    "sqlState" : "0A000"
  },
  "UNSUPPORTED_TRANSACTION_BY_JDBC_SERVER" : {
    "message" : [ "The target JDBC server does not support transaction and can only support ALTER TABLE with a single action." ],
    "sqlState" : "0A000"
  },
  "WRITING_AMBIGUOUS_DATES_AFTER_UPGRADE" : {
    "message" : [ "writing dates before 1582-10-15 or timestamps before 1900-01-01T00:00:00Z into %s files can be dangerous, as the files may be read by Spark 2.x or legacy versions of Hive later, which uses a legacy hybrid calendar that is different from Spark 3.0+'s Proleptic Gregorian calendar. See more details in SPARK-31404. You can set %s to 'LEGACY' to rebase the datetime values w.r.t. the calendar difference during writing, to get maximum interoperability. Or set %s to 'CORRECTED' to write the datetime values as it is, if you are 100%% sure that the written files will only be read by Spark 3.0+ or other systems that use Proleptic Gregorian calendar." ]
  },
  "WRITING_JOB_ABORTED" : {
    "message" : [ "Writing job aborted" ],
    "sqlState" : "40000"
  }
}