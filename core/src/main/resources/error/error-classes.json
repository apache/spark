{
  "AMBIGUOUS_FIELD_NAME" : {
    "message" : [ "Field name %s is ambiguous and has %s matching fields in the struct." ],
    "sqlState" : "42000"
  },
  "ARITHMETIC_OVERFLOW" : {
    "message" : [ "%s.%s If necessary set %s to false (except for ANSI interval type) to bypass this error.%s" ],
    "sqlState" : "22003"
  },
  "CANNOT_CAST_DATATYPE" : {
    "message" : [ "Cannot cast %s to %s." ],
    "sqlState" : "22005"
  },
  "CANNOT_CHANGE_DECIMAL_PRECISION" : {
    "message" : [ "%s cannot be represented as Decimal(%s, %s). If necessary set %s to false to bypass this error.%s" ],
    "sqlState" : "22005"
  },
  "CANNOT_PARSE_DECIMAL" : {
    "message" : [ "Cannot parse decimal" ],
    "sqlState" : "42000"
  },
  "CANNOT_UP_CAST_DATATYPE" : {
    "message" : [ "Cannot up cast %s from %s to %s.\n%s" ]
  },
  "CAST_CAUSES_OVERFLOW" : {
    "message" : [ "Casting %s to %s causes overflow. To return NULL instead, use 'try_cast'. If necessary set %s to false to bypass this error." ],
    "sqlState" : "22005"
  },
  "CONCURRENT_QUERY" : {
    "message" : [ "Another instance of this query was just started by a concurrent session." ]
  },
  "DATETIME_OVERFLOW" : {
    "message" : [ "Datetime operation overflow: %s." ],
    "sqlState" : "22008"
  },
  "DIVIDE_BY_ZERO" : {
    "message" : [ "divide by zero. To return NULL instead, use 'try_divide'. If necessary set %s to false (except for ANSI interval type) to bypass this error.%s" ],
    "sqlState" : "22012"
  },
  "DUPLICATE_KEY" : {
    "message" : [ "Found duplicate keys %s" ],
    "sqlState" : "23000"
  },
  "FAILED_EXECUTE_UDF" : {
    "message" : [ "Failed to execute user defined function (%s: (%s) => %s)" ]
  },
  "FAILED_RENAME_PATH" : {
    "message" : [ "Failed to rename %s to %s as destination already exists" ],
    "sqlState" : "22023"
  },
  "FAILED_SET_ORIGINAL_PERMISSION_BACK" : {
    "message" : [ "Failed to set original permission %s back to the created path: %s. Exception: %s" ]
  },
  "FORBIDDEN_OPERATION" : {
    "message" : [ "The operation %s is not allowed on %s: %s" ]
  },
  "GRAPHITE_SINK_INVALID_PROTOCOL" : {
    "message" : [ "Invalid Graphite protocol: %s" ]
  },
  "GRAPHITE_SINK_PROPERTY_MISSING" : {
    "message" : [ "Graphite sink requires '%s' property." ]
  },
  "GROUPING_COLUMN_MISMATCH" : {
    "message" : [ "Column of grouping (%s) can't be found in grouping columns %s" ],
    "sqlState" : "42000"
  },
  "GROUPING_ID_COLUMN_MISMATCH" : {
    "message" : [ "Columns of grouping_id (%s) does not match grouping columns (%s)" ],
    "sqlState" : "42000"
  },
  "GROUPING_SIZE_LIMIT_EXCEEDED" : {
    "message" : [ "Grouping sets size cannot be greater than %s" ]
  },
  "ILLEGAL_SUBSTRING" : {
    "message" : [ "%s cannot contain %s." ]
  },
  "INCOMPARABLE_PIVOT_COLUMN" : {
    "message" : [ "Invalid pivot column '%s'. Pivot columns must be comparable." ],
    "sqlState" : "42000"
  },
  "INCOMPATIBLE_DATASOURCE_REGISTER" : {
    "message" : [ "Detected an incompatible DataSourceRegister. Please remove the incompatible library from classpath or upgrade it. Error: %s" ]
  },
  "INCONSISTENT_BEHAVIOR_CROSS_VERSION" : {
    "message" : [ "You may get a different result due to the upgrading to Spark >= %s: %s" ]
  },
  "INDEX_OUT_OF_BOUNDS" : {
    "message" : [ "Index %s must be between 0 and the length of the ArrayData." ],
    "sqlState" : "22023"
  },
  "INTERNAL_ERROR" : {
    "message" : [ "%s" ]
  },
  "INVALID_ARRAY_INDEX" : {
    "message" : [ "Invalid index: %s, numElements: %s. If necessary set %s to false to bypass this error." ]
  },
  "INVALID_ARRAY_INDEX_IN_ELEMENT_AT" : {
    "message" : [ "Invalid index: %s, numElements: %s. To return NULL instead, use 'try_element_at'. If necessary set %s to false to bypass this error." ]
  },
  "INVALID_FIELD_NAME" : {
    "message" : [ "Field name %s is invalid: %s is not a struct." ],
    "sqlState" : "42000"
  },
  "INVALID_FRACTION_OF_SECOND" : {
    "message" : [ "The fraction of sec must be zero. Valid range is [0, 60]. If necessary set %s to false to bypass this error. " ],
    "sqlState" : "22023"
  },
  "INVALID_JSON_SCHEMA_MAPTYPE" : {
    "message" : [ "Input schema %s can only contain StringType as a key type for a MapType." ]
  },
  "INVALID_PANDAS_UDF_PLACEMENT" : {
    "message" : [ "The group aggregate pandas UDF %s cannot be invoked together with as other, non-pandas aggregate functions." ]
  },
  "INVALID_PARAMETER_VALUE" : {
    "message" : [ "The value of parameter(s) '%s' in %s is invalid: %s" ],
    "sqlState" : "22023"
  },
  "INVALID_SQL_SYNTAX" : {
    "message" : [ "Invalid SQL syntax: %s" ],
    "sqlState" : "42000"
  },
  "INVALID_SYNTAX_FOR_CAST" : {
    "message" : [ "Invalid input syntax for type %s: %s. To return NULL instead, use 'try_cast'. If necessary set %s to false to bypass this error.%s" ],
    "sqlState" : "42000"
  },
  "MAP_KEY_DOES_NOT_EXIST" : {
    "message" : [ "Key %s does not exist. If necessary set %s to false to bypass this error.%s" ]
  },
  "MAP_KEY_DOES_NOT_EXIST_IN_ELEMENT_AT" : {
    "message" : [ "Key %s does not exist. To return NULL instead, use 'try_element_at'. If necessary set %s to false to bypass this error.%s" ]
  },
  "MISSING_COLUMN" : {
    "message" : [ "Column '%s' does not exist. Did you mean one of the following? [%s]" ],
    "sqlState" : "42000"
  },
  "MISSING_STATIC_PARTITION_COLUMN" : {
    "message" : [ "Unknown static partition column: %s" ],
    "sqlState" : "42000"
  },
  "MULTI_UDF_INTERFACE_ERROR" : {
    "message" : [ "Not allowed to implement multiple UDF interfaces, UDF class %s" ]
  },
  "NON_LITERAL_PIVOT_VALUES" : {
    "message" : [ "Literal expressions required for pivot values, found '%s'" ],
    "sqlState" : "42000"
  },
  "NON_PARTITION_COLUMN" : {
    "message" : [ "PARTITION clause cannot contain a non-partition column name: %s" ],
    "sqlState" : "42000"
  },
  "NO_HANDLER_FOR_UDAF" : {
    "message" : [ "No handler for UDAF '%s'. Use sparkSession.udf.register(...) instead." ]
  },
  "NO_UDF_INTERFACE_ERROR" : {
    "message" : [ "UDF class %s doesn't implement any UDF interface" ]
  },
  "PARSE_CHAR_MISSING_LENGTH" : {
    "message" : [ "DataType %s requires a length parameter, for example %s(10). Please specify the length." ],
    "sqlState" : "42000"
  },
  "PARSE_EMPTY_STATEMENT" : {
    "message" : [ "Syntax error, unexpected empty statement" ],
    "sqlState" : "42000"
  },
  "PARSE_SYNTAX_ERROR" : {
    "message" : [ "Syntax error at or near %s%s" ],
    "sqlState" : "42000"
  },
  "PIVOT_VALUE_DATA_TYPE_MISMATCH" : {
    "message" : [ "Invalid pivot value '%s': value data type %s does not match pivot column data type %s" ],
    "sqlState" : "42000"
  },
  "RENAME_SRC_PATH_NOT_FOUND" : {
    "message" : [ "Failed to rename as %s was not found" ],
    "sqlState" : "22023"
  },
  "SECOND_FUNCTION_ARGUMENT_NOT_INTEGER" : {
    "message" : [ "The second argument of '%s' function needs to be an integer." ],
    "sqlState" : "22023"
  },
  "UNABLE_TO_ACQUIRE_MEMORY" : {
    "message" : [ "Unable to acquire %s bytes of memory, got %s" ]
  },
  "UNRECOGNIZED_SQL_TYPE" : {
    "message" : [ "Unrecognized SQL type %s" ],
    "sqlState" : "42000"
  },
  "UNSUPPORTED_DATATYPE" : {
    "message" : [ "Unsupported data type %s" ],
    "sqlState" : "0A000"
  },
  "UNSUPPORTED_FEATURE" : {
    "message" : [ "The feature is not supported: %s" ],
    "sqlState" : "0A000"
  },
  "UNSUPPORTED_GROUPING_EXPRESSION" : {
    "message" : [ "grouping()/grouping_id() can only be used with GroupingSets/Cube/Rollup" ]
  },
  "UNSUPPORTED_OPERATION" : {
    "message" : [ "The operation is not supported: %s" ]
  },
  "UNTYPED_SCALA_UDF" : {
    "message" : [ "You're using untyped Scala UDF, which does not have the input type information. Spark may blindly pass null to the Scala closure with primitive-type argument, and the closure will see the default value of the Java type for the null argument, e.g. `udf((x: Int) => x, IntegerType)`, the result is 0 for null input. To get rid of this error, you could:\n1. use typed Scala UDF APIs(without return type parameter), e.g. `udf((x: Int) => x)`\n2. use Java UDF APIs, e.g. `udf(new UDF1[String, Integer] { override def call(s: String): Integer = s.length() }, IntegerType)`, if input types are all non primitive\n3. set spark.sql.legacy.allowUntypedScalaUDF to true and use this API with caution" ]
  },
  "WRITING_JOB_ABORTED" : {
    "message" : [ "Writing job aborted" ],
    "sqlState" : "40000"
  }
}
