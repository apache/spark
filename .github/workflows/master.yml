name: master

on:
  push:
    branches:
    - master
  pull_request:
    branches:
    - master

jobs:
  # TODO(SPARK-32248): Recover JDK 11 builds
  # Build: build Spark and run the tests for specified modules.
  build:
    name: "Build modules: ${{ matrix.modules }} ${{ matrix.comment }} (JDK ${{ matrix.java }}, ${{ matrix.hadoop }}, ${{ matrix.hive }})"
    runs-on: ubuntu-latest
    strategy:
      fail-fast: false
      matrix:
        java:
          - 1.8
        hadoop:
          - hadoop3.2
        hive:
          - hive2.3
        # TODO(SPARK-32246): We don't test 'streaming-kinesis-asl' for now.
        # Kinesis tests depends on external Amazon kinesis service.
        # Note that the modules below are from sparktestsupport/modules.py.
        modules:
          - |-
            core, unsafe, kvstore, avro,
            network_common, network_shuffle, repl, launcher
            examples, sketch, graphx
          - |-
            catalyst, hive-thriftserver
          - |-
            streaming, sql-kafka-0-10, streaming-kafka-0-10,
            mllib-local, mllib,
            yarn, mesos, kubernetes, hadoop-cloud, spark-ganglia-lgpl
          - |-
            pyspark-sql, pyspark-mllib, pyspark-resource
          - |-
            pyspark-core, pyspark-streaming, pyspark-ml
          - |-
            sparkr
        # Here, we split Hive and SQL tests into some of slow ones and the rest of them.
        included-tags: [""]
        excluded-tags: [""]
        comment: [""]
        include:
          # Hive tests
          - modules: hive
            java: 1.8
            hadoop: hadoop3.2
            hive: hive2.3
            included-tags: org.apache.spark.tags.SlowHiveTest
            comment: "- slow tests"
          - modules: hive
            java: 1.8
            hadoop: hadoop3.2
            hive: hive2.3
            excluded-tags: org.apache.spark.tags.SlowHiveTest
            comment: "- other tests"
          # SQL tests
          - modules: sql
            java: 1.8
            hadoop: hadoop3.2
            hive: hive2.3
            included-tags: org.apache.spark.tags.ExtendedSQLTest
            comment: "- slow tests"
          - modules: sql
            java: 1.8
            hadoop: hadoop3.2
            hive: hive2.3
            excluded-tags: org.apache.spark.tags.ExtendedSQLTest
            comment: "- other tests"
    env:
      MODULES_TO_TEST: ${{ matrix.modules }}
      EXCLUDED_TAGS: ${{ matrix.excluded-tags }}
      INCLUDED_TAGS: ${{ matrix.included-tags }}
      HADOOP_PROFILE: ${{ matrix.hadoop }}
      HIVE_PROFILE: ${{ matrix.hive }}
      # GitHub Actions' default miniconda to use in pip packaging test.
      CONDA_PREFIX: /usr/share/miniconda
    steps:
    - name: Checkout Spark repository
      uses: actions/checkout@v2
    # PySpark
    - name: Install Python 3.6
      uses: actions/setup-python@v2
      if: contains(matrix.modules, 'pyspark') || (contains(matrix.modules, 'sql') && !contains(matrix.modules, 'sql-'))
      with:
        python-version: 3.6
        architecture: x64
    - name: "Run tests: ${{ matrix.modules }}"
      run: |
        # Hive tests become flaky when running in parallel as it's too intensive.
        if [[ "$MODULES_TO_TEST" == "hive" ]]; then export SERIAL_SBT_TESTS=1; fi
        mkdir -p ~/.m2
        ./dev/run-tests --parallelism 2 --modules "$MODULES_TO_TEST" --included-tags "$INCLUDED_TAGS" --excluded-tags "$EXCLUDED_TAGS"
        rm -rf ~/.m2/repository/org/apache/spark
