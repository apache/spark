#
# Licensed to the Apache Software Foundation (ASF) under one
# or more contributor license agreements.  See the NOTICE file
# distributed with this work for additional information
# regarding copyright ownership.  The ASF licenses this file
# to you under the Apache License, Version 2.0 (the
# "License"); you may not use this file except in compliance
# with the License.  You may obtain a copy of the License at
#
#   http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing,
# software distributed under the License is distributed on an
# "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
# KIND, either express or implied.  See the License for the
# specific language governing permissions and limitations
# under the License.
#

name: "Build pyspark spark-pipelines"

on:
  push:
    branches:
      - '**'

jobs:
  # Build: build Spark and run the tests for specified modules using SBT
  build:
    name: "Build spark-pipelines"
    runs-on: ubuntu-latest
    timeout-minutes: 120
    if: github.repository == 'apache/spark'
    steps:
      - name: Checkout Spark repository
        uses: actions/checkout@v4
      - name: Cache SBT and Maven
        uses: actions/cache@v4
        with:
          path: |
            build/apache-maven-*
            build/*.jar
            ~/.sbt
          key: build-pyspark-spark-pipelines-${{ hashFiles('**/pom.xml', 'project/build.properties', 'build/mvn', 'build/sbt', 'build/sbt-launch-lib.bash', 'build/spark-build-info') }}
          restore-keys: |
            build-pyspark-spark-pipelines-
      - name: Cache Coursier local repository
        uses: actions/cache@v4
        with:
          path: ~/.cache/coursier
          key: build-pyspark-spark-pipelines-${{ hashFiles('**/pom.xml') }}
          restore-keys: |
            build-pyspark-spark-pipelines-
      - name: Install Java 17
        uses: actions/setup-java@v4
        with:
          distribution: zulu
          java-version: 17
      - name: Install Python 3.11
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'
          architecture: x64
      - name: Build Spark
        run: |
          ./build/sbt -Phive Test/package
      - name: Install pure Python package (pyspark-client)
        env:
          SPARK_TESTING: 1
        run: |
          cd python
          python packaging/client/setup.py sdist
          cd dist
          pip install pyspark*client-*.tar.gz
      - name: List Python packages
        run: python -m pip list
      - name: Run tests (local)
        env:
          SPARK_TESTING: 1
          SPARK_CONNECT_TESTING_REMOTE: sc://localhost
        run: |
          # Make less noisy
          cp conf/log4j2.properties.template conf/log4j2.properties
          sed -i 's/rootLogger.level = info/rootLogger.level = warn/g' conf/log4j2.properties

          # Start a Spark Connect server for local
          PYTHONPATH="python/lib/pyspark.zip:python/lib/py4j-0.10.9.9-src.zip:$PYTHONPATH" ./sbin/start-connect-server.sh \
            --driver-java-options "-Dlog4j.configurationFile=file:$GITHUB_WORKSPACE/conf/log4j2.properties" \
            --jars "`find connector/protobuf/target -name spark-protobuf-*SNAPSHOT.jar`,`find connector/avro/target -name spark-avro*SNAPSHOT.jar`"

          # Remove Py4J and PySpark zipped library to make sure there is no JVM connection
          mv python/lib lib.back
          mv python/pyspark pyspark.back

          python ./python/pyspark/pipelines/tests/test_spark_pipelines_pyspark_installation.py

          # Stop Spark Connect server.
          ./sbin/stop-connect-server.sh
          mv lib.back python/lib
          mv pyspark.back python/pyspark
      - name: Upload test results to report
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: test-results-pyspark-spark-pipelines
          path: "**/target/test-reports/*.xml"
      - name: Upload Spark Connect server log file
        if: ${{ !success() }}
        uses: actions/upload-artifact@v4
        with:
          name: unit-tests-log-pyspark-spark-pipelines
          path: logs/*.out
