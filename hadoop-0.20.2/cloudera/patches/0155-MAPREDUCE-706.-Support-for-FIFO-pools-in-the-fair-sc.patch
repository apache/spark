From 1198ef1375387ba107d46f0ab5e9a7c6a7645931 Mon Sep 17 00:00:00 2001
From: Aaron Kimball <aaron@cloudera.com>
Date: Fri, 12 Mar 2010 17:28:15 -0800
Subject: [PATCH 0155/1179] MAPREDUCE-706. Support for FIFO pools in the fair scheduler

Description: The fair scheduler should support making the internal scheduling algorithm for some pools be FIFO instead of fair sharing in order to work better for batch workloads. FIFO pools will behave exactly like the current default scheduler, sorting jobs by priority and then submission time. Pools will have their scheduling algorithm set through the pools config file, and it will be changeable at runtime.

<p>To support this feature, I'm also changing the internal logic of the fair scheduler to no longer use deficits. Instead, for fair sharing, we will assign tasks to the job farthest below its share as a ratio of its share. This is easier to combine with other scheduling algorithms and leads to a more stable sharing situation, avoiding unfairness issues brought up in <a href="http://issues.apache.org/jira/browse/MAPREDUCE-543" title="large pending jobs hog resources"><del>MAPREDUCE-543</del></a> and <a href="http://issues.apache.org/jira/browse/MAPREDUCE-544" title="deficit computation is biased by historical load">MAPREDUCE-544</a> that happen when some jobs have long tasks. The new preemption (<a href="http://issues.apache.org/jira/browse/MAPREDUCE-551" title="Add preemption to the fair scheduler"><del>MAPREDUCE-551</del></a>) will ensure that critical jobs can gain their fair share within a bounded amount of time.</p>
Reason: New feature
Author: Matei Zaharia
Ref: UNKNOWN
---
 .../fairscheduler/designdoc/fsdesigndoc.tex        |  238 +++
 .../apache/hadoop/mapred/DefaultTaskSelector.java  |    6 +-
 .../org/apache/hadoop/mapred/FairScheduler.java    |  972 +++++--------
 .../apache/hadoop/mapred/FairSchedulerServlet.java |  117 +-
 .../apache/hadoop/mapred/FifoJobComparator.java    |    3 +-
 .../org/apache/hadoop/mapred/JobSchedulable.java   |  145 ++
 .../org/apache/hadoop/mapred/LocalityLevel.java    |   65 +
 .../src/java/org/apache/hadoop/mapred/Pool.java    |   36 +-
 .../java/org/apache/hadoop/mapred/PoolManager.java |   49 +-
 .../org/apache/hadoop/mapred/PoolSchedulable.java  |  188 +++
 .../java/org/apache/hadoop/mapred/Schedulable.java |  131 ++
 .../apache/hadoop/mapred/SchedulingAlgorithms.java |  209 +++
 .../org/apache/hadoop/mapred/SchedulingMode.java   |   26 +
 .../org/apache/hadoop/mapred/TaskSelector.java     |    2 +-
 .../org/apache/hadoop/mapred/FakeSchedulable.java  |  108 ++
 .../hadoop/mapred/TestComputeFairShares.java       |  184 +++
 .../apache/hadoop/mapred/TestFairScheduler.java    | 1618 ++++++++++++++------
 .../documentation/content/xdocs/fair_scheduler.xml |   61 +-
 .../org/apache/hadoop/mapred/JobInProgress.java    |   97 +-
 19 files changed, 2996 insertions(+), 1259 deletions(-)
 create mode 100644 src/contrib/fairscheduler/designdoc/fsdesigndoc.tex
 create mode 100644 src/contrib/fairscheduler/src/java/org/apache/hadoop/mapred/JobSchedulable.java
 create mode 100644 src/contrib/fairscheduler/src/java/org/apache/hadoop/mapred/LocalityLevel.java
 create mode 100644 src/contrib/fairscheduler/src/java/org/apache/hadoop/mapred/PoolSchedulable.java
 create mode 100644 src/contrib/fairscheduler/src/java/org/apache/hadoop/mapred/Schedulable.java
 create mode 100644 src/contrib/fairscheduler/src/java/org/apache/hadoop/mapred/SchedulingAlgorithms.java
 create mode 100644 src/contrib/fairscheduler/src/java/org/apache/hadoop/mapred/SchedulingMode.java
 create mode 100644 src/contrib/fairscheduler/src/test/org/apache/hadoop/mapred/FakeSchedulable.java
 create mode 100644 src/contrib/fairscheduler/src/test/org/apache/hadoop/mapred/TestComputeFairShares.java

diff --git a/src/contrib/fairscheduler/designdoc/fsdesigndoc.tex b/src/contrib/fairscheduler/designdoc/fsdesigndoc.tex
new file mode 100644
index 0000000..27cdaa3
--- /dev/null
+++ b/src/contrib/fairscheduler/designdoc/fsdesigndoc.tex
@@ -0,0 +1,238 @@
+\documentclass[11pt]{article}
+\usepackage{geometry}
+\geometry{letterpaper}
+
+
+\begin{document}
+
+\title{Hadoop Fair Scheduler Design Document}
+\author{}
+\maketitle
+\tableofcontents
+
+\section{Introduction}
+
+The Hadoop Fair Scheduler started as a simple means to share MapReduce clusters. Over time, it has grown in functionality to support hierarchical scheduling, preemption, and multiple ways of organizing and weighing jobs. This document explains the goals and features of the Fair Scheduler and its internal design.
+
+\section{Fair Scheduler Goals}
+
+The Fair Scheduler was designed with four main goals:
+\begin{enumerate}
+  \item Run small jobs quickly even if they are sharing a cluster with large jobs. Unlike Hadoop's built-in FIFO scheduler, fair scheduling lets small jobs make progress even if a large job is running, without starving the large job.
+  \item Provide guaranteed service levels to ``production" jobs, to let them run alongside experimental jobs in a shared cluster.
+  \item Be simple to administer and configure. The scheduler should do something reasonable ``out of the box," and users should only need to configure it as they discover that they want to use more advanced features.
+  \item Support reconfiguration at runtime, without requiring a cluster restart.
+\end{enumerate}
+
+\section{Scheduler Features}
+
+This section provides a quick overview of the features of the Fair Scheduler. A detailed usage guide is available in the Hadoop documentation in {\tt build/docs/fair\_scheduler.html}.
+
+\subsection{Pools}
+
+The Fair Scheduler groups jobs into ``pools" and performs fair sharing between these pools. Each pool can use either FIFO or fair sharing to schedule jobs internal to the pool. The pool that a job is placed in is determined by a JobConf property, the ``pool name property". By default, this is {\tt user.name}, so that there is one pool per user. However, different properties can be used, e.g.~{\tt group.name} to have one pool per Unix group.
+
+A common trick is to set the pool name property to an unused property name such as {\tt pool.name} and make this default to {\tt user.name}, so that there is one pool per user but it is also possible to place jobs into ``special" pools by setting their {\tt pool.name} directly. The {\tt mapred-site.xml} snippet below shows how to do this:
+
+\begin{verbatim}
+<property>
+  <name>mapred.fairscheduler.poolnameproperty</name>
+  <value>pool.name</value>
+</property>
+
+<property>
+  <name>pool.name</name>
+  <value>${user.name}</value>
+</property>
+\end{verbatim}
+
+\subsection{Minimum Shares}
+
+Normally, active pools (those that contain jobs) will get equal shares of the map and reduce task slots in the cluster. However, it is also possible to set a \emph{minimum share} of map and reduce slots on a given pool, which is a number of slots that it will always get when it is active, even if its fair share would be below this number. This is useful for guaranteeing that production jobs get a certain desired level of service when sharing a cluster with non-production jobs. Minimum shares have three effects:
+\begin{enumerate}
+  \item The pool's fair share will always be at least as large as its minimum share. Slots are taken from the share of other pools to achieve this. The only exception is if the minimum shares of the active pools add up to more than the total number of slots in the cluster; in this case, each pool's share will be scaled down proportionally.
+  \item Pools whose running task count is below their minimum share get assigned slots first when slots are available.
+  \item It is possible to set a \emph{preemption timeout} on the pool after which, if it has not received enough task slots to meet its minimum share, it is allowed to kill tasks in other jobs to meet its share. Minimum shares with preemption timeouts thus act like SLAs.
+\end{enumerate}
+
+Note that when a pool is inactive (contains no jobs), its minimum share is not ``reserved" for it -- the slots are split up among the other pools.
+
+\subsection{Preemption}
+
+As explained above, the scheduler may kill tasks from a job in one pool in order to meet the minimum share of another pool. We call this preemption, although this usage of the word is somewhat strange given the normal definition of preemption as pausing; really it is the \emph{job} that gets preempted, while the task gets killed. The feature explained above is called \emph{min share preemption}. In addition, the scheduler supports \emph{fair share preemption}, to kill tasks when a pool's fair share is not being met. Fair share preemption is much more conservative than min share preemption, because pools without min shares are expected to be non-production jobs where some amount of unfairness is tolerable. In particular, fair share preemption activates if a pool has been below \emph{half} of its fair share for a configurable fair share preemption timeout, which is recommended to be set fairly high (e.g. 10 minutes).
+
+In both types of preemption, the scheduler kills the most recently launched tasks from over-scheduled pools, to minimize the amount of computation wasted by preemption.
+
+\subsection{Running Job Limits}
+
+The fair scheduler can limit the number of concurrently running jobs from each user and from each pool. This is useful for limiting the amount of intermediate data generated on the cluster. The jobs that will run are chosen in order of submit time and priority. Jobs submitted beyond the limit wait for one of the running jobs to finish.
+
+\subsection{Job Priorities}
+
+Within a pool, job priorities can be used to control the scheduling of jobs, whether the pool's internal scheduling mode is FIFO or fair sharing:
+\begin{itemize}
+  \item In FIFO pools, jobs are ordered first by priority and then by submit time, as in Hadoop's default scheduler.
+  \item In fair sharing pools, job priorities are used as weights to control how much share a job gets. The normal priority corresponds to a weight of 1.0, and each level gives 2x more weight. For example, a high-priority job gets a weight of 2.0, and will therefore get 2x the share of a normal-priority job. 
+\end{itemize}
+
+\subsection{Pool Weights}
+
+Pools can be given weights to achieve unequal sharing of the cluster. For example, a pool with weight 2.0 gets 2x the share of a pool with weight 1.0.
+
+\subsection{Delay Scheduling}
+
+The Fair Scheduler contains an algorithm called delay scheduling to improve data locality. Jobs that cannot launch a data-local map task wait for some period of time before they are allowed to launch non-data-local tasks, ensuring that they will run locally if some node in the cluster has the relevant data. Delay scheduling is described in detail in Section \ref{sec:delay-scheduling}.
+
+\subsection{Administration}
+
+The Fair Scheduler includes a web UI displaying the active pools and jobs and their fair shares, moving jobs between pools, and changing job priorities.
+In addition, the Fair Scheduler's allocation file (specifying min shares and preemption timeouts for the pools) is automatically reloaded if it is modified on disk, to allow runtime reconfiguration.
+
+\section{Implementation}
+
+\subsection{Hadoop Scheduling Background}
+
+Hadoop jobs consist of a number of map and reduce \emph{tasks}. These task run in \emph{slots} on the nodes on the cluster. Each node is configured with a number of map slots and reduce slots based on its computational resources (typically one slot per core). The role of the scheduler is to assign tasks to any slots that are free.
+
+All schedulers in Hadoop, including the Fair Scheduler, inherit from the {\tt TaskScheduler} abstract class. This class provides access to a {\tt TaskTrackerManager} -- an interface to the JobTracker -- as well as a {\tt Configuration} instance. It also ask the scheduler to implement three abstract methods: the lifecycle methods {\tt start} and {\tt terminate}, and a method called {\tt assignTasks} to launch tasks on a given TaskTracker.
+Task assignment in Hadoop is reactive. TaskTrackers periodically send heartbeats to the JobTracker with their {\tt TaskTrackerStatus}, which contains a list of running tasks, the number of slots on the node, and other information. The JobTracker then calls {\tt assignTasks} on the scheduler to obtain tasks to launch. These are returned with the heartbeat response.
+
+Apart from reacting to heartbeats through {\tt assignTasks}, schedulers can also be notified when jobs have been submitted to the cluster, killed, or removed by adding listeners to the {\tt TaskTrackerManager}. The Fair Scheduler sets up these listeners in its {\tt start} method. An important role of the listeners is to initialize jobs that are submitted -- until a job is initialized, it cannot launch tasks. The Fair Scheduler currently initializes all jobs right away, but it may also be desirable to hold off initializing jobs if too many are submitted to limit memory usage on the JobTracker.
+
+Selection of tasks \emph{within} a job is mostly done by the {\tt JobInProgress} class, and not by individual schedulers. {\tt JobInProgress} exposes two methods, {\tt obtainNewMapTask} and {\tt obtainNewReduceTask}, to launch a task of either type. Both methods may either return a {\tt Task} object or {\tt null} if the job does not wish to launch a task. Whether a job wishes to launch a task may change back and forth during its lifetime. Even after all tasks in the job have been started, the job may wish to run another task for speculative execution. In addition, if the node containing a map task failed, the job will wish to re-run it to rebuild its output for use in the reduce tasks. Schedulers may therefore need to poll multiple jobs until they find one with a task to run.
+
+Finally, for map tasks, an important scheduling criterion is data locality: running the task on a node or rack that contains its input data. Normally, {\tt JobInProgress.obtainNewMapTask} returns the ``closest" map task to a given node. However, to give schedulers slightly more control over data locality, there is also a version of {\tt obtainNewMapTask} that allow the scheduler to cap the level of non-locality allowed for the task (e.g.~request a task only on the same node, or {\tt null} if none is available). The Fair Scheduler uses this method with an algorithm called delay scheduling (Section \ref{sec:delay-scheduling}) to optimize data locality.
+
+\subsection{Fair Scheduler Basics}
+
+At a high level, the Fair Scheduler uses hierarchical scheduling to assign tasks. First it selects a pool to assign a task to according to the fair sharing algorithm in Section \ref{sec:fair-sharing-alg}. Then it asks the pool obtain a task. The pool chooses among its jobs according to its internal scheduling order (FIFO or fair sharing).
+
+In fact, because jobs might not have tasks to launch ({\tt obtainNew(Map|Reduce)Task} can return null), the scheduler actually establishes an ordering on jobs and asks them for tasks in turn. Within a pool, jobs are sorted either by priority and start time (for FIFO) or by distance below fair share. If the first job in the ordering does not have a task to launch, the pool will ask the second, third, etc jobs. Pools themselves are sorted by distance below min share and fair share, so if the first pool does not have any jobs that can launch tasks, the second pool is asked, etc. This makes it straightforward to implement features like delay scheduling (Section \ref{sec:delay-scheduling}) that may cause jobs to ``pass" on a slot.
+
+Apart from the assign tasks code path, the Fair Scheduler also has a periodic update thread that calls {\tt update} every few seconds. This thread is responsible for recomputing fair shares to display them on the UI (Section \ref{sec:fair-share-computation}), checking whether jobs need to be preempted (Section \ref{sec:preemption}), and checking whether the allocations file has changed to reload pool allocations (through {\tt PoolManager}).
+
+\subsection{The {\tt Schedulable} Class}
+
+To allow the same fair sharing algorithm to be used both between pools and within a pool, the Fair Scheduler uses an abstract class called {\tt Schedulable} to represent both pools and jobs. Its subclasses for these roles are {\tt PoolSchedulable} and {\tt JobSchedulable}. A {\tt Schedulable} is responsible for three roles:
+\begin{enumerate}
+  \item It can be asked to obtain a task through {\tt assignTask}. This may return {\tt null} if the {\tt Schedulable} has no tasks to launch.
+  \item It can be queried for information about the pool/job to use in scheduling, such as:
+  \begin{itemize}
+    \item Number of running tasks.
+    \item Demand (number of tasks the {\tt Schedulable} \emph{wants} to run; this is equal to number of running tasks + number of unlaunched tasks).
+    \item Min share assigned through config file.
+    \item Weight (for fair sharing).
+    \item Priority and start time (for FIFO scheduling).
+  \end{itemize}
+  \item It can be assigned a fair share through {\tt setFairShare}.
+\end{enumerate}
+
+There are separate {\tt Schedulable}s for map and reduce tasks, to make it possible to use the same algorithm on both types of tasks.
+
+\subsection{Fair Sharing Algorithm}
+\label{sec:fair-sharing-alg}
+
+A simple way to achieve fair sharing is the following: whenever a slot is available, assign it to the pool that has the fewest running tasks. This will ensure that all pool get an equal number of slots, unless a pool's demand is less than its fair share, in which case the extra slots are divided evenly among the other pools. Two features of the Fair Scheduler complicate this algorithm a little:
+\begin{itemize}
+  \item Pool weights mean that some pools should get more slots than others. For example, a pool with weight 2 should get 2x more slots than a pool with weight 1. This is accomplished by changing the scheduling rule to ``assign the slot to the pool whose value of $runningTasks/weight$ is smallest."
+  \item Minimum shares mean that pools below their min share should get slots first. When we sort pools to choose which ones to schedule next, we place pools below their min share ahead of pools above their min share. We order the pools below their min share by how far they are below it as a percentage of the share.
+\end{itemize}
+
+This fair sharing algorithm is implemented in {\tt FairShareComparator} in the {\tt SchedulingAlgorithms} class. The comparator orders jobs by distance below min share and then by $runningTasks/weight$.
+
+\subsection{Preemption}
+\label{sec:preemption}
+
+To determine when to preempt tasks, the Fair Schedulers maintains two values for each {\tt PoolSchedulable}: the last time when the pool was at its min share, and the last time when the pool was at half its fair share. These conditions are checked periodically by the update thread in {\tt FairScheduler.updatePreemptionVariables}, using the methods {\tt isStarvedForMinShare} and {\tt isStarvedForFairShare}. These methods also take into account the demand of the pool, so that a pool is not counted as starving if its demand is below its min/fair share but is otherwise met.
+
+When preempting tasks, the scheduler kills the most recently launched tasks from over-scheduled pools. This minimizes the amount of computation wasted by preemption and ensures that all jobs can eventually finish (it is as if the preempted jobs just never got their last few slots). The tasks are chosen and preempted in {\tt preemptTasks}.
+
+Note that for min share preemption, it is clear when a pool is below its min share because the min share is given as a number of slots, but for fair share preemption, we must be able to compute a pool's fair share to determine when it is being starved. This computation is trickier than dividing the number of slots by the number of pools due to weights, min shares and demands. Section \ref{sec:fair-share-computation} explains how fair shares are computed.
+
+\subsection{Fair Share Computation}
+\label{sec:fair-share-computation}
+
+The scheduling algorithm in Section \ref{sec:fair-sharing-alg} achieves fair shares without actually needing to compute pools' numerical shares beforehand. However, for preemption and for displaying shares in the Web UI, we want to know what a pool's fair share is even if the pool is not currently at its share. That is, we want to know how many slots the pool \emph{would} get if we started with all slots being empty and ran the algorithm in Section \ref{sec:fair-sharing-alg} until we filled them.
+One way to compute these shares would be to simulate starting out with empty slots and calling {\tt assignTasks} repeatedly until they filled, but this is expensive, because each scheduling decision takes $O(numJobs)$ time and we need to make $O(numSlots)$ decisions.
+
+To compute fair shares efficiently, the Fair Scheduler includes an algorithm based on binary search in {\tt SchedulingAlgorithms.computeFairShares}. This algorithm is based on the following observation. If all slots had been assigned according to weighted fair sharing respecting pools' demands and min shares, then there would exist a ratio $r$ such that:
+\begin{enumerate}
+  \item Pools whose demand $d_i$ is less than $r w_i$ (where $w_i$ is the weight of the pool) are assigned $d_i$ slots.
+  \item Pools whose min share $m_i$ is more than $r w_i$ are assigned $\min(m_i, d_i)$ slots.
+  \item All other pools are assigned $r w_i$ slots.
+  \item The pools' shares sum up to the total number of slots $t$.
+\end{enumerate}
+
+The Fair Scheduler uses binary search to compute the correct $r$. We define a function $f(r)$ as the number of slots that would be used for a given $r$ if conditions 1-3 above were met, and then find a value of $r$ that makes $f(r)=t$. More precisely, $f(r)$ is defined as:
+$$f(r) = \sum_i{\min(d_i, \max(r w_i, m_i)).}$$
+
+Note that $f(r)$ is increasing in $r$ because every term of the sum is increasing, so the equation $f(r) = t$ can be solved by binary search. We choose 0 as a lower bound of our binary search because with $r=0$, only min shares are assigned. (An earlier check in {\tt computeFairShares} checks whether the min shares add up to more than the total number of slots, and if so, computes fair shares by scaling down the min shares proportionally and returns.) To compute an upper bound for the binary search, we try $r=1,2,4,8,\dots$ until we find a value large enough that either more than $t$ slots are used or all pools' demands are met (in case the demands added up to less than $t$).
+
+The steps of the algorithm are explained in detail in {\tt SchedulingAlgorithms.java}.
+
+This algorithm runs in time $O(NP)$, where $N$ is the number of jobs/pools and $P$ is the desired number of bits of precision in the computed values (number of iterations of binary search), which we've set to 25. It thus scales linearly in the number of jobs and pools.
+
+\subsection{Running Job Limits}
+
+Running job limits are implemented by marking jobs as not runnable if there are too many jobs submitted by the same user or pool. This is done in {\tt FairScheduler.updateRunnability}. A job that is not runnable declares its demand as 0 and always returns {\tt null} from {\tt assignTasks}.
+
+\subsection{Delay Scheduling}
+\label{sec:delay-scheduling}
+
+In Hadoop, running map tasks on the nodes or racks that contain their input data is critical for performance, because it avoids shipping the data over the network. However, always assigning slots to the first job in order of pool shares and in-pool ordering (the ``head-of-line job") can sometimes lead to poor locality:
+\begin{itemize}
+  \item If the head-of-line job is small, the chance of it having data on the node that a heartbeat was received from is small. Therefore, locality would be poor in a small-job workload if we always assigned slots to the head-of-line job.
+  \item When fair sharing is used, there is a strong bias for a job to be reassigned into a slot that it just finished a task in, because when it finishes the task, the job falls below its fair share. This can mean that jobs have a difficult time running in slots that other jobs have taken and thus achieve poor locality.
+\end{itemize}
+
+To deal with both of these situations, the Fair Scheduler can sacrifice fairness temporarily to improve locality through an algorithm called delay scheduling. If the head-of-line job cannot launch a local task on the TaskTracker that sent a heartbeat, then it is skipped, and other running jobs are looked at in order of pool shares and in-pool scheduling rules to find a job with a local task. However, if the head-of-line job has been skipped for a sufficiently long time, it is allowed to launch rack-local tasks. Then, if it is skipped for a longer time, it is also allowed to launch off-rack tasks. These skip times are called locality delays. Delays of a few seconds are sufficient to drastically increase locality.
+
+The Fair Scheduler allows locality delays to be set through {\tt mapred-site.xml} or to be turned off by setting them to zero. However, by default, it computes the delay automatically based on the heartbeat interval of the cluster. The delay is set to 1.5x the heartbeat interval.
+
+When a job that has been allowed to launch non-local tasks ends up launching a local task again, its ``locality level" resets and it must wait again before launching non-local tasks. This is done so that a job that gets ``unlucky" early in its lifetime does not continue to launch non-local tasks throughout its life.
+
+Delay scheduling is implemented by keeping track of two variables on each job: the locality level of the last map it launched (0 for node-local, 1 for rack-local and 2 for off-rack) and the time it has spent being skipped for a task. These are kept in a {\tt JobInfo} structure associated with each job in {\tt FairScheduler.java}. Whenever a job is asked for tasks, it checks the locality level it is allowed to launch them at through {\tt FairScheduler.getAllowedLocalityLevel}. If it does not launch a task, it is marked as ``visited" on that heartbeat by appending itself to a {\tt visited} job list that is passed around between calls to {\tt assignTasks} on the same heartbeat. Jobs that are visited on a heartbeat but do not launch any tasks during it are considered as skipped for the time interval between this heartbeat and the next. Code at the beginning of {\tt FairScheduler.assignTasks} increments the wait time of each skipped job by the time elapsed since the last heartbeat. Once a job has been skipped for more than the locality delay, {\tt getAllowedLocalityLevel} starts returning higher locality so that it is allowed to launch less-local tasks. Whenever the job launches a task, its wait time is reset, but we remember the locality level of the launched task so that the job is allowed to launch more tasks at this level without further waiting.
+
+\subsection{Locking Order}
+
+Fair Scheduler data structures can be touched by several threads. Most commonly, the JobTracker invokes {\tt assignTasks}. This happens inside a block of code where the JobTracker has locked itself already. Therefore, to prevent deadlocks, we always ensure that \emph{if both the FairScheduler and the JobTracker must be locked, the JobTracker is locked first}. Other threads that can lock the FairScheduler include the update thread and the web UI.
+
+\subsection{Unit Tests}
+
+The Fair Scheduler contains extensive unit tests using mock {\tt TaskTrackerManager}, {\tt JobInProgress}, {\tt TaskInProgress}, and {\tt Schedulable} objects. Scheduler tests are in {\tt TestFairScheduler.java}. The {\tt computeFairShares} algorithm is tested separately in {\tt TestComputeFairShares.java}. All tests use accelerated time via a fake {\tt Clock} class.
+
+\pagebreak
+\section{Code Guide}
+
+The following table lists some key source files in the Fair Scheduler:
+
+\begin{center}
+\begin{tabular}{|l|p{0.7\columnwidth}|}
+  \hline
+  {\bf File} & {\bf Contents} 
+  \\ \hline
+  {\tt FairScheduler.java} & Scheduler entry point. Also contains update thread, and logic for preemption, delay scheduling, and running job limits.
+  \\ \hline
+  {\tt Schedulable.java} & Definition of the {\tt Schedulable} class. Extended by {\tt JobSchedulable} and {\tt PoolSchedulable}.
+  \\ \hline
+  {\tt SchedulingAlgorithms.java} & Contains FIFO and fair sharing comparators, as well as the {\tt computeFairShares} algorithm in Section \ref{sec:fair-share-computation}.
+  \\ \hline
+  {\tt PoolManager.java} & Reads pool properties from the allocation file and maintains a collection of {\tt Pool} objects. Pools are created on demand.
+  \\ \hline
+  {\tt Pool.java} & Represents a pool and stores its map and reduce {\tt Schedulables}.
+  \\ \hline
+  {\tt FairSchedulerServlet.java} & Implements the scheduler's web UI.
+  \\ \hline
+  {\tt FairSchedulerEventLog.java} & An easy-to-parse event log for debugging. Must be enabled through {\tt mapred.fairscheduler.eventlog.enabled}.
+  If enabled, logs are placed in {\tt \$HADOOP\_LOG\_DIR/fairscheduler}.
+  \\ \hline
+  {\tt TaskSelector.java} & A pluggable class responsible for picking tasks within a job. Currently, {\tt DefaultTaskSelector} delegates to {\tt JobInProgress}, but this would be a useful place to experiment with new algorithms for speculative execution and locality.
+  \\ \hline
+  {\tt LoadManager.java} & A pluggable class responsible for determining when to launch more tasks on a TaskTracker. Currently, {\tt CapBasedLoadManager} uses slot counts, but this would be a useful place to experiment with scheduling based on machine load.
+  \\ \hline
+  {\tt WeightAdjuster.java} & A pluggable class responsible for setting job weights. An example, {\tt NewJobWeightBooster}, is provided, which increases weight temporarily for new jobs.
+  \\ \hline
+\end{tabular}
+\end{center}
+
+\end{document}
\ No newline at end of file
diff --git a/src/contrib/fairscheduler/src/java/org/apache/hadoop/mapred/DefaultTaskSelector.java b/src/contrib/fairscheduler/src/java/org/apache/hadoop/mapred/DefaultTaskSelector.java
index e19c676..d079e55 100644
--- a/src/contrib/fairscheduler/src/java/org/apache/hadoop/mapred/DefaultTaskSelector.java
+++ b/src/contrib/fairscheduler/src/java/org/apache/hadoop/mapred/DefaultTaskSelector.java
@@ -56,12 +56,12 @@ public class DefaultTaskSelector extends TaskSelector {
   }
 
   @Override
-  public Task obtainNewMapTask(TaskTrackerStatus taskTracker, JobInProgress job)
-      throws IOException {
+  public Task obtainNewMapTask(TaskTrackerStatus taskTracker, JobInProgress job,
+      int localityLevel) throws IOException {
     ClusterStatus clusterStatus = taskTrackerManager.getClusterStatus();
     int numTaskTrackers = clusterStatus.getTaskTrackers();
     return job.obtainNewMapTask(taskTracker, numTaskTrackers,
-        taskTrackerManager.getNumberOfUniqueHosts());
+        taskTrackerManager.getNumberOfUniqueHosts(), localityLevel);
   }
 
   @Override
diff --git a/src/contrib/fairscheduler/src/java/org/apache/hadoop/mapred/FairScheduler.java b/src/contrib/fairscheduler/src/java/org/apache/hadoop/mapred/FairScheduler.java
index 4c188f4..d8f88de 100644
--- a/src/contrib/fairscheduler/src/java/org/apache/hadoop/mapred/FairScheduler.java
+++ b/src/contrib/fairscheduler/src/java/org/apache/hadoop/mapred/FairScheduler.java
@@ -25,11 +25,9 @@ import java.util.Collections;
 import java.util.Comparator;
 import java.util.HashMap;
 import java.util.HashSet;
-import java.util.Iterator;
 import java.util.List;
 import java.util.Map;
 import java.util.Set;
-import java.util.Map.Entry;
 
 import org.apache.commons.logging.Log;
 import org.apache.commons.logging.LogFactory;
@@ -58,6 +56,9 @@ public class FairScheduler extends TaskScheduler {
   private static final TaskType[] MAP_AND_REDUCE = 
     new TaskType[] {TaskType.MAP, TaskType.REDUCE};
 
+  // Maximum locality delay when auto-computing locality delays
+  private static final long MAX_AUTOCOMPUTED_LOCALITY_DELAY = 15000;
+  
   protected PoolManager poolMgr;
   protected LoadManager loadMgr;
   protected TaskSelector taskSelector;
@@ -67,8 +68,12 @@ public class FairScheduler extends TaskScheduler {
   protected long lastUpdateTime;           // Time when we last updated infos
   protected boolean initialized;  // Are we initialized?
   protected volatile boolean running; // Are we running?
-  protected boolean useFifo;      // Set if we want to revert to FIFO behavior
   protected boolean assignMultiple; // Simultaneously assign map and reduce?
+  protected int mapAssignCap = -1;    // Max maps to launch per heartbeat
+  protected int reduceAssignCap = -1; // Max reduces to launch per heartbeat
+  protected long localityDelay;       // Time to wait for node and rack locality
+  protected boolean autoComputeLocalityDelay = false; // Compute locality delay
+                                                      // from heartbeat interval
   protected boolean sizeBasedWeight; // Give larger weights to larger jobs
   protected boolean waitForMapsBeforeLaunchingReduces = true;
   protected boolean preemptionEnabled;
@@ -93,30 +98,17 @@ public class FairScheduler extends TaskScheduler {
    */
   static class JobInfo {
     boolean runnable = false;   // Can the job run given user/pool limits?
-    double mapWeight = 0;       // Weight of job in calculation of map share
-    double reduceWeight = 0;    // Weight of job in calculation of reduce share
-    long mapDeficit = 0;        // Time deficit for maps
-    long reduceDeficit = 0;     // Time deficit for reduces
-    int runningMaps = 0;        // Maps running at last update
-    int runningReduces = 0;     // Reduces running at last update
-    int neededMaps;             // Maps needed at last update
-    int neededReduces;          // Reduces needed at last update
-    int minMaps = 0;            // Minimum maps as guaranteed by pool
-    int minReduces = 0;         // Minimum reduces as guaranteed by pool
-    double mapFairShare = 0;    // Fair share of map slots at last update
-    double reduceFairShare = 0; // Fair share of reduce slots at last update
-
-    // Variables used for preemption
-    long lastTimeAtMapMinShare;      // When was the job last at its min maps?
-    long lastTimeAtReduceMinShare;   // Similar for reduces.
-    long lastTimeAtMapHalfFairShare; // When was the job last at half fair maps?
-    long lastTimeAtReduceHalfFairShare;  // Similar for reduces.
-    
-    public JobInfo(long currentTime) {
-      lastTimeAtMapMinShare = currentTime;
-      lastTimeAtReduceMinShare = currentTime;
-      lastTimeAtMapHalfFairShare = currentTime;
-      lastTimeAtReduceHalfFairShare = currentTime;
+    public JobSchedulable mapSchedulable;
+    public JobSchedulable reduceSchedulable;
+    // Variables used for delay scheduling
+    LocalityLevel lastMapLocalityLevel; // Locality level of last map launched
+    long timeWaitedForLocalMap; // Time waiting for local map since last map
+    boolean skippedAtLastHeartbeat;  // Was job skipped at previous assignTasks?
+                                     // (used to update timeWaitedForLocalMap)
+    public JobInfo(JobSchedulable mapSched, JobSchedulable reduceSched) {
+      this.mapSchedulable = mapSched;
+      this.reduceSchedulable = reduceSched;
+      this.lastMapLocalityLevel = LocalityLevel.NODE;
     }
   }
   
@@ -166,7 +158,8 @@ public class FairScheduler extends TaskScheduler {
         taskTrackerManager.addJobInProgressListener(eagerInitListener);
       }
 
-      poolMgr = new PoolManager(conf);
+      poolMgr = new PoolManager(this);
+      poolMgr.initialize();
       loadMgr = (LoadManager) ReflectionUtils.newInstance(
           conf.getClass("mapred.fairscheduler.loadmanager", 
               CapBasedLoadManager.class, LoadManager.class), conf);
@@ -193,12 +186,20 @@ public class FairScheduler extends TaskScheduler {
           "mapred.fairscheduler.preemption.interval", 15000);
       assignMultiple = conf.getBoolean(
           "mapred.fairscheduler.assignmultiple", true);
+      mapAssignCap = conf.getInt(
+          "mapred.fairscheduler.assignmultiple.maps", -1);
+      reduceAssignCap = conf.getInt(
+          "mapred.fairscheduler.assignmultiple.reduces", -1);
       sizeBasedWeight = conf.getBoolean(
           "mapred.fairscheduler.sizebasedweight", false);
       preemptionEnabled = conf.getBoolean(
           "mapred.fairscheduler.preemption", false);
       onlyLogPreemption = conf.getBoolean(
           "mapred.fairscheduler.preemption.only.log", false);
+      localityDelay = conf.getLong(
+          "mapred.fairscheduler.locality.delay", -1);
+      if (localityDelay == -1)
+        autoComputeLocalityDelay = true; // Compute from heartbeat interval
 
 
       initialized = true;
@@ -246,9 +247,10 @@ public class FairScheduler extends TaskScheduler {
     public void jobAdded(JobInProgress job) {
       synchronized (FairScheduler.this) {
         eventLog.log("JOB_ADDED", job.getJobID());
-        poolMgr.addJob(job);
-        JobInfo info = new JobInfo(clock.getTime());
+        JobInfo info = new JobInfo(new JobSchedulable(FairScheduler.this, job, TaskType.MAP),
+            new JobSchedulable(FairScheduler.this, job, TaskType.REDUCE));
         infos.put(job, info);
+        poolMgr.addJob(job); // Also adds job into the right PoolScheduable
         update();
       }
     }
@@ -298,20 +300,18 @@ public class FairScheduler extends TaskScheduler {
       return null;
     String trackerName = tracker.getTrackerName();
     eventLog.log("HEARTBEAT", trackerName);
+    long currentTime = clock.getTime();
 
-    // Reload allocations file if it hasn't been loaded in a while
-    poolMgr.reloadAllocsIfNecessary();
-    
     // Compute total runnable maps and reduces, and currently running ones
     int runnableMaps = 0;
     int runningMaps = 0;
     int runnableReduces = 0;
     int runningReduces = 0;
-    for (JobInProgress job: infos.keySet()) {
-      runnableMaps += runnableTasks(job, TaskType.MAP);
-      runningMaps += runningTasks(job, TaskType.MAP);
-      runnableReduces += runnableTasks(job, TaskType.REDUCE);
-      runningReduces += runningTasks(job, TaskType.REDUCE);
+    for (Pool pool: poolMgr.getPools()) {
+      runnableMaps += pool.getMapSchedulable().getDemand();
+      runningMaps += pool.getMapSchedulable().getRunningTasks();
+      runnableReduces += pool.getReduceSchedulable().getDemand();
+      runningReduces += pool.getReduceSchedulable().getRunningTasks();
     }
 
     ClusterStatus clusterStatus = taskTrackerManager.getClusterStatus();
@@ -324,107 +324,175 @@ public class FairScheduler extends TaskScheduler {
     eventLog.log("RUNNABLE_TASKS", 
         runnableMaps, runningMaps, runnableReduces, runningReduces);
 
+    // Update time waited for local maps for jobs skipped on last heartbeat
+    updateLocalityWaitTimes(currentTime);
+    
+    TaskTrackerStatus tts = tracker;
+
     // Scan to see whether any job needs to run a map, then a reduce
     ArrayList<Task> tasks = new ArrayList<Task>();
     for (TaskType taskType: MAP_AND_REDUCE) {
-      // Continue if all runnable tasks of this type are already running
-      if (taskType == TaskType.MAP && runningMaps == runnableMaps ||
-          taskType == TaskType.REDUCE && runningReduces == runnableReduces)
-        continue;
-      // Continue if the node can't support another task of the given type
-      boolean canAssign = (taskType == TaskType.MAP) ? 
-          loadMgr.canAssignMap(tracker, runnableMaps, totalMapSlots) :
-          loadMgr.canAssignReduce(tracker, runnableReduces, totalReduceSlots);
-      if (canAssign) {
-        // Figure out the jobs that need this type of task
-        List<JobInProgress> candidates = new ArrayList<JobInProgress>();
-        for (JobInProgress job: infos.keySet()) {
-          if (job.getStatus().getRunState() == JobStatus.RUNNING && 
-              neededTasks(job, taskType) > 0) {
-            candidates.add(job);
-          }
+      // Keep track of which jobs were visited and which had tasks launched,
+      // so that we can later mark skipped jobs for delay scheduling
+      Set<JobInProgress> visited = new HashSet<JobInProgress>();
+      Set<JobInProgress> launched = new HashSet<JobInProgress>();
+      // Compute a maximum number of tasks to assign on this task tracker
+      int cap = maxTasksToAssign(taskType, tts);
+      // Assign up to cap tasks
+      for (int i = 0; i < cap; i++) {
+        // Break if all runnable tasks of this type are already running
+        if (taskType == TaskType.MAP && runningMaps == runnableMaps ||
+            taskType == TaskType.REDUCE && runningReduces == runnableReduces)
+          break;
+        // Break if the node can't support another task of this type
+        boolean canAssign = (taskType == TaskType.MAP) ? 
+            loadMgr.canAssignMap(tts, runnableMaps, totalMapSlots) :
+            loadMgr.canAssignReduce(tts, runnableReduces, totalReduceSlots);
+        if (canAssign) {
+          // Get the map or reduce schedulables and sort them by fair sharing
+          List<PoolSchedulable> scheds = getPoolSchedulables(taskType);
+          Collections.sort(scheds, new SchedulingAlgorithms.FairShareComparator());
+          for (Schedulable sched: scheds) {
+            eventLog.log("INFO", "Checking for " + taskType + 
+                " task in " + sched.getName());
+            Task task = sched.assignTask(tts, currentTime, visited);
+            if (task != null) {
+              JobInProgress job = taskTrackerManager.getJob(task.getJobID());
+              eventLog.log("ASSIGN", trackerName, taskType,
+                  job.getJobID(), task.getTaskID());
+              launched.add(job);
+              // Update running task counts, and the job's locality level
+              if (taskType == TaskType.MAP) {
+                runningMaps++;
+                updateLastMapLocalityLevel(job, task, tts);
+              } else {
+                runningReduces++;
+              }
+              // Add task to the list of assignments
+              tasks.add(task);
+              break;
+            } // end if(task != null)
+          } // end for(Schedulable sched: scheds)
+        } else {
+          eventLog.log("INFO", "Can't assign another " + taskType +
+              " to " + trackerName);
+          break;
         }
-        // Sort jobs by deficit (for Fair Sharing) or submit time (for FIFO)
-        Comparator<JobInProgress> comparator = useFifo ?
-            new FifoJobComparator() : new DeficitComparator(taskType);
-        Collections.sort(candidates, comparator);
-        for (JobInProgress job: candidates) {
-          eventLog.log("INFO", 
-              "Checking for " + taskType + " task in " + job.getJobID());
-          Task task;
-          if (taskType == TaskType.MAP) {
-            task = taskSelector.obtainNewMapTask(tracker, job);
-          } else {
-            task = taskSelector.obtainNewReduceTask(tracker, job);
-          }
-          if (task != null) {
-            eventLog.log("ASSIGN", trackerName, taskType,
-                job.getJobID(), task.getTaskID());
-            // Update the JobInfo for this job so we account for the launched
-            // tasks during this update interval and don't try to launch more
-            // tasks than the job needed on future heartbeats
-            JobInfo info = infos.get(job);
-            if (taskType == TaskType.MAP) {
-              info.runningMaps++;
-              info.neededMaps--;
-            } else {
-              info.runningReduces++;
-              info.neededReduces--;
-            }
-            // Add task to the list of assignments
-            tasks.add(task);
-            // If not allowed to assign multiple tasks per heartbeat, return
-            if (!assignMultiple)
-              return tasks;
-            break;
+      } // end for(i = 0; i < cap; i++)
+      // If we were assigning maps, mark any jobs that were visited but
+      // did not launch a task as skipped on this heartbeat
+      if (taskType == TaskType.MAP) {
+        for (JobInProgress job: visited) {
+          if (!launched.contains(job)) {
+            infos.get(job).skippedAtLastHeartbeat = true;
           }
         }
-      } else {
-        eventLog.log("INFO", 
-            "Can't assign another " + taskType + " to " + trackerName);
-
       }
-    }
+      // Return if assignMultiple was disabled and we found a task
+      if (!assignMultiple && tasks.size() > 0)
+        return tasks;
+    } // end for(TaskType taskType: MAP_AND_REDUCE)
     
     // If no tasks were found, return null
     return tasks.isEmpty() ? null : tasks;
   }
 
   /**
-   * Compare jobs by deficit for a given task type, putting jobs whose current
-   * allocation is less than their minimum share always ahead of others. This is
-   * the default job comparator used for Fair Sharing.
+   * Get maximum number of tasks to assign on a TaskTracker on a heartbeat.
+   * The scheduler may launch fewer than this many tasks if the LoadManager
+   * says not to launch more, but it will never launch more than this number.
    */
-  private class DeficitComparator implements Comparator<JobInProgress> {
-    private final TaskType taskType;
+  private int maxTasksToAssign(TaskType type, TaskTrackerStatus tts) {
+    if (!assignMultiple)
+      return 1;
+    int cap = (type == TaskType.MAP) ? mapAssignCap : reduceAssignCap;
+    if (cap == -1) // Infinite cap; use the TaskTracker's slot count
+      return (type == TaskType.MAP) ?
+          (tts.getMaxMapTasks() - tts.countMapTasks()) :
+          (tts.getMaxReduceTasks() - tts.countReduceTasks());
+    else
+      return cap;
+  }
 
-    private DeficitComparator(TaskType taskType) {
-      this.taskType = taskType;
+  /**
+   * Update locality wait times for jobs that were skipped at last heartbeat.
+   */
+  private void updateLocalityWaitTimes(long currentTime) {
+    long timeSinceLastHeartbeat = 
+      (lastHeartbeatTime == 0 ? 0 : currentTime - lastHeartbeatTime);
+    lastHeartbeatTime = currentTime;
+    for (JobInfo info: infos.values()) {
+      if (info.skippedAtLastHeartbeat) {
+        info.timeWaitedForLocalMap += timeSinceLastHeartbeat;
+        info.skippedAtLastHeartbeat = false;
+      }
     }
+  }
 
-    public int compare(JobInProgress j1, JobInProgress j2) {
-      // Put needy jobs ahead of non-needy jobs (where needy means must receive
-      // new tasks to meet slot minimum), comparing among jobs of the same type
-      // by deficit so as to put jobs with higher deficit ahead.
-      JobInfo j1Info = infos.get(j1);
-      JobInfo j2Info = infos.get(j2);
-      double deficitDif;
-      boolean j1Needy, j2Needy;
-      if (taskType == TaskType.MAP) {
-        j1Needy = j1.runningMaps() < Math.floor(j1Info.minMaps);
-        j2Needy = j2.runningMaps() < Math.floor(j2Info.minMaps);
-        deficitDif = j2Info.mapDeficit - j1Info.mapDeficit;
-      } else {
-        j1Needy = j1.runningReduces() < Math.floor(j1Info.minReduces);
-        j2Needy = j2.runningReduces() < Math.floor(j2Info.minReduces);
-        deficitDif = j2Info.reduceDeficit - j1Info.reduceDeficit;
-      }
-      if (j1Needy && !j2Needy)
-        return -1;
-      else if (j2Needy && !j1Needy)
-        return 1;
-      else // Both needy or both non-needy; compare by deficit
-        return (int) Math.signum(deficitDif);
+  /**
+   * Update a job's locality level and locality wait variables given that that 
+   * it has just launched a map task on a given task tracker.
+   */
+  private void updateLastMapLocalityLevel(JobInProgress job,
+      Task mapTaskLaunched, TaskTrackerStatus tracker) {
+    JobInfo info = infos.get(job);
+    LocalityLevel localityLevel = LocalityLevel.fromTask(
+        job, mapTaskLaunched, tracker);
+    info.lastMapLocalityLevel = localityLevel;
+    info.timeWaitedForLocalMap = 0;
+    eventLog.log("ASSIGNED_LOC_LEVEL", job.getJobID(), localityLevel);
+  }
+
+  /**
+   * Get the maximum locality level at which a given job is allowed to
+   * launch tasks, based on how long it has been waiting for local tasks.
+   * This is used to implement the "delay scheduling" feature of the Fair
+   * Scheduler for optimizing data locality.
+   * If the job has no locality information (e.g. it does not use HDFS), this 
+   * method returns LocalityLevel.ANY, allowing tasks at any level.
+   * Otherwise, the job can only launch tasks at its current locality level
+   * or lower, unless it has waited at least localityDelay milliseconds
+   * (in which case it can go one level beyond) or 2 * localityDelay millis
+   * (in which case it can go to any level).
+   */
+  protected LocalityLevel getAllowedLocalityLevel(JobInProgress job,
+      long currentTime) {
+    JobInfo info = infos.get(job);
+    if (info == null) { // Job not in infos (shouldn't happen)
+      LOG.error("getAllowedLocalityLevel called on job " + job
+          + ", which does not have a JobInfo in infos");
+      return LocalityLevel.ANY;
+    }
+    if (job.nonLocalMaps.size() > 0) { // Job doesn't have locality information
+      return LocalityLevel.ANY;
+    }
+    // Don't wait for locality if the job's pool is starving for maps
+    Pool pool = poolMgr.getPool(job);
+    PoolSchedulable sched = pool.getMapSchedulable();
+    long minShareTimeout = poolMgr.getMinSharePreemptionTimeout(pool.getName());
+    long fairShareTimeout = poolMgr.getFairSharePreemptionTimeout();
+    if (currentTime - sched.getLastTimeAtMinShare() > minShareTimeout ||
+        currentTime - sched.getLastTimeAtHalfFairShare() > fairShareTimeout) {
+      eventLog.log("INFO", "No delay scheduling for "
+          + job.getJobID() + " because it is being starved");
+      return LocalityLevel.ANY;
+    }
+    // In the common case, compute locality level based on time waited
+    switch(info.lastMapLocalityLevel) {
+    case NODE: // Last task launched was node-local
+      if (info.timeWaitedForLocalMap >= 2 * localityDelay)
+        return LocalityLevel.ANY;
+      else if (info.timeWaitedForLocalMap >= localityDelay)
+        return LocalityLevel.RACK;
+      else
+        return LocalityLevel.NODE;
+    case RACK: // Last task launched was rack-local
+      if (info.timeWaitedForLocalMap >= localityDelay)
+        return LocalityLevel.ANY;
+      else
+        return LocalityLevel.RACK;
+    default: // Last task was non-local; can launch anywhere
+      return LocalityLevel.ANY;
     }
   }
   
@@ -434,11 +502,24 @@ public class FairScheduler extends TaskScheduler {
    * and needed tasks of each type. 
    */
   protected void update() {
-    //Making more granual locking so that clusterStatus can be fetched from Jobtracker.
+    // Making more granular locking so that clusterStatus can be fetched 
+    // from Jobtracker without locking the scheduler.
     ClusterStatus clusterStatus = taskTrackerManager.getClusterStatus();
-    // Got clusterStatus hence acquiring scheduler lock now
-    // Remove non-running jobs
-    synchronized(this){
+    
+    // Recompute locality delay from JobTracker heartbeat interval if enabled.
+    // This will also lock the JT, so do it outside of a fair scheduler lock.
+    if (autoComputeLocalityDelay) {
+      JobTracker jobTracker = (JobTracker) taskTrackerManager;
+      localityDelay = Math.min(MAX_AUTOCOMPUTED_LOCALITY_DELAY,
+          (long) (1.5 * jobTracker.getNextHeartbeatInterval()));
+    }
+    
+    // Got clusterStatus hence acquiring scheduler lock now.
+    synchronized (this) {
+      // Reload allocations file if it hasn't been loaded in a while
+      poolMgr.reloadAllocsIfNecessary();
+      
+      // Remove any jobs that have stopped running
       List<JobInProgress> toRemove = new ArrayList<JobInProgress>();
       for (JobInProgress job: infos.keySet()) { 
         int runState = job.getStatus().getRunState();
@@ -451,29 +532,40 @@ public class FairScheduler extends TaskScheduler {
         infos.remove(job);
         poolMgr.removeJob(job);
       }
-      // Update running jobs with deficits since last update, and compute new
-      // slot allocations, weight, shares and task counts
-      long now = clock.getTime();
-      long timeDelta = now - lastUpdateTime;
-      updateDeficits(timeDelta);
-      updateRunnability();
-      updateTaskCounts();
-      updateWeights();
-      updateMinSlots();
-      updateFairShares(clusterStatus);
+      
+      updateRunnability(); // Set job runnability based on user/pool limits 
+      
+      // Update demands of jobs and pools
+      for (Pool pool: poolMgr.getPools()) {
+        pool.getMapSchedulable().updateDemand();
+        pool.getReduceSchedulable().updateDemand();
+      }
+      
+      // Compute fair shares based on updated demands
+      List<PoolSchedulable> mapScheds = getPoolSchedulables(TaskType.MAP);
+      List<PoolSchedulable> reduceScheds = getPoolSchedulables(TaskType.REDUCE);
+      SchedulingAlgorithms.computeFairShares(
+          mapScheds, clusterStatus.getMaxMapTasks());
+      SchedulingAlgorithms.computeFairShares(
+          reduceScheds, clusterStatus.getMaxReduceTasks());
+      
+      // Use the computed shares to assign shares within each pool
+      for (Pool pool: poolMgr.getPools()) {
+        pool.getMapSchedulable().redistributeShare();
+        pool.getReduceSchedulable().redistributeShare();
+      }
+      
       if (preemptionEnabled)
         updatePreemptionVariables();
-      lastUpdateTime = now;
     }
   }
   
-  private void updateDeficits(long timeDelta) {
-    for (JobInfo info: infos.values()) {
-      info.mapDeficit +=
-        (info.mapFairShare - info.runningMaps) * timeDelta;
-      info.reduceDeficit +=
-        (info.reduceFairShare - info.runningReduces) * timeDelta;
+  public List<PoolSchedulable> getPoolSchedulables(TaskType type) {
+    List<PoolSchedulable> scheds = new ArrayList<PoolSchedulable>();
+    for (Pool pool: poolMgr.getPools()) {
+      scheds.add(pool.getSchedulable(type));
     }
+    return scheds;
   }
   
   private void updateRunnability() {
@@ -504,285 +596,19 @@ public class FairScheduler extends TaskScheduler {
     }
   }
 
-  private void updateTaskCounts() {
-    for (Map.Entry<JobInProgress, JobInfo> entry: infos.entrySet()) {
-      JobInProgress job = entry.getKey();
-      JobInfo info = entry.getValue();
-      if (job.getStatus().getRunState() != JobStatus.RUNNING) {
-        // Job is still in PREP state and tasks aren't initialized; skip it.
-        continue;
-      }
-      int totalMaps = job.numMapTasks;
-      int finishedMaps = 0;
-      int runningMaps = 0;
-      for (TaskInProgress tip: job.getMapTasks()) {
-        if (tip.isComplete()) {
-          finishedMaps += 1;
-        } else if (tip.isRunning()) {
-          runningMaps += tip.getActiveTasks().size();
-        }
-      }
-      info.runningMaps = runningMaps;
-      info.neededMaps = (totalMaps - runningMaps - finishedMaps
-          + taskSelector.neededSpeculativeMaps(job));
-      // Count reduces
-      int totalReduces = job.numReduceTasks;
-      int finishedReduces = 0;
-      int runningReduces = 0;
-      for (TaskInProgress tip: job.getReduceTasks()) {
-        if (tip.isComplete()) {
-          finishedReduces += 1;
-        } else if (tip.isRunning()) {
-          runningReduces += tip.getActiveTasks().size();
-        }
-      }
-      info.runningReduces = runningReduces;
-      if (enoughMapsFinishedToRunReduces(finishedMaps, totalMaps)) {
-        info.neededReduces = (totalReduces - runningReduces - finishedReduces 
-            + taskSelector.neededSpeculativeReduces(job));
-      } else {
-        info.neededReduces = 0;
-      }
-      // If the job was marked as not runnable due to its user or pool having
-      // too many active jobs, set the neededMaps/neededReduces to 0. We still
-      // count runningMaps/runningReduces however so we can give it a deficit.
-      if (!info.runnable) {
-        info.neededMaps = 0;
-        info.neededReduces = 0;
-      }
-    }
-  }
-
-  /**
-   * Has a job finished enough maps to allow launching its reduces?
-   */
-  protected boolean enoughMapsFinishedToRunReduces(
-      int finishedMaps, int totalMaps) {
-    if (waitForMapsBeforeLaunchingReduces) {
-      return finishedMaps >= Math.max(1, totalMaps * 0.05);
-    } else {
-      return true;
-    }
-  }
-
-  private void updateWeights() {
-    // First, calculate raw weights for each job
-    for (Map.Entry<JobInProgress, JobInfo> entry: infos.entrySet()) {
-      JobInProgress job = entry.getKey();
-      JobInfo info = entry.getValue();
-      info.mapWeight = calculateRawWeight(job, TaskType.MAP);
-      info.reduceWeight = calculateRawWeight(job, TaskType.REDUCE);
-    }
-    // Now calculate job weight sums for each pool
-    Map<String, Double> mapWeightSums = new HashMap<String, Double>();
-    Map<String, Double> reduceWeightSums = new HashMap<String, Double>();
-    for (Pool pool: poolMgr.getPools()) {
-      double mapWeightSum = 0;
-      double reduceWeightSum = 0;
-      for (JobInProgress job: pool.getJobs()) {
-        if (isRunnable(job)) {
-          if (runnableTasks(job, TaskType.MAP) > 0) {
-            mapWeightSum += infos.get(job).mapWeight;
-          }
-          if (runnableTasks(job, TaskType.REDUCE) > 0) {
-            reduceWeightSum += infos.get(job).reduceWeight;
-          }
-        }
-      }
-      mapWeightSums.put(pool.getName(), mapWeightSum);
-      reduceWeightSums.put(pool.getName(), reduceWeightSum);
-    }
-    // And normalize the weights based on pool sums and pool weights
-    // to share fairly across pools (proportional to their weights)
-    for (Map.Entry<JobInProgress, JobInfo> entry: infos.entrySet()) {
-      JobInProgress job = entry.getKey();
-      JobInfo info = entry.getValue();
-      String pool = poolMgr.getPoolName(job);
-      double poolWeight = poolMgr.getPoolWeight(pool);
-      double mapWeightSum = mapWeightSums.get(pool);
-      double reduceWeightSum = reduceWeightSums.get(pool);
-      if (mapWeightSum == 0)
-        info.mapWeight = 0;
-      else
-        info.mapWeight *= (poolWeight / mapWeightSum); 
-      if (reduceWeightSum == 0)
-        info.reduceWeight = 0;
-      else
-        info.reduceWeight *= (poolWeight / reduceWeightSum); 
-    }
-  }
-  
-  private void updateMinSlots() {
-    // Clear old minSlots
-    for (JobInfo info: infos.values()) {
-      info.minMaps = 0;
-      info.minReduces = 0;
-    }
-    // For each pool, distribute its task allocation among jobs in it that need
-    // slots. This is a little tricky since some jobs in the pool might not be
-    // able to use all the slots, e.g. they might have only a few tasks left.
-    // To deal with this, we repeatedly split up the available task slots
-    // between the jobs left, give each job min(its alloc, # of slots it needs),
-    // and redistribute any slots that are left over between jobs that still
-    // need slots on the next pass. If, in total, the jobs in our pool don't
-    // need all its allocation, we leave the leftover slots for general use.
-    PoolManager poolMgr = getPoolManager();
-    for (Pool pool: poolMgr.getPools()) {
-      for (final TaskType type: MAP_AND_REDUCE) {
-        Set<JobInProgress> jobs = new HashSet<JobInProgress>(pool.getJobs());
-        int slotsLeft = poolMgr.getAllocation(pool.getName(), type);
-        // Keep assigning slots until none are left
-        while (slotsLeft > 0) {
-          // Figure out total weight of jobs that still need slots
-          double totalWeight = 0;
-          for (Iterator<JobInProgress> it = jobs.iterator(); it.hasNext();) {
-            JobInProgress job = it.next();
-            if (isRunnable(job) &&
-                runnableTasks(job, type) > minTasks(job, type)) {
-              totalWeight += weight(job, type);
-            } else {
-              it.remove();
-            }
-          }
-          if (totalWeight == 0) // No jobs that can use more slots are left 
-            break;
-          // Assign slots to jobs, using the floor of their weight divided by
-          // total weight. This ensures that all jobs get some chance to take
-          // a slot. Then, if no slots were assigned this way, we do another
-          // pass where we use ceil, in case some slots were still left over.
-          int oldSlots = slotsLeft; // Copy slotsLeft so we can modify it
-          for (JobInProgress job: jobs) {
-            double weight = weight(job, type);
-            int share = (int) Math.floor(oldSlots * weight / totalWeight);
-            slotsLeft = giveMinSlots(job, type, slotsLeft, share);
-          }
-          if (slotsLeft == oldSlots) {
-            // No tasks were assigned; do another pass using ceil, giving the
-            // extra slots to jobs in order of weight then deficit
-            List<JobInProgress> sortedJobs = new ArrayList<JobInProgress>(jobs);
-            Collections.sort(sortedJobs, new Comparator<JobInProgress>() {
-              public int compare(JobInProgress j1, JobInProgress j2) {
-                double dif = weight(j2, type) - weight(j1, type);
-                if (dif == 0) // Weights are equal, compare by deficit 
-                  dif = deficit(j2, type) - deficit(j1, type);
-                return (int) Math.signum(dif);
-              }
-            });
-            for (JobInProgress job: sortedJobs) {
-              double weight = weight(job, type);
-              int share = (int) Math.ceil(oldSlots * weight / totalWeight);
-              slotsLeft = giveMinSlots(job, type, slotsLeft, share);
-            }
-            if (slotsLeft > 0) {
-              LOG.warn("Had slotsLeft = " + slotsLeft + " after the final "
-                  + "loop in updateMinSlots. This probably means some fair "
-                  + "scheduler weights are being set to NaN or Infinity.");
-            }
-            break;
-          }
-        }
-      }
-    }
-  }
-
-  /**
-   * Give up to <code>tasksToGive</code> min slots to a job (potentially fewer
-   * if either the job needs fewer slots or there aren't enough slots left).
-   * Returns the number of slots left over.
-   */
-  private int giveMinSlots(JobInProgress job, TaskType type,
-      int slotsLeft, int slotsToGive) {
-    int runnable = runnableTasks(job, type);
-    int curMin = minTasks(job, type);
-    slotsToGive = Math.min(Math.min(slotsLeft, runnable - curMin), slotsToGive);
-    slotsLeft -= slotsToGive;
-    JobInfo info = infos.get(job);
-    if (type == TaskType.MAP)
-      info.minMaps += slotsToGive;
-    else
-      info.minReduces += slotsToGive;
-    return slotsLeft;
-  }
-
-  private void updateFairShares(ClusterStatus clusterStatus) {
-    // Clear old fairShares
-    for (JobInfo info: infos.values()) {
-      info.mapFairShare = 0;
-      info.reduceFairShare = 0;
-    }
-    // Assign new shares, based on weight and minimum share. This is done
-    // as follows. First, we split up the available slots between all
-    // jobs according to weight. Then if there are any jobs whose minSlots is
-    // larger than their fair allocation, we give them their minSlots and
-    // remove them from the list, and start again with the amount of slots
-    // left over. This continues until all jobs' minSlots are less than their
-    // fair allocation, and at this point we know that we've met everyone's
-    // guarantee and we've split the excess capacity fairly among jobs left.
-    for (TaskType type: MAP_AND_REDUCE) {
-      // Select only jobs that still need this type of task
-      HashSet<JobInfo> jobsLeft = new HashSet<JobInfo>();
-      for (Entry<JobInProgress, JobInfo> entry: infos.entrySet()) {
-        JobInProgress job = entry.getKey();
-        JobInfo info = entry.getValue();
-        if (isRunnable(job) && runnableTasks(job, type) > 0) {
-          jobsLeft.add(info);
-        }
-      }
-      double slotsLeft = getTotalSlots(type, clusterStatus);
-      while (!jobsLeft.isEmpty()) {
-        double totalWeight = 0;
-        for (JobInfo info: jobsLeft) {
-          double weight = (type == TaskType.MAP ?
-              info.mapWeight : info.reduceWeight);
-          totalWeight += weight;
-        }
-        boolean recomputeSlots = false;
-        double oldSlots = slotsLeft; // Copy slotsLeft so we can modify it
-        for (Iterator<JobInfo> iter = jobsLeft.iterator(); iter.hasNext();) {
-          JobInfo info = iter.next();
-          double minSlots = (type == TaskType.MAP ?
-              info.minMaps : info.minReduces);
-          double weight = (type == TaskType.MAP ?
-              info.mapWeight : info.reduceWeight);
-          double fairShare = weight / totalWeight * oldSlots;
-          if (minSlots > fairShare) {
-            // Job needs more slots than its fair share; give it its minSlots,
-            // remove it from the list, and set recomputeSlots = true to 
-            // remember that we must loop again to redistribute unassigned slots
-            if (type == TaskType.MAP)
-              info.mapFairShare = minSlots;
-            else
-              info.reduceFairShare = minSlots;
-            slotsLeft -= minSlots;
-            iter.remove();
-            recomputeSlots = true;
-          }
-        }
-        if (!recomputeSlots) {
-          // All minimums are met. Give each job its fair share of excess slots.
-          for (JobInfo info: jobsLeft) {
-            double weight = (type == TaskType.MAP ?
-                info.mapWeight : info.reduceWeight);
-            double fairShare = weight / totalWeight * oldSlots;
-            if (type == TaskType.MAP)
-              info.mapFairShare = fairShare;
-            else
-              info.reduceFairShare = fairShare;
-          }
-          break;
-        }
-      }
-    }
-  }
-
-  private double calculateRawWeight(JobInProgress job, TaskType taskType) {
+  public double getJobWeight(JobInProgress job, TaskType taskType) {
     if (!isRunnable(job)) {
-      return 0;
+      // Job won't launch tasks, but don't return 0 to avoid division errors
+      return 1.0;
     } else {
       double weight = 1.0;
       if (sizeBasedWeight) {
         // Set weight based on runnable tasks
-        weight = Math.log1p(runnableTasks(job, taskType)) / Math.log(2);
+        JobInfo info = infos.get(job);
+        int runnableTasks = (taskType == TaskType.MAP) ?
+            info.mapSchedulable.getDemand() : 
+            info.reduceSchedulable.getDemand();
+        weight = Math.log1p(runnableTasks) / Math.log(2);
       }
       weight *= getPriorityFactor(job.getPriority());
       if (weightAdjuster != null) {
@@ -813,69 +639,57 @@ public class FairScheduler extends TaskScheduler {
   }
 
   /**
-   * Update the preemption JobInfo fields for all jobs, i.e. the times since
-   * each job last was at its guaranteed share and at > 1/2 of its fair share
+   * Update the preemption fields for all PoolScheduables, i.e. the times since
+   * each pool last was at its guaranteed share and at > 1/2 of its fair share
    * for each type of task.
    */
   private void updatePreemptionVariables() {
     long now = clock.getTime();
-    for (Map.Entry<JobInProgress, JobInfo> entry: infos.entrySet()) {
-      JobInProgress job = entry.getKey();
-      JobInfo info = entry.getValue();
-      if (job.getStatus().getRunState() != JobStatus.RUNNING) {
-        // Job is still in PREP state and tasks aren't initialized. Count it as
-        // both at min and fair share since we shouldn't start any timeouts now.
-        info.lastTimeAtMapMinShare = now;
-        info.lastTimeAtReduceMinShare = now;
-        info.lastTimeAtMapHalfFairShare = now;
-        info.lastTimeAtReduceHalfFairShare = now;
-      } else {
-        if (!isStarvedForMinShare(job, TaskType.MAP))
-          info.lastTimeAtMapMinShare = now;
-        if (!isStarvedForMinShare(job, TaskType.REDUCE))
-          info.lastTimeAtReduceMinShare = now;
-        if (!isStarvedForFairShare(job, TaskType.MAP))
-          info.lastTimeAtMapHalfFairShare = now;
-        if (!isStarvedForFairShare(job, TaskType.REDUCE))
-          info.lastTimeAtReduceHalfFairShare = now;
+    for (TaskType type: MAP_AND_REDUCE) {
+      for (PoolSchedulable sched: getPoolSchedulables(type)) {
+        if (!isStarvedForMinShare(sched)) {
+          sched.setLastTimeAtMinShare(now);
+        }
+        if (!isStarvedForFairShare(sched)) {
+          sched.setLastTimeAtHalfFairShare(now);
+        }
+        eventLog.log("PREEMPT_VARS", sched.getName(), type,
+            now - sched.getLastTimeAtMinShare(),
+            now - sched.getLastTimeAtHalfFairShare());
       }
-      eventLog.log("PREEMPT_VARS", job.getJobID(),
-          now - info.lastTimeAtMapMinShare,
-          now - info.lastTimeAtMapHalfFairShare);
     }
   }
 
   /**
-   * Is a job below its min share for the given task type?
+   * Is a pool below its min share for the given task type?
    */
-  boolean isStarvedForMinShare(JobInProgress job, TaskType taskType) {
-    return runningTasks(job, taskType) < minTasks(job, taskType);
+  boolean isStarvedForMinShare(PoolSchedulable sched) {
+    int desiredShare = Math.min(sched.getMinShare(), sched.getDemand());
+    return (sched.getRunningTasks() < desiredShare);
   }
   
   /**
-   * Is a job being starved for fair share for the given task type?
-   * This is defined as being below half its fair share *and* having a
-   * positive deficit.
+   * Is a pool being starved for fair share for the given task type?
+   * This is defined as being below half its fair share.
    */
-  boolean isStarvedForFairShare(JobInProgress job, TaskType type) {
+  boolean isStarvedForFairShare(PoolSchedulable sched) {
     int desiredFairShare = (int) Math.floor(Math.min(
-        fairTasks(job, type) / 2, neededTasks(job, type)));
-    return (runningTasks(job, type) < desiredFairShare &&
-            deficit(job, type) > 0);
+        sched.getFairShare() / 2, sched.getDemand()));
+    return (sched.getRunningTasks() < desiredFairShare);
   }
-  
+
   /**
-   * Check for jobs that need tasks preempted, either because they have been
-   * below their guaranteed share for their pool's preemptionTimeout or they
+   * Check for pools that need tasks preempted, either because they have been
+   * below their guaranteed share for minSharePreemptionTimeout or they
    * have been below half their fair share for the fairSharePreemptionTimeout.
-   * If such jobs exist, compute how many tasks of each type need to be
-   * preempted and then select the right ones using selectTasksToPreempt.
+   * If such pools exist, compute how many tasks of each type need to be
+   * preempted and then select the right ones using preemptTasks.
    * 
    * This method computes and logs the number of tasks we want to preempt even
    * if preemption is disabled, for debugging purposes.
    */
   protected void preemptTasksIfNecessary() {
-    if (!preemptionEnabled || useFifo)
+    if (!preemptionEnabled)
       return;
     
     long curTime = clock.getTime();
@@ -887,31 +701,16 @@ public class FairScheduler extends TaskScheduler {
     // because we might need to call some JobTracker methods (killTask).
     synchronized (taskTrackerManager) {
       synchronized (this) {
-        List<JobInProgress> jobs = new ArrayList<JobInProgress>(infos.keySet());
         for (TaskType type: MAP_AND_REDUCE) {
+          List<PoolSchedulable> scheds = getPoolSchedulables(type);
           int tasksToPreempt = 0;
-          for (JobInProgress job: jobs) {
-            tasksToPreempt += tasksToPreempt(job, type, curTime);
+          for (PoolSchedulable sched: scheds) {
+            tasksToPreempt += tasksToPreempt(sched, curTime);
           }
           if (tasksToPreempt > 0) {
             eventLog.log("SHOULD_PREEMPT", type, tasksToPreempt);
             if (!onlyLogPreemption) {
-              // Actually preempt the tasks. The policy for this is to pick
-              // tasks from jobs that are above their min share and have very 
-              // negative deficits (meaning they've been over-scheduled). 
-              // However, we also want to minimize the amount of computation
-              // wasted by preemption, so prefer tasks that started recently.
-              // We go through all jobs in order of deficit (highest first), 
-              // and for each job, we preempt tasks in order of start time 
-              // until we hit either minTasks or fairTasks tasks left (so as
-              // not to create a new starved job).
-              Collections.sort(jobs, new DeficitComparator(type));
-              for (int i = jobs.size() - 1; i >= 0; i--) {
-                JobInProgress job = jobs.get(i);
-                int tasksPreempted = preemptTasks(job, type, tasksToPreempt);
-                tasksToPreempt -= tasksPreempted;
-                if (tasksToPreempt == 0) break;
-              }
+              preemptTasks(scheds, tasksToPreempt);
             }
           }
         }
@@ -920,44 +719,98 @@ public class FairScheduler extends TaskScheduler {
   }
 
   /**
-   * Count how many tasks of a given type the job needs to preempt, if any.
-   * If the job has been below its min share for at least its pool's preemption
+   * Preempt a given number of tasks from a list of PoolSchedulables. 
+   * The policy for this is to pick tasks from pools that are over their fair 
+   * share, but make sure that no pool is placed below its fair share in the 
+   * process. Furthermore, we want to minimize the amount of computation
+   * wasted by preemption, so out of the tasks in over-scheduled pools, we
+   * prefer to preempt tasks that started most recently.
+   */
+  private void preemptTasks(List<PoolSchedulable> scheds, int tasksToPreempt) {
+    if (scheds.isEmpty() || tasksToPreempt == 0)
+      return;
+    
+    TaskType taskType = scheds.get(0).getTaskType();
+    
+    // Collect running tasks of our type from over-scheduled pools
+    List<TaskStatus> runningTasks = new ArrayList<TaskStatus>();
+    for (PoolSchedulable sched: scheds) {
+      if (sched.getRunningTasks() > sched.getFairShare())
+      for (JobSchedulable js: sched.getJobSchedulables()) {
+        runningTasks.addAll(getRunningTasks(js.getJob(), taskType));
+      }
+    }
+    
+    // Sort tasks into reverse order of start time
+    Collections.sort(runningTasks, new Comparator<TaskStatus>() {
+      public int compare(TaskStatus t1, TaskStatus t2) {
+        if (t1.getStartTime() < t2.getStartTime())
+          return 1;
+        else if (t1.getStartTime() == t2.getStartTime())
+          return 0;
+        else
+          return -1;
+      }
+    });
+    
+    // Maintain a count of tasks left in each pool; this is a bit
+    // faster than calling runningTasks() on the pool repeatedly
+    // because the latter must scan through jobs in the pool
+    HashMap<Pool, Integer> tasksLeft = new HashMap<Pool, Integer>(); 
+    for (Pool p: poolMgr.getPools()) {
+      tasksLeft.put(p, p.getSchedulable(taskType).getRunningTasks());
+    }
+    
+    // Scan down the sorted list of task statuses until we've killed enough
+    // tasks, making sure we don't kill too many from any pool
+    for (TaskStatus status: runningTasks) {
+      JobID jobID = status.getTaskID().getJobID();
+      JobInProgress job = taskTrackerManager.getJob(jobID);
+      Pool pool = poolMgr.getPool(job);
+      PoolSchedulable sched = pool.getSchedulable(taskType);
+      if (tasksLeft.get(pool) > sched.getFairShare()) {
+        eventLog.log("PREEMPT", status.getTaskID(),
+            status.getTaskTracker());
+        try {
+          taskTrackerManager.killTask(status.getTaskID(), false);
+          tasksToPreempt--;
+          if (tasksToPreempt == 0)
+            break;
+        } catch (IOException e) {
+          LOG.error("Failed to kill task " + status.getTaskID(), e);
+        }
+      }
+    }
+  }
+
+  /**
+   * Count how many tasks of a given type the pool needs to preempt, if any.
+   * If the pool has been below its min share for at least its preemption
    * timeout, it should preempt the difference between its current share and
    * this min share. If it has been below half its fair share for at least the
    * fairSharePreemptionTimeout, it should preempt enough tasks to get up to
-   * its full fair share. If both situations hold, we preempt the max of the
+   * its full fair share. If both conditions hold, we preempt the max of the
    * two amounts (this shouldn't happen unless someone sets the timeouts to
    * be identical for some reason).
    */
-  protected int tasksToPreempt(JobInProgress job, TaskType type, long curTime) {
-    JobInfo info = infos.get(job);
-    if (info == null) return 0;
-    String pool = poolMgr.getPoolName(job);
+  protected int tasksToPreempt(PoolSchedulable sched, long curTime) {
+    String pool = sched.getName();
     long minShareTimeout = poolMgr.getMinSharePreemptionTimeout(pool);
     long fairShareTimeout = poolMgr.getFairSharePreemptionTimeout();
     int tasksDueToMinShare = 0;
     int tasksDueToFairShare = 0;
-    if (type == TaskType.MAP) {
-      if (curTime - info.lastTimeAtMapMinShare > minShareTimeout) {
-        tasksDueToMinShare = info.minMaps - info.runningMaps;
-      }
-      if (curTime - info.lastTimeAtMapHalfFairShare > fairShareTimeout) {
-        double fairShare = Math.min(info.mapFairShare, info.neededMaps);
-        tasksDueToFairShare = (int) (fairShare - info.runningMaps);
-      }
-    } else { // type == TaskType.REDUCE
-      if (curTime - info.lastTimeAtReduceMinShare > minShareTimeout) {
-        tasksDueToMinShare = info.minReduces - info.runningReduces;
-      }
-      if (curTime - info.lastTimeAtReduceHalfFairShare > fairShareTimeout) {
-        double fairShare = Math.min(info.reduceFairShare, info.neededReduces);
-        tasksDueToFairShare = (int) (fairShare - info.runningReduces);
-      }
+    if (curTime - sched.getLastTimeAtMinShare() > minShareTimeout) {
+      int target = Math.min(sched.getMinShare(), sched.getDemand());
+      tasksDueToMinShare = target - sched.getRunningTasks();
+    }
+    if (curTime - sched.getLastTimeAtHalfFairShare() > fairShareTimeout) {
+      int target = (int) Math.min(sched.getFairShare(), sched.getDemand());
+      tasksDueToFairShare = target - sched.getRunningTasks();
     }
     int tasksToPreempt = Math.max(tasksDueToMinShare, tasksDueToFairShare);
     if (tasksToPreempt > 0) {
       String message = "Should preempt " + tasksToPreempt + " " 
-          + type + " tasks for " + job.getJobID() 
+          + sched.getTaskType() + " tasks for pool " + sched.getName() 
           + ": tasksDueToMinShare = " + tasksDueToMinShare
           + ", tasksDueToFairShare = " + tasksDueToFairShare;
       eventLog.log("INFO", message);
@@ -966,24 +819,7 @@ public class FairScheduler extends TaskScheduler {
     return tasksToPreempt;
   }
 
-  /**
-   * Preempt up to maxToPreempt tasks of the given type from the given job,
-   * without having it go below its min share or below half its fair share.
-   * Selects the tasks so as to preempt the least recently launched one first,
-   * thus minimizing wasted compute time. Returns the number of tasks preempted.
-   */
-  private int preemptTasks(JobInProgress job, TaskType type, int maxToPreempt) {
-    // Figure out how many tasks to preempt. NOTE: We use the runningTasks, etc
-    // values in JobInfo rather than re-counting them, but this should be safe
-    // because we are being called only inside update(), which has a lock on
-    // the JobTracker, so all the values are fresh.
-    int desiredFairShare = (int) Math.floor(Math.min(
-        fairTasks(job, type) / 2, neededTasks(job, type)));
-    int tasksToLeave = Math.max(minTasks(job, type), desiredFairShare);
-    int tasksToPreempt = Math.min(
-        maxToPreempt, runningTasks(job, type) - tasksToLeave);
-    if (tasksToPreempt == 0)
-      return 0;
+  private List<TaskStatus> getRunningTasks(JobInProgress job, TaskType type) {
     // Create a list of all running TaskInProgress'es in the job
     List<TaskInProgress> tips = new ArrayList<TaskInProgress>();
     if (type == TaskType.MAP) {
@@ -1010,86 +846,10 @@ public class FairScheduler extends TaskScheduler {
         }
       }
     }
-
-    // Sort the statuses in order of start time, with the latest launched first
-    Collections.sort(statuses, new Comparator<TaskStatus>() {
-      public int compare(TaskStatus t1, TaskStatus t2) {
-        return (int) Math.signum(t2.getStartTime() - t1.getStartTime());
-      }
-    });
-    // Preempt the tasks in order of start time until we've done enough
-    int numKilled = 0;
-    for (int i = 0; i < tasksToPreempt; i++) {
-      if (i > statuses.size() - tasksToLeave) {
-        // Sanity check in case we computed maxToPreempt incorrectly due to
-        // stale data in JobInfos. Shouldn't happen if we are called from update.
-        LOG.error("Stale task counts in the JobInfos in preemptTasks - "
-            + "probaly due to calling preemptTasks() from outside update(). ");
-        break;
-      }
-      TaskStatus status = statuses.get(i);
-      eventLog.log("PREEMPT", status.getTaskID(), status.getTaskTracker());
-      try {
-        taskTrackerManager.killTask(status.getTaskID(), false);
-        numKilled++;
-      } catch (IOException e) {
-        LOG.error("Failed to kill task " + status.getTaskID(), e);
-      }
-    }
-    return numKilled;
+    return statuses;
   }
 
 
-  public synchronized boolean getUseFifo() {
-    return useFifo;
-  }
-  
-  public synchronized void setUseFifo(boolean useFifo) {
-    this.useFifo = useFifo;
-  }
-  
-  // Getter methods for reading JobInfo values based on TaskType, safely
-  // returning 0's for jobs with no JobInfo present.
-
-  protected int neededTasks(JobInProgress job, TaskType taskType) {
-    JobInfo info = infos.get(job);
-    if (info == null) return 0;
-    return taskType == TaskType.MAP ? info.neededMaps : info.neededReduces;
-  }
-  
-  protected int runningTasks(JobInProgress job, TaskType taskType) {
-    JobInfo info = infos.get(job);
-    if (info == null) return 0;
-    return taskType == TaskType.MAP ? info.runningMaps : info.runningReduces;
-  }
-
-  protected int runnableTasks(JobInProgress job, TaskType type) {
-    return neededTasks(job, type) + runningTasks(job, type);
-  }
-
-  protected int minTasks(JobInProgress job, TaskType type) {
-    JobInfo info = infos.get(job);
-    if (info == null) return 0;
-    return (type == TaskType.MAP) ? info.minMaps : info.minReduces;
-  }
-
-  protected double fairTasks(JobInProgress job, TaskType type) {
-    JobInfo info = infos.get(job);
-    if (info == null) return 0;
-    return (type == TaskType.MAP) ? info.mapFairShare : info.reduceFairShare;
-  }
-
-  protected double weight(JobInProgress job, TaskType taskType) {
-    JobInfo info = infos.get(job);
-    if (info == null) return 0;
-    return (taskType == TaskType.MAP ? info.mapWeight : info.reduceWeight);
-  }
-
-  protected double deficit(JobInProgress job, TaskType taskType) {
-    JobInfo info = infos.get(job);
-    if (info == null) return 0;
-    return taskType == TaskType.MAP ? info.mapDeficit : info.reduceDeficit;
-  }
 
   protected boolean isRunnable(JobInProgress job) {
     JobInfo info = infos.get(job);
@@ -1130,13 +890,14 @@ public class FairScheduler extends TaskScheduler {
       for (JobInProgress job: jobs) {
         JobProfile profile = job.getProfile();
         JobInfo info = infos.get(job);
-        eventLog.log("JOB",
+        // TODO: Fix
+        /*eventLog.log("JOB",
             profile.getJobID(), profile.name, profile.user,
             job.getPriority(), poolMgr.getPoolName(job),
             job.numMapTasks, info.runningMaps, info.neededMaps, 
             info.mapFairShare, info.mapWeight, info.mapDeficit,
             job.numReduceTasks, info.runningReduces, info.neededReduces, 
-            info.reduceFairShare, info.reduceWeight, info.reduceDeficit);
+            info.reduceFairShare, info.reduceWeight, info.reduceDeficit);*/
       }
       // List pools in alphabetical order
       List<Pool> pools = new ArrayList<Pool>(poolMgr.getPools());
@@ -1154,8 +915,9 @@ public class FairScheduler extends TaskScheduler {
         for (JobInProgress job: pool.getJobs()) {
           JobInfo info = infos.get(job);
           if (info != null) {
-            runningMaps += info.runningMaps;
-            runningReduces += info.runningReduces;
+            // TODO: Fix
+            //runningMaps += info.runningMaps;
+            //runningReduces += info.runningReduces;
           }
         }
         String name = pool.getName();
@@ -1168,4 +930,16 @@ public class FairScheduler extends TaskScheduler {
       eventLog.log("END_DUMP");
     }
   }
+
+  public Clock getClock() {
+    return clock;
+  }
+  
+  public FairSchedulerEventLog getEventLog() {
+    return eventLog;
+  }
+
+  public JobInfo getJobInfo(JobInProgress job) {
+    return infos.get(job);
+  }
 }
diff --git a/src/contrib/fairscheduler/src/java/org/apache/hadoop/mapred/FairSchedulerServlet.java b/src/contrib/fairscheduler/src/java/org/apache/hadoop/mapred/FairSchedulerServlet.java
index cbb8109..b09ecf2 100644
--- a/src/contrib/fairscheduler/src/java/org/apache/hadoop/mapred/FairSchedulerServlet.java
+++ b/src/contrib/fairscheduler/src/java/org/apache/hadoop/mapred/FairSchedulerServlet.java
@@ -45,10 +45,8 @@ import org.apache.hadoop.util.StringUtils;
  * Servlet for displaying fair scheduler information, installed at
  * [job tracker URL]/scheduler when the {@link FairScheduler} is in use.
  * 
- * The main features are viewing each job's task count and fair share, ability
- * to change job priorities and pools from the UI, and ability to switch the
- * scheduler to FIFO mode without restarting the JobTracker if this is required
- * for any reason.
+ * The main features are viewing each job's task count and fair share,
+ * and admin controls to change job priorities and pools from the UI.
  * 
  * There is also an "advanced" view for debugging that can be turned on by
  * going to [job tracker URL]/scheduler?advanced.
@@ -82,11 +80,6 @@ public class FairSchedulerServlet extends HttpServlet {
     // If the request has a set* param, handle that and redirect to the regular
     // view page so that the user won't resubmit the data if they hit refresh.
     boolean advancedView = request.getParameter("advanced") != null;
-    if (request.getParameter("setFifo") != null) {
-      scheduler.setUseFifo(request.getParameter("setFifo").equals("true"));
-      response.sendRedirect("/scheduler" + (advancedView ? "?advanced" : ""));
-      return;
-    }
     if (request.getParameter("setPool") != null) {
       Collection<JobInProgress> runningJobs = jobTracker.getRunningJobs();
       PoolManager poolMgr = null;
@@ -133,15 +126,14 @@ public class FairSchedulerServlet extends HttpServlet {
     String hostname = StringUtils.simpleHostname(
         jobTracker.getJobTrackerMachine());
     out.print("<html><head>");
-    out.printf("<title>%s Job Scheduler Admininstration</title>\n", hostname);
+    out.printf("<title>%s Fair Scheduler Admininstration</title>\n", hostname);
     out.print("<link rel=\"stylesheet\" type=\"text/css\" " + 
         "href=\"/static/hadoop.css\">\n");
     out.print("</head><body>\n");
     out.printf("<h1><a href=\"/jobtracker.jsp\">%s</a> " + 
-        "Job Scheduler Administration</h1>\n", hostname);
+        "Fair Scheduler Administration</h1>\n", hostname);
     showPools(out, advancedView);
     showJobs(out, advancedView);
-    showAdminForm(out, advancedView);
     out.print("</body></html>\n");
     out.close();
 
@@ -159,9 +151,13 @@ public class FairSchedulerServlet extends HttpServlet {
       PoolManager poolManager = scheduler.getPoolManager();
       out.print("<h2>Pools</h2>\n");
       out.print("<table border=\"2\" cellpadding=\"5\" cellspacing=\"2\">\n");
-      out.print("<tr><th>Pool</th><th>Running Jobs</th>" + 
-          "<th>Min Maps</th><th>Min Reduces</th>" + 
-          "<th>Running Maps</th><th>Running Reduces</th></tr>\n");
+      out.print("<tr><th rowspan=2>Pool</th>" +
+          "<th rowspan=2>Running Jobs</th>" + 
+          "<th colspan=3>Map Tasks</th>" + 
+          "<th colspan=3>Reduce Tasks</th>" +
+          "<th rowspan=2>Scheduling Mode</th></tr>\n<tr>" + 
+          "<th>Min Share</th><th>Running</th><th>Fair Share</th>" + 
+          "<th>Min Share</th><th>Running</th><th>Fair Share</th></tr>\n");
       List<Pool> pools = new ArrayList<Pool>(poolManager.getPools());
       Collections.sort(pools, new Comparator<Pool>() {
         public int compare(Pool p1, Pool p2) {
@@ -172,24 +168,21 @@ public class FairSchedulerServlet extends HttpServlet {
           else return p1.getName().compareTo(p2.getName());
         }});
       for (Pool pool: pools) {
-        int runningMaps = 0;
-        int runningReduces = 0;
-        for (JobInProgress job: pool.getJobs()) {
-          JobInfo info = scheduler.infos.get(job);
-          if (info != null) {
-            runningMaps += info.runningMaps;
-            runningReduces += info.runningReduces;
-          }
-        }
-        out.print("<tr>\n");
-        out.printf("<td>%s</td>\n", pool.getName());
-        out.printf("<td>%s</td>\n", pool.getJobs().size());
-        out.printf("<td>%s</td>\n", poolManager.getAllocation(pool.getName(),
+        String name = pool.getName();
+        int runningMaps = pool.getMapSchedulable().getRunningTasks();
+        int runningReduces = pool.getReduceSchedulable().getRunningTasks();
+        out.print("<tr>");
+        out.printf("<td>%s</td>", name);
+        out.printf("<td>%d</td>", pool.getJobs().size());
+        out.printf("<td>%d</td>", poolManager.getAllocation(name,
             TaskType.MAP));
-        out.printf("<td>%s</td>\n", poolManager.getAllocation(pool.getName(), 
+        out.printf("<td>%d</td>", runningMaps);
+        out.printf("<td>%.1f</td>", pool.getMapSchedulable().getFairShare());
+        out.printf("<td>%d</td>", poolManager.getAllocation(name,
             TaskType.REDUCE));
-        out.printf("<td>%s</td>\n", runningMaps);
-        out.printf("<td>%s</td>\n", runningReduces);
+        out.printf("<td>%d</td>", runningReduces);
+        out.printf("<td>%.1f</td>", pool.getReduceSchedulable().getFairShare());
+        out.printf("<td>%s</td>", pool.getSchedulingMode());
         out.print("</tr>\n");
       }
       out.print("</table>\n");
@@ -202,21 +195,21 @@ public class FairSchedulerServlet extends HttpServlet {
   private void showJobs(PrintWriter out, boolean advancedView) {
     out.print("<h2>Running Jobs</h2>\n");
     out.print("<table border=\"2\" cellpadding=\"5\" cellspacing=\"2\">\n");
-    int colsPerTaskType = advancedView ? 6 : 3;
+    int colsPerTaskType = advancedView ? 4 : 3;
     out.printf("<tr><th rowspan=2>Submitted</th>" + 
         "<th rowspan=2>JobID</th>" +
         "<th rowspan=2>User</th>" +
         "<th rowspan=2>Name</th>" +
         "<th rowspan=2>Pool</th>" +
         "<th rowspan=2>Priority</th>" +
-        "<th colspan=%d>Maps</th>" +
-        "<th colspan=%d>Reduces</th>",
+        "<th colspan=%d>Map Tasks</th>" +
+        "<th colspan=%d>Reduce Tasks</th>",
         colsPerTaskType, colsPerTaskType);
     out.print("</tr><tr>\n");
     out.print("<th>Finished</th><th>Running</th><th>Fair Share</th>" +
-        (advancedView ? "<th>Weight</th><th>Deficit</th><th>minMaps</th>" : ""));
+        (advancedView ? "<th>Weight</th>" : ""));
     out.print("<th>Finished</th><th>Running</th><th>Fair Share</th>" +
-        (advancedView ? "<th>Weight</th><th>Deficit</th><th>minReduces</th>" : ""));
+        (advancedView ? "<th>Weight</th>" : ""));
     out.print("</tr>\n");
     synchronized (jobTracker) {
       Collection<JobInProgress> runningJobs = jobTracker.getRunningJobs();
@@ -225,7 +218,7 @@ public class FairSchedulerServlet extends HttpServlet {
           JobProfile profile = job.getProfile();
           JobInfo info = scheduler.infos.get(job);
           if (info == null) { // Job finished, but let's show 0's for info
-            info = new JobInfo(0);
+            info = new JobInfo(null, null);
           }
           out.print("<tr>\n");
           out.printf("<td>%s</td>\n", DATE_FORMAT.format(
@@ -245,25 +238,25 @@ public class FairSchedulerServlet extends HttpServlet {
                        job.getPriority().toString(),
                        "/scheduler?setPriority=<CHOICE>&jobid=" + profile.getJobID() +
                        (advancedView ? "&advanced" : "")));
-          out.printf("<td>%d / %d</td><td>%d</td><td>%8.1f</td>\n",
-                     job.finishedMaps(), job.desiredMaps(), info.runningMaps,
-                     info.mapFairShare);
+          Pool pool = scheduler.getPoolManager().getPool(job);
+          String mapShare = (pool.getSchedulingMode() == SchedulingMode.FAIR) ?
+            String.format("%.1f", info.mapSchedulable.getFairShare()) : "NA";
+          out.printf("<td>%d / %d</td><td>%d</td><td>%s</td>\n",
+                     job.finishedMaps(), job.desiredMaps(), 
+                     info.mapSchedulable.getRunningTasks(),
+                     mapShare);
           if (advancedView) {
-            out.printf("<td>%8.1f</td>\n", info.mapWeight);
-            out.printf("<td>%s</td>\n", info.neededMaps > 0 ?
-                       (info.mapDeficit / 1000) + "s" : "--");
-            out.printf("<td>%d</td>\n", info.minMaps);
+            out.printf("<td>%.1f</td>\n", info.mapSchedulable.getWeight());
           }
-          out.printf("<td>%d / %d</td><td>%d</td><td>%8.1f</td>\n",
-                     job.finishedReduces(), job.desiredReduces(), info.runningReduces,
-                     info.reduceFairShare);
+          String reduceShare = (pool.getSchedulingMode() == SchedulingMode.FAIR) ?
+            String.format("%.1f", info.reduceSchedulable.getFairShare()) : "NA";
+          out.printf("<td>%d / %d</td><td>%d</td><td>%s</td>\n",
+                     job.finishedReduces(), job.desiredReduces(), 
+                     info.reduceSchedulable.getRunningTasks(),
+                     reduceShare);
           if (advancedView) {
-            out.printf("<td>%8.1f</td>\n", info.reduceWeight);
-            out.printf("<td>%s</td>\n", info.neededReduces > 0 ?
-                       (info.reduceDeficit / 1000) + "s" : "--");
-            out.printf("<td>%d</td>\n", info.minReduces);
+            out.printf("<td>%.1f</td>\n", info.reduceSchedulable.getWeight());
           }
-          out.print("</tr>\n");
         }
       }
     }
@@ -292,24 +285,4 @@ public class FairSchedulerServlet extends HttpServlet {
     html.append("</select>\n");
     return html.toString();
   }
-
-  /**
-   * Print the administration form at the bottom of the page, which currently
-   * only includes the button for switching between FIFO and Fair Scheduling.
-   */
-  private void showAdminForm(PrintWriter out, boolean advancedView) {
-    out.print("<h2>Scheduling Mode</h2>\n");
-    String curMode = scheduler.getUseFifo() ? "FIFO" : "Fair Sharing";
-    String otherMode = scheduler.getUseFifo() ? "Fair Sharing" : "FIFO";
-    String advParam = advancedView ? "?advanced" : "";
-    out.printf("<form method=\"post\" action=\"/scheduler%s\">\n", advParam);
-    out.printf("<p>The scheduler is currently using <b>%s mode</b>. " +
-        "<input type=\"submit\" value=\"Switch to %s mode.\" " + 
-        "onclick=\"return confirm('Are you sure you want to change " +
-        "scheduling mode to %s?')\" />\n",
-        curMode, otherMode, otherMode);
-    out.printf("<input type=\"hidden\" name=\"setFifo\" value=\"%s\" />",
-        !scheduler.getUseFifo());
-    out.print("</form>\n");
-  }
 }
diff --git a/src/contrib/fairscheduler/src/java/org/apache/hadoop/mapred/FifoJobComparator.java b/src/contrib/fairscheduler/src/java/org/apache/hadoop/mapred/FifoJobComparator.java
index 04f1815..75d7e81 100644
--- a/src/contrib/fairscheduler/src/java/org/apache/hadoop/mapred/FifoJobComparator.java
+++ b/src/contrib/fairscheduler/src/java/org/apache/hadoop/mapred/FifoJobComparator.java
@@ -35,7 +35,8 @@ public class FifoJobComparator implements Comparator<JobInProgress> {
       }
     }
     if (res == 0) {
-      res = j1.hashCode() - j2.hashCode();
+      // If there is a tie, break it by job ID to get a deterministic order
+      res = j1.getJobID().compareTo(j2.getJobID());
     }
     return res;
   }
diff --git a/src/contrib/fairscheduler/src/java/org/apache/hadoop/mapred/JobSchedulable.java b/src/contrib/fairscheduler/src/java/org/apache/hadoop/mapred/JobSchedulable.java
new file mode 100644
index 0000000..ed8860b
--- /dev/null
+++ b/src/contrib/fairscheduler/src/java/org/apache/hadoop/mapred/JobSchedulable.java
@@ -0,0 +1,145 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.hadoop.mapred;
+
+import java.io.IOException;
+import java.util.Collection;
+
+import org.apache.hadoop.mapred.FairScheduler.JobInfo;
+import org.apache.hadoop.mapred.TaskType;
+
+public class JobSchedulable extends Schedulable {
+  private FairScheduler scheduler;
+  private JobInProgress job;
+  private TaskType taskType;
+  private int demand = 0;
+
+  public JobSchedulable(FairScheduler scheduler, JobInProgress job, 
+      TaskType taskType) {
+    this.scheduler = scheduler;
+    this.job = job;
+    this.taskType = taskType;
+  }
+  
+  @Override
+  public String getName() {
+    return job.getJobID().toString();
+  }
+
+  public JobInProgress getJob() {
+    return job;
+  }
+  
+  @Override
+  public void updateDemand() {
+    demand = 0;
+    if (isRunnable()) {
+      // For reduces, make sure enough maps are done that reduces can launch
+      if (taskType == TaskType.REDUCE && !job.scheduleReduces())
+        return;
+      // Add up demand from each TaskInProgress; each TIP can either
+      // - have no attempts running, in which case it demands 1 slot
+      // - have N attempts running, in which case it demands N slots, and may
+      //   potentially demand one more slot if it needs to be speculated
+      TaskInProgress[] tips = (taskType == TaskType.MAP ? 
+          job.getMapTasks() : job.getReduceTasks());
+      boolean speculationEnabled = (taskType == TaskType.MAP ?
+          job.hasSpeculativeMaps() : job.hasSpeculativeReduces());
+      long time = scheduler.getClock().getTime();
+      for (TaskInProgress tip: tips) {
+        if (!tip.isComplete()) {
+          if (tip.isRunning()) {
+            // Count active tasks and any speculative task we want to launch
+            demand += tip.getActiveTasks().size();
+            if (speculationEnabled
+                && tip.hasSpeculativeTask(time, job.getStatus().mapProgress()))
+              demand += 1;
+          } else {
+            // Need to launch 1 task
+            demand += 1;
+          }
+        }
+      }
+    }
+  }
+
+  private boolean isRunnable() {
+    JobInfo info = scheduler.getJobInfo(job);
+    int runState = job.getStatus().getRunState();
+    return (info != null && info.runnable && runState == JobStatus.RUNNING);
+  }
+
+  @Override
+  public int getDemand() {
+    return demand;
+  }
+  
+  @Override
+  public void redistributeShare() {}
+
+  @Override
+  public JobPriority getPriority() {
+    return job.getPriority();
+  }
+
+  @Override
+  public int getRunningTasks() {
+    return taskType == TaskType.MAP ? job.runningMaps() : job.runningReduces();
+  }
+
+  @Override
+  public long getStartTime() {
+    return job.startTime;
+  }
+  
+  @Override
+  public double getWeight() {
+    return scheduler.getJobWeight(job, taskType);
+  }
+  
+  @Override
+  public int getMinShare() {
+    return 0;
+  }
+
+  @Override
+  public Task assignTask(TaskTrackerStatus tts, long currentTime,
+      Collection<JobInProgress> visited) throws IOException {
+    if (isRunnable()) {
+      visited.add(job);
+      TaskTrackerManager ttm = scheduler.taskTrackerManager;
+      ClusterStatus clusterStatus = ttm.getClusterStatus();
+      int numTaskTrackers = clusterStatus.getTaskTrackers();
+      if (taskType == TaskType.MAP) {
+        LocalityLevel localityLevel = scheduler.getAllowedLocalityLevel(
+            job, currentTime);
+        scheduler.getEventLog().log(
+            "ALLOWED_LOC_LEVEL", job.getJobID(), localityLevel);
+        // obtainNewMapTask needs to be passed 1 + the desired locality level
+        return job.obtainNewMapTask(tts, numTaskTrackers,
+            ttm.getNumberOfUniqueHosts(), localityLevel.toCacheLevelCap());
+      } else {
+        return job.obtainNewReduceTask(tts, numTaskTrackers,
+            ttm.getNumberOfUniqueHosts());
+      }
+    } else {
+      return null;
+    }
+  }
+}
diff --git a/src/contrib/fairscheduler/src/java/org/apache/hadoop/mapred/LocalityLevel.java b/src/contrib/fairscheduler/src/java/org/apache/hadoop/mapred/LocalityLevel.java
new file mode 100644
index 0000000..5d1e91d
--- /dev/null
+++ b/src/contrib/fairscheduler/src/java/org/apache/hadoop/mapred/LocalityLevel.java
@@ -0,0 +1,65 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.hadoop.mapred;
+
+/**
+ * Represents the level of data-locality at which a job in the fair scheduler
+ * is allowed to launch tasks. By default, jobs are not allowed to launch
+ * non-data-local tasks until they have waited a small number of seconds to
+ * find a slot on a node that they have data on. If a job has waited this
+ * long, it is allowed to launch rack-local tasks as well (on nodes that may
+ * not have the task's input data, but share a rack with a node that does).
+ * Finally, after a further wait, jobs are allowed to launch tasks anywhere
+ * in the cluster.
+ * 
+ * This enum defines three levels - NODE, RACK and ANY (for allowing tasks
+ * to be launched on any node). A map task's level can be obtained from
+ * its job through {@link #fromTask(JobInProgress, Task, TaskTrackerStatus)}. In
+ * addition, for any locality level, it is possible to get a "level cap" to pass
+ * to {@link JobInProgress#obtainNewMapTask(TaskTrackerStatus, int, int, int)}
+ * to ensure that only tasks at this level or lower are launched, through
+ * the {@link #toCacheLevelCap()} method.
+ */
+public enum LocalityLevel {
+  NODE, RACK, ANY;
+  
+  public static LocalityLevel fromTask(JobInProgress job, Task mapTask,
+      TaskTrackerStatus tracker) {
+    TaskID tipID = mapTask.getTaskID().getTaskID();
+    TaskInProgress tip = job.getTaskInProgress(tipID);
+    switch (job.getLocalityLevel(tip, tracker)) {
+    case 0: return LocalityLevel.NODE;
+    case 1: return LocalityLevel.RACK;
+    default: return LocalityLevel.ANY;
+    }
+  }
+  
+  /**
+   * Obtain a JobInProgress cache level cap to pass to
+   * {@link JobInProgress#obtainNewMapTask(TaskTrackerStatus, int, int, int)}
+   * to ensure that only tasks of this locality level and lower are launched.
+   */
+  public int toCacheLevelCap() {
+    switch(this) {
+    case NODE: return 1;
+    case RACK: return 2;
+    default: return Integer.MAX_VALUE;
+    }
+  }
+}
diff --git a/src/contrib/fairscheduler/src/java/org/apache/hadoop/mapred/Pool.java b/src/contrib/fairscheduler/src/java/org/apache/hadoop/mapred/Pool.java
index 660c840..502c0ca 100644
--- a/src/contrib/fairscheduler/src/java/org/apache/hadoop/mapred/Pool.java
+++ b/src/contrib/fairscheduler/src/java/org/apache/hadoop/mapred/Pool.java
@@ -21,6 +21,8 @@ package org.apache.hadoop.mapred;
 import java.util.ArrayList;
 import java.util.Collection;
 
+import org.apache.hadoop.mapred.TaskType;
+
 /**
  * A schedulable pool of jobs.
  */
@@ -33,9 +35,17 @@ public class Pool {
   
   /** Jobs in this specific pool; does not include children pools' jobs. */
   private Collection<JobInProgress> jobs = new ArrayList<JobInProgress>();
+  
+  /** Scheduling mode for jobs inside the pool (fair or FIFO) */
+  private SchedulingMode schedulingMode;
 
-  public Pool(String name) {
+  private PoolSchedulable mapSchedulable;
+  private PoolSchedulable reduceSchedulable;
+
+  public Pool(FairScheduler scheduler, String name) {
     this.name = name;
+    mapSchedulable = new PoolSchedulable(scheduler, this, TaskType.MAP);
+    reduceSchedulable = new PoolSchedulable(scheduler, this, TaskType.REDUCE);
   }
   
   public Collection<JobInProgress> getJobs() {
@@ -44,17 +54,41 @@ public class Pool {
   
   public void addJob(JobInProgress job) {
     jobs.add(job);
+    mapSchedulable.addJob(job);
+    reduceSchedulable.addJob(job);
   }
   
   public void removeJob(JobInProgress job) {
     jobs.remove(job);
+    mapSchedulable.removeJob(job);
+    reduceSchedulable.removeJob(job);
   }
   
   public String getName() {
     return name;
   }
 
+  public SchedulingMode getSchedulingMode() {
+    return schedulingMode;
+  }
+  
+  public void setSchedulingMode(SchedulingMode schedulingMode) {
+    this.schedulingMode = schedulingMode;
+  }
+
   public boolean isDefaultPool() {
     return Pool.DEFAULT_POOL_NAME.equals(name);
   }
+  
+  public PoolSchedulable getMapSchedulable() {
+    return mapSchedulable;
+  }
+  
+  public PoolSchedulable getReduceSchedulable() {
+    return reduceSchedulable;
+  }
+  
+  public PoolSchedulable getSchedulable(TaskType type) {
+    return type == TaskType.MAP ? mapSchedulable : reduceSchedulable;
+  }
 }
diff --git a/src/contrib/fairscheduler/src/java/org/apache/hadoop/mapred/PoolManager.java b/src/contrib/fairscheduler/src/java/org/apache/hadoop/mapred/PoolManager.java
index f41ee4f..ce8d0cd 100644
--- a/src/contrib/fairscheduler/src/java/org/apache/hadoop/mapred/PoolManager.java
+++ b/src/contrib/fairscheduler/src/java/org/apache/hadoop/mapred/PoolManager.java
@@ -57,6 +57,8 @@ public class PoolManager {
    * (this is done to prevent loading a file that hasn't been fully written).
    */
   public static final long ALLOC_RELOAD_WAIT = 5 * 1000; 
+
+  private final FairScheduler scheduler;
   
   // Map and reduce minimum allocations for each pool
   private Map<String, Integer> mapAllocs = new HashMap<String, Integer>();
@@ -86,6 +88,7 @@ public class PoolManager {
   // below half its fair share for this long, it is allowed to preempt tasks.
   private long fairSharePreemptionTimeout = Long.MAX_VALUE;
 
+  SchedulingMode defaultSchedulingMode = SchedulingMode.FAIR;
 
   private String allocFile; // Path to XML file containing allocations
   private String poolNameProperty; // Jobconf property to use for determining a
@@ -97,8 +100,13 @@ public class PoolManager {
   private long lastSuccessfulReload; // Last time we successfully reloaded pools
   private boolean lastReloadAttemptFailed = false;
 
-  public PoolManager(Configuration conf) throws IOException, SAXException,
+  public PoolManager(FairScheduler scheduler) {
+    this.scheduler = scheduler;
+  }
+  
+  public void initialize() throws IOException, SAXException,
       AllocationConfigurationException, ParserConfigurationException {
+    Configuration conf = scheduler.getConf();
     this.poolNameProperty = conf.get(
         "mapred.fairscheduler.poolnameproperty", "user.name");
     this.allocFile = conf.get("mapred.fairscheduler.allocation.file");
@@ -119,11 +127,19 @@ public class PoolManager {
   public synchronized Pool getPool(String name) {
     Pool pool = pools.get(name);
     if (pool == null) {
-      pool = new Pool(name);
+      pool = new Pool(scheduler, name);
+      pool.setSchedulingMode(defaultSchedulingMode);
       pools.put(name, pool);
     }
     return pool;
   }
+  
+  /**
+   * Get the pool that a given job is in.
+   */
+  public Pool getPool(JobInProgress job) {
+    return getPool(getPoolName(job));
+  }
 
   /**
    * Reload allocations file if it hasn't been loaded in a while
@@ -183,6 +199,7 @@ public class PoolManager {
     Map<String, Integer> poolMaxJobs = new HashMap<String, Integer>();
     Map<String, Integer> userMaxJobs = new HashMap<String, Integer>();
     Map<String, Double> poolWeights = new HashMap<String, Double>();
+    Map<String, SchedulingMode> poolModes = new HashMap<String, SchedulingMode>();
     Map<String, Long> minSharePreemptionTimeouts = new HashMap<String, Long>();
     int userMaxJobsDefault = Integer.MAX_VALUE;
     int poolMaxJobsDefault = Integer.MAX_VALUE;
@@ -191,6 +208,7 @@ public class PoolManager {
     List<String> poolNamesInAllocFile = new ArrayList<String>();
     long fairSharePreemptionTimeout = Long.MAX_VALUE;
     long defaultMinSharePreemptionTimeout = Long.MAX_VALUE;
+    SchedulingMode defaultSchedulingMode = SchedulingMode.FAIR;
     
     // Read and parse the allocations file.
     DocumentBuilderFactory docBuilderFactory =
@@ -237,6 +255,9 @@ public class PoolManager {
             String text = ((Text)field.getFirstChild()).getData().trim();
             long val = Long.parseLong(text) * 1000L;
             minSharePreemptionTimeouts.put(poolName, val);
+          } else if ("schedulingMode".equals(field.getTagName())) {
+            String text = ((Text)field.getFirstChild()).getData().trim();
+            poolModes.put(poolName, parseSchedulingMode(text));
           }
         }
       } else if ("user".equals(element.getTagName())) {
@@ -269,6 +290,9 @@ public class PoolManager {
         String text = ((Text)element.getFirstChild()).getData().trim();
         long val = Long.parseLong(text) * 1000L;
         defaultMinSharePreemptionTimeout = val;
+      } else if ("defaultPoolSchedulingMode".equals(element.getTagName())) {
+        String text = ((Text)element.getFirstChild()).getData().trim();
+        defaultSchedulingMode = parseSchedulingMode(text);
       } else {
         LOG.warn("Bad element in allocations file: " + element.getTagName());
       }
@@ -287,12 +311,31 @@ public class PoolManager {
       this.minSharePreemptionTimeouts = minSharePreemptionTimeouts;
       this.fairSharePreemptionTimeout = fairSharePreemptionTimeout;
       this.defaultMinSharePreemptionTimeout = defaultMinSharePreemptionTimeout;
+      this.defaultSchedulingMode = defaultSchedulingMode;
       for (String name: poolNamesInAllocFile) {
-        getPool(name);
+        Pool pool = getPool(name);
+        if (poolModes.containsKey(name)) {
+          pool.setSchedulingMode(poolModes.get(name));
+        } else {
+          pool.setSchedulingMode(defaultSchedulingMode);
+        }
       }
     }
   }
 
+  private SchedulingMode parseSchedulingMode(String text)
+      throws AllocationConfigurationException {
+    text = text.toLowerCase();
+    if (text.equals("fair")) {
+      return SchedulingMode.FAIR;
+    } else if (text.equals("fifo")) {
+      return SchedulingMode.FIFO;
+    } else {
+      throw new AllocationConfigurationException(
+          "Unknown scheduling mode : " + text + "; expected 'fifo' or 'fair'");
+    }
+  }
+
   /**
    * Get the allocation for a particular pool
    */
diff --git a/src/contrib/fairscheduler/src/java/org/apache/hadoop/mapred/PoolSchedulable.java b/src/contrib/fairscheduler/src/java/org/apache/hadoop/mapred/PoolSchedulable.java
new file mode 100644
index 0000000..855b4f7
--- /dev/null
+++ b/src/contrib/fairscheduler/src/java/org/apache/hadoop/mapred/PoolSchedulable.java
@@ -0,0 +1,188 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.hadoop.mapred;
+
+import java.io.IOException;
+import java.util.Collection;
+import java.util.Collections;
+import java.util.Comparator;
+import java.util.Iterator;
+import java.util.LinkedList;
+import java.util.List;
+
+import org.apache.commons.logging.Log;
+import org.apache.commons.logging.LogFactory;
+import org.apache.hadoop.mapred.FairScheduler.JobInfo;
+import org.apache.hadoop.mapred.TaskType;
+
+public class PoolSchedulable extends Schedulable {
+  public static final Log LOG = LogFactory.getLog(
+      PoolSchedulable.class.getName());
+  
+  private FairScheduler scheduler;
+  private Pool pool;
+  private TaskType taskType;
+  private PoolManager poolMgr;
+  private List<JobSchedulable> jobScheds = new LinkedList<JobSchedulable>();
+  private int demand = 0;
+  
+  // Variables used for preemption
+  long lastTimeAtMinShare;
+  long lastTimeAtHalfFairShare;
+
+  public PoolSchedulable(FairScheduler scheduler, Pool pool, TaskType type) {
+    this.scheduler = scheduler;
+    this.pool = pool;
+    this.taskType = type;
+    this.poolMgr = scheduler.getPoolManager();
+    long currentTime = scheduler.getClock().getTime();
+    this.lastTimeAtMinShare = currentTime;
+    this.lastTimeAtHalfFairShare = currentTime;
+  }
+
+  public void addJob(JobInProgress job) {
+    JobInfo info = scheduler.getJobInfo(job);
+    jobScheds.add(taskType == TaskType.MAP ?
+        info.mapSchedulable : info.reduceSchedulable);
+  }
+  
+  public void removeJob(JobInProgress job) {
+    for (Iterator<JobSchedulable> it = jobScheds.iterator(); it.hasNext();) {
+      JobSchedulable jobSched = it.next();
+      if (jobSched.getJob() == job) {
+        it.remove();
+        break;
+      }
+    }
+  }
+
+  /**
+   * Update demand by asking jobs in the pool to update
+   */
+  @Override
+  public void updateDemand() {
+    demand = 0;
+    for (JobSchedulable sched: jobScheds) {
+      sched.updateDemand();
+      demand += sched.getDemand();
+    }
+  }
+  
+  /**
+   * Distribute the pool's fair share among its jobs
+   */
+  @Override
+  public void redistributeShare() {
+    if (pool.getSchedulingMode() == SchedulingMode.FAIR) {
+      SchedulingAlgorithms.computeFairShares(jobScheds, getFairShare());
+    } else {
+      for (JobSchedulable sched: jobScheds) {
+        sched.setFairShare(0);
+      }
+    } 
+  }
+
+  @Override
+  public int getDemand() {
+    return demand;
+  }
+
+  @Override
+  public int getMinShare() {
+    return poolMgr.getAllocation(pool.getName(), taskType);
+  }
+
+  @Override
+  public double getWeight() {
+    return poolMgr.getPoolWeight(pool.getName());
+  }
+
+  @Override
+  public JobPriority getPriority() {
+    return JobPriority.NORMAL;
+  }
+
+  @Override
+  public int getRunningTasks() {
+    int ans = 0;
+    for (JobSchedulable sched: jobScheds) {
+      ans += sched.getRunningTasks();
+    }
+    return ans;
+  }
+
+  @Override
+  public long getStartTime() {
+    return 0;
+  }
+
+  @Override
+  public Task assignTask(TaskTrackerStatus tts, long currentTime,
+      Collection<JobInProgress> visited) throws IOException {
+    SchedulingMode mode = pool.getSchedulingMode();
+    Comparator<Schedulable> comparator;
+    if (mode == SchedulingMode.FIFO) {
+      comparator = new SchedulingAlgorithms.FifoComparator();
+    } else if (mode == SchedulingMode.FAIR) {
+      comparator = new SchedulingAlgorithms.FairShareComparator();
+    } else {
+      throw new RuntimeException("Unsupported pool scheduling mode " + mode);
+    }
+    Collections.sort(jobScheds, comparator);
+    for (JobSchedulable sched: jobScheds) {
+      Task task = sched.assignTask(tts, currentTime, visited);
+      if (task != null)
+        return task;
+    }
+    return null;
+  }
+  
+  @Override
+  public String getName() {
+    return pool.getName();
+  }
+
+  Pool getPool() {
+    return pool;
+  }
+
+  public TaskType getTaskType() {
+    return taskType;
+  }
+  
+  public Collection<JobSchedulable> getJobSchedulables() {
+    return jobScheds;
+  }
+  
+  public long getLastTimeAtMinShare() {
+    return lastTimeAtMinShare;
+  }
+  
+  public void setLastTimeAtMinShare(long lastTimeAtMinShare) {
+    this.lastTimeAtMinShare = lastTimeAtMinShare;
+  }
+  
+  public long getLastTimeAtHalfFairShare() {
+    return lastTimeAtHalfFairShare;
+  }
+  
+  public void setLastTimeAtHalfFairShare(long lastTimeAtHalfFairShare) {
+    this.lastTimeAtHalfFairShare = lastTimeAtHalfFairShare;
+  }
+}
diff --git a/src/contrib/fairscheduler/src/java/org/apache/hadoop/mapred/Schedulable.java b/src/contrib/fairscheduler/src/java/org/apache/hadoop/mapred/Schedulable.java
new file mode 100644
index 0000000..c4922f9
--- /dev/null
+++ b/src/contrib/fairscheduler/src/java/org/apache/hadoop/mapred/Schedulable.java
@@ -0,0 +1,131 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.hadoop.mapred;
+
+import java.io.IOException;
+import java.util.Collection;
+
+/**
+ * A Schedulable represents an entity that can launch tasks, such as a job
+ * or a pool. It provides a common interface so that algorithms such as fair
+ * sharing can be applied both within a pool and across pools. There are 
+ * currently two types of Schedulables: JobSchedulables, which represent a
+ * single job, and PoolSchedulables, which allocate among jobs in their pool.
+ * 
+ * Separate sets of Schedulables are used for maps and reduces. Each pool has
+ * both a mapSchedulable and a reduceSchedulable, and so does each job.
+ * 
+ * A Schedulable is responsible for three roles:
+ * 1) It can launch tasks through assignTask().
+ * 2) It provides information about the job/pool to the scheduler, including:
+ *    - Demand (maximum number of tasks required)
+ *    - Number of currently running tasks
+ *    - Minimum share (for pools)
+ *    - Job/pool weight (for fair sharing)
+ *    - Start time and priority (for FIFO)
+ * 3) It can be assigned a fair share, for use with fair scheduling.
+ * 
+ * Schedulable also contains two methods for performing scheduling computations:
+ * - updateDemand() is called periodically to compute the demand of the various
+ *   jobs and pools, which may be expensive (e.g. jobs must iterate through all
+ *   their tasks to count failed tasks, tasks that can be speculated, etc).
+ * - redistributeShare() is called after demands are updated and a Schedulable's
+ *   fair share has been set by its parent to let it distribute its share among
+ *   the other Schedulables within it (e.g. for pools that want to perform fair
+ *   sharing among their jobs).
+ */
+abstract class Schedulable {
+  /** Fair share assigned to this Schedulable */
+  private double fairShare = 0;
+
+  /**
+   * Name of job/pool, used for debugging as well as for breaking ties in
+   * scheduling order deterministically. 
+   */
+  public abstract String getName();
+  
+  /**
+   * Maximum number of tasks required by this Schedulable. This is defined as
+   * number of currently running tasks + number of unlaunched tasks (tasks that
+   * are either not yet launched or need to be speculated).
+   */
+  public abstract int getDemand();
+  
+  /** Number of tasks the schedulable is currently running. */
+  public abstract int getRunningTasks();
+  
+  /** Minimum share slots assigned to the schedulable. */
+  public abstract int getMinShare();
+  
+  /** Job/pool weight in fair sharing. */
+  public abstract double getWeight();
+  
+  /** Job priority for jobs in FIFO pools; meaningless for PoolSchedulables. */
+  public abstract JobPriority getPriority();
+  
+  /** Start time for jobs in FIFO pools; meaningless for PoolSchedulables. */
+  public abstract long getStartTime();
+  
+  /** Refresh the Schedulable's demand and those of its children if any. */
+  public abstract void updateDemand();
+  
+  /** 
+   * Distribute the fair share assigned to this Schedulable among its 
+   * children (used in pools where the internal scheduler is fair sharing). 
+   */
+  public abstract void redistributeShare();
+  
+  /**
+   * Obtain a task for a given TaskTracker, or null if the Schedulable has
+   * no tasks to launch at this moment or does not wish to launch a task on
+   * this TaskTracker (e.g. is waiting for a TaskTracker with local data). 
+   * In addition, if a job is skipped during this search because it is waiting
+   * for a TaskTracker with local data, this method is expected to add it to
+   * the <tt>visited</tt> collection passed in, so that the scheduler can
+   * properly mark it as skipped during this heartbeat. Please see
+   * {@link FairScheduler#getAllowedLocalityLevel(JobInProgress, long)}
+   * for details of delay scheduling (waiting for trackers with local data).
+   * 
+   * @param tts      TaskTracker that the task will be launched on
+   * @param currentTime Cached time (to prevent excessive calls to gettimeofday)
+   * @param visited  A Collection to which this method must add all jobs that
+   *                 were considered during the search for a job to assign.
+   * @return Task to launch, or null if Schedulable cannot currently launch one.
+   * @throws IOException Possible if obtainNew(Map|Reduce)Task throws exception.
+   */
+  public abstract Task assignTask(TaskTrackerStatus tts, long currentTime,
+      Collection<JobInProgress> visited) throws IOException;
+
+  /** Assign a fair share to this Schedulable. */
+  public void setFairShare(double fairShare) {
+    this.fairShare = fairShare;
+  }
+  
+  /** Get the fair share assigned to this Schedulable. */
+  public double getFairShare() {
+    return fairShare;
+  }
+  
+  /** Convenient toString implementation for debugging. */
+  @Override
+  public String toString() {
+    return String.format("[%s, demand=%d, running=%d, share=%.1f, w=%.1f]",
+        getName(), getDemand(), getRunningTasks(), fairShare, getWeight());
+  }
+}
diff --git a/src/contrib/fairscheduler/src/java/org/apache/hadoop/mapred/SchedulingAlgorithms.java b/src/contrib/fairscheduler/src/java/org/apache/hadoop/mapred/SchedulingAlgorithms.java
new file mode 100644
index 0000000..4896856
--- /dev/null
+++ b/src/contrib/fairscheduler/src/java/org/apache/hadoop/mapred/SchedulingAlgorithms.java
@@ -0,0 +1,209 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.hadoop.mapred;
+
+import java.util.Collection;
+import java.util.Comparator;
+
+import org.apache.commons.logging.Log;
+import org.apache.commons.logging.LogFactory;
+
+/**
+ * Utility class containing scheduling algorithms used in the fair scheduler.
+ */
+class SchedulingAlgorithms {
+  public static final Log LOG = LogFactory.getLog(
+      SchedulingAlgorithms.class.getName());
+  
+  /**
+   * Compare Schedulables in order of priority and then submission time, as in
+   * the default FIFO scheduler in Hadoop.
+   */
+  public static class FifoComparator implements Comparator<Schedulable> {
+    @Override
+    public int compare(Schedulable s1, Schedulable s2) {
+      int res = s1.getPriority().compareTo(s2.getPriority());
+      if (res == 0) {
+        res = (int) Math.signum(s1.getStartTime() - s2.getStartTime());
+      }
+      if (res == 0) {
+        // In the rare case where jobs were submitted at the exact same time,
+        // compare them by name (which will be the JobID) to get a deterministic
+        // ordering, so we don't alternately launch tasks from different jobs.
+        res = s1.getName().compareTo(s2.getName());
+      }
+      return res;
+    }
+  }
+
+  /**
+   * Compare Schedulables via weighted fair sharing. In addition, Schedulables
+   * below their min share get priority over those whose min share is met. 
+   * 
+   * Schedulables below their min share are compared by how far below it they
+   * are as a ratio. For example, if job A has 8 out of a min share of 10 tasks
+   * and job B has 50 out of a min share of 100, then job B is scheduled next, 
+   * because B is at 50% of its min share and A is at 80% of its min share.
+   * 
+   * Schedulables above their min share are compared by (runningTasks / weight).
+   * If all weights are equal, slots are given to the job with the fewest tasks;
+   * otherwise, jobs with more weight get proportionally more slots.
+   */
+  public static class FairShareComparator implements Comparator<Schedulable> {
+    @Override
+    public int compare(Schedulable s1, Schedulable s2) {
+      double minShareRatio1, minShareRatio2;
+      double tasksToWeightRatio1, tasksToWeightRatio2;
+      int minShare1 = Math.min(s1.getMinShare(), s1.getDemand());
+      int minShare2 = Math.min(s2.getMinShare(), s2.getDemand());
+      boolean s1Needy = s1.getRunningTasks() < minShare1;
+      boolean s2Needy = s2.getRunningTasks() < minShare2;
+      minShareRatio1 = s1.getRunningTasks() / Math.max(minShare1, 1.0);
+      minShareRatio2 = s2.getRunningTasks() / Math.max(minShare2, 1.0);
+      tasksToWeightRatio1 = s1.getRunningTasks() / s1.getWeight();
+      tasksToWeightRatio2 = s2.getRunningTasks() / s2.getWeight();
+      int res = 0;
+      if (s1Needy && !s2Needy)
+        res = -1;
+      else if (s2Needy && !s1Needy)
+        res = 1;
+      else if (s1Needy && s2Needy)
+        res = (int) Math.signum(minShareRatio1 - minShareRatio2);
+      else // Neither schedulable is needy
+        res = (int) Math.signum(tasksToWeightRatio1 - tasksToWeightRatio2);
+      if (res == 0) {
+        // Jobs are tied in fairness ratio. Break the tie by submit time and job 
+        // name to get a deterministic ordering, which is useful for unit tests.
+        res = (int) Math.signum(s1.getStartTime() - s2.getStartTime());
+        if (res == 0)
+          res = s1.getName().compareTo(s2.getName());
+      }
+      return res;
+    }
+  }
+
+  /** 
+   * Number of iterations for the binary search in computeFairShares. This is 
+   * equivalent to the number of bits of precision in the output. 25 iterations 
+   * gives precision better than 0.1 slots in clusters with one million slots.
+   */
+  private static final int COMPUTE_FAIR_SHARES_ITERATIONS = 25;
+  
+  /**
+   * Given a set of Schedulables and a number of slots, compute their weighted
+   * fair shares. The min shares and demands of the Schedulables are assumed to
+   * be set beforehand. We compute the fairest possible allocation of shares 
+   * to the Schedulables that respects their min shares and demands.
+   * 
+   * To understand what this method does, we must first define what weighted
+   * fair sharing means in the presence of minimum shares and demands. If there
+   * were no minimum shares and every Schedulable had an infinite demand (i.e.
+   * could launch infinitely many tasks), then weighted fair sharing would be
+   * achieved if the ratio of slotsAssigned / weight was equal for each
+   * Schedulable and all slots were assigned. Minimum shares and demands add
+   * two further twists:
+   * - Some Schedulables may not have enough tasks to fill all their share.
+   * - Some Schedulables may have a min share higher than their assigned share.
+   * 
+   * To deal with these possibilities, we define an assignment of slots as
+   * being fair if there exists a ratio R such that:
+   * - Schedulables S where S.demand < R * S.weight are assigned share S.demand
+   * - Schedulables S where S.minShare > R * S.weight are given share S.minShare
+   * - All other Schedulables S are assigned share R * S.weight
+   * - The sum of all the shares is totalSlots.
+   * 
+   * We call R the weight-to-slots ratio because it converts a Schedulable's
+   * weight to the number of slots it is assigned.
+   * 
+   * We compute a fair allocation by finding a suitable weight-to-slot ratio R.
+   * To do this, we use binary search. Given a ratio R, we compute the number
+   * of slots that would be used in total with this ratio (the sum of the shares
+   * computed using the conditions above). If this number of slots is less than
+   * totalSlots, then R is too small and more slots could be assigned. If the
+   * number of slots is more than totalSlots, then R is too large. 
+   * 
+   * We begin the binary search with a lower bound on R of 0 (which means that
+   * all Schedulables are only given their minShare) and an upper bound computed
+   * to be large enough that too many slots are given (by doubling R until we
+   * either use more than totalSlots slots or we fulfill all jobs' demands).
+   * The helper method slotsUsedWithWeightToSlotRatio computes the total number
+   * of slots used with a given value of R.
+   * 
+   * The running time of this algorithm is linear in the number of Schedulables,
+   * because slotsUsedWithWeightToSlotRatio is linear-time and the number of
+   * iterations of binary search is a constant (dependent on desired precision).
+   */
+  public static void computeFairShares(
+      Collection<? extends Schedulable> schedulables, double totalSlots) {
+    // Find an upper bound on R that we can use in our binary search. We start 
+    // at R = 1 and double it until we have either used totalSlots slots or we
+    // have met all Schedulables' demands (if total demand < totalSlots).
+    double totalDemand = 0;
+    for (Schedulable sched: schedulables) {
+      totalDemand += sched.getDemand();
+    }
+    double cap = Math.min(totalDemand, totalSlots);
+    double rMax = 1.0;
+    while (slotsUsedWithWeightToSlotRatio(rMax, schedulables) < cap) {
+      rMax *= 2.0;
+    }
+    // Perform the binary search for up to COMPUTE_FAIR_SHARES_ITERATIONS steps
+    double left = 0;
+    double right = rMax;
+    for (int i = 0; i < COMPUTE_FAIR_SHARES_ITERATIONS; i++) {
+      double mid = (left + right) / 2.0;
+      if (slotsUsedWithWeightToSlotRatio(mid, schedulables) < cap) {
+        left = mid;
+      } else {
+        right = mid;
+      }
+    }
+    // Set the fair shares based on the value of R we've converged to
+    for (Schedulable sched: schedulables) {
+      sched.setFairShare(computeShare(sched, right));
+    }
+  }
+  
+  /**
+   * Compute the number of slots that would be used given a weight-to-slot
+   * ratio w2sRatio, for use in the computeFairShares algorithm as described
+   * in #{@link SchedulingAlgorithms#computeFairShares(Collection, double)}.
+   */
+  private static double slotsUsedWithWeightToSlotRatio(double w2sRatio,
+      Collection<? extends Schedulable> schedulables) {
+    double slotsTaken = 0;
+    for (Schedulable sched: schedulables) {
+      double share = computeShare(sched, w2sRatio);
+      slotsTaken += share;
+    }
+    return slotsTaken;
+  }
+
+  /**
+   * Compute the number of slots assigned to a Schedulable given a particular
+   * weight-to-slot ratio w2sRatio, for use in computeFairShares as described
+   * in #{@link SchedulingAlgorithms#computeFairShares(Collection, double)}.
+   */
+  private static double computeShare(Schedulable sched, double w2sRatio) {
+    double share = sched.getWeight() * w2sRatio;
+    share = Math.max(share, sched.getMinShare());
+    share = Math.min(share, sched.getDemand());
+    return share;
+  }
+}
diff --git a/src/contrib/fairscheduler/src/java/org/apache/hadoop/mapred/SchedulingMode.java b/src/contrib/fairscheduler/src/java/org/apache/hadoop/mapred/SchedulingMode.java
new file mode 100644
index 0000000..e6d7bd8
--- /dev/null
+++ b/src/contrib/fairscheduler/src/java/org/apache/hadoop/mapred/SchedulingMode.java
@@ -0,0 +1,26 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.hadoop.mapred;
+
+/**
+ * Internal scheduling modes for pools.
+ */
+public enum SchedulingMode {
+  FAIR, FIFO
+}
diff --git a/src/contrib/fairscheduler/src/java/org/apache/hadoop/mapred/TaskSelector.java b/src/contrib/fairscheduler/src/java/org/apache/hadoop/mapred/TaskSelector.java
index 3e1f2ad..f47142e 100644
--- a/src/contrib/fairscheduler/src/java/org/apache/hadoop/mapred/TaskSelector.java
+++ b/src/contrib/fairscheduler/src/java/org/apache/hadoop/mapred/TaskSelector.java
@@ -86,7 +86,7 @@ public abstract class TaskSelector implements Configurable {
    * @throws IOException 
    */
   public abstract Task obtainNewMapTask(TaskTrackerStatus taskTracker,
-      JobInProgress job) throws IOException;
+      JobInProgress job, int localityLevel) throws IOException;
 
   /**
    * Choose a reduce task to run from the given job on the given TaskTracker.
diff --git a/src/contrib/fairscheduler/src/test/org/apache/hadoop/mapred/FakeSchedulable.java b/src/contrib/fairscheduler/src/test/org/apache/hadoop/mapred/FakeSchedulable.java
new file mode 100644
index 0000000..f457ae2
--- /dev/null
+++ b/src/contrib/fairscheduler/src/test/org/apache/hadoop/mapred/FakeSchedulable.java
@@ -0,0 +1,108 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.hadoop.mapred;
+
+import java.io.IOException;
+import java.util.Collection;
+
+/**
+ * Dummy implementation of Schedulable for unit testing.
+ */
+public class FakeSchedulable extends Schedulable {
+  private int demand;
+  private int runningTasks;
+  private int minShare;
+  private double weight;
+  private JobPriority priority;
+  private long startTime;
+  
+  public FakeSchedulable() {
+    this(0, 0, 1, 0, 0, JobPriority.NORMAL, 0);
+  }
+  
+  public FakeSchedulable(int demand) {
+    this(demand, 0, 1, 0, 0, JobPriority.NORMAL, 0);
+  }
+  
+  public FakeSchedulable(int demand, int minShare) {
+    this(demand, minShare, 1, 0, 0, JobPriority.NORMAL, 0);
+  }
+  
+  public FakeSchedulable(int demand, int minShare, double weight) {
+    this(demand, minShare, weight, 0, 0, JobPriority.NORMAL, 0);
+  }
+  
+  public FakeSchedulable(int demand, int minShare, double weight, int fairShare,
+      int runningTasks, JobPriority priority, long startTime) {
+    this.demand = demand;
+    this.minShare = minShare;
+    this.weight = weight;
+    setFairShare(fairShare);
+    this.runningTasks = runningTasks;
+    this.priority = priority;
+    this.startTime = startTime;
+  }
+  
+  @Override
+  public Task assignTask(TaskTrackerStatus tts, long currentTime,
+      Collection<JobInProgress> visited) throws IOException {
+    return null;
+  }
+
+  @Override
+  public int getDemand() {
+    return demand;
+  }
+
+  @Override
+  public String getName() {
+    return "FakeSchedulable" + this.hashCode();
+  }
+
+  @Override
+  public JobPriority getPriority() {
+    return priority;
+  }
+
+  @Override
+  public int getRunningTasks() {
+    return runningTasks;
+  }
+
+  @Override
+  public long getStartTime() {
+    return startTime;
+  }
+  
+  @Override
+  public double getWeight() {
+    return weight;
+  }
+  
+  @Override
+  public int getMinShare() {
+    return minShare;
+  }
+
+  @Override
+  public void redistributeShare() {}
+
+  @Override
+  public void updateDemand() {}
+}
diff --git a/src/contrib/fairscheduler/src/test/org/apache/hadoop/mapred/TestComputeFairShares.java b/src/contrib/fairscheduler/src/test/org/apache/hadoop/mapred/TestComputeFairShares.java
new file mode 100644
index 0000000..ebc5b95
--- /dev/null
+++ b/src/contrib/fairscheduler/src/test/org/apache/hadoop/mapred/TestComputeFairShares.java
@@ -0,0 +1,184 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.hadoop.mapred;
+
+import java.util.ArrayList;
+import java.util.List;
+
+import junit.framework.TestCase;
+
+/**
+ * Exercise the computeFairShares method in SchedulingAlgorithms.
+ */
+public class TestComputeFairShares extends TestCase {
+  private List<Schedulable> scheds;
+  
+  @Override
+  protected void setUp() throws Exception {
+    scheds = new ArrayList<Schedulable>();
+  }
+  
+  /** 
+   * Basic test - pools with different demands that are all higher than their
+   * fair share (of 10 slots) should each get their fair share.
+   */
+  public void testEqualSharing() {
+    scheds.add(new FakeSchedulable(100));
+    scheds.add(new FakeSchedulable(50));
+    scheds.add(new FakeSchedulable(30));
+    scheds.add(new FakeSchedulable(20));
+    SchedulingAlgorithms.computeFairShares(scheds, 40);
+    verifyShares(10, 10, 10, 10);
+  }
+  
+  /**
+   * In this test, pool 4 has a smaller demand than the 40 / 4 = 10 slots that
+   * it would be assigned with equal sharing. It should only get the 3 slots
+   * it demands. The other pools must then split the remaining 37 slots, but
+   * pool 3, with 11 slots demanded, is now below its share of 37/3 ~= 12.3,
+   * so it only gets 11 slots. Pools 1 and 2 split the rest and get 13 each. 
+   */
+  public void testLowDemands() {
+    scheds.add(new FakeSchedulable(100));
+    scheds.add(new FakeSchedulable(50));
+    scheds.add(new FakeSchedulable(11));
+    scheds.add(new FakeSchedulable(3));
+    SchedulingAlgorithms.computeFairShares(scheds, 40);
+    verifyShares(13, 13, 11, 3);
+  }
+  
+  /**
+   * In this test, some pools have minimum shares set. Pool 1 has a min share
+   * of 20 so it gets 20 slots. Pool 2 also has a min share of 20, but its
+   * demand is only 10 so it can only get 10 slots. The remaining pools have
+   * 10 slots to split between them. Pool 4 gets 3 slots because its demand is
+   * only 3, and pool 3 gets the remaining 7 slots. Pool 4 also had a min share
+   * of 2 slots but this should not affect the outcome.
+   */
+  public void testMinShares() {
+    scheds.add(new FakeSchedulable(100, 20));
+    scheds.add(new FakeSchedulable(10, 20));
+    scheds.add(new FakeSchedulable(10, 0));
+    scheds.add(new FakeSchedulable(3, 2));
+    SchedulingAlgorithms.computeFairShares(scheds, 40);
+    verifyShares(20, 10, 7, 3);
+  }
+  
+  /**
+   * Basic test for weighted shares with no minimum shares and no low demands.
+   * Each pool should get slots in proportion to its weight.
+   */
+  public void testWeightedSharing() {
+    scheds.add(new FakeSchedulable(100, 0, 2.0));
+    scheds.add(new FakeSchedulable(50,  0, 1.0));
+    scheds.add(new FakeSchedulable(30,  0, 1.0));
+    scheds.add(new FakeSchedulable(20,  0, 0.5));
+    SchedulingAlgorithms.computeFairShares(scheds, 45);
+    verifyShares(20, 10, 10, 5);
+  }
+
+  /**
+   * Weighted sharing test where pools 1 and 2 are now given lower demands than
+   * above. Pool 1 stops at 10 slots, leaving 35. If the remaining pools split
+   * this into a 1:1:0.5 ratio, they would get 14:14:7 slots respectively, but
+   * pool 2's demand is only 11, so it only gets 11. The remaining 2 pools split
+   * the 24 slots left into a 1:0.5 ratio, getting 16 and 8 slots respectively.
+   */
+  public void testWeightedSharingWithLowDemands() {
+    scheds.add(new FakeSchedulable(10, 0, 2.0));
+    scheds.add(new FakeSchedulable(11, 0, 1.0));
+    scheds.add(new FakeSchedulable(30, 0, 1.0));
+    scheds.add(new FakeSchedulable(20, 0, 0.5));
+    SchedulingAlgorithms.computeFairShares(scheds, 45);
+    verifyShares(10, 11, 16, 8);
+  }
+
+  /**
+   * Weighted fair sharing test with min shares. As in the min share test above,
+   * pool 1 has a min share greater than its demand so it only gets its demand.
+   * Pool 3 has a min share of 15 even though its weight is very small, so it
+   * gets 15 slots. The remaining pools share the remaining 20 slots equally,
+   * getting 10 each. Pool 3's min share of 5 slots doesn't affect this.
+   */
+  public void testWeightedSharingWithMinShares() {
+    scheds.add(new FakeSchedulable(10, 20, 2.0));
+    scheds.add(new FakeSchedulable(11, 0, 1.0));
+    scheds.add(new FakeSchedulable(30, 5, 1.0));
+    scheds.add(new FakeSchedulable(20, 15, 0.5));
+    SchedulingAlgorithms.computeFairShares(scheds, 45);
+    verifyShares(10, 10, 10, 15);
+  }
+
+  /**
+   * Test that shares are computed accurately even when there are many more
+   * frameworks than available slots.
+   */
+  public void testSmallShares() {
+    scheds.add(new FakeSchedulable(10));
+    scheds.add(new FakeSchedulable(5));
+    scheds.add(new FakeSchedulable(3));
+    scheds.add(new FakeSchedulable(2));
+    SchedulingAlgorithms.computeFairShares(scheds, 1);
+    verifyShares(0.25, 0.25, 0.25, 0.25);
+  }
+
+  /**
+   * Test that shares are computed accurately even when the number of slots is
+   * very large.
+   */  
+  public void testLargeShares() {
+    int million = 1000 * 1000;
+    scheds.add(new FakeSchedulable(100 * million));
+    scheds.add(new FakeSchedulable(50 * million));
+    scheds.add(new FakeSchedulable(30 * million));
+    scheds.add(new FakeSchedulable(20 * million));
+    SchedulingAlgorithms.computeFairShares(scheds, 40 * million);
+    verifyShares(10 * million, 10 * million, 10 * million, 10 * million);
+  }
+
+  /**
+   * Test that having a pool with 0 demand doesn't confuse the algorithm.
+   */
+  public void testZeroDemand() {
+    scheds.add(new FakeSchedulable(100));
+    scheds.add(new FakeSchedulable(50));
+    scheds.add(new FakeSchedulable(30));
+    scheds.add(new FakeSchedulable(0));
+    SchedulingAlgorithms.computeFairShares(scheds, 30);
+    verifyShares(10, 10, 10, 0);
+  }
+  
+  /**
+   * Test that being called on an empty list doesn't confuse the algorithm.
+   */
+  public void testEmptyList() {
+    SchedulingAlgorithms.computeFairShares(scheds, 40);
+    verifyShares();
+  }
+  
+  /**
+   * Check that a given list of shares have been assigned to this.scheds.
+   */
+  private void verifyShares(double... shares) {
+    assertEquals(scheds.size(), shares.length);
+    for (int i = 0; i < shares.length; i++) {
+      assertEquals(shares[i], scheds.get(i).getFairShare(), 0.01);
+    }
+  }
+}
diff --git a/src/contrib/fairscheduler/src/test/org/apache/hadoop/mapred/TestFairScheduler.java b/src/contrib/fairscheduler/src/test/org/apache/hadoop/mapred/TestFairScheduler.java
index 2e88a38..904f9ed 100644
--- a/src/contrib/fairscheduler/src/test/org/apache/hadoop/mapred/TestFairScheduler.java
+++ b/src/contrib/fairscheduler/src/test/org/apache/hadoop/mapred/TestFairScheduler.java
@@ -56,11 +56,14 @@ public class TestFairScheduler extends TestCase {
     private FakeTaskTrackerManager taskTrackerManager;
     private int mapCounter = 0;
     private int reduceCounter = 0;
+    private final String[][] mapInputLocations; // Array of hosts for each map
     
     public FakeJobInProgress(JobConf jobConf,
-        FakeTaskTrackerManager taskTrackerManager) throws IOException {
+        FakeTaskTrackerManager taskTrackerManager, 
+        String[][] mapInputLocations) throws IOException {
       super(new JobID("test", ++jobCounter), jobConf);
       this.taskTrackerManager = taskTrackerManager;
+      this.mapInputLocations = mapInputLocations;
       this.startTime = System.currentTimeMillis();
       this.status = new JobStatus();
       this.status.setRunState(JobStatus.PREP);
@@ -101,58 +104,71 @@ public class TestFairScheduler extends TestCase {
       setup[1].setJobSetupTask();
       // create maps
       numMapTasks = conf.getNumMapTasks();
-      System.out.println("numMapTasks = " + numMapTasks);
       maps = new TaskInProgress[numMapTasks];
       for (int i = 0; i < numMapTasks; i++) {
-        maps[i] = new FakeTaskInProgress(getJobID(), 
-            getJobConf(), true, this);
+        String[] inputLocations = null;
+        if (mapInputLocations != null)
+          inputLocations = mapInputLocations[i];
+        maps[i] = new FakeTaskInProgress(getJobID(), i,
+            getJobConf(), this, inputLocations);
+        if (mapInputLocations == null) // Job has no locality info
+          nonLocalMaps.add(maps[i]);
       }
       // create reduces
       numReduceTasks = conf.getNumReduceTasks();
-      System.out.println("numReduceTasks = " + numReduceTasks);
       reduces = new TaskInProgress[numReduceTasks];
       for (int i = 0; i < numReduceTasks; i++) {
-        reduces[i] = new FakeTaskInProgress(getJobID(), 
-            getJobConf(), false, this);
+        reduces[i] = new FakeTaskInProgress(getJobID(), i,
+            getJobConf(), this);
       }
    }
 
     @Override
     public Task obtainNewMapTask(final TaskTrackerStatus tts, int clusterSize,
-        int numUniqueHosts) throws IOException {
-      TaskAttemptID attemptId = getTaskAttemptID(true);
-      Task task = new MapTask("", attemptId, 0, "", new BytesWritable()) {
-        @Override
-        public String toString() {
-          return String.format("%s on %s", getTaskID(), tts.getTrackerName());
+        int numUniqueHosts, int localityLevel) throws IOException {
+      for (int map = 0; map < maps.length; map++) {
+        FakeTaskInProgress tip = (FakeTaskInProgress) maps[map];
+        if (!tip.isRunning() && !tip.isComplete() &&
+            getLocalityLevel(tip, tts) < localityLevel) {
+          TaskAttemptID attemptId = getTaskAttemptID(tip);
+          Task task = new MapTask("", attemptId, 0, "", new BytesWritable()) {
+            @Override
+            public String toString() {
+              return String.format("%s on %s", getTaskID(), tts.getTrackerName());
+            }
+          };
+          runningMapTasks++;
+          tip.createTaskAttempt(task, tts.getTrackerName());
+          nonLocalRunningMaps.add(tip);
+          taskTrackerManager.startTask(tts.getTrackerName(), task, tip);
+          return task;
         }
-      };
-      runningMapTasks++;
-      FakeTaskInProgress tip = 
-        (FakeTaskInProgress) maps[attemptId.getTaskID().getId()];
-      tip.createTaskAttempt(task, tts.getTrackerName());
-      nonLocalRunningMaps.add(tip);
-      taskTrackerManager.startTask(tts.getTrackerName(), task, tip);
-     return task;
+      }
+      return null;
     }
     
     @Override
     public Task obtainNewReduceTask(final TaskTrackerStatus tts,
         int clusterSize, int ignored) throws IOException {
-      TaskAttemptID attemptId = getTaskAttemptID(false);
-      Task task = new ReduceTask("", attemptId, 0, 10) {
-        @Override
-        public String toString() {
-          return String.format("%s on %s", getTaskID(), tts.getTrackerName());
+      for (int reduce = 0; reduce < reduces.length; reduce++) {
+        FakeTaskInProgress tip = 
+          (FakeTaskInProgress) reduces[reduce];
+        if (!tip.isRunning() && !tip.isComplete()) {
+          TaskAttemptID attemptId = getTaskAttemptID(tip);
+          Task task = new ReduceTask("", attemptId, 0, maps.length) {
+            @Override
+            public String toString() {
+              return String.format("%s on %s", getTaskID(), tts.getTrackerName());
+            }
+          };
+          runningReduceTasks++;
+          tip.createTaskAttempt(task, tts.getTrackerName());
+          runningReduces.add(tip);
+          taskTrackerManager.startTask(tts.getTrackerName(), task, tip);
+          return task;
         }
-      };
-      runningReduceTasks++;
-      FakeTaskInProgress tip = 
-        (FakeTaskInProgress) reduces[attemptId.getTaskID().getId()];
-      tip.createTaskAttempt(task, tts.getTrackerName());
-      runningReduces.add(tip);
-      taskTrackerManager.startTask(tts.getTrackerName(), task, tip);
-      return task;
+      }
+      return null;
     }
 
     public void mapTaskFinished(TaskInProgress tip) {
@@ -167,16 +183,33 @@ public class TestFairScheduler extends TestCase {
       runningReduces.remove(tip);
     }    
 
-    private TaskAttemptID getTaskAttemptID(boolean isMap) {
+    private TaskAttemptID getTaskAttemptID(TaskInProgress tip) {
       JobID jobId = getJobID();
-      TaskType t = TaskType.REDUCE;
-      if (isMap) {
-        t = TaskType.MAP;
-        return new TaskAttemptID(jobId.getJtIdentifier(),
-            jobId.getId(), isMap, mapCounter++, 0);
+      return new TaskAttemptID(jobId.getJtIdentifier(),
+          jobId.getId(), tip.isMapTask(), tip.getIdWithinJob(), tip.nextTaskId++);
+    }
+
+    @Override
+    int getLocalityLevel(TaskInProgress tip, TaskTrackerStatus tts) {
+      FakeTaskInProgress ftip = (FakeTaskInProgress) tip;
+      if (ftip.inputLocations != null) {
+        // Check whether we're on the same host as an input split
+        for (String location: ftip.inputLocations) {
+          if (location.equals(tts.host)) {
+            return 0;
+          }
+        }
+        // Check whether we're on the same rack as an input split
+        for (String location: ftip.inputLocations) {
+          if (getRack(location).equals(getRack(tts.host))) {
+            return 1;
+          }
+        }
+        // Not on same rack or host
+        return 2;
       } else {
-        return new TaskAttemptID(jobId.getJtIdentifier(),
-            jobId.getId(), isMap, reduceCounter++, 0);
+        // Job has no locality info
+        return -1;
       }
     }    
   }
@@ -187,11 +220,25 @@ public class TestFairScheduler extends TestCase {
     private TreeMap<TaskAttemptID, String> activeTasks;
     private TaskStatus taskStatus;
     private boolean isComplete = false;
-    
-    FakeTaskInProgress(JobID jId, JobConf jobConf, boolean isMap,
+    private String[] inputLocations;
+     
+    // Constructor for map
+    FakeTaskInProgress(JobID jId, int id, JobConf jobConf,
+        FakeJobInProgress job, String[] inputLocations) {
+      super(jId, "", new JobClient.RawSplit(), null, jobConf, job, id);
+      this.isMap = true;
+      this.fakeJob = job;
+      this.inputLocations = inputLocations;
+      activeTasks = new TreeMap<TaskAttemptID, String>();
+      taskStatus = TaskStatus.createTaskStatus(isMap);
+      taskStatus.setRunState(TaskStatus.State.UNASSIGNED);
+    }
+
+    // Constructor for reduce
+    FakeTaskInProgress(JobID jId, int id, JobConf jobConf,
                        FakeJobInProgress job) {
-      super(jId, "", new JobClient.RawSplit(), null, jobConf, job, 0);
-      this.isMap = isMap;
+      super(jId, "", jobConf.getNumMapTasks(), id, null, jobConf, job);
+      this.isMap = false;
       this.fakeJob = job;
       activeTasks = new TreeMap<TaskAttemptID, String>();
       taskStatus = TaskStatus.createTaskStatus(isMap);
@@ -268,6 +315,7 @@ public class TestFairScheduler extends TestCase {
     int maxReduceTasksPerTracker = 2;
     List<JobInProgressListener> listeners =
       new ArrayList<JobInProgressListener>();
+    Map<JobID, JobInProgress> jobs = new HashMap<JobID, JobInProgress>();
     
     private Map<String, TaskTrackerStatus> trackers =
       new HashMap<String, TaskTrackerStatus>();
@@ -279,14 +327,20 @@ public class TestFairScheduler extends TestCase {
       new HashMap<String, TaskTrackerStatus>();
 
 
-    public FakeTaskTrackerManager(int numTrackers) {
-      for (int i = 1; i <= numTrackers; i++) {
-        TaskTrackerStatus tt = new TaskTrackerStatus("tt" + i,  "host" + i, i,
-            new ArrayList<TaskStatus>(), 0,
-            maxMapTasksPerTracker, maxReduceTasksPerTracker);
-        trackers.put("tt" + i, tt);
-      }
-    }
+   public FakeTaskTrackerManager(int numRacks, int numTrackersPerRack) {
+     int nextTrackerId = 1;
+     for (int rack = 1; rack <= numRacks; rack++) {
+       for (int node = 1; node <= numTrackersPerRack; node++) {
+         int id = nextTrackerId++;
+         String host = "rack" + rack + ".node" + node;
+         System.out.println("Creating TaskTracker tt" + id + " on " + host);
+         TaskTrackerStatus tts = new TaskTrackerStatus("tt" + id, host, 0,
+             new ArrayList<TaskStatus>(), 0,
+             maxMapTasksPerTracker, maxReduceTasksPerTracker);
+         trackers.put("tt" + id, tts);
+       }
+     }
+   }
 
     @Override
     public ClusterStatus getClusterStatus() {
@@ -304,7 +358,7 @@ public class TestFairScheduler extends TestCase {
     
     @Override
     public int getNumberOfUniqueHosts() {
-      return 0;
+      return trackers.size();
     }
 
     @Override
@@ -335,7 +389,7 @@ public class TestFairScheduler extends TestCase {
 
     @Override
     public JobInProgress getJob(JobID jobid) {
-      return null;
+      return jobs.get(jobid);
     }
 
     public void initJob (JobInProgress job) {
@@ -349,6 +403,7 @@ public class TestFairScheduler extends TestCase {
     // Test methods
     
     public void submitJob(JobInProgress job) throws IOException {
+      jobs.put(job.getJobID(), job);
       for (JobInProgressListener listener : listeners) {
         listener.jobAdded(job);
       }
@@ -430,15 +485,24 @@ public class TestFairScheduler extends TestCase {
     fileWriter.write("<?xml version=\"1.0\"?>\n");
     fileWriter.write("<allocations />\n");
     fileWriter.close();
-    setUpCluster(2);
+    setUpCluster(1, 2, false);
+  }
+
+  public String getRack(String hostname) {
+    // Host names are of the form rackN.nodeM, so split at the dot.
+    return hostname.split("\\.")[0];
   }
 
-  private void setUpCluster(int numTaskTrackers) {
+  private void setUpCluster(int numRacks, int numNodesPerRack,
+      boolean assignMultiple) {
     conf = new JobConf();
     conf.set("mapred.fairscheduler.allocation.file", ALLOC_FILE);
     conf.set("mapred.fairscheduler.poolnameproperty", POOL_PROPERTY);
-    conf.set("mapred.fairscheduler.assignmultiple", "false");
-    taskTrackerManager = new FakeTaskTrackerManager(numTaskTrackers);
+    conf.setBoolean("mapred.fairscheduler.assignmultiple", assignMultiple);
+    // Manually set locality delay because we aren't using a JobTracker so
+    // we can't auto-compute it from the heartbeat interval.
+    conf.setLong("mapred.fairscheduler.locality.delay", 10000);
+    taskTrackerManager = new FakeTaskTrackerManager(numRacks, numNodesPerRack);
     clock = new FakeClock();
     scheduler = new FairScheduler(clock, true);
     scheduler.waitForMapsBeforeLaunchingReduces = false;
@@ -456,17 +520,23 @@ public class TestFairScheduler extends TestCase {
   
   private JobInProgress submitJob(int state, int maps, int reduces)
       throws IOException {
-    return submitJob(state, maps, reduces, null);
+    return submitJob(state, maps, reduces, null, null);
   }
   
   private JobInProgress submitJob(int state, int maps, int reduces, String pool)
       throws IOException {
+    return submitJob(state, maps, reduces, pool, null);
+  }
+  
+  private JobInProgress submitJob(int state, int maps, int reduces, String pool,
+      String[][] mapInputLocations) throws IOException {
     JobConf jobConf = new JobConf(conf);
     jobConf.setNumMapTasks(maps);
     jobConf.setNumReduceTasks(reduces);
     if (pool != null)
       jobConf.set(POOL_PROPERTY, pool);
-    JobInProgress job = new FakeJobInProgress(jobConf, taskTrackerManager);
+    JobInProgress job = new FakeJobInProgress(jobConf, taskTrackerManager,
+        mapInputLocations);
     job.getStatus().setRunState(state);
     taskTrackerManager.submitJob(job);
     job.startTime = clock.time;
@@ -582,14 +652,12 @@ public class TestFairScheduler extends TestCase {
     JobInfo info1 = scheduler.infos.get(job1);
     
     // Check scheduler variables
-    assertEquals(0,    info1.runningMaps);
-    assertEquals(0,    info1.runningReduces);
-    assertEquals(2,    info1.neededMaps);
-    assertEquals(1,    info1.neededReduces);
-    assertEquals(0,    info1.mapDeficit);
-    assertEquals(0,    info1.reduceDeficit);
-    assertEquals(4.0,  info1.mapFairShare);
-    assertEquals(4.0,  info1.reduceFairShare);
+    assertEquals(0,    info1.mapSchedulable.getRunningTasks());
+    assertEquals(0,    info1.reduceSchedulable.getRunningTasks());
+    assertEquals(2,    info1.mapSchedulable.getDemand());
+    assertEquals(1,    info1.reduceSchedulable.getDemand());
+    assertEquals(2.0,  info1.mapSchedulable.getFairShare());
+    assertEquals(1.0,  info1.reduceSchedulable.getFairShare());
     
     // Advance time before submitting another job j2, to make j1 run before j2
     // deterministically.
@@ -597,70 +665,119 @@ public class TestFairScheduler extends TestCase {
     JobInProgress job2 = submitJob(JobStatus.RUNNING, 1, 2);
     JobInfo info2 = scheduler.infos.get(job2);
     
-    // Check scheduler variables; the fair shares should now have been allocated
-    // equally between j1 and j2, but j1 should have (4 slots)*(100 ms) deficit
-    assertEquals(0,    info1.runningMaps);
-    assertEquals(0,    info1.runningReduces);
-    assertEquals(2,    info1.neededMaps);
-    assertEquals(1,    info1.neededReduces);
-    assertEquals(400,  info1.mapDeficit);
-    assertEquals(400,  info1.reduceDeficit);
-    assertEquals(2.0,  info1.mapFairShare);
-    assertEquals(2.0,  info1.reduceFairShare);
-    assertEquals(0,    info2.runningMaps);
-    assertEquals(0,    info2.runningReduces);
-    assertEquals(1,    info2.neededMaps);
-    assertEquals(2,    info2.neededReduces);
-    assertEquals(0,    info2.mapDeficit);
-    assertEquals(0,    info2.reduceDeficit);
-    assertEquals(2.0,  info2.mapFairShare);
-    assertEquals(2.0,  info2.reduceFairShare);
-    
-    // Assign tasks and check that all slots are filled with j1, then j2
+    // Check scheduler variables
+    assertEquals(0,    info1.mapSchedulable.getRunningTasks());
+    assertEquals(0,    info1.reduceSchedulable.getRunningTasks());
+    assertEquals(2,    info1.mapSchedulable.getDemand());
+    assertEquals(1,    info1.reduceSchedulable.getDemand());
+    assertEquals(2.0,  info1.mapSchedulable.getFairShare());
+    assertEquals(1.0,  info1.reduceSchedulable.getFairShare());
+    assertEquals(0,    info2.mapSchedulable.getRunningTasks());
+    assertEquals(0,    info2.reduceSchedulable.getRunningTasks());
+    assertEquals(1,    info2.mapSchedulable.getDemand());
+    assertEquals(2,    info2.reduceSchedulable.getDemand());
+    assertEquals(1.0,  info2.mapSchedulable.getFairShare());
+    assertEquals(2.0,  info2.reduceSchedulable.getFairShare());
+    
+    // Assign tasks and check that jobs alternate in filling slots
     checkAssignment("tt1", "attempt_test_0001_m_000000_0 on tt1");
-    checkAssignment("tt1", "attempt_test_0001_m_000001_0 on tt1");
+    checkAssignment("tt1", "attempt_test_0002_m_000000_0 on tt1");
     checkAssignment("tt1", "attempt_test_0001_r_000000_0 on tt1");
     checkAssignment("tt1", "attempt_test_0002_r_000000_0 on tt1");
-    checkAssignment("tt2", "attempt_test_0002_m_000000_0 on tt2");
+    checkAssignment("tt2", "attempt_test_0001_m_000001_0 on tt2");
     checkAssignment("tt2", "attempt_test_0002_r_000001_0 on tt2");
     assertNull(scheduler.assignTasks(tracker("tt2")));
     
     // Check that the scheduler has started counting the tasks as running
     // as soon as it launched them.
-    assertEquals(2,  info1.runningMaps);
-    assertEquals(1,  info1.runningReduces);
-    assertEquals(0,  info1.neededMaps);
-    assertEquals(0,  info1.neededReduces);
-    assertEquals(1,  info2.runningMaps);
-    assertEquals(2,  info2.runningReduces);
-    assertEquals(0, info2.neededMaps);
-    assertEquals(0, info2.neededReduces);
+    assertEquals(2,  info1.mapSchedulable.getRunningTasks());
+    assertEquals(1,  info1.reduceSchedulable.getRunningTasks());
+    assertEquals(2,  info1.mapSchedulable.getDemand());
+    assertEquals(1,  info1.reduceSchedulable.getDemand());
+    assertEquals(1,  info2.mapSchedulable.getRunningTasks());
+    assertEquals(2,  info2.reduceSchedulable.getRunningTasks());
+    assertEquals(1, info2.mapSchedulable.getDemand());
+    assertEquals(2, info2.reduceSchedulable.getDemand());
+  }
+  
+  /**
+   * This test is identical to testSmallJobs but sets assignMultiple to
+   * true so that multiple tasks can be assigned per heartbeat.
+   */
+  public void testSmallJobsWithAssignMultiple() throws IOException {
+    setUpCluster(1, 2, true);
+    
+    JobInProgress job1 = submitJob(JobStatus.RUNNING, 2, 1);
+    JobInfo info1 = scheduler.infos.get(job1);
+    
+    // Check scheduler variables
+    assertEquals(0,    info1.mapSchedulable.getRunningTasks());
+    assertEquals(0,    info1.reduceSchedulable.getRunningTasks());
+    assertEquals(2,    info1.mapSchedulable.getDemand());
+    assertEquals(1,    info1.reduceSchedulable.getDemand());
+    assertEquals(2.0,  info1.mapSchedulable.getFairShare());
+    assertEquals(1.0,  info1.reduceSchedulable.getFairShare());
+    
+    // Advance time before submitting another job j2, to make j1 run before j2
+    // deterministically.
+    advanceTime(100);
+    JobInProgress job2 = submitJob(JobStatus.RUNNING, 1, 2);
+    JobInfo info2 = scheduler.infos.get(job2);
+    
+    // Check scheduler variables
+    assertEquals(0,    info1.mapSchedulable.getRunningTasks());
+    assertEquals(0,    info1.reduceSchedulable.getRunningTasks());
+    assertEquals(2,    info1.mapSchedulable.getDemand());
+    assertEquals(1,    info1.reduceSchedulable.getDemand());
+    assertEquals(2.0,  info1.mapSchedulable.getFairShare());
+    assertEquals(1.0,  info1.reduceSchedulable.getFairShare());
+    assertEquals(0,    info2.mapSchedulable.getRunningTasks());
+    assertEquals(0,    info2.reduceSchedulable.getRunningTasks());
+    assertEquals(1,    info2.mapSchedulable.getDemand());
+    assertEquals(2,    info2.reduceSchedulable.getDemand());
+    assertEquals(1.0,  info2.mapSchedulable.getFairShare());
+    assertEquals(2.0,  info2.reduceSchedulable.getFairShare());
+    
+    // Assign tasks and check that jobs alternate in filling slots
+    checkAssignment("tt1", "attempt_test_0001_m_000000_0 on tt1",
+                           "attempt_test_0002_m_000000_0 on tt1",
+                           "attempt_test_0001_r_000000_0 on tt1",
+                           "attempt_test_0002_r_000000_0 on tt1");
+    checkAssignment("tt2", "attempt_test_0001_m_000001_0 on tt2",
+                           "attempt_test_0002_r_000001_0 on tt2");
+    assertNull(scheduler.assignTasks(tracker("tt2")));
+    
+    // Check that the scheduler has started counting the tasks as running
+    // as soon as it launched them.
+    assertEquals(2,  info1.mapSchedulable.getRunningTasks());
+    assertEquals(1,  info1.reduceSchedulable.getRunningTasks());
+    assertEquals(2,  info1.mapSchedulable.getDemand());
+    assertEquals(1,  info1.reduceSchedulable.getDemand());
+    assertEquals(1,  info2.mapSchedulable.getRunningTasks());
+    assertEquals(2,  info2.reduceSchedulable.getRunningTasks());
+    assertEquals(1, info2.mapSchedulable.getDemand());
+    assertEquals(2, info2.reduceSchedulable.getDemand());
   }
   
   /**
    * This test begins by submitting two jobs with 10 maps and reduces each.
-   * The first job is submitted 100ms after the second, during which time no
-   * tasks run. After this, we assign tasks to all slots, which should all be
-   * from job 1. These run for 200ms, at which point job 2 now has a deficit
-   * of 400 while job 1 is down to a deficit of 0. We then finish all tasks and
-   * assign new ones, which should all be from job 2. These run for 50 ms,
-   * which is not enough time for job 2 to make up its deficit (it only makes up
-   * 100 ms of deficit). Finally we assign a new round of tasks, which should
-   * all be from job 2 again.
+   * The first job is submitted 100ms after the second, to make it get slots
+   * first deterministically. We then assign a wave of tasks and check that
+   * they are given alternately to job1, job2, job1, job2, etc. We finish
+   * these tasks and assign a second wave, which should continue to be
+   * allocated in this manner.
    */
   public void testLargeJobs() throws IOException {
     JobInProgress job1 = submitJob(JobStatus.RUNNING, 10, 10);
     JobInfo info1 = scheduler.infos.get(job1);
     
     // Check scheduler variables
-    assertEquals(0,    info1.runningMaps);
-    assertEquals(0,    info1.runningReduces);
-    assertEquals(10,   info1.neededMaps);
-    assertEquals(10,   info1.neededReduces);
-    assertEquals(0,    info1.mapDeficit);
-    assertEquals(0,    info1.reduceDeficit);
-    assertEquals(4.0,  info1.mapFairShare);
-    assertEquals(4.0,  info1.reduceFairShare);
+    assertEquals(0,    info1.mapSchedulable.getRunningTasks());
+    assertEquals(0,    info1.reduceSchedulable.getRunningTasks());
+    assertEquals(10,   info1.mapSchedulable.getDemand());
+    assertEquals(10,   info1.reduceSchedulable.getDemand());
+    assertEquals(4.0,  info1.mapSchedulable.getFairShare());
+    assertEquals(4.0,  info1.reduceSchedulable.getFairShare());
     
     // Advance time before submitting another job j2, to make j1 run before j2
     // deterministically.
@@ -668,34 +785,29 @@ public class TestFairScheduler extends TestCase {
     JobInProgress job2 = submitJob(JobStatus.RUNNING, 10, 10);
     JobInfo info2 = scheduler.infos.get(job2);
     
-    // Check scheduler variables; the fair shares should now have been allocated
-    // equally between j1 and j2, but j1 should have (4 slots)*(100 ms) deficit
-    assertEquals(0,    info1.runningMaps);
-    assertEquals(0,    info1.runningReduces);
-    assertEquals(10,   info1.neededMaps);
-    assertEquals(10,   info1.neededReduces);
-    assertEquals(400,  info1.mapDeficit);
-    assertEquals(400,  info1.reduceDeficit);
-    assertEquals(2.0,  info1.mapFairShare);
-    assertEquals(2.0,  info1.reduceFairShare);
-    assertEquals(0,    info2.runningMaps);
-    assertEquals(0,    info2.runningReduces);
-    assertEquals(10,   info2.neededMaps);
-    assertEquals(10,   info2.neededReduces);
-    assertEquals(0,    info2.mapDeficit);
-    assertEquals(0,    info2.reduceDeficit);
-    assertEquals(2.0,  info2.mapFairShare);
-    assertEquals(2.0,  info2.reduceFairShare);
-    
-    // Assign tasks and check that all slots are initially filled with job 1
+    // Check scheduler variables
+    assertEquals(0,    info1.mapSchedulable.getRunningTasks());
+    assertEquals(0,    info1.reduceSchedulable.getRunningTasks());
+    assertEquals(10,   info1.mapSchedulable.getDemand());
+    assertEquals(10,   info1.reduceSchedulable.getDemand());
+    assertEquals(2.0,  info1.mapSchedulable.getFairShare());
+    assertEquals(2.0,  info1.reduceSchedulable.getFairShare());
+    assertEquals(0,    info2.mapSchedulable.getRunningTasks());
+    assertEquals(0,    info2.reduceSchedulable.getRunningTasks());
+    assertEquals(10,   info2.mapSchedulable.getDemand());
+    assertEquals(10,   info2.reduceSchedulable.getDemand());
+    assertEquals(2.0,  info2.mapSchedulable.getFairShare());
+    assertEquals(2.0,  info2.reduceSchedulable.getFairShare());
+    
+    // Check that tasks are filled alternately by the jobs
     checkAssignment("tt1", "attempt_test_0001_m_000000_0 on tt1");
-    checkAssignment("tt1", "attempt_test_0001_m_000001_0 on tt1");
+    checkAssignment("tt1", "attempt_test_0002_m_000000_0 on tt1");
     checkAssignment("tt1", "attempt_test_0001_r_000000_0 on tt1");
-    checkAssignment("tt1", "attempt_test_0001_r_000001_0 on tt1");
-    checkAssignment("tt2", "attempt_test_0001_m_000002_0 on tt2");
-    checkAssignment("tt2", "attempt_test_0001_m_000003_0 on tt2");
-    checkAssignment("tt2", "attempt_test_0001_r_000002_0 on tt2");
-    checkAssignment("tt2", "attempt_test_0001_r_000003_0 on tt2");
+    checkAssignment("tt1", "attempt_test_0002_r_000000_0 on tt1");
+    checkAssignment("tt2", "attempt_test_0001_m_000001_0 on tt2");
+    checkAssignment("tt2", "attempt_test_0002_m_000001_0 on tt2");
+    checkAssignment("tt2", "attempt_test_0001_r_000001_0 on tt2");
+    checkAssignment("tt2", "attempt_test_0002_r_000001_0 on tt2");
     
     // Check that no new tasks can be launched once the tasktrackers are full
     assertNull(scheduler.assignTasks(tracker("tt1")));
@@ -703,84 +815,175 @@ public class TestFairScheduler extends TestCase {
     
     // Check that the scheduler has started counting the tasks as running
     // as soon as it launched them.
-    assertEquals(4,  info1.runningMaps);
-    assertEquals(4,  info1.runningReduces);
-    assertEquals(6,  info1.neededMaps);
-    assertEquals(6,  info1.neededReduces);
-    assertEquals(0,  info2.runningMaps);
-    assertEquals(0,  info2.runningReduces);
-    assertEquals(10, info2.neededMaps);
-    assertEquals(10, info2.neededReduces);
+    assertEquals(2,  info1.mapSchedulable.getRunningTasks());
+    assertEquals(2,  info1.reduceSchedulable.getRunningTasks());
+    assertEquals(10,  info1.mapSchedulable.getDemand());
+    assertEquals(10,  info1.reduceSchedulable.getDemand());
+    assertEquals(2,  info2.mapSchedulable.getRunningTasks());
+    assertEquals(2,  info2.reduceSchedulable.getRunningTasks());
+    assertEquals(10, info2.mapSchedulable.getDemand());
+    assertEquals(10, info2.reduceSchedulable.getDemand());
     
     // Finish up the tasks and advance time again. Note that we must finish
     // the task since FakeJobInProgress does not properly maintain running
     // tasks, so the scheduler will always get an empty task list from
     // the JobInProgress's getMapTasks/getReduceTasks and think they finished.
     taskTrackerManager.finishTask("tt1", "attempt_test_0001_m_000000_0");
-    taskTrackerManager.finishTask("tt1", "attempt_test_0001_m_000001_0");
+    taskTrackerManager.finishTask("tt1", "attempt_test_0002_m_000000_0");
     taskTrackerManager.finishTask("tt1", "attempt_test_0001_r_000000_0");
-    taskTrackerManager.finishTask("tt1", "attempt_test_0001_r_000001_0");
-    taskTrackerManager.finishTask("tt2", "attempt_test_0001_m_000002_0");
-    taskTrackerManager.finishTask("tt2", "attempt_test_0001_m_000003_0");
-    taskTrackerManager.finishTask("tt2", "attempt_test_0001_r_000002_0");
-    taskTrackerManager.finishTask("tt2", "attempt_test_0001_r_000003_0");
+    taskTrackerManager.finishTask("tt1", "attempt_test_0002_r_000000_0");
+    taskTrackerManager.finishTask("tt2", "attempt_test_0001_m_000001_0");
+    taskTrackerManager.finishTask("tt2", "attempt_test_0002_m_000001_0");
+    taskTrackerManager.finishTask("tt2", "attempt_test_0001_r_000001_0");
+    taskTrackerManager.finishTask("tt2", "attempt_test_0002_r_000001_0");
     advanceTime(200);
-    assertEquals(0,   info1.runningMaps);
-    assertEquals(0,   info1.runningReduces);
-    assertEquals(0,   info1.mapDeficit);
-    assertEquals(0,   info1.reduceDeficit);
-    assertEquals(0,   info2.runningMaps);
-    assertEquals(0,   info2.runningReduces);
-    assertEquals(400, info2.mapDeficit);
-    assertEquals(400, info2.reduceDeficit);
+    assertEquals(0,   info1.mapSchedulable.getRunningTasks());
+    assertEquals(0,   info1.reduceSchedulable.getRunningTasks());
+    assertEquals(0,   info2.mapSchedulable.getRunningTasks());
+    assertEquals(0,   info2.reduceSchedulable.getRunningTasks());
 
-    // Assign tasks and check that all slots are now filled with job 2
-    checkAssignment("tt1", "attempt_test_0002_m_000000_0 on tt1");
-    checkAssignment("tt1", "attempt_test_0002_m_000001_0 on tt1");
-    checkAssignment("tt1", "attempt_test_0002_r_000000_0 on tt1");
-    checkAssignment("tt1", "attempt_test_0002_r_000001_0 on tt1");
-    checkAssignment("tt2", "attempt_test_0002_m_000002_0 on tt2");
+    // Check that tasks are filled alternately by the jobs
+    checkAssignment("tt1", "attempt_test_0001_m_000002_0 on tt1");
+    checkAssignment("tt1", "attempt_test_0002_m_000002_0 on tt1");
+    checkAssignment("tt1", "attempt_test_0001_r_000002_0 on tt1");
+    checkAssignment("tt1", "attempt_test_0002_r_000002_0 on tt1");
+    checkAssignment("tt2", "attempt_test_0001_m_000003_0 on tt2");
     checkAssignment("tt2", "attempt_test_0002_m_000003_0 on tt2");
-    checkAssignment("tt2", "attempt_test_0002_r_000002_0 on tt2");
+    checkAssignment("tt2", "attempt_test_0001_r_000003_0 on tt2");
     checkAssignment("tt2", "attempt_test_0002_r_000003_0 on tt2");
-
-    // Finish up the tasks and advance time again, but give job 2 only 50ms.
+    
+    // Check scheduler variables; the demands should now be 8 because 2 tasks
+    // of each type have finished in each job
+    assertEquals(2,    info1.mapSchedulable.getRunningTasks());
+    assertEquals(2,    info1.reduceSchedulable.getRunningTasks());
+    assertEquals(8,   info1.mapSchedulable.getDemand());
+    assertEquals(8,   info1.reduceSchedulable.getDemand());
+    assertEquals(2.0,  info1.mapSchedulable.getFairShare());
+    assertEquals(2.0,  info1.reduceSchedulable.getFairShare());
+    assertEquals(2,    info2.mapSchedulable.getRunningTasks());
+    assertEquals(2,    info2.reduceSchedulable.getRunningTasks());
+    assertEquals(8,   info2.mapSchedulable.getDemand());
+    assertEquals(8,   info2.reduceSchedulable.getDemand());
+    assertEquals(2.0,  info2.mapSchedulable.getFairShare());
+    assertEquals(2.0,  info2.reduceSchedulable.getFairShare());
+  }
+  
+  /**
+   * A copy of testLargeJobs that enables the assignMultiple feature to launch
+   * multiple tasks per heartbeat. Results should be the same as testLargeJobs.
+   */
+  public void testLargeJobsWithAssignMultiple() throws IOException {
+    setUpCluster(1, 2, true);
+    
+    JobInProgress job1 = submitJob(JobStatus.RUNNING, 10, 10);
+    JobInfo info1 = scheduler.infos.get(job1);
+    
+    // Check scheduler variables
+    assertEquals(0,    info1.mapSchedulable.getRunningTasks());
+    assertEquals(0,    info1.reduceSchedulable.getRunningTasks());
+    assertEquals(10,   info1.mapSchedulable.getDemand());
+    assertEquals(10,   info1.reduceSchedulable.getDemand());
+    assertEquals(4.0,  info1.mapSchedulable.getFairShare());
+    assertEquals(4.0,  info1.reduceSchedulable.getFairShare());
+    
+    // Advance time before submitting another job j2, to make j1 run before j2
+    // deterministically.
+    advanceTime(100);
+    JobInProgress job2 = submitJob(JobStatus.RUNNING, 10, 10);
+    JobInfo info2 = scheduler.infos.get(job2);
+    
+    // Check scheduler variables; the fair shares should now have been allocated
+    // equally between j1 and j2, but j1 should have (4 slots)*(100 ms) deficit
+    assertEquals(0,    info1.mapSchedulable.getRunningTasks());
+    assertEquals(0,    info1.reduceSchedulable.getRunningTasks());
+    assertEquals(10,   info1.mapSchedulable.getDemand());
+    assertEquals(10,   info1.reduceSchedulable.getDemand());
+    assertEquals(2.0,  info1.mapSchedulable.getFairShare());
+    assertEquals(2.0,  info1.reduceSchedulable.getFairShare());
+    assertEquals(0,    info2.mapSchedulable.getRunningTasks());
+    assertEquals(0,    info2.reduceSchedulable.getRunningTasks());
+    assertEquals(10,   info2.mapSchedulable.getDemand());
+    assertEquals(10,   info2.reduceSchedulable.getDemand());
+    assertEquals(2.0,  info2.mapSchedulable.getFairShare());
+    assertEquals(2.0,  info2.reduceSchedulable.getFairShare());
+    
+    // Check that tasks are filled alternately by the jobs
+    checkAssignment("tt1", "attempt_test_0001_m_000000_0 on tt1",
+                           "attempt_test_0002_m_000000_0 on tt1",
+                           "attempt_test_0001_r_000000_0 on tt1",
+                           "attempt_test_0002_r_000000_0 on tt1");
+    checkAssignment("tt2", "attempt_test_0001_m_000001_0 on tt2",
+                           "attempt_test_0002_m_000001_0 on tt2",
+                           "attempt_test_0001_r_000001_0 on tt2",
+                           "attempt_test_0002_r_000001_0 on tt2");
+    
+    // Check that no new tasks can be launched once the tasktrackers are full
+    assertNull(scheduler.assignTasks(tracker("tt1")));
+    assertNull(scheduler.assignTasks(tracker("tt2")));
+    
+    // Check that the scheduler has started counting the tasks as running
+    // as soon as it launched them.
+    assertEquals(2,  info1.mapSchedulable.getRunningTasks());
+    assertEquals(2,  info1.reduceSchedulable.getRunningTasks());
+    assertEquals(10,  info1.mapSchedulable.getDemand());
+    assertEquals(10,  info1.reduceSchedulable.getDemand());
+    assertEquals(2,  info2.mapSchedulable.getRunningTasks());
+    assertEquals(2,  info2.reduceSchedulable.getRunningTasks());
+    assertEquals(10, info2.mapSchedulable.getDemand());
+    assertEquals(10, info2.reduceSchedulable.getDemand());
+    
+    // Finish up the tasks and advance time again. Note that we must finish
+    // the task since FakeJobInProgress does not properly maintain running
+    // tasks, so the scheduler will always get an empty task list from
+    // the JobInProgress's getMapTasks/getReduceTasks and think they finished.
+    taskTrackerManager.finishTask("tt1", "attempt_test_0001_m_000000_0");
     taskTrackerManager.finishTask("tt1", "attempt_test_0002_m_000000_0");
-    taskTrackerManager.finishTask("tt1", "attempt_test_0002_m_000001_0");
+    taskTrackerManager.finishTask("tt1", "attempt_test_0001_r_000000_0");
     taskTrackerManager.finishTask("tt1", "attempt_test_0002_r_000000_0");
-    taskTrackerManager.finishTask("tt1", "attempt_test_0002_r_000001_0");
-    taskTrackerManager.finishTask("tt2", "attempt_test_0002_m_000002_0");
-    taskTrackerManager.finishTask("tt2", "attempt_test_0002_m_000003_0");
-    taskTrackerManager.finishTask("tt2", "attempt_test_0002_r_000002_0");
-    taskTrackerManager.finishTask("tt2", "attempt_test_0002_r_000003_0");
-    advanceTime(50);
-    assertEquals(0,   info1.runningMaps);
-    assertEquals(0,   info1.runningReduces);
-    assertEquals(100, info1.mapDeficit);
-    assertEquals(100, info1.reduceDeficit);
-    assertEquals(0,   info2.runningMaps);
-    assertEquals(0,   info2.runningReduces);
-    assertEquals(300, info2.mapDeficit);
-    assertEquals(300, info2.reduceDeficit);
+    taskTrackerManager.finishTask("tt2", "attempt_test_0001_m_000001_0");
+    taskTrackerManager.finishTask("tt2", "attempt_test_0002_m_000001_0");
+    taskTrackerManager.finishTask("tt2", "attempt_test_0001_r_000001_0");
+    taskTrackerManager.finishTask("tt2", "attempt_test_0002_r_000001_0");
+    advanceTime(200);
+    assertEquals(0,   info1.mapSchedulable.getRunningTasks());
+    assertEquals(0,   info1.reduceSchedulable.getRunningTasks());
+    assertEquals(0,   info2.mapSchedulable.getRunningTasks());
+    assertEquals(0,   info2.reduceSchedulable.getRunningTasks());
 
-    // Assign tasks and check that all slots are now still with job 2
-    checkAssignment("tt1", "attempt_test_0002_m_000004_0 on tt1");
-    checkAssignment("tt1", "attempt_test_0002_m_000005_0 on tt1");
-    checkAssignment("tt1", "attempt_test_0002_r_000004_0 on tt1");
-    checkAssignment("tt1", "attempt_test_0002_r_000005_0 on tt1");
-    checkAssignment("tt2", "attempt_test_0002_m_000006_0 on tt2");
-    checkAssignment("tt2", "attempt_test_0002_m_000007_0 on tt2");
-    checkAssignment("tt2", "attempt_test_0002_r_000006_0 on tt2");
-    checkAssignment("tt2", "attempt_test_0002_r_000007_0 on tt2");
+    // Check that tasks are filled alternately by the jobs
+    checkAssignment("tt1", "attempt_test_0001_m_000002_0 on tt1",
+                           "attempt_test_0002_m_000002_0 on tt1",
+                           "attempt_test_0001_r_000002_0 on tt1",
+                           "attempt_test_0002_r_000002_0 on tt1");
+    checkAssignment("tt2", "attempt_test_0001_m_000003_0 on tt2",
+                           "attempt_test_0002_m_000003_0 on tt2",
+                           "attempt_test_0001_r_000003_0 on tt2",
+                           "attempt_test_0002_r_000003_0 on tt2");
+    
+    // Check scheduler variables; the demands should now be 8 because 2 tasks
+    // of each type have finished in each job
+    assertEquals(2,    info1.mapSchedulable.getRunningTasks());
+    assertEquals(2,    info1.reduceSchedulable.getRunningTasks());
+    assertEquals(8,   info1.mapSchedulable.getDemand());
+    assertEquals(8,   info1.reduceSchedulable.getDemand());
+    assertEquals(2.0,  info1.mapSchedulable.getFairShare());
+    assertEquals(2.0,  info1.reduceSchedulable.getFairShare());
+    assertEquals(2,    info2.mapSchedulable.getRunningTasks());
+    assertEquals(2,    info2.reduceSchedulable.getRunningTasks());
+    assertEquals(8,   info2.mapSchedulable.getDemand());
+    assertEquals(8,   info2.reduceSchedulable.getDemand());
+    assertEquals(2.0,  info2.mapSchedulable.getFairShare());
+    assertEquals(2.0,  info2.reduceSchedulable.getFairShare());
   }
-  
 
   /**
-   * We submit two jobs such that one has 2x the priority of the other, wait
-   * for 100 ms, and check that the weights/deficits are okay and that the
-   * tasks all go to the high-priority job.
+   * We submit two jobs such that one has 2x the priority of the other to 
+   * a cluster of 3 nodes, wait for 100 ms, and check that the weights/shares 
+   * the high-priority job gets 4 tasks while the normal-priority job gets 2.
    */
   public void testJobsWithPriorities() throws IOException {
+    setUpCluster(1, 3, false);
+    
     JobInProgress job1 = submitJob(JobStatus.RUNNING, 10, 10);
     JobInfo info1 = scheduler.infos.get(job1);
     JobInProgress job2 = submitJob(JobStatus.RUNNING, 10, 10);
@@ -789,56 +992,52 @@ public class TestFairScheduler extends TestCase {
     scheduler.update();
     
     // Check scheduler variables
-    assertEquals(0,    info1.runningMaps);
-    assertEquals(0,    info1.runningReduces);
-    assertEquals(10,   info1.neededMaps);
-    assertEquals(10,   info1.neededReduces);
-    assertEquals(0,    info1.mapDeficit);
-    assertEquals(0,    info1.reduceDeficit);
-    assertEquals(1.33, info1.mapFairShare, 0.1);
-    assertEquals(1.33, info1.reduceFairShare, 0.1);
-    assertEquals(0,    info2.runningMaps);
-    assertEquals(0,    info2.runningReduces);
-    assertEquals(10,   info2.neededMaps);
-    assertEquals(10,   info2.neededReduces);
-    assertEquals(0,    info2.mapDeficit);
-    assertEquals(0,    info2.reduceDeficit);
-    assertEquals(2.66, info2.mapFairShare, 0.1);
-    assertEquals(2.66, info2.reduceFairShare, 0.1);
-    
-    // Advance time and check deficits
+    assertEquals(0,   info1.mapSchedulable.getRunningTasks());
+    assertEquals(0,   info1.reduceSchedulable.getRunningTasks());
+    assertEquals(10,  info1.mapSchedulable.getDemand());
+    assertEquals(10,  info1.reduceSchedulable.getDemand());
+    assertEquals(2.0, info1.mapSchedulable.getFairShare(), 0.1);
+    assertEquals(2.0, info1.reduceSchedulable.getFairShare(), 0.1);
+    assertEquals(0,   info2.mapSchedulable.getRunningTasks());
+    assertEquals(0,   info2.reduceSchedulable.getRunningTasks());
+    assertEquals(10,  info2.mapSchedulable.getDemand());
+    assertEquals(10,  info2.reduceSchedulable.getDemand());
+    assertEquals(4.0, info2.mapSchedulable.getFairShare(), 0.1);
+    assertEquals(4.0, info2.reduceSchedulable.getFairShare(), 0.1);
+    
+    // Advance time
     advanceTime(100);
-    assertEquals(133,  info1.mapDeficit, 1.0);
-    assertEquals(133,  info1.reduceDeficit, 1.0);
-    assertEquals(266,  info2.mapDeficit, 1.0);
-    assertEquals(266,  info2.reduceDeficit, 1.0);
     
-    // Assign tasks and check that all slots are filled with j1, then j2
+    // Assign tasks and check that j2 gets 2x more tasks than j1. In addition,
+    // whenever the jobs' runningTasks/weight ratios are tied, j1 should get
+    // the new task first because it started first; thus the tasks of each
+    // type should be handed out alternately to 1, 2, 2, 1, 2, 2, etc.
+    System.out.println("HEREEEE");
+    checkAssignment("tt1", "attempt_test_0001_m_000000_0 on tt1");
     checkAssignment("tt1", "attempt_test_0002_m_000000_0 on tt1");
-    checkAssignment("tt1", "attempt_test_0002_m_000001_0 on tt1");
+    checkAssignment("tt1", "attempt_test_0001_r_000000_0 on tt1");
     checkAssignment("tt1", "attempt_test_0002_r_000000_0 on tt1");
-    checkAssignment("tt1", "attempt_test_0002_r_000001_0 on tt1");
-    checkAssignment("tt2", "attempt_test_0002_m_000002_0 on tt2");
-    checkAssignment("tt2", "attempt_test_0002_m_000003_0 on tt2");
-    checkAssignment("tt2", "attempt_test_0002_r_000002_0 on tt2");
-    checkAssignment("tt2", "attempt_test_0002_r_000003_0 on tt2");
+    checkAssignment("tt2", "attempt_test_0002_m_000001_0 on tt2");
+    checkAssignment("tt2", "attempt_test_0001_m_000001_0 on tt2");
+    checkAssignment("tt2", "attempt_test_0002_r_000001_0 on tt2");
+    checkAssignment("tt2", "attempt_test_0001_r_000001_0 on tt2");
+    checkAssignment("tt3", "attempt_test_0002_m_000002_0 on tt3");
+    checkAssignment("tt3", "attempt_test_0002_m_000003_0 on tt3");
+    checkAssignment("tt3", "attempt_test_0002_r_000002_0 on tt3");
+    checkAssignment("tt3", "attempt_test_0002_r_000003_0 on tt3");
   }
   
   /**
    * This test starts by submitting three large jobs:
    * - job1 in the default pool, at time 0
    * - job2 in poolA, with an allocation of 1 map / 2 reduces, at time 200
-   * - job3 in poolB, with an allocation of 2 maps / 1 reduce, at time 200
-   * 
-   * After this, we sleep 100ms, until time 300. At this point, job1 has the
-   * highest map deficit, job3 the second, and job2 the third. This is because
-   * job3 has more maps in its min share than job2, but job1 has been around
-   * a long time at the beginning. The reduce deficits are similar, except job2
-   * comes before job3 because it had a higher reduce minimum share.
+   * - job3 in poolB, with an allocation of 2 maps / 1 reduce, at time 300
    * 
-   * Finally, assign tasks to all slots. The maps should be assigned in the
-   * order job3, job2, job1 because 3 and 2 both have guaranteed slots and 3
-   * has a higher deficit. The reduces should be assigned as job2, job3, job1.
+   * We then assign tasks to all slots. The maps should be assigned in the
+   * order job2, job3, job 3, job1 because jobs 3 and 2 have guaranteed slots
+   * (1 and 2 respectively). Job2 comes before job3 when they are both at 0
+   * slots because it has an earlier start time. In a similar manner,
+   * reduces should be assigned as job2, job3, job2, job1.
    */
   public void testLargeJobsWithPools() throws Exception {
     // Set up pools file
@@ -858,60 +1057,54 @@ public class TestFairScheduler extends TestCase {
     out.println("</allocations>");
     out.close();
     scheduler.getPoolManager().reloadAllocs();
+    Pool defaultPool = scheduler.getPoolManager().getPool("default");
+    Pool poolA = scheduler.getPoolManager().getPool("poolA");
+    Pool poolB = scheduler.getPoolManager().getPool("poolB");
     
     JobInProgress job1 = submitJob(JobStatus.RUNNING, 10, 10);
     JobInfo info1 = scheduler.infos.get(job1);
     
     // Check scheduler variables
-    assertEquals(0,    info1.runningMaps);
-    assertEquals(0,    info1.runningReduces);
-    assertEquals(10,   info1.neededMaps);
-    assertEquals(10,   info1.neededReduces);
-    assertEquals(0,    info1.mapDeficit);
-    assertEquals(0,    info1.reduceDeficit);
-    assertEquals(4.0,  info1.mapFairShare);
-    assertEquals(4.0,  info1.reduceFairShare);
+    assertEquals(0,    info1.mapSchedulable.getRunningTasks());
+    assertEquals(0,    info1.reduceSchedulable.getRunningTasks());
+    assertEquals(10,   info1.mapSchedulable.getDemand());
+    assertEquals(10,   info1.reduceSchedulable.getDemand());
+    assertEquals(4.0,  info1.mapSchedulable.getFairShare());
+    assertEquals(4.0,  info1.reduceSchedulable.getFairShare());
     
     // Advance time 200ms and submit jobs 2 and 3
     advanceTime(200);
-    assertEquals(800,  info1.mapDeficit);
-    assertEquals(800,  info1.reduceDeficit);
     JobInProgress job2 = submitJob(JobStatus.RUNNING, 10, 10, "poolA");
     JobInfo info2 = scheduler.infos.get(job2);
+    advanceTime(100);
     JobInProgress job3 = submitJob(JobStatus.RUNNING, 10, 10, "poolB");
     JobInfo info3 = scheduler.infos.get(job3);
     
     // Check that minimum and fair shares have been allocated
-    assertEquals(0,    info1.minMaps);
-    assertEquals(0,    info1.minReduces);
-    assertEquals(1.0,  info1.mapFairShare);
-    assertEquals(1.0,  info1.reduceFairShare);
-    assertEquals(1,    info2.minMaps);
-    assertEquals(2,    info2.minReduces);
-    assertEquals(1.0,  info2.mapFairShare);
-    assertEquals(2.0,  info2.reduceFairShare);
-    assertEquals(2,    info3.minMaps);
-    assertEquals(1,    info3.minReduces);
-    assertEquals(2.0,  info3.mapFairShare);
-    assertEquals(1.0,  info3.reduceFairShare);
-    
-    // Advance time 100ms and check deficits
+    assertEquals(0,    defaultPool.getMapSchedulable().getMinShare());
+    assertEquals(0,    defaultPool.getReduceSchedulable().getMinShare());
+    assertEquals(1.0,  info1.mapSchedulable.getFairShare());
+    assertEquals(1.0,  info1.reduceSchedulable.getFairShare());
+    assertEquals(1,    poolA.getMapSchedulable().getMinShare());
+    assertEquals(2,    poolA.getReduceSchedulable().getMinShare());
+    assertEquals(1.0,  info2.mapSchedulable.getFairShare());
+    assertEquals(2.0,  info2.reduceSchedulable.getFairShare());
+    assertEquals(2,    poolB.getMapSchedulable().getMinShare());
+    assertEquals(1,    poolB.getReduceSchedulable().getMinShare());
+    assertEquals(2.0,  info3.mapSchedulable.getFairShare());
+    assertEquals(1.0,  info3.reduceSchedulable.getFairShare());
+    
+    // Advance time 100ms
     advanceTime(100);
-    assertEquals(900,  info1.mapDeficit);
-    assertEquals(900,  info1.reduceDeficit);
-    assertEquals(100,  info2.mapDeficit);
-    assertEquals(200,  info2.reduceDeficit);
-    assertEquals(200,  info3.mapDeficit);
-    assertEquals(100,  info3.reduceDeficit);
     
     // Assign tasks and check that slots are first given to needy jobs
+    checkAssignment("tt1", "attempt_test_0002_m_000000_0 on tt1");
     checkAssignment("tt1", "attempt_test_0003_m_000000_0 on tt1");
-    checkAssignment("tt1", "attempt_test_0003_m_000001_0 on tt1");
     checkAssignment("tt1", "attempt_test_0002_r_000000_0 on tt1");
-    checkAssignment("tt1", "attempt_test_0002_r_000001_0 on tt1");
-    checkAssignment("tt2", "attempt_test_0002_m_000000_0 on tt2");
+    checkAssignment("tt1", "attempt_test_0003_r_000000_0 on tt1");
+    checkAssignment("tt2", "attempt_test_0003_m_000001_0 on tt2");
     checkAssignment("tt2", "attempt_test_0001_m_000000_0 on tt2");
-    checkAssignment("tt2", "attempt_test_0003_r_000000_0 on tt2");
+    checkAssignment("tt2", "attempt_test_0002_r_000001_0 on tt2");
     checkAssignment("tt2", "attempt_test_0001_r_000000_0 on tt2");
   }
 
@@ -921,8 +1114,7 @@ public class TestFairScheduler extends TestCase {
    * - job2 in poolA, with an allocation of 2 maps / 2 reduces, at time 200
    * - job3 in poolA, with an allocation of 2 maps / 2 reduces, at time 300
    * 
-   * After this, we sleep 100ms, until time 400. At this point, job1 has the
-   * highest deficit, job2 the second, and job3 the third. The first two tasks
+   * After this, we start assigning tasks. The first two tasks of each type
    * should be assigned to job2 and job3 since they are in a pool with an
    * allocation guarantee, but the next two slots should be assigned to job 3
    * because the pool will no longer be needy.
@@ -940,71 +1132,52 @@ public class TestFairScheduler extends TestCase {
     out.println("</allocations>");
     out.close();
     scheduler.getPoolManager().reloadAllocs();
+    Pool poolA = scheduler.getPoolManager().getPool("poolA");
     
     JobInProgress job1 = submitJob(JobStatus.RUNNING, 10, 10);
     JobInfo info1 = scheduler.infos.get(job1);
     
     // Check scheduler variables
-    assertEquals(0,    info1.runningMaps);
-    assertEquals(0,    info1.runningReduces);
-    assertEquals(10,   info1.neededMaps);
-    assertEquals(10,   info1.neededReduces);
-    assertEquals(0,    info1.mapDeficit);
-    assertEquals(0,    info1.reduceDeficit);
-    assertEquals(4.0,  info1.mapFairShare);
-    assertEquals(4.0,  info1.reduceFairShare);
+    assertEquals(0,    info1.mapSchedulable.getRunningTasks());
+    assertEquals(0,    info1.reduceSchedulable.getRunningTasks());
+    assertEquals(10,   info1.mapSchedulable.getDemand());
+    assertEquals(10,   info1.reduceSchedulable.getDemand());
+    assertEquals(4.0,  info1.mapSchedulable.getFairShare());
+    assertEquals(4.0,  info1.reduceSchedulable.getFairShare());
     
     // Advance time 200ms and submit job 2
     advanceTime(200);
-    assertEquals(800,  info1.mapDeficit);
-    assertEquals(800,  info1.reduceDeficit);
     JobInProgress job2 = submitJob(JobStatus.RUNNING, 10, 10, "poolA");
     JobInfo info2 = scheduler.infos.get(job2);
     
     // Check that minimum and fair shares have been allocated
-    assertEquals(0,    info1.minMaps);
-    assertEquals(0,    info1.minReduces);
-    assertEquals(2.0,  info1.mapFairShare);
-    assertEquals(2.0,  info1.reduceFairShare);
-    assertEquals(2,    info2.minMaps);
-    assertEquals(2,    info2.minReduces);
-    assertEquals(2.0,  info2.mapFairShare);
-    assertEquals(2.0,  info2.reduceFairShare);
+    assertEquals(2,    poolA.getMapSchedulable().getMinShare());
+    assertEquals(2,    poolA.getReduceSchedulable().getMinShare());
+    assertEquals(2.0,  info1.mapSchedulable.getFairShare());
+    assertEquals(2.0,  info1.reduceSchedulable.getFairShare());
+    assertEquals(2.0,  info2.mapSchedulable.getFairShare());
+    assertEquals(2.0,  info2.reduceSchedulable.getFairShare());
     
     // Advance time 100ms and submit job 3
     advanceTime(100);
-    assertEquals(1000, info1.mapDeficit);
-    assertEquals(1000, info1.reduceDeficit);
-    assertEquals(200,  info2.mapDeficit);
-    assertEquals(200,  info2.reduceDeficit);
     JobInProgress job3 = submitJob(JobStatus.RUNNING, 10, 10, "poolA");
     JobInfo info3 = scheduler.infos.get(job3);
     
     // Check that minimum and fair shares have been allocated
-    assertEquals(0,    info1.minMaps);
-    assertEquals(0,    info1.minReduces);
-    assertEquals(2,    info1.mapFairShare, 0.1);
-    assertEquals(2,    info1.reduceFairShare, 0.1);
-    assertEquals(1,    info2.minMaps);
-    assertEquals(1,    info2.minReduces);
-    assertEquals(1,    info2.mapFairShare, 0.1);
-    assertEquals(1,    info2.reduceFairShare, 0.1);
-    assertEquals(1,    info3.minMaps);
-    assertEquals(1,    info3.minReduces);
-    assertEquals(1,    info3.mapFairShare, 0.1);
-    assertEquals(1,    info3.reduceFairShare, 0.1);
-    
-    // Advance time 100ms and check deficits
+    assertEquals(2,    poolA.getMapSchedulable().getMinShare());
+    assertEquals(2,    poolA.getReduceSchedulable().getMinShare());
+    assertEquals(2.0,  info1.mapSchedulable.getFairShare());
+    assertEquals(2.0,  info1.reduceSchedulable.getFairShare());
+    assertEquals(1.0,  info2.mapSchedulable.getFairShare());
+    assertEquals(1.0,  info2.reduceSchedulable.getFairShare());
+    assertEquals(1.0,  info3.mapSchedulable.getFairShare());
+    assertEquals(1.0,  info3.reduceSchedulable.getFairShare());
+    
+    // Advance time
     advanceTime(100);
-    assertEquals(1200, info1.mapDeficit, 1.0);
-    assertEquals(1200, info1.reduceDeficit, 1.0);
-    assertEquals(300,  info2.mapDeficit, 1.0);
-    assertEquals(300,  info2.reduceDeficit, 1.0);
-    assertEquals(100,  info3.mapDeficit, 1.0);
-    assertEquals(100,  info3.reduceDeficit, 1.0);
     
     // Assign tasks and check that slots are first given to needy jobs, but
-    // that job 1 gets two tasks after due to having a larger deficit.
+    // that job 1 gets two tasks after due to having a larger share.
     checkAssignment("tt1", "attempt_test_0002_m_000000_0 on tt1");
     checkAssignment("tt1", "attempt_test_0003_m_000000_0 on tt1");
     checkAssignment("tt1", "attempt_test_0002_r_000000_0 on tt1");
@@ -1016,16 +1189,88 @@ public class TestFairScheduler extends TestCase {
   }
   
   /**
+   * A copy of testLargeJobsWithExcessCapacity that enables assigning multiple
+   * tasks per heartbeat. Results should match testLargeJobsWithExcessCapacity.
+   */
+  public void testLargeJobsWithExcessCapacityAndAssignMultiple() 
+      throws Exception {
+    setUpCluster(1, 2, true);
+    
+    // Set up pools file
+    PrintWriter out = new PrintWriter(new FileWriter(ALLOC_FILE));
+    out.println("<?xml version=\"1.0\"?>");
+    out.println("<allocations>");
+    // Give pool A a minimum of 2 maps, 2 reduces
+    out.println("<pool name=\"poolA\">");
+    out.println("<minMaps>2</minMaps>");
+    out.println("<minReduces>2</minReduces>");
+    out.println("</pool>");
+    out.println("</allocations>");
+    out.close();
+    scheduler.getPoolManager().reloadAllocs();
+    Pool poolA = scheduler.getPoolManager().getPool("poolA");
+    
+    JobInProgress job1 = submitJob(JobStatus.RUNNING, 10, 10);
+    JobInfo info1 = scheduler.infos.get(job1);
+    
+    // Check scheduler variables
+    assertEquals(0,    info1.mapSchedulable.getRunningTasks());
+    assertEquals(0,    info1.reduceSchedulable.getRunningTasks());
+    assertEquals(10,   info1.mapSchedulable.getDemand());
+    assertEquals(10,   info1.reduceSchedulable.getDemand());
+    assertEquals(4.0,  info1.mapSchedulable.getFairShare());
+    assertEquals(4.0,  info1.reduceSchedulable.getFairShare());
+    
+    // Advance time 200ms and submit job 2
+    advanceTime(200);
+    JobInProgress job2 = submitJob(JobStatus.RUNNING, 10, 10, "poolA");
+    JobInfo info2 = scheduler.infos.get(job2);
+    
+    // Check that minimum and fair shares have been allocated
+    assertEquals(2,    poolA.getMapSchedulable().getMinShare());
+    assertEquals(2,    poolA.getReduceSchedulable().getMinShare());
+    assertEquals(2.0,  info1.mapSchedulable.getFairShare());
+    assertEquals(2.0,  info1.reduceSchedulable.getFairShare());
+    assertEquals(2.0,  info2.mapSchedulable.getFairShare());
+    assertEquals(2.0,  info2.reduceSchedulable.getFairShare());
+    
+    // Advance time 100ms and submit job 3
+    advanceTime(100);
+    JobInProgress job3 = submitJob(JobStatus.RUNNING, 10, 10, "poolA");
+    JobInfo info3 = scheduler.infos.get(job3);
+    
+    // Check that minimum and fair shares have been allocated
+    assertEquals(2,    poolA.getMapSchedulable().getMinShare());
+    assertEquals(2,    poolA.getReduceSchedulable().getMinShare());
+    assertEquals(2.0,  info1.mapSchedulable.getFairShare());
+    assertEquals(2.0,  info1.reduceSchedulable.getFairShare());
+    assertEquals(1.0,  info2.mapSchedulable.getFairShare());
+    assertEquals(1.0,  info2.reduceSchedulable.getFairShare());
+    assertEquals(1.0,  info3.mapSchedulable.getFairShare());
+    assertEquals(1.0,  info3.reduceSchedulable.getFairShare());
+    
+    // Advance time
+    advanceTime(100);
+    
+    // Assign tasks and check that slots are first given to needy jobs, but
+    // that job 1 gets two tasks after due to having a larger share.
+    checkAssignment("tt1", "attempt_test_0002_m_000000_0 on tt1",
+                           "attempt_test_0003_m_000000_0 on tt1",
+                           "attempt_test_0002_r_000000_0 on tt1",
+                           "attempt_test_0003_r_000000_0 on tt1");
+    checkAssignment("tt2", "attempt_test_0001_m_000000_0 on tt2",
+                           "attempt_test_0001_m_000001_0 on tt2",
+                           "attempt_test_0001_r_000000_0 on tt2",
+                           "attempt_test_0001_r_000001_0 on tt2");
+  }
+  
+  /**
    * This test starts by submitting two jobs at time 0:
    * - job1 in the default pool
    * - job2, with 1 map and 1 reduce, in poolA, which has an alloc of 4
    *   maps and 4 reduces
    * 
    * When we assign the slots, job2 should only get 1 of each type of task.
-   * 
-   * The fair share for job 2 should be 2.0 however, because even though it is
-   * running only one task, it accumulates deficit in case it will have failures
-   * or need speculative tasks later. (TODO: This may not be a good policy.)
    */
   public void testSmallJobInLargePool() throws Exception {
     // Set up pools file
@@ -1047,22 +1292,18 @@ public class TestFairScheduler extends TestCase {
     JobInfo info2 = scheduler.infos.get(job2);
     
     // Check scheduler variables
-    assertEquals(0,    info1.runningMaps);
-    assertEquals(0,    info1.runningReduces);
-    assertEquals(10,   info1.neededMaps);
-    assertEquals(10,   info1.neededReduces);
-    assertEquals(0,    info1.mapDeficit);
-    assertEquals(0,    info1.reduceDeficit);
-    assertEquals(2.0,  info1.mapFairShare);
-    assertEquals(2.0,  info1.reduceFairShare);
-    assertEquals(0,    info2.runningMaps);
-    assertEquals(0,    info2.runningReduces);
-    assertEquals(1,    info2.neededMaps);
-    assertEquals(1,    info2.neededReduces);
-    assertEquals(0,    info2.mapDeficit);
-    assertEquals(0,    info2.reduceDeficit);
-    assertEquals(2.0,  info2.mapFairShare);
-    assertEquals(2.0,  info2.reduceFairShare);
+    assertEquals(0,    info1.mapSchedulable.getRunningTasks());
+    assertEquals(0,    info1.reduceSchedulable.getRunningTasks());
+    assertEquals(10,   info1.mapSchedulable.getDemand());
+    assertEquals(10,   info1.reduceSchedulable.getDemand());
+    assertEquals(3.0,  info1.mapSchedulable.getFairShare());
+    assertEquals(3.0,  info1.reduceSchedulable.getFairShare());
+    assertEquals(0,    info2.mapSchedulable.getRunningTasks());
+    assertEquals(0,    info2.reduceSchedulable.getRunningTasks());
+    assertEquals(1,    info2.mapSchedulable.getDemand());
+    assertEquals(1,    info2.reduceSchedulable.getDemand());
+    assertEquals(1.0,  info2.mapSchedulable.getFairShare());
+    assertEquals(1.0,  info2.reduceSchedulable.getFairShare());
     
     // Assign tasks and check that slots are first given to needy jobs
     checkAssignment("tt1", "attempt_test_0002_m_000000_0 on tt1");
@@ -1107,24 +1348,24 @@ public class TestFairScheduler extends TestCase {
     JobInfo info4 = scheduler.infos.get(job4);
     
     // Check scheduler variables
-    assertEquals(2.0,  info1.mapFairShare);
-    assertEquals(2.0,  info1.reduceFairShare);
-    assertEquals(2.0,  info2.mapFairShare);
-    assertEquals(2.0,  info2.reduceFairShare);
-    assertEquals(0.0,  info3.mapFairShare);
-    assertEquals(0.0,  info3.reduceFairShare);
-    assertEquals(0.0,  info4.mapFairShare);
-    assertEquals(0.0,  info4.reduceFairShare);
-    
-    // Assign tasks and check that slots are first to jobs 1 and 2
+    assertEquals(2.0,  info1.mapSchedulable.getFairShare());
+    assertEquals(2.0,  info1.reduceSchedulable.getFairShare());
+    assertEquals(2.0,  info2.mapSchedulable.getFairShare());
+    assertEquals(2.0,  info2.reduceSchedulable.getFairShare());
+    assertEquals(0.0,  info3.mapSchedulable.getFairShare());
+    assertEquals(0.0,  info3.reduceSchedulable.getFairShare());
+    assertEquals(0.0,  info4.mapSchedulable.getFairShare());
+    assertEquals(0.0,  info4.reduceSchedulable.getFairShare());
+    
+    // Assign tasks and check that only jobs 1 and 2 get them
     checkAssignment("tt1", "attempt_test_0001_m_000000_0 on tt1");
-    checkAssignment("tt1", "attempt_test_0001_m_000001_0 on tt1");
+    checkAssignment("tt1", "attempt_test_0002_m_000000_0 on tt1");
     checkAssignment("tt1", "attempt_test_0001_r_000000_0 on tt1");
-    checkAssignment("tt1", "attempt_test_0001_r_000001_0 on tt1");
+    checkAssignment("tt1", "attempt_test_0002_r_000000_0 on tt1");
     advanceTime(100);
-    checkAssignment("tt2", "attempt_test_0002_m_000000_0 on tt2");
+    checkAssignment("tt2", "attempt_test_0001_m_000001_0 on tt2");
     checkAssignment("tt2", "attempt_test_0002_m_000001_0 on tt2");
-    checkAssignment("tt2", "attempt_test_0002_r_000000_0 on tt2");
+    checkAssignment("tt2", "attempt_test_0001_r_000001_0 on tt2");
     checkAssignment("tt2", "attempt_test_0002_r_000001_0 on tt2");
   }
 
@@ -1164,25 +1405,25 @@ public class TestFairScheduler extends TestCase {
     JobInfo info4 = scheduler.infos.get(job4);
     
     // Check scheduler variables
-    assertEquals(1.33,  info1.mapFairShare, 0.1);
-    assertEquals(1.33,  info1.reduceFairShare, 0.1);
-    assertEquals(0.0,   info2.mapFairShare);
-    assertEquals(0.0,   info2.reduceFairShare);
-    assertEquals(1.33,  info3.mapFairShare, 0.1);
-    assertEquals(1.33,  info3.reduceFairShare, 0.1);
-    assertEquals(1.33,  info4.mapFairShare, 0.1);
-    assertEquals(1.33,  info4.reduceFairShare, 0.1);
-    
-    // Assign tasks and check that slots are first to jobs 1 and 3
+    assertEquals(1.33,  info1.mapSchedulable.getFairShare(), 0.1);
+    assertEquals(1.33,  info1.reduceSchedulable.getFairShare(), 0.1);
+    assertEquals(0.0,   info2.mapSchedulable.getFairShare());
+    assertEquals(0.0,   info2.reduceSchedulable.getFairShare());
+    assertEquals(1.33,  info3.mapSchedulable.getFairShare(), 0.1);
+    assertEquals(1.33,  info3.reduceSchedulable.getFairShare(), 0.1);
+    assertEquals(1.33,  info4.mapSchedulable.getFairShare(), 0.1);
+    assertEquals(1.33,  info4.reduceSchedulable.getFairShare(), 0.1);
+    
+    // Assign tasks and check that slots are given only to jobs 1, 3 and 4
     checkAssignment("tt1", "attempt_test_0001_m_000000_0 on tt1");
-    checkAssignment("tt1", "attempt_test_0001_m_000001_0 on tt1");
+    checkAssignment("tt1", "attempt_test_0003_m_000000_0 on tt1");
     checkAssignment("tt1", "attempt_test_0001_r_000000_0 on tt1");
-    checkAssignment("tt1", "attempt_test_0001_r_000001_0 on tt1");
+    checkAssignment("tt1", "attempt_test_0003_r_000000_0 on tt1");
     advanceTime(100);
-    checkAssignment("tt2", "attempt_test_0003_m_000000_0 on tt2");
-    checkAssignment("tt2", "attempt_test_0003_m_000001_0 on tt2");
-    checkAssignment("tt2", "attempt_test_0003_r_000000_0 on tt2");
-    checkAssignment("tt2", "attempt_test_0003_r_000001_0 on tt2");
+    checkAssignment("tt2", "attempt_test_0004_m_000000_0 on tt2");
+    checkAssignment("tt2", "attempt_test_0001_m_000001_0 on tt2");
+    checkAssignment("tt2", "attempt_test_0004_r_000000_0 on tt2");
+    checkAssignment("tt2", "attempt_test_0001_r_000001_0 on tt2");
   }
   
   /**
@@ -1265,60 +1506,36 @@ public class TestFairScheduler extends TestCase {
     // the other half. This works out to 2 slots each for the jobs
     // in poolA and 1/3 each for the jobs in the default pool because
     // there are 2 runnable jobs in poolA and 6 jobs in the default pool.
-    assertEquals(0.33,   info1.mapFairShare, 0.1);
-    assertEquals(0.33,   info1.reduceFairShare, 0.1);
-    assertEquals(0.0,    info2.mapFairShare);
-    assertEquals(0.0,    info2.reduceFairShare);
-    assertEquals(0.33,   info3.mapFairShare, 0.1);
-    assertEquals(0.33,   info3.reduceFairShare, 0.1);
-    assertEquals(0.33,   info4.mapFairShare, 0.1);
-    assertEquals(0.33,   info4.reduceFairShare, 0.1);
-    assertEquals(0.33,   info5.mapFairShare, 0.1);
-    assertEquals(0.33,   info5.reduceFairShare, 0.1);
-    assertEquals(0.33,   info6.mapFairShare, 0.1);
-    assertEquals(0.33,   info6.reduceFairShare, 0.1);
-    assertEquals(0.33,   info7.mapFairShare, 0.1);
-    assertEquals(0.33,   info7.reduceFairShare, 0.1);
-    assertEquals(0.0,    info8.mapFairShare);
-    assertEquals(0.0,    info8.reduceFairShare);
-    assertEquals(2.0,    info9.mapFairShare, 0.1);
-    assertEquals(2.0,    info9.reduceFairShare, 0.1);
-    assertEquals(0.0,    info10.mapFairShare);
-    assertEquals(0.0,    info10.reduceFairShare);
+    assertEquals(0.33,   info1.mapSchedulable.getFairShare(), 0.1);
+    assertEquals(0.33,   info1.reduceSchedulable.getFairShare(), 0.1);
+    assertEquals(0.0,    info2.mapSchedulable.getFairShare());
+    assertEquals(0.0,    info2.reduceSchedulable.getFairShare());
+    assertEquals(0.33,   info3.mapSchedulable.getFairShare(), 0.1);
+    assertEquals(0.33,   info3.reduceSchedulable.getFairShare(), 0.1);
+    assertEquals(0.33,   info4.mapSchedulable.getFairShare(), 0.1);
+    assertEquals(0.33,   info4.reduceSchedulable.getFairShare(), 0.1);
+    assertEquals(0.33,   info5.mapSchedulable.getFairShare(), 0.1);
+    assertEquals(0.33,   info5.reduceSchedulable.getFairShare(), 0.1);
+    assertEquals(0.33,   info6.mapSchedulable.getFairShare(), 0.1);
+    assertEquals(0.33,   info6.reduceSchedulable.getFairShare(), 0.1);
+    assertEquals(0.33,   info7.mapSchedulable.getFairShare(), 0.1);
+    assertEquals(0.33,   info7.reduceSchedulable.getFairShare(), 0.1);
+    assertEquals(0.0,    info8.mapSchedulable.getFairShare());
+    assertEquals(0.0,    info8.reduceSchedulable.getFairShare());
+    assertEquals(2.0,    info9.mapSchedulable.getFairShare(), 0.1);
+    assertEquals(2.0,    info9.reduceSchedulable.getFairShare(), 0.1);
+    assertEquals(0.0,    info10.mapSchedulable.getFairShare());
+    assertEquals(0.0,    info10.reduceSchedulable.getFairShare());
   }
   
   public void testSizeBasedWeight() throws Exception {
     scheduler.sizeBasedWeight = true;
     JobInProgress job1 = submitJob(JobStatus.RUNNING, 2, 10);
     JobInProgress job2 = submitJob(JobStatus.RUNNING, 20, 1);
-    assertTrue(scheduler.infos.get(job2).mapFairShare >
-               scheduler.infos.get(job1).mapFairShare);
-    assertTrue(scheduler.infos.get(job1).reduceFairShare >
-               scheduler.infos.get(job2).reduceFairShare);
-  }
-  
-  public void testWaitForMapsBeforeLaunchingReduces() {
-    // We have set waitForMapsBeforeLaunchingReduces to false by default in
-    // this class, so this should return true
-    assertTrue(scheduler.enoughMapsFinishedToRunReduces(0, 100));
-    
-    // However, if we set waitForMapsBeforeLaunchingReduces to true, we should
-    // now no longer be able to assign reduces until 5 have finished
-    scheduler.waitForMapsBeforeLaunchingReduces = true;
-    assertFalse(scheduler.enoughMapsFinishedToRunReduces(0, 100));
-    assertFalse(scheduler.enoughMapsFinishedToRunReduces(1, 100));
-    assertFalse(scheduler.enoughMapsFinishedToRunReduces(2, 100));
-    assertFalse(scheduler.enoughMapsFinishedToRunReduces(3, 100));
-    assertFalse(scheduler.enoughMapsFinishedToRunReduces(4, 100));
-    assertTrue(scheduler.enoughMapsFinishedToRunReduces(5, 100));
-    assertTrue(scheduler.enoughMapsFinishedToRunReduces(6, 100));
-    
-    // Also test some jobs that have very few maps, in which case we will
-    // wait for at least 1 map to finish
-    assertFalse(scheduler.enoughMapsFinishedToRunReduces(0, 5));
-    assertTrue(scheduler.enoughMapsFinishedToRunReduces(1, 5));
-    assertFalse(scheduler.enoughMapsFinishedToRunReduces(0, 1));
-    assertTrue(scheduler.enoughMapsFinishedToRunReduces(1, 1));
+    assertTrue(scheduler.infos.get(job2).mapSchedulable.getFairShare() >
+               scheduler.infos.get(job1).mapSchedulable.getFairShare());
+    assertTrue(scheduler.infos.get(job1).reduceSchedulable.getFairShare() >
+               scheduler.infos.get(job2).reduceSchedulable.getFairShare());
   }
   
 
@@ -1354,25 +1571,25 @@ public class TestFairScheduler extends TestCase {
     JobInfo info3 = scheduler.infos.get(job3);
     advanceTime(10);
     
-    assertEquals(1.14,  info1.mapFairShare, 0.01);
-    assertEquals(1.14,  info1.reduceFairShare, 0.01);
-    assertEquals(2.28,  info2.mapFairShare, 0.01);
-    assertEquals(2.28,  info2.reduceFairShare, 0.01);
-    assertEquals(0.57,  info3.mapFairShare, 0.01);
-    assertEquals(0.57,  info3.reduceFairShare, 0.01);
+    assertEquals(1.14,  info1.mapSchedulable.getFairShare(), 0.01);
+    assertEquals(1.14,  info1.reduceSchedulable.getFairShare(), 0.01);
+    assertEquals(2.28,  info2.mapSchedulable.getFairShare(), 0.01);
+    assertEquals(2.28,  info2.reduceSchedulable.getFairShare(), 0.01);
+    assertEquals(0.57,  info3.mapSchedulable.getFairShare(), 0.01);
+    assertEquals(0.57,  info3.reduceSchedulable.getFairShare(), 0.01);
     
     JobInProgress job4 = submitJob(JobStatus.RUNNING, 10, 10, "poolB");
     JobInfo info4 = scheduler.infos.get(job4);
     advanceTime(10);
     
-    assertEquals(1.14,  info1.mapFairShare, 0.01);
-    assertEquals(1.14,  info1.reduceFairShare, 0.01);
-    assertEquals(2.28,  info2.mapFairShare, 0.01);
-    assertEquals(2.28,  info2.reduceFairShare, 0.01);
-    assertEquals(0.28,  info3.mapFairShare, 0.01);
-    assertEquals(0.28,  info3.reduceFairShare, 0.01);
-    assertEquals(0.28,  info4.mapFairShare, 0.01);
-    assertEquals(0.28,  info4.reduceFairShare, 0.01);
+    assertEquals(1.14,  info1.mapSchedulable.getFairShare(), 0.01);
+    assertEquals(1.14,  info1.reduceSchedulable.getFairShare(), 0.01);
+    assertEquals(2.28,  info2.mapSchedulable.getFairShare(), 0.01);
+    assertEquals(2.28,  info2.reduceSchedulable.getFairShare(), 0.01);
+    assertEquals(0.28,  info3.mapSchedulable.getFairShare(), 0.01);
+    assertEquals(0.28,  info3.reduceSchedulable.getFairShare(), 0.01);
+    assertEquals(0.28,  info4.mapSchedulable.getFairShare(), 0.01);
+    assertEquals(0.28,  info4.reduceSchedulable.getFairShare(), 0.01);
   }
 
   /**
@@ -1405,19 +1622,21 @@ public class TestFairScheduler extends TestCase {
     JobInfo info3 = scheduler.infos.get(job3);
     advanceTime(10);
     
+    /*
     assertEquals(0,     info1.mapWeight, 0.01);
     assertEquals(1.0,   info1.reduceWeight, 0.01);
     assertEquals(0,     info2.mapWeight, 0.01);
     assertEquals(1.0,   info2.reduceWeight, 0.01);
     assertEquals(1.0,   info3.mapWeight, 0.01);
     assertEquals(1.0,   info3.reduceWeight, 0.01);
-    
-    assertEquals(0,     info1.mapFairShare, 0.01);
-    assertEquals(1.33,  info1.reduceFairShare, 0.01);
-    assertEquals(0,     info2.mapFairShare, 0.01);
-    assertEquals(1.33,  info2.reduceFairShare, 0.01);
-    assertEquals(4,     info3.mapFairShare, 0.01);
-    assertEquals(1.33,  info3.reduceFairShare, 0.01);
+    */
+    
+    assertEquals(0,     info1.mapSchedulable.getFairShare(), 0.01);
+    assertEquals(1.33,  info1.reduceSchedulable.getFairShare(), 0.01);
+    assertEquals(0,     info2.mapSchedulable.getFairShare(), 0.01);
+    assertEquals(1.33,  info2.reduceSchedulable.getFairShare(), 0.01);
+    assertEquals(4,     info3.mapSchedulable.getFairShare(), 0.01);
+    assertEquals(1.33,  info3.reduceSchedulable.getFairShare(), 0.01);
   }
 
   /**
@@ -1442,7 +1661,7 @@ public class TestFairScheduler extends TestCase {
    * This test starts by launching a job in the default pool that takes
    * all the slots in the cluster. We then submit a job in a pool with
    * min share of 2 maps and 1 reduce task. After the min share preemption
-   * timeout, this job should be allowed to preempt tasks. 
+   * timeout, this pool should be allowed to preempt tasks. 
    */
   public void testMinSharePreemption() throws Exception {
     // Enable preemption in scheduler
@@ -1461,6 +1680,7 @@ public class TestFairScheduler extends TestCase {
     out.println("</allocations>");
     out.close();
     scheduler.getPoolManager().reloadAllocs();
+    Pool poolA = scheduler.getPoolManager().getPool("poolA");
 
     // Submit job 1 and assign all slots to it. Sleep a bit before assigning
     // tasks on tt1 and tt2 to ensure that the ones on tt2 get preempted first.
@@ -1481,32 +1701,32 @@ public class TestFairScheduler extends TestCase {
     
     // Ten seconds later, check that job 2 is not able to preempt tasks.
     advanceTime(10000);
-    assertEquals(0, scheduler.tasksToPreempt(job2, TaskType.MAP,
+    assertEquals(0, scheduler.tasksToPreempt(poolA.getMapSchedulable(),
         clock.getTime()));
-    assertEquals(0, scheduler.tasksToPreempt(job2, TaskType.REDUCE,
+    assertEquals(0, scheduler.tasksToPreempt(poolA.getReduceSchedulable(),
         clock.getTime()));
     
     // Advance time by 49 more seconds, putting us at 59s after the
     // submission of job 2. It should still not be able to preempt.
     advanceTime(49000);
-    assertEquals(0, scheduler.tasksToPreempt(job2, TaskType.MAP,
+    assertEquals(0, scheduler.tasksToPreempt(poolA.getMapSchedulable(),
         clock.getTime()));
-    assertEquals(0, scheduler.tasksToPreempt(job2, TaskType.REDUCE,
+    assertEquals(0, scheduler.tasksToPreempt(poolA.getReduceSchedulable(),
         clock.getTime()));
     
     // Advance time by 2 seconds, putting us at 61s after the submission
     // of job 2. It should now be able to preempt 2 maps and 1 reduce.
     advanceTime(2000);
-    assertEquals(2, scheduler.tasksToPreempt(job2, TaskType.MAP,
+    assertEquals(2, scheduler.tasksToPreempt(poolA.getMapSchedulable(),
         clock.getTime()));
-    assertEquals(1, scheduler.tasksToPreempt(job2, TaskType.REDUCE,
+    assertEquals(1, scheduler.tasksToPreempt(poolA.getReduceSchedulable(),
         clock.getTime()));
-
+    
     // Test that the tasks actually get preempted and we can assign new ones
     scheduler.preemptTasksIfNecessary();
     scheduler.update();
-    assertEquals(2, scheduler.runningTasks(job1, TaskType.MAP));
-    assertEquals(3, scheduler.runningTasks(job1, TaskType.REDUCE));
+    assertEquals(2, job1.runningMaps());
+    assertEquals(3, job1.runningReduces());
     checkAssignment("tt2", "attempt_test_0002_m_000000_0 on tt2");
     checkAssignment("tt2", "attempt_test_0002_m_000001_0 on tt2");
     checkAssignment("tt2", "attempt_test_0002_r_000000_0 on tt2");
@@ -1518,7 +1738,7 @@ public class TestFairScheduler extends TestCase {
    * This test starts by launching a job in the default pool that takes
    * all the slots in the cluster. We then submit a job in a pool with
    * min share of 3 maps and 3 reduce tasks, but which only actually
-   * needs 1 map and 2 reduces. We check that this job does not prempt
+   * needs 1 map and 2 reduces. We check that this pool does not prempt
    * more than this many tasks despite its min share being higher. 
    */
   public void testMinSharePreemptionWithSmallJob() throws Exception {
@@ -1538,6 +1758,7 @@ public class TestFairScheduler extends TestCase {
     out.println("</allocations>");
     out.close();
     scheduler.getPoolManager().reloadAllocs();
+    Pool poolA = scheduler.getPoolManager().getPool("poolA");
 
     // Submit job 1 and assign all slots to it. Sleep a bit before assigning
     // tasks on tt1 and tt2 to ensure that the ones on tt2 get preempted first.
@@ -1558,24 +1779,24 @@ public class TestFairScheduler extends TestCase {
     
     // Advance time by 59 seconds and check that no preemption occurs.
     advanceTime(59000);
-    assertEquals(0, scheduler.tasksToPreempt(job2, TaskType.MAP,
+    assertEquals(0, scheduler.tasksToPreempt(poolA.getMapSchedulable(),
         clock.getTime()));
-    assertEquals(0, scheduler.tasksToPreempt(job2, TaskType.REDUCE,
+    assertEquals(0, scheduler.tasksToPreempt(poolA.getReduceSchedulable(),
         clock.getTime()));
     
     // Advance time by 2 seconds, putting us at 61s after the submission
     // of job 2. Job 2 should now preempt 1 map and 2 reduces.
     advanceTime(2000);
-    assertEquals(1, scheduler.tasksToPreempt(job2, TaskType.MAP,
+    assertEquals(1, scheduler.tasksToPreempt(poolA.getMapSchedulable(),
         clock.getTime()));
-    assertEquals(2, scheduler.tasksToPreempt(job2, TaskType.REDUCE,
+    assertEquals(2, scheduler.tasksToPreempt(poolA.getReduceSchedulable(),
         clock.getTime()));
 
     // Test that the tasks actually get preempted and we can assign new ones
     scheduler.preemptTasksIfNecessary();
     scheduler.update();
-    assertEquals(3, scheduler.runningTasks(job1, TaskType.MAP));
-    assertEquals(2, scheduler.runningTasks(job1, TaskType.REDUCE));
+    assertEquals(3, job1.runningMaps());
+    assertEquals(2, job1.runningReduces());
     checkAssignment("tt2", "attempt_test_0002_m_000000_0 on tt2");
     checkAssignment("tt2", "attempt_test_0002_r_000000_0 on tt2");
     checkAssignment("tt2", "attempt_test_0002_r_000001_0 on tt2");
@@ -1584,21 +1805,21 @@ public class TestFairScheduler extends TestCase {
   }
 
   /**
-   * This test runs on a 4-node (8-slot) cluster to allow 3 jobs with fair
+   * This test runs on a 4-node (8-slot) cluster to allow 3 pools with fair
    * shares greater than 2 slots to coexist (which makes the half-fair-share 
-   * of each job more than 1 so that fair share preemption can kick in). 
+   * of each pool more than 1 so that fair share preemption can kick in). 
    * 
-   * The test first launches job 1, which takes 6 map slots and 6 reduce slots. 
-   * We then submit job 2, which takes 2 slots of each type. Finally, we submit 
-   * a third job, job 3, which gets no slots. At this point the fair share
-   * of each job will be 8/3 ~= 2.7 slots. Job 1 will be above its fair share,
-   * job 2 will be below it but at half fair share, and job 3 will
-   * be below half fair share. Therefore job 3 should be allowed to
-   * preempt a task (after a timeout) but jobs 1 and 2 shouldn't. 
+   * The test first starts job 1, which takes 6 map slots and 6 reduce slots,
+   * in pool 1.  We then submit job 2 in pool 2, which takes 2 slots of each
+   * type. Finally, we submit a third job, job 3 in pool3, which gets no slots. 
+   * At this point the fair share of each pool will be 8/3 ~= 2.7 slots. 
+   * Pool 1 will be above its fair share, pool 2 will be below it but at half
+   * fair share, and pool 3 will be below half fair share. Therefore pool 3 
+   * should preempt a task (after a timeout) but pools 1 and 2 shouldn't. 
    */
   public void testFairSharePreemption() throws Exception {
     // Create a bigger cluster than normal (4 tasktrackers instead of 2)
-    setUpCluster(4);
+    setUpCluster(1, 4, false);
     // Enable preemption in scheduler
     scheduler.preemptionEnabled = true;
     // Set up pools file with a fair share preemtion timeout of 1 minute
@@ -1609,14 +1830,18 @@ public class TestFairScheduler extends TestCase {
     out.println("</allocations>");
     out.close();
     scheduler.getPoolManager().reloadAllocs();
+    
+    // Grab pools (they'll be created even though they're not in the alloc file)
+    Pool pool1 = scheduler.getPoolManager().getPool("pool1");
+    Pool pool2 = scheduler.getPoolManager().getPool("pool2");
+    Pool pool3 = scheduler.getPoolManager().getPool("pool3");
 
-    // Submit jobs 1 and 2. We advance time by 100 between each task tracker
+    // Submit job 1. We advance time by 100 between each task tracker
     // assignment stage to ensure that the tasks from job1 on tt3 are the ones
     // that are deterministically preempted first (being the latest launched
     // tasks in an over-allocated job).
-    JobInProgress job1 = submitJob(JobStatus.RUNNING, 6, 6);
-    advanceTime(100); // Makes job 1 deterministically launch before job 2
-    JobInProgress job2 = submitJob(JobStatus.RUNNING, 10, 10);
+    JobInProgress job1 = submitJob(JobStatus.RUNNING, 6, 6, "pool1");
+    advanceTime(100);
     checkAssignment("tt1", "attempt_test_0001_m_000000_0 on tt1");
     checkAssignment("tt1", "attempt_test_0001_m_000001_0 on tt1");
     checkAssignment("tt1", "attempt_test_0001_r_000000_0 on tt1");
@@ -1632,42 +1857,47 @@ public class TestFairScheduler extends TestCase {
     checkAssignment("tt3", "attempt_test_0001_r_000004_0 on tt3");
     checkAssignment("tt3", "attempt_test_0001_r_000005_0 on tt3");
     advanceTime(100);
+    
+    // Submit job 2. It should get the last 2 slots.
+    JobInProgress job2 = submitJob(JobStatus.RUNNING, 10, 10, "pool2");
+    advanceTime(100);
     checkAssignment("tt4", "attempt_test_0002_m_000000_0 on tt4");
     checkAssignment("tt4", "attempt_test_0002_m_000001_0 on tt4");
     checkAssignment("tt4", "attempt_test_0002_r_000000_0 on tt4");
     checkAssignment("tt4", "attempt_test_0002_r_000001_0 on tt4");
     
     // Submit job 3.
-    JobInProgress job3 = submitJob(JobStatus.RUNNING, 10, 10);
+    JobInProgress job3 = submitJob(JobStatus.RUNNING, 10, 10, "pool3");
     
-    // Check that after 59 seconds, neither job can preempt
+    // Check that after 59 seconds, neither pool can preempt
     advanceTime(59000);
-    assertEquals(0, scheduler.tasksToPreempt(job2, TaskType.MAP,
+    assertEquals(0, scheduler.tasksToPreempt(pool2.getMapSchedulable(),
         clock.getTime()));
-    assertEquals(0, scheduler.tasksToPreempt(job2, TaskType.REDUCE,
+    assertEquals(0, scheduler.tasksToPreempt(pool2.getReduceSchedulable(),
         clock.getTime()));
-    assertEquals(0, scheduler.tasksToPreempt(job3, TaskType.MAP,
+    assertEquals(0, scheduler.tasksToPreempt(pool3.getMapSchedulable(),
         clock.getTime()));
-    assertEquals(0, scheduler.tasksToPreempt(job3, TaskType.REDUCE,
+    assertEquals(0, scheduler.tasksToPreempt(pool3.getReduceSchedulable(),
         clock.getTime()));
     
     // Wait 2 more seconds, so that job 3 has now been in the system for 61s.
-    // Now job 3 should be able to preempt 1 task but job 2 shouldn't.
+    // Now pool 3 should be able to preempt 2 tasks (its share of 2.7 rounded
+    // down to its floor), but pool 2 shouldn't.
     advanceTime(2000);
-    assertEquals(0, scheduler.tasksToPreempt(job2, TaskType.MAP,
+    assertEquals(0, scheduler.tasksToPreempt(pool2.getMapSchedulable(),
         clock.getTime()));
-    assertEquals(0, scheduler.tasksToPreempt(job2, TaskType.REDUCE,
+    assertEquals(0, scheduler.tasksToPreempt(pool2.getReduceSchedulable(),
         clock.getTime()));
-    assertEquals(2, scheduler.tasksToPreempt(job3, TaskType.MAP,
+    assertEquals(2, scheduler.tasksToPreempt(pool3.getMapSchedulable(),
         clock.getTime()));
-    assertEquals(2, scheduler.tasksToPreempt(job3, TaskType.REDUCE,
+    assertEquals(2, scheduler.tasksToPreempt(pool3.getReduceSchedulable(),
         clock.getTime()));
     
     // Test that the tasks actually get preempted and we can assign new ones
     scheduler.preemptTasksIfNecessary();
     scheduler.update();
-    assertEquals(4, scheduler.runningTasks(job1, TaskType.MAP));
-    assertEquals(4, scheduler.runningTasks(job1, TaskType.REDUCE));
+    assertEquals(4, job1.runningMaps());
+    assertEquals(4, job1.runningReduces());
     checkAssignment("tt3", "attempt_test_0003_m_000000_0 on tt3");
     checkAssignment("tt3", "attempt_test_0003_m_000001_0 on tt3");
     checkAssignment("tt3", "attempt_test_0003_r_000000_0 on tt3");
@@ -1679,9 +1909,9 @@ public class TestFairScheduler extends TestCase {
   }
   
   /**
-   * This test submits a job that takes all 4 slots, and then a second
-   * job that has both a min share of 2 slots with a 60s timeout and a
-   * fair share timeout of 60s. After 60 seconds, this job will be starved
+   * This test submits a job that takes all 4 slots, and then a second job in
+   * a pool that has both a min share of 2 slots with a 60s timeout and a
+   * fair share timeout of 60s. After 60 seconds, this pool will be starved
    * of both min share (2 slots of each type) and fair share (2 slots of each
    * type), and we test that it does not kill more than 2 tasks of each type
    * in total.
@@ -1704,6 +1934,7 @@ public class TestFairScheduler extends TestCase {
     out.println("</allocations>");
     out.close();
     scheduler.getPoolManager().reloadAllocs();
+    Pool poolA = scheduler.getPoolManager().getPool("poolA");
 
     // Submit job 1 and assign all slots to it. Sleep a bit before assigning
     // tasks on tt1 and tt2 to ensure that the ones on tt2 get preempted first.
@@ -1724,32 +1955,32 @@ public class TestFairScheduler extends TestCase {
     
     // Ten seconds later, check that job 2 is not able to preempt tasks.
     advanceTime(10000);
-    assertEquals(0, scheduler.tasksToPreempt(job2, TaskType.MAP,
+    assertEquals(0, scheduler.tasksToPreempt(poolA.getMapSchedulable(),
         clock.getTime()));
-    assertEquals(0, scheduler.tasksToPreempt(job2, TaskType.REDUCE,
+    assertEquals(0, scheduler.tasksToPreempt(poolA.getReduceSchedulable(),
         clock.getTime()));
     
     // Advance time by 49 more seconds, putting us at 59s after the
     // submission of job 2. It should still not be able to preempt.
     advanceTime(49000);
-    assertEquals(0, scheduler.tasksToPreempt(job2, TaskType.MAP,
+    assertEquals(0, scheduler.tasksToPreempt(poolA.getMapSchedulable(),
         clock.getTime()));
-    assertEquals(0, scheduler.tasksToPreempt(job2, TaskType.REDUCE,
+    assertEquals(0, scheduler.tasksToPreempt(poolA.getReduceSchedulable(),
         clock.getTime()));
     
     // Advance time by 2 seconds, putting us at 61s after the submission
     // of job 2. It should now be able to preempt 2 maps and 1 reduce.
     advanceTime(2000);
-    assertEquals(2, scheduler.tasksToPreempt(job2, TaskType.MAP,
+    assertEquals(2, scheduler.tasksToPreempt(poolA.getMapSchedulable(),
         clock.getTime()));
-    assertEquals(2, scheduler.tasksToPreempt(job2, TaskType.REDUCE,
+    assertEquals(2, scheduler.tasksToPreempt(poolA.getReduceSchedulable(),
         clock.getTime()));
 
     // Test that the tasks actually get preempted and we can assign new ones
     scheduler.preemptTasksIfNecessary();
     scheduler.update();
-    assertEquals(2, scheduler.runningTasks(job1, TaskType.MAP));
-    assertEquals(2, scheduler.runningTasks(job1, TaskType.REDUCE));
+    assertEquals(2, job1.runningMaps());
+    assertEquals(2, job1.runningReduces());
     checkAssignment("tt2", "attempt_test_0002_m_000000_0 on tt2");
     checkAssignment("tt2", "attempt_test_0002_m_000001_0 on tt2");
     checkAssignment("tt2", "attempt_test_0002_r_000000_0 on tt2");
@@ -1801,8 +2032,8 @@ public class TestFairScheduler extends TestCase {
     advanceTime(61000);
     scheduler.preemptTasksIfNecessary();
     scheduler.update();
-    assertEquals(4, scheduler.runningTasks(job1, TaskType.MAP));
-    assertEquals(4, scheduler.runningTasks(job1, TaskType.REDUCE));
+    assertEquals(4, job1.runningMaps());
+    assertEquals(4, job1.runningReduces());
     assertNull(scheduler.assignTasks(tracker("tt1")));
     assertNull(scheduler.assignTasks(tracker("tt2")));
   }
@@ -1855,12 +2086,392 @@ public class TestFairScheduler extends TestCase {
     advanceTime(61000);
     scheduler.preemptTasksIfNecessary();
     scheduler.update();
-    assertEquals(4, scheduler.runningTasks(job1, TaskType.MAP));
-    assertEquals(4, scheduler.runningTasks(job1, TaskType.REDUCE));
+    assertEquals(4, job1.runningMaps());
+    assertEquals(4, job1.runningReduces());
     assertNull(scheduler.assignTasks(tracker("tt1")));
     assertNull(scheduler.assignTasks(tracker("tt2")));
   }
 
+  /**
+   * This test exercises delay scheduling at the node level. We submit a job
+   * with data on rack1.node2 and check that it doesn't get assigned on earlier
+   * nodes. A second job with no locality info should get assigned instead.
+   * 
+   * TaskTracker names in this test map to nodes as follows:
+   * - tt1 = rack1.node1
+   * - tt2 = rack1.node2
+   * - tt3 = rack2.node1
+   * - tt4 = rack2.node2
+   */
+  public void testDelaySchedulingAtNodeLevel() throws IOException {
+    setUpCluster(2, 2, true);
+    scheduler.assignMultiple = true;
+    
+    JobInProgress job1 = submitJob(JobStatus.RUNNING, 1, 0, "pool1",
+        new String[][] {
+          {"rack2.node2"}
+        });
+    JobInfo info1 = scheduler.infos.get(job1);
+    
+    // Advance time before submitting another job j2, to make j1 be ahead
+    // of j2 in the queue deterministically.
+    advanceTime(100);
+    JobInProgress job2 = submitJob(JobStatus.RUNNING, 10, 0);
+    
+    // Assign tasks on nodes 1-3 and check that j2 gets them
+    checkAssignment("tt1", "attempt_test_0002_m_000000_0 on tt1", 
+                           "attempt_test_0002_m_000001_0 on tt1");
+    checkAssignment("tt2", "attempt_test_0002_m_000002_0 on tt2",
+                           "attempt_test_0002_m_000003_0 on tt2");
+    checkAssignment("tt3", "attempt_test_0002_m_000004_0 on tt3",
+                           "attempt_test_0002_m_000005_0 on tt3");
+    
+    // Assign a task on node 4 now and check that j1 gets it. The other slot
+    // on the node should be given to j2 because j1 will be out of tasks.
+    checkAssignment("tt4", "attempt_test_0001_m_000000_0 on tt4",
+                           "attempt_test_0002_m_000006_0 on tt4");
+    
+    // Check that delay scheduling info is properly set
+    assertEquals(info1.lastMapLocalityLevel, LocalityLevel.NODE);
+    assertEquals(info1.timeWaitedForLocalMap, 0);
+    assertEquals(info1.skippedAtLastHeartbeat, false);
+  }
+
+  /**
+   * This test submits a job and causes it to exceed its node-level delay,
+   * and thus to go on to launch a rack-local task. We submit one job with data
+   * on rack2.node4 and check that it does not get assigned on any of the other
+   * nodes until 10 seconds (the delay configured in setUpCluster) pass.
+   * Finally, after some delay, we let the job assign local tasks and check
+   * that it has returned to waiting for node locality.
+   * 
+   * TaskTracker names in this test map to nodes as follows:
+   * - tt1 = rack1.node1
+   * - tt2 = rack1.node2
+   * - tt3 = rack2.node1
+   * - tt4 = rack2.node2
+   */
+  public void testDelaySchedulingAtRackLevel() throws IOException {
+    setUpCluster(2, 2, true);
+    scheduler.assignMultiple = true;
+    
+    JobInProgress job1 = submitJob(JobStatus.RUNNING, 4, 0, "pool1",
+        new String[][] {
+          {"rack2.node2"}, {"rack2.node2"}, {"rack2.node2"}, {"rack2.node2"}
+        });
+    JobInfo info1 = scheduler.infos.get(job1);
+    
+    // Advance time before submitting another job j2, to make j1 be ahead
+    // of j2 in the queue deterministically.
+    advanceTime(100);
+    JobInProgress job2 = submitJob(JobStatus.RUNNING, 20, 0);
+    
+    // Assign tasks on nodes 1-3 and check that j2 gets them
+    checkAssignment("tt1", "attempt_test_0002_m_000000_0 on tt1", 
+                           "attempt_test_0002_m_000001_0 on tt1");
+    checkAssignment("tt2", "attempt_test_0002_m_000002_0 on tt2",
+                           "attempt_test_0002_m_000003_0 on tt2");
+    checkAssignment("tt3", "attempt_test_0002_m_000004_0 on tt3",
+                           "attempt_test_0002_m_000005_0 on tt3");
+    
+    // Advance time by 11 seconds to put us past the 10-second locality delay
+    advanceTime(11000);
+    
+    // Finish some tasks on each node
+    taskTrackerManager.finishTask("tt1", "attempt_test_0002_m_000000_0");
+    taskTrackerManager.finishTask("tt2", "attempt_test_0002_m_000002_0");
+    taskTrackerManager.finishTask("tt3", "attempt_test_0002_m_000004_0");
+    advanceTime(100);
+    
+    // Check that job 1 is only assigned on node 3 (which is rack-local)
+    checkAssignment("tt1", "attempt_test_0002_m_000006_0 on tt1");
+    checkAssignment("tt2", "attempt_test_0002_m_000007_0 on tt2");
+    checkAssignment("tt3", "attempt_test_0001_m_000000_0 on tt3");
+    
+    // Check that delay scheduling info is properly set
+    assertEquals(info1.lastMapLocalityLevel, LocalityLevel.RACK);
+    assertEquals(info1.timeWaitedForLocalMap, 0);
+    assertEquals(info1.skippedAtLastHeartbeat, false);
+    
+    // Also give job 1 some tasks on node 4. Its lastMapLocalityLevel
+    // should go back to 0 after it gets assigned these.
+    checkAssignment("tt4", "attempt_test_0001_m_000001_0 on tt4",
+                           "attempt_test_0001_m_000002_0 on tt4");
+    
+    // Check that delay scheduling info is properly set
+    assertEquals(info1.lastMapLocalityLevel, LocalityLevel.NODE);
+    assertEquals(info1.timeWaitedForLocalMap, 0);
+    assertEquals(info1.skippedAtLastHeartbeat, false);
+    
+    // Check that job 1 no longer assigns tasks in the same rack now
+    // that it has obtained a node-local task
+    taskTrackerManager.finishTask("tt1", "attempt_test_0002_m_000001_0");
+    taskTrackerManager.finishTask("tt2", "attempt_test_0002_m_000003_0");
+    taskTrackerManager.finishTask("tt3", "attempt_test_0002_m_000005_0");
+    advanceTime(100);
+    checkAssignment("tt1", "attempt_test_0002_m_000008_0 on tt1");
+    checkAssignment("tt2", "attempt_test_0002_m_000009_0 on tt2");
+    checkAssignment("tt3", "attempt_test_0002_m_000010_0 on tt3");
+    advanceTime(100);
+    
+    // However, job 1 should still be able to launch tasks on node 4
+    taskTrackerManager.finishTask("tt4", "attempt_test_0001_m_000001_0");
+    advanceTime(100);
+    checkAssignment("tt4", "attempt_test_0001_m_000003_0 on tt4");
+  }
+  
+  /**
+   * This test submits a job and causes it to exceed its node-level delay,
+   * then its rack-level delay. It should then launch tasks off-rack.
+   * However, once the job gets a rack-local slot it should stay in-rack,
+   * and once it gets a node-local slot it should stay in-node.
+   * For simplicity, we don't submit a second job in this test.
+   * 
+   * TaskTracker names in this test map to nodes as follows:
+   * - tt1 = rack1.node1
+   * - tt2 = rack1.node2
+   * - tt3 = rack2.node1
+   * - tt4 = rack2.node2
+   */
+  public void testDelaySchedulingOffRack() throws IOException {
+    setUpCluster(2, 2, true);
+    scheduler.assignMultiple = true;
+    
+    JobInProgress job1 = submitJob(JobStatus.RUNNING, 8, 0, "pool1",
+        new String[][] {
+          {"rack2.node2"}, {"rack2.node2"}, {"rack2.node2"}, {"rack2.node2"},
+          {"rack2.node2"}, {"rack2.node2"}, {"rack2.node2"}, {"rack2.node2"},
+        });
+    JobInfo info1 = scheduler.infos.get(job1);
+    advanceTime(100);
+    
+    // Check that nothing is assigned on trackers 1-3
+    assertNull(scheduler.assignTasks(tracker("tt1")));
+    assertNull(scheduler.assignTasks(tracker("tt2")));
+    assertNull(scheduler.assignTasks(tracker("tt3")));
+    
+    // Advance time by 11 seconds to put us past the 10-sec node locality delay
+    advanceTime(11000);
+
+    // Check that nothing is assigned on trackers 1-2; the job would assign
+    // a task on tracker 3 (rack1.node2) so we skip that one 
+    assertNull(scheduler.assignTasks(tracker("tt1")));
+    assertNull(scheduler.assignTasks(tracker("tt2")));
+    
+    // Repeat to see that receiving multiple heartbeats works
+    advanceTime(100);
+    assertNull(scheduler.assignTasks(tracker("tt1")));
+    assertNull(scheduler.assignTasks(tracker("tt2")));
+    advanceTime(100);
+    assertNull(scheduler.assignTasks(tracker("tt1")));
+    assertNull(scheduler.assignTasks(tracker("tt2")));
+
+    // Check that delay scheduling info is properly set
+    assertEquals(info1.lastMapLocalityLevel, LocalityLevel.NODE);
+    assertEquals(info1.timeWaitedForLocalMap, 11200);
+    assertEquals(info1.skippedAtLastHeartbeat, true);
+    
+    // Advance time by 11 seconds to put us past the 10-sec rack locality delay
+    advanceTime(11000);
+    
+    // Now the job should be able to assign tasks on tt1 and tt2
+    checkAssignment("tt1", "attempt_test_0001_m_000000_0 on tt1",
+                           "attempt_test_0001_m_000001_0 on tt1");
+    checkAssignment("tt2", "attempt_test_0001_m_000002_0 on tt2",
+                           "attempt_test_0001_m_000003_0 on tt2");
+
+    // Check that delay scheduling info is properly set
+    assertEquals(info1.lastMapLocalityLevel, LocalityLevel.ANY);
+    assertEquals(info1.timeWaitedForLocalMap, 0);
+    assertEquals(info1.skippedAtLastHeartbeat, false);
+    
+    // Now assign a task on tt3. This should make the job stop assigning
+    // on tt1 and tt2 (checked after we finish some tasks there)
+    checkAssignment("tt3", "attempt_test_0001_m_000004_0 on tt3",
+                           "attempt_test_0001_m_000005_0 on tt3");
+
+    // Check that delay scheduling info is properly set
+    assertEquals(info1.lastMapLocalityLevel, LocalityLevel.RACK);
+    assertEquals(info1.timeWaitedForLocalMap, 0);
+    assertEquals(info1.skippedAtLastHeartbeat, false);
+    
+    // Check that j1 no longer assigns tasks on rack 1 now
+    taskTrackerManager.finishTask("tt1", "attempt_test_0001_m_000001_0");
+    taskTrackerManager.finishTask("tt2", "attempt_test_0001_m_000003_0");
+    advanceTime(100);
+    assertNull(scheduler.assignTasks(tracker("tt1")));
+    assertNull(scheduler.assignTasks(tracker("tt2")));
+    
+    // However, tasks on rack 2 should still be assigned
+    taskTrackerManager.finishTask("tt3", "attempt_test_0001_m_000004_0");
+    advanceTime(100);
+    checkAssignment("tt3", "attempt_test_0001_m_000006_0 on tt3");
+    
+    // Now assign a task on node 4
+    checkAssignment("tt4", "attempt_test_0001_m_000007_0 on tt4");
+
+    // Check that delay scheduling info is set so we are looking for node-local
+    // tasks at this point
+    assertEquals(info1.lastMapLocalityLevel, LocalityLevel.NODE);
+    assertEquals(info1.timeWaitedForLocalMap, 0);
+    assertEquals(info1.skippedAtLastHeartbeat, false);
+  }
+  
+  /**
+   * This test submits two jobs with 4 maps and 3 reduces in total to a
+   * 4-node cluster. Although the cluster has 2 map slots and 2 reduce
+   * slots per node, it should only launch one map and one reduce on each
+   * node to balance the load. We check that this happens even if
+   * assignMultiple is set to true so the scheduler has the opportunity
+   * to launch multiple tasks per heartbeat.
+   */
+  public void testAssignMultipleWithUnderloadedCluster() throws IOException {
+    setUpCluster(1, 4, true);
+    JobInProgress job1 = submitJob(JobStatus.RUNNING, 2, 2);
+    
+    // Advance to make j1 be scheduled before j2 deterministically.
+    advanceTime(100);
+    JobInProgress job2 = submitJob(JobStatus.RUNNING, 2, 1);
+    
+    // Assign tasks and check that at most one map and one reduce slot is used
+    // on each node, and that no tasks are assigned on subsequent heartbeats
+    checkAssignment("tt1", "attempt_test_0001_m_000000_0 on tt1",
+                           "attempt_test_0001_r_000000_0 on tt1");
+    assertNull(scheduler.assignTasks(tracker("tt1")));
+    checkAssignment("tt2", "attempt_test_0002_m_000000_0 on tt2",
+                           "attempt_test_0002_r_000000_0 on tt2");
+    assertNull(scheduler.assignTasks(tracker("tt2")));
+    checkAssignment("tt3", "attempt_test_0001_m_000001_0 on tt3",
+                           "attempt_test_0001_r_000001_0 on tt3");
+    assertNull(scheduler.assignTasks(tracker("tt3")));
+    checkAssignment("tt4", "attempt_test_0002_m_000001_0 on tt4");
+    assertNull(scheduler.assignTasks(tracker("tt4")));
+  }
+  
+  /**
+   * This test submits four jobs in the default pool, which is set to FIFO mode:
+   * - job1, with 1 map and 1 reduce
+   * - job2, with 3 maps and 3 reduces
+   * - job3, with 1 map, 1 reduce, and priority set to HIGH
+   * - job4, with 3 maps and 3 reduces
+   * 
+   * We check that the scheduler assigns tasks first to job3 (because it is
+   * high priority), then to job1, then to job2.
+   */
+  public void testFifoPool() throws Exception {
+    // Set up pools file
+    PrintWriter out = new PrintWriter(new FileWriter(ALLOC_FILE));
+    out.println("<?xml version=\"1.0\"?>");
+    out.println("<allocations>");
+    out.println("<pool name=\"default\">");
+    out.println("<schedulingMode>fifo</schedulingMode>");
+    out.println("</pool>");
+    out.println("</allocations>");
+    out.close();
+    scheduler.getPoolManager().reloadAllocs();
+    
+    // Submit jobs, advancing time in-between to make sure that they are
+    // all submitted at distinct times.
+    JobInProgress job1 = submitJob(JobStatus.RUNNING, 1, 1);
+    advanceTime(10);
+    JobInProgress job2 = submitJob(JobStatus.RUNNING, 3, 3);
+    advanceTime(10);
+    JobInProgress job3 = submitJob(JobStatus.RUNNING, 1, 1);
+    job3.setPriority(JobPriority.HIGH);
+    advanceTime(10);
+    JobInProgress job4 = submitJob(JobStatus.RUNNING, 3, 3);
+    
+    // Assign tasks and check that they're given first to job3 (because it is
+    // high priority), then to job1, then to job2.
+    checkAssignment("tt1", "attempt_test_0003_m_000000_0 on tt1");
+    checkAssignment("tt1", "attempt_test_0001_m_000000_0 on tt1");
+    checkAssignment("tt1", "attempt_test_0003_r_000000_0 on tt1");
+    checkAssignment("tt1", "attempt_test_0001_r_000000_0 on tt1");
+    checkAssignment("tt2", "attempt_test_0002_m_000000_0 on tt2");
+    checkAssignment("tt2", "attempt_test_0002_m_000001_0 on tt2");
+    checkAssignment("tt2", "attempt_test_0002_r_000000_0 on tt2");
+    checkAssignment("tt2", "attempt_test_0002_r_000001_0 on tt2");
+  }
+  
+  /**
+   * This test submits 2 large jobs each to 2 pools, which are both set to FIFO
+   * mode through the global defaultPoolSchedulingMode setting. We check that
+   * the scheduler assigns tasks only to the first job within each pool, but
+   * alternates between the pools to give each pool a fair share.
+   */
+  public void testMultipleFifoPools() throws Exception {
+    // Set up pools file
+    PrintWriter out = new PrintWriter(new FileWriter(ALLOC_FILE));
+    out.println("<?xml version=\"1.0\"?>");
+    out.println("<allocations>");
+    out.println("<defaultPoolSchedulingMode>fifo</defaultPoolSchedulingMode>");
+    out.println("</allocations>");
+    out.close();
+    scheduler.getPoolManager().reloadAllocs();
+    
+    // Submit jobs, advancing time in-between to make sure that they are
+    // all submitted at distinct times.
+    JobInProgress job1 = submitJob(JobStatus.RUNNING, 10, 10, "poolA");
+    advanceTime(10);
+    JobInProgress job2 = submitJob(JobStatus.RUNNING, 10, 10, "poolA");
+    advanceTime(10);
+    JobInProgress job3 = submitJob(JobStatus.RUNNING, 10, 10, "poolB");
+    advanceTime(10);
+    JobInProgress job4 = submitJob(JobStatus.RUNNING, 10, 10, "poolB");
+    
+    // Assign tasks and check that they alternate between jobs 1 and 3, the
+    // head-of-line jobs in their respective pools.
+    checkAssignment("tt1", "attempt_test_0001_m_000000_0 on tt1");
+    checkAssignment("tt1", "attempt_test_0003_m_000000_0 on tt1");
+    checkAssignment("tt1", "attempt_test_0001_r_000000_0 on tt1");
+    checkAssignment("tt1", "attempt_test_0003_r_000000_0 on tt1");
+    checkAssignment("tt2", "attempt_test_0001_m_000001_0 on tt2");
+    checkAssignment("tt2", "attempt_test_0003_m_000001_0 on tt2");
+    checkAssignment("tt2", "attempt_test_0001_r_000001_0 on tt2");
+    checkAssignment("tt2", "attempt_test_0003_r_000001_0 on tt2");
+  }
+  
+  /**
+   * This test submits 2 large jobs each to 2 pools, one of which is set to FIFO
+   * mode through the global defaultPoolSchedulingMode setting, and one of which
+   * is set to fair mode. We check that the scheduler assigns tasks only to the
+   * first job in the FIFO pool but to both jobs in the fair sharing pool.
+   */
+  public void testFifoAndFairPools() throws Exception {
+    // Set up pools file
+    PrintWriter out = new PrintWriter(new FileWriter(ALLOC_FILE));
+    out.println("<?xml version=\"1.0\"?>");
+    out.println("<allocations>");
+    out.println("<defaultPoolSchedulingMode>fifo</defaultPoolSchedulingMode>");
+    out.println("<pool name=\"poolB\">");
+    out.println("<schedulingMode>fair</schedulingMode>");
+    out.println("</pool>");
+    out.println("</allocations>");
+    out.close();
+    scheduler.getPoolManager().reloadAllocs();
+    
+    // Submit jobs, advancing time in-between to make sure that they are
+    // all submitted at distinct times.
+    JobInProgress job1 = submitJob(JobStatus.RUNNING, 10, 10, "poolA");
+    advanceTime(10);
+    JobInProgress job2 = submitJob(JobStatus.RUNNING, 10, 10, "poolA");
+    advanceTime(10);
+    JobInProgress job3 = submitJob(JobStatus.RUNNING, 10, 10, "poolB");
+    advanceTime(10);
+    JobInProgress job4 = submitJob(JobStatus.RUNNING, 10, 10, "poolB");
+    
+    // Assign tasks and check that only job 1 gets tasks in pool A, but
+    // jobs 3 and 4 both get tasks in pool B.
+    checkAssignment("tt1", "attempt_test_0001_m_000000_0 on tt1");
+    checkAssignment("tt1", "attempt_test_0003_m_000000_0 on tt1");
+    checkAssignment("tt1", "attempt_test_0001_r_000000_0 on tt1");
+    checkAssignment("tt1", "attempt_test_0003_r_000000_0 on tt1");
+    checkAssignment("tt2", "attempt_test_0001_m_000001_0 on tt2");
+    checkAssignment("tt2", "attempt_test_0004_m_000000_0 on tt2");
+    checkAssignment("tt2", "attempt_test_0001_r_000001_0 on tt2");
+    checkAssignment("tt2", "attempt_test_0004_r_000000_0 on tt2");
+  }
+  
   private void advanceTime(long time) {
     clock.advance(time);
     scheduler.update();
@@ -1871,11 +2482,14 @@ public class TestFairScheduler extends TestCase {
   }
   
   protected void checkAssignment(String taskTrackerName,
-      String expectedTaskString) throws IOException {
+      String... expectedTasks) throws IOException {
     List<Task> tasks = scheduler.assignTasks(tracker(taskTrackerName));
-    assertNotNull(expectedTaskString, tasks);
-    assertEquals(expectedTaskString, 1, tasks.size());
-    assertEquals(expectedTaskString, tasks.get(0).toString());
+    System.out.println("Assigned tasks:");
+    for (int i = 0; i < tasks.size(); i++)
+      System.out.println("- " + tasks.get(i));
+    assertNotNull(tasks);
+    assertEquals(expectedTasks.length, tasks.size());
+    for (int i = 0; i < tasks.size(); i++)
+      assertEquals("assignment " + i, expectedTasks[i], tasks.get(i).toString());
   }
-  
 }
diff --git a/src/docs/src/documentation/content/xdocs/fair_scheduler.xml b/src/docs/src/documentation/content/xdocs/fair_scheduler.xml
index 26826ca..624d9c2 100644
--- a/src/docs/src/documentation/content/xdocs/fair_scheduler.xml
+++ b/src/docs/src/documentation/content/xdocs/fair_scheduler.xml
@@ -48,13 +48,11 @@
       <p>
         The fair scheduler organizes jobs into <em>pools</em>, and 
         divides resources fairly between these pools. By default, there is a 
-        separate pool for each user, so that each user gets the same share 
-        of the cluster no matter how many jobs they submit. It is also
-        possible to set a job's pool based on the user's Unix group or
-        any jobconf property. 
-        Within each pool, fair sharing is used to divide capacity between 
-        the running jobs. Pools can also be given weights to share the 
-        cluster non-proportionally.
+        separate pool for each user, so that each user gets an equal share 
+        of the cluster. It is also possible to set a job's pool based on the
+        user's Unix group or any jobconf property. 
+        Within each pool, jobs can be scheduled using either fair sharing or 
+        first-in-first-out (FIFO) scheduling.
       </p>
       <p>
         In addition to providing fair sharing, the Fair Scheduler allows
@@ -65,19 +63,15 @@
         guaranteed share, the excess is split between other pools.
       </p>
       <p>
-        In normal operation, when a new job is submitted, the scheduler 
-        waits for tasks from existing jobs to finish in order to free up
-        slots for the new job. However, the scheduler also optionally supports
-        <em>preemption</em> of running jobs after configurable timeouts.
-        If the new job's minimum share is not reached after
-        a certain amount of time, the job is allowed to kill tasks from
-        existing jobs to make room to run.
-        Preemption can thus be used to guarantee
-        that "production" jobs run at specified times while allowing
+        If a pool's minimum share is not met for some period of time, the
+        scheduler optionally supports <em>preemption</em> of jobs in other
+        pools. The pool will be allowed to kill tasks from other pools to make
+        room to run. Preemption can be used to guarantee
+        that "production" jobs are not starved while also allowing
         the Hadoop cluster to also be used for experimental and research jobs.
-        In addition, a job can also be allowed to preempt tasks if it is
+        In addition, a pool can also be allowed to preempt tasks if it is
         below half of its fair share for a configurable timeout (generally
-        set larger than the minimum share timeout).
+        set larger than the minimum share preemption timeout).
         When choosing tasks to kill, the fair scheduler picks the
         most-recently-launched tasks from over-allocated jobs, 
         to minimize wasted computation.
@@ -87,11 +81,11 @@
       <p>
         Finally, the Fair Scheduler can limit the number of concurrent
         running jobs per user and per pool. This can be useful when a 
-        user must submit hundreds of jobs at once, and for ensuring that
-        intermediate data does not fill up disk space on a cluster if too many
+        user must submit hundreds of jobs at once, or for ensuring that
+        intermediate data does not fill up disk space on a cluster when too many
         concurrent jobs are running.
-        Setting job limits causes jobs submitted beyond the limit to wait in the
-        scheduler's queue until some of the user/pool's earlier jobs finish.
+        Setting job limits causes jobs submitted beyond the limit to wait
+        until some of the user/pool's earlier jobs finish.
         Jobs to run from each user/pool are chosen in order of priority and then
         submit time.
       </p>
@@ -356,6 +350,9 @@
           <ul>
           <li><em>minMaps</em> and <em>minReduces</em>,
             to set the pool's minimum share of task slots.</li>
+          <li><em>schedulingMode</em>, the pool's internal scheduling mode,
+          which can be <em>fair</em> for fair sharing or <em>fifo</em> for
+          first-in-first-out.</li>
           <li><em>maxRunningJobs</em>, 
           to limit the number of jobs from the 
           pool to run at once (defaults to infinite).</li>
@@ -381,6 +378,9 @@
         <li><em>fairSharePreemptionTimeout</em>, 
         which sets the preemption timeout used when jobs are below half
         their fair share.</li>
+        <li><em>defaultPoolSchedulingMode</em>, which sets the default scheduling 
+        mode (<em>fair</em> or <em>fifo</em>) for pools whose mode is
+        not specified.</li>
         </ul>
         <p>
         Pool and user elements only required if you are setting
@@ -430,9 +430,9 @@
     </p> 
     <ol>
     <li>
-      It is possible to modify minimum shares, limits, weights and preemption
-      timeouts at runtime by editing the allocation file.
-      The scheduler will reload this file 10-15 seconds after it 
+      It is possible to modify minimum shares, limits, weights, preemption
+      timeouts and pool scheduling modes at runtime by editing the allocation
+      file. The scheduler will reload this file 10-15 seconds after it 
       sees that it was modified.
      </li>
      <li>
@@ -465,22 +465,16 @@
      <p>
      In addition, it is possible to view an "advanced" version of the web 
      UI by going to <em>http://&lt;JobTracker URL&gt;/scheduler?advanced</em>. 
-     This view shows four more columns:
+     This view shows two more columns:
      </p>
      <ul>
      <li><em>Maps/Reduce Weight</em>: Weight of the job in the fair sharing 
      calculations. This depends on priority and potentially also on 
      job size and job age if the <em>sizebasedweight</em> and 
      <em>NewJobWeightBooster</em> are enabled.</li>
-     <li><em>Map/Reduce Deficit</em>: The job's scheduling deficit in machine-
-     seconds - the amount of resources it should have gotten according to 
-     its fair share, minus how many it actually got. Positive deficit means
-      the job will be scheduled again in the near future because it needs to 
-      catch up to its fair share. The scheduler schedules jobs with higher 
-      deficit ahead of others. Please see the Implementation section of 
-      this document for details.</li>
      </ul>
     </section>
+    <!--
     <section>
     <title>Implementation</title>
     <p>There are two aspects to implementing fair scheduling: Calculating 
@@ -535,5 +529,6 @@
      <a href="#Advanced+Parameters">advanced mapred-site.xml properties</a>.
      </p>
     </section>
+    -->
   </body>  
 </document>
diff --git a/src/mapred/org/apache/hadoop/mapred/JobInProgress.java b/src/mapred/org/apache/hadoop/mapred/JobInProgress.java
index f85d916..6a8a927 100644
--- a/src/mapred/org/apache/hadoop/mapred/JobInProgress.java
+++ b/src/mapred/org/apache/hadoop/mapred/JobInProgress.java
@@ -304,6 +304,14 @@ public class JobInProgress {
     this.resourceEstimator = new ResourceEstimator(this);
   }
 
+  public boolean hasSpeculativeMaps() {
+    return hasSpeculativeMaps;
+  }
+
+  public boolean hasSpeculativeReduces() {
+    return hasSpeculativeReduces;
+  }
+
   /**
    * Called periodically by JobTrackerMetrics to update the metrics for
    * this job.
@@ -951,15 +959,16 @@ public class JobInProgress {
    */
   public synchronized Task obtainNewMapTask(TaskTrackerStatus tts, 
                                             int clusterSize, 
-                                            int numUniqueHosts
+                                            int numUniqueHosts,
+                                            int maxCacheLevel
                                            ) throws IOException {
     if (status.getRunState() != JobStatus.RUNNING) {
       LOG.info("Cannot create task split for " + profile.getJobID());
       return null;
     }
         
-    int target = findNewMapTask(tts, clusterSize, numUniqueHosts, anyCacheLevel,
-                                status.mapProgress());
+    int target = findNewMapTask(tts, clusterSize, numUniqueHosts, maxCacheLevel,
+         getStatus().mapProgress());
     if (target == -1) {
       return null;
     }
@@ -970,6 +979,16 @@ public class JobInProgress {
     }
 
     return result;
+  } 
+  
+  /**
+   * Return a MapTask, if appropriate, to run on the given tasktracker
+   */
+  public synchronized Task obtainNewMapTask(TaskTrackerStatus tts, 
+                                            int clusterSize, 
+                                            int numUniqueHosts
+                                           ) throws IOException {
+    return obtainNewMapTask(tts, clusterSize, numUniqueHosts, anyCacheLevel);
   }    
 
   /*
@@ -1019,18 +1038,7 @@ public class JobInProgress {
       return null;
     }
 
-    int target = findNewMapTask(tts, clusterSize, numUniqueHosts, maxLevel, 
-                                status.mapProgress());
-    if (target == -1) {
-      return null;
-    }
-
-    Task result = maps[target].getTaskToRun(tts.getTrackerName());
-    if (result != null) {
-      addRunningTaskToTIP(maps[target], result.getTaskID(), tts, true);
-    }
-
-    return result;
+    return obtainNewMapTask(tts, clusterSize, numUniqueHosts, maxLevel);
   }
   
   public synchronized Task obtainNewNonLocalMapTask(TaskTrackerStatus tts,
@@ -1042,18 +1050,8 @@ public class JobInProgress {
       return null;
     }
 
-    int target = findNewMapTask(tts, clusterSize, numUniqueHosts, 
-                                NON_LOCAL_CACHE_LEVEL, status.mapProgress());
-    if (target == -1) {
-      return null;
-    }
-
-    Task result = maps[target].getTaskToRun(tts.getTrackerName());
-    if (result != null) {
-      addRunningTaskToTIP(maps[target], result.getTaskID(), tts, true);
-    }
-
-    return result;
+    return obtainNewMapTask(tts, clusterSize, numUniqueHosts,
+        NON_LOCAL_CACHE_LEVEL);
   }
   
   /**
@@ -1319,23 +1317,7 @@ public class JobInProgress {
     // data locality.
     if (tip.isMapTask() && !tip.isJobSetupTask() && !tip.isJobCleanupTask()) {
       // increment the data locality counter for maps
-      Node tracker = jobtracker.getNode(tts.getHost());
-      int level = this.maxLevel;
-      // find the right level across split locations
-      for (String local : maps[tip.getIdWithinJob()].getSplitLocations()) {
-        Node datanode = jobtracker.getNode(local);
-        int newLevel = this.maxLevel;
-        if (tracker != null && datanode != null) {
-          newLevel = getMatchingLevelForNodes(tracker, datanode);
-        }
-        if (newLevel < level) {
-          level = newLevel;
-          // an optimization
-          if (level == 0) {
-            break;
-          }
-        }
-      }
+      int level = getLocalityLevel(tip, tts);
       switch (level) {
       case 0 :
         LOG.info("Choosing data-local task " + tip.getTIPId());
@@ -2711,4 +2693,31 @@ public class JobInProgress {
       return Values.REDUCE.name();
     }
   }
+
+  /*
+  * Get the level of locality that a given task would have if launched on
+  * a particular TaskTracker. Returns 0 if the task has data on that machine,
+  * 1 if it has data on the same rack, etc (depending on number of levels in
+  * the network hierarchy).
+  */
+ int getLocalityLevel(TaskInProgress tip, TaskTrackerStatus tts) {
+   Node tracker = jobtracker.getNode(tts.getHost());
+   int level = this.maxLevel;
+   // find the right level across split locations
+   for (String local : maps[tip.getIdWithinJob()].getSplitLocations()) {
+     Node datanode = jobtracker.getNode(local);
+     int newLevel = this.maxLevel;
+     if (tracker != null && datanode != null) {
+       newLevel = getMatchingLevelForNodes(tracker, datanode);
+     }
+     if (newLevel < level) {
+       level = newLevel;
+       // an optimization
+       if (level == 0) {
+         break;
+       }
+     }
+   }
+   return level;
+ }
 }
-- 
1.7.0.4

