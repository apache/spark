<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN" "http://www.w3.org/TR/html4/loose.dtd">
<html>
<head>
<META http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta content="Apache Forrest" name="Generator">
<meta name="Forrest-version" content="0.8">
<meta name="Forrest-skin-name" content="pelt">
<title>
      HOD Scheduler
    </title>
<link type="text/css" href="skin/basic.css" rel="stylesheet">
<link media="screen" type="text/css" href="skin/screen.css" rel="stylesheet">
<link media="print" type="text/css" href="skin/print.css" rel="stylesheet">
<link type="text/css" href="skin/profile.css" rel="stylesheet">
<script src="skin/getBlank.js" language="javascript" type="text/javascript"></script><script src="skin/getMenu.js" language="javascript" type="text/javascript"></script><script src="skin/fontsize.js" language="javascript" type="text/javascript"></script>
<link rel="shortcut icon" href="images/favicon.ico">
</head>
<body onload="init()">
<script type="text/javascript">ndeSetTextSize();</script>
<div id="top">
<!--+
    |breadtrail
    +-->
<div class="breadtrail">
<a href="http://www.apache.org/">Apache</a> &gt; <a href="http://hadoop.apache.org/">Hadoop</a> &gt; <a href="http://hadoop.apache.org/core/">Core</a><script src="skin/breadcrumbs.js" language="JavaScript" type="text/javascript"></script>
</div>
<!--+
    |header
    +-->
<div class="header">
<!--+
    |start group logo
    +-->
<div class="grouplogo">
<a href="http://hadoop.apache.org/"><img class="logoImage" alt="Hadoop" src="images/hadoop-logo.jpg" title="Apache Hadoop"></a>
</div>
<!--+
    |end group logo
    +-->
<!--+
    |start Project Logo
    +-->
<div class="projectlogo">
<a href="http://hadoop.apache.org/core/"><img class="logoImage" alt="Hadoop" src="images/hadoop-logo-2.gif" title="Scalable Computing Platform"></a>
</div>
<!--+
    |end Project Logo
    +-->
<!--+
    |start Search
    +-->
<div class="searchbox">
<form action="http://www.google.com/search" method="get" class="roundtopsmall">
<input value="archive.cloudera.com" name="sitesearch" type="hidden"><input onFocus="getBlank (this, 'Search the site with google');" size="25" name="q" id="query" type="text" value="Search the site with google">&nbsp; 
                    <input name="Search" value="Search" type="submit">
</form>
</div>
<!--+
    |end search
    +-->
<!--+
    |start Tabs
    +-->
<ul id="tabs">
<li>
<a class="unselected" href="http://hadoop.apache.org/core/">Project</a>
</li>
<li>
<a class="unselected" href="http://wiki.apache.org/hadoop">Wiki</a>
</li>
<li class="current">
<a class="selected" href="index.html">Hadoop 0.20 Documentation</a>
</li>
</ul>
<!--+
    |end Tabs
    +-->
</div>
</div>
<div id="main">
<div id="publishedStrip">
<!--+
    |start Subtabs
    +-->
<div id="level2tabs"></div>
<!--+
    |end Endtabs
    +-->
<script type="text/javascript"><!--
document.write("Last Published: " + document.lastModified);
//  --></script>
</div>
<!--+
    |breadtrail
    +-->
<div class="breadtrail">

             &nbsp;
           </div>
<!--+
    |start Menu, mainarea
    +-->
<!--+
    |start Menu
    +-->
<div id="menu">
<div onclick="SwitchMenu('menu_1.1', 'skin/')" id="menu_1.1Title" class="menutitle">Getting Started</div>
<div id="menu_1.1" class="menuitemgroup">
<div class="menuitem">
<a href="index.html">Overview</a>
</div>
<div class="menuitem">
<a href="single_node_setup.html">Single Node Setup</a>
</div>
<div class="menuitem">
<a href="cluster_setup.html">Cluster Setup</a>
</div>
</div>
<div onclick="SwitchMenu('menu_1.2', 'skin/')" id="menu_1.2Title" class="menutitle">Guides</div>
<div id="menu_1.2" class="menuitemgroup">
<div class="menuitem">
<a href="HttpAuthentication.html">Authentication for Hadoop HTTP web-consoles</a>
</div>
</div>
<div onclick="SwitchMenu('menu_selected_1.3', 'skin/')" id="menu_selected_1.3Title" class="menutitle" style="background-image: url('skin/images/chapter_open.gif');">MapReduce</div>
<div id="menu_selected_1.3" class="selectedmenuitemgroup" style="display: block;">
<div class="menuitem">
<a href="mapred_tutorial.html">MapReduce Tutorial</a>
</div>
<div class="menuitem">
<a href="streaming.html">Hadoop Streaming</a>
</div>
<div class="menuitem">
<a href="commands_manual.html">Hadoop Commands</a>
</div>
<div class="menuitem">
<a href="distcp.html">DistCp</a>
</div>
<div class="menuitem">
<a href="vaidya.html">Vaidya</a>
</div>
<div class="menuitem">
<a href="hadoop_archives.html">Hadoop Archives</a>
</div>
<div class="menuitem">
<a href="gridmix.html">Gridmix</a>
</div>
<div class="menuitem">
<a href="capacity_scheduler.html">Capacity Scheduler</a>
</div>
<div class="menuitem">
<a href="fair_scheduler.html">Fair Scheduler</a>
</div>
<div class="menupage">
<div class="menupagetitle">Hod Scheduler</div>
</div>
</div>
<div onclick="SwitchMenu('menu_1.4', 'skin/')" id="menu_1.4Title" class="menutitle">HDFS</div>
<div id="menu_1.4" class="menuitemgroup">
<div class="menuitem">
<a href="hdfs_user_guide.html">HDFS Users </a>
</div>
<div class="menuitem">
<a href="hdfs_design.html">HDFS Architecture</a>
</div>
<div class="menuitem">
<a href="hdfs_permissions_guide.html">Permissions</a>
</div>
<div class="menuitem">
<a href="hdfs_quota_admin_guide.html">Quotas</a>
</div>
<div class="menuitem">
<a href="hdfs_imageviewer.html">Offline Image Viewer Guide</a>
</div>
<div class="menuitem">
<a href="SLG_user_guide.html">Synthetic Load Generator</a>
</div>
<div class="menuitem">
<a href="libhdfs.html">C API libhdfs</a>
</div>
</div>
<div onclick="SwitchMenu('menu_1.5', 'skin/')" id="menu_1.5Title" class="menutitle">Common</div>
<div id="menu_1.5" class="menuitemgroup">
<div class="menuitem">
<a href="file_system_shell.html">File System Shell</a>
</div>
<div class="menuitem">
<a href="service_level_auth.html">Service Level Authorization</a>
</div>
<div class="menuitem">
<a href="native_libraries.html">Native Libraries</a>
</div>
</div>
<div onclick="SwitchMenu('menu_1.6', 'skin/')" id="menu_1.6Title" class="menutitle">Miscellaneous</div>
<div id="menu_1.6" class="menuitemgroup">
<div class="menuitem">
<a href="Secure_Impersonation.html">Secure Impersonation</a>
</div>
<div class="menuitem">
<a href="api/index.html">API Docs</a>
</div>
<div class="menuitem">
<a href="jdiff/changes.html">API Changes</a>
</div>
<div class="menuitem">
<a href="http://wiki.apache.org/hadoop/">Wiki</a>
</div>
<div class="menuitem">
<a href="http://wiki.apache.org/hadoop/FAQ">FAQ</a>
</div>
<div class="menuitem">
<a href="releasenotes.html">Release Notes</a>
</div>
<div class="menuitem">
<a href="changes.html">Change Log</a>
</div>
</div>
<div id="credit"></div>
<div id="roundbottom">
<img style="display: none" class="corner" height="15" width="15" alt="" src="skin/images/rc-b-l-15-1body-2menu-3menu.png"></div>
<!--+
  |alternative credits
  +-->
<div id="credit2"></div>
</div>
<!--+
    |end Menu
    +-->
<!--+
    |start content
    +-->
<div id="content">
<div title="Portable Document Format" class="pdflink">
<a class="dida" href="hod_scheduler.pdf"><img alt="PDF -icon" src="skin/images/pdfdoc.gif" class="skin"><br>
        PDF</a>
</div>
<h1>
      HOD Scheduler
    </h1>
<div id="minitoc-area">
<ul class="minitoc">
<li>
<a href="#Introduction">Introduction</a>
</li>
<li>
<a href="#HOD+Users">HOD Users</a>
<ul class="minitoc">
<li>
<a href="#Getting+Started"> Getting Started</a>
<ul class="minitoc">
<li>
<a href="#A+Typical+HOD+Session">A Typical HOD Session</a>
</li>
<li>
<a href="#Running+Hadoop+Scripts+Using+HOD">Running Hadoop Scripts Using HOD</a>
</li>
</ul>
</li>
<li>
<a href="#HOD+Features"> HOD Features </a>
<ul class="minitoc">
<li>
<a href="#Provisioning+and+Managing+Hadoop+Clusters"> Provisioning and Managing Hadoop Clusters </a>
</li>
<li>
<a href="#Using+a+Tarball+to+Distribute+Hadoop"> Using a Tarball to Distribute Hadoop </a>
</li>
<li>
<a href="#Using+an+External+HDFS"> Using an External HDFS </a>
</li>
<li>
<a href="#Options+for+Configuring+Hadoop"> Options for Configuring Hadoop </a>
</li>
<li>
<a href="#Viewing+Hadoop+Web-UIs"> Viewing Hadoop Web-UIs </a>
</li>
<li>
<a href="#Collecting+and+Viewing+Hadoop+Logs"> Collecting and Viewing Hadoop Logs </a>
</li>
<li>
<a href="#Auto-deallocation+of+Idle+Clusters"> Auto-deallocation of Idle Clusters </a>
</li>
<li>
<a href="#Specifying+Additional+Job+Attributes"> Specifying Additional Job Attributes </a>
</li>
<li>
<a href="#Capturing+HOD+Exit+Codes+in+Torque"> Capturing HOD Exit Codes in Torque </a>
</li>
<li>
<a href="#Command+Line"> Command Line</a>
</li>
<li>
<a href="#Options+Configuring+HOD"> Options Configuring HOD </a>
</li>
</ul>
</li>
<li>
<a href="#Troubleshooting-N104A6"> Troubleshooting </a>
<ul class="minitoc">
<li>
<a href="#HOD+Hangs+During+Allocation">HOD Hangs During Allocation </a>
</li>
<li>
<a href="#HOD+Hangs+During+Deallocation">HOD Hangs During Deallocation </a>
</li>
<li>
<a href="#HOD+Fails+With+an+Error+Code+and+Error+Message">HOD Fails With an Error Code and Error Message </a>
</li>
<li>
<a href="#Hadoop+DFSClient+Warns+with+a%0A++NotReplicatedYetException">Hadoop DFSClient Warns with a
  NotReplicatedYetException</a>
</li>
<li>
<a href="#Hadoop+Jobs+Not+Running+on+a+Successfully+Allocated+Cluster"> Hadoop Jobs Not Running on a Successfully Allocated Cluster </a>
</li>
<li>
<a href="#My+Hadoop+Job+Got+Killed"> My Hadoop Job Got Killed </a>
</li>
<li>
<a href="#Hadoop+Job+Fails+with+Message%3A+%27Job+tracker+still+initializing%27"> Hadoop Job Fails with Message: 'Job tracker still initializing' </a>
</li>
<li>
<a href="#The+Exit+Codes+For+HOD+Are+Not+Getting+Into+Torque"> The Exit Codes For HOD Are Not Getting Into Torque </a>
</li>
<li>
<a href="#The+Hadoop+Logs+are+Not+Uploaded+to+HDFS"> The Hadoop Logs are Not Uploaded to HDFS </a>
</li>
<li>
<a href="#Locating+Ringmaster+Logs"> Locating Ringmaster Logs </a>
</li>
<li>
<a href="#Locating+Hodring+Logs"> Locating Hodring Logs </a>
</li>
</ul>
</li>
</ul>
</li>
<li>
<a href="#HOD+Administrators">HOD Administrators</a>
<ul class="minitoc">
<li>
<a href="#Getting+Started-N10781">Getting Started</a>
</li>
<li>
<a href="#Prerequisites">Prerequisites</a>
</li>
<li>
<a href="#Resource+Manager">Resource Manager</a>
</li>
<li>
<a href="#Installing+HOD">Installing HOD</a>
</li>
<li>
<a href="#Configuring+HOD">Configuring HOD</a>
<ul class="minitoc">
<li>
<a href="#Minimal+Configuration">Minimal Configuration</a>
</li>
<li>
<a href="#Advanced+Configuration">Advanced Configuration</a>
</li>
</ul>
</li>
<li>
<a href="#Running+HOD">Running HOD</a>
</li>
<li>
<a href="#Supporting+Tools+and+Utilities">Supporting Tools and Utilities</a>
<ul class="minitoc">
<li>
<a href="#logcondense.py+-+Manage+Log+Files">logcondense.py - Manage Log Files</a>
</li>
<li>
<a href="#checklimits.sh+-+Monitor+Resource+Limits">checklimits.sh - Monitor Resource Limits</a>
</li>
<li>
<a href="#verify-account+Script">verify-account Script</a>
</li>
</ul>
</li>
</ul>
</li>
<li>
<a href="#HOD+Configuration">HOD Configuration</a>
<ul class="minitoc">
<li>
<a href="#Getting+Started-N109E3">Getting Started</a>
</li>
<li>
<a href="#Configuation+Options">Configuation Options</a>
<ul class="minitoc">
<li>
<a href="#common+options">common options</a>
</li>
<li>
<a href="#hod+options">hod options</a>
</li>
<li>
<a href="#resource_manager+options">resource_manager options</a>
</li>
<li>
<a href="#ringmaster+options">ringmaster options</a>
</li>
<li>
<a href="#gridservice-hdfs+options">gridservice-hdfs options</a>
</li>
<li>
<a href="#gridservice-mapred+options">gridservice-mapred options</a>
</li>
<li>
<a href="#hodring+options">hodring options</a>
</li>
</ul>
</li>
</ul>
</li>
</ul>
</div>


<a name="N1000F"></a><a name="Introduction"></a>
<h2 class="h3">Introduction</h2>
<div class="section">
<p>Hadoop On Demand (HOD) is a system for provisioning and managing independent Hadoop MapReduce and 
Hadoop Distributed File System (HDFS) instances on a shared cluster of nodes. HOD is a tool that makes it easy 
for administrators and users to quickly setup and use Hadoop. HOD is also a very useful tool for Hadoop developers 
and testers who need to share a physical cluster for testing their own Hadoop versions. </p>
<p>HOD uses the Torque resource manager to do node allocation. On the allocated nodes, it can start Hadoop 
MapReduce and HDFS daemons. It automatically generates the appropriate configuration files (hadoop-site.xml) 
for the Hadoop daemons and client. HOD also has the capability to distribute Hadoop to the nodes in the virtual 
cluster that it allocates. HOD supports Hadoop from version 0.15 onwards.</p>
</div>

  
<a name="N1001C"></a><a name="HOD+Users"></a>
<h2 class="h3">HOD Users</h2>
<div class="section">
<p>This section shows users how to get started using HOD, reviews various HOD features and command line options, 
  and provides detailed troubleshooting help.</p>
<a name="N10025"></a><a name="Getting+Started"></a>
<h3 class="h4"> Getting Started</h3>
<a name="Getting_Started_Using_HOD_0_4" id="Getting_Started_Using_HOD_0_4"></a>
<p>In this section, we shall see a step-by-step introduction on how to use HOD for the most basic operations. Before 
  following these steps, it is assumed that HOD and its dependent hardware and software components are setup and 
  configured correctly. This is a step that is generally performed by system administrators of the cluster.</p>
<p>The HOD user interface is a command line utility called <span class="codefrag">hod</span>. It is driven by a configuration file, 
  that is typically setup for users by system administrators. Users can override this configuration when using 
  the <span class="codefrag">hod</span>, which is described later in this documentation. The configuration file can be specified in 
  two ways when using <span class="codefrag">hod</span>, as described below: </p>
<ul>
    
<li> Specify it on command line, using the -c option. Such as 
    <span class="codefrag">hod &lt;operation&gt; &lt;required-args&gt; -c path-to-the-configuration-file [other-options]</span>
</li>
    
<li> Set up an environment variable <em>HOD_CONF_DIR</em> where <span class="codefrag">hod</span> will be run. 
    This should be pointed to a directory on the local file system, containing a file called <em>hodrc</em>. 
    Note that this is analogous to the <em>HADOOP_CONF_DIR</em> and <em>hadoop-site.xml</em> file for Hadoop. 
    If no configuration file is specified on the command line, <span class="codefrag">hod</span> shall look for the <em>HOD_CONF_DIR</em> 
    environment variable and a <em>hodrc</em> file under that.</li>
    
</ul>
<p>In examples listed below, we shall not explicitly point to the configuration option, assuming it is correctly specified.</p>
<a name="N10062"></a><a name="A+Typical+HOD+Session"></a>
<h4>A Typical HOD Session</h4>
<a name="HOD_Session" id="HOD_Session"></a>
<p>A typical session of HOD will involve at least three steps: allocate, run hadoop jobs, deallocate. In order to do this, 
  perform the following steps.</p>
<p>
<strong> Create a Cluster Directory </strong>
</p>
<a name="Create_a_Cluster_Directory" id="Create_a_Cluster_Directory"></a>
<p>The <em>cluster directory</em> is a directory on the local file system where <span class="codefrag">hod</span> will generate the 
  Hadoop configuration, <em>hadoop-site.xml</em>, corresponding to the cluster it allocates. Pass this directory to the 
  <span class="codefrag">hod</span> operations as stated below. If the cluster directory passed doesn't already exist, HOD will automatically 
  try to create it and use it. Once a cluster is allocated, a user can utilize it to run Hadoop jobs by specifying the cluster 
  directory as the Hadoop --config option. </p>
<p>
<strong>Operation allocate</strong>
</p>
<a name="Operation_allocate" id="Operation_allocate"></a>
<p>The <em>allocate</em> operation is used to allocate a set of nodes and install and provision Hadoop on them. 
  It has the following syntax. Note that it requires a cluster_dir ( -d, --hod.clusterdir) and the number of nodes 
  (-n, --hod.nodecount) needed to be allocated:</p>
<pre class="code">$ hod allocate -d cluster_dir -n number_of_nodes [OPTIONS]</pre>
<p>If the command completes successfully, then <span class="codefrag">cluster_dir/hadoop-site.xml</span> will be generated and 
  will contain information about the allocated cluster. It will also print out the information about the Hadoop web UIs.</p>
<p>An example run of this command produces the following output. Note in this example that <span class="codefrag">~/hod-clusters/test</span> 
  is the cluster directory, and we are allocating 5 nodes:</p>
<pre class="code">
$ hod allocate -d ~/hod-clusters/test -n 5 
INFO - HDFS UI on http://foo1.bar.com:53422 
INFO - Mapred UI on http://foo2.bar.com:55380</pre>
<p>
<strong> Running Hadoop jobs using the allocated cluster </strong>
</p>
<a name="Running_Hadoop_jobs_using_the_al" id="Running_Hadoop_jobs_using_the_al"></a>
<p>Now, one can run Hadoop jobs using the allocated cluster in the usual manner. This assumes variables like <em>JAVA_HOME</em> 
  and path to the Hadoop installation are set up correctly.:</p>
<pre class="code">$ hadoop --config cluster_dir hadoop_command hadoop_command_args</pre>
<p>or</p>
<pre class="code">
$ export HADOOP_CONF_DIR=cluster_dir
$ hadoop hadoop_command hadoop_command_args</pre>
<p>Continuing our example, the following command will run a wordcount example on the allocated cluster:</p>
<pre class="code">$ hadoop --config ~/hod-clusters/test jar /path/to/hadoop/hadoop-examples.jar wordcount /path/to/input /path/to/output</pre>
<p>or</p>
<pre class="code">
$ export HADOOP_CONF_DIR=~/hod-clusters/test
$ hadoop jar /path/to/hadoop/hadoop-examples.jar wordcount /path/to/input /path/to/output</pre>
<p>
<strong> Operation deallocate</strong>
</p>
<a name="Operation_deallocate" id="Operation_deallocate"></a>
<p>The <em>deallocate</em> operation is used to release an allocated cluster. When finished with a cluster, deallocate must be 
  run so that the nodes become free for others to use. The <em>deallocate</em> operation has the following syntax. Note that it 
  requires the cluster_dir (-d, --hod.clusterdir) argument:</p>
<pre class="code">$ hod deallocate -d cluster_dir</pre>
<p>Continuing our example, the following command will deallocate the cluster:</p>
<pre class="code">$ hod deallocate -d ~/hod-clusters/test</pre>
<p>As can be seen, HOD allows the users to allocate a cluster, and use it flexibly for running Hadoop jobs. For example, users 
  can run multiple jobs in parallel on the same cluster, by running hadoop from multiple shells pointing to the same configuration.</p>
<a name="N100E4"></a><a name="Running+Hadoop+Scripts+Using+HOD"></a>
<h4>Running Hadoop Scripts Using HOD</h4>
<a name="HOD_Script_Mode" id="HOD_Script_Mode"></a>
<p>The HOD <em>script operation</em> combines the operations of allocating, using and deallocating a cluster into a single operation. 
  This is very useful for users who want to run a script of hadoop jobs and let HOD handle the cleanup automatically once the script completes. 
  In order to run hadoop scripts using <span class="codefrag">hod</span>, do the following:</p>
<p>
<strong> Create a script file </strong>
</p>
<a name="Create_a_script_file" id="Create_a_script_file"></a>
<p>This will be a regular shell script that will typically contain hadoop commands, such as:</p>
<pre class="code">$ hadoop jar jar_file options</pre>
<p>However, the user can add any valid commands as part of the script. HOD will execute this script setting <em>HADOOP_CONF_DIR</em> 
  automatically to point to the allocated cluster. So users do not need to worry about this. The users however need to specify a cluster directory 
  just like when using the allocate operation.</p>
<p>
<strong> Running the script </strong>
</p>
<a name="Running_the_script" id="Running_the_script"></a>
<p>The syntax for the <em>script operation</em> as is as follows. Note that it requires a cluster directory ( -d, --hod.clusterdir), number of 
  nodes (-n, --hod.nodecount) and a script file (-s, --hod.script):</p>
<pre class="code">$ hod script -d cluster_directory -n number_of_nodes -s script_file</pre>
<p>Note that HOD will deallocate the cluster as soon as the script completes, and this means that the script must not complete until the 
  hadoop jobs themselves are completed. Users must take care of this while writing the script. </p>
<a name="N1011C"></a><a name="HOD+Features"></a>
<h3 class="h4"> HOD Features </h3>
<a name="HOD_0_4_Features" id="HOD_0_4_Features"></a><a name="N10124"></a><a name="Provisioning+and+Managing+Hadoop+Clusters"></a>
<h4> Provisioning and Managing Hadoop Clusters </h4>
<a name="Provisioning_and_Managing_Hadoop" id="Provisioning_and_Managing_Hadoop"></a>
<p>The primary feature of HOD is to provision Hadoop MapReduce and HDFS clusters. This is described above in the Getting Started section. 
  Also, as long as nodes are available, and organizational policies allow, a user can use HOD to allocate multiple MapReduce clusters simultaneously. 
  The user would need to specify different paths for the <span class="codefrag">cluster_dir</span> parameter mentioned above for each cluster he/she allocates. 
  HOD provides the <em>list</em> and the <em>info</em> operations to enable managing multiple clusters.</p>
<p>
<strong> Operation list</strong>
</p>
<a name="Operation_list" id="Operation_list"></a>
<p>The list operation lists all the clusters allocated so far by a user. The cluster directory where the hadoop-site.xml is stored for the cluster, 
  and its status vis-a-vis connectivity with the JobTracker and/or HDFS is shown. The list operation has the following syntax:</p>
<pre class="code">$ hod list</pre>
<p>
<strong> Operation info</strong>
</p>
<a name="Operation_info" id="Operation_info"></a>
<p>The info operation shows information about a given cluster. The information shown includes the Torque job id, and locations of the important 
  daemons like the HOD Ringmaster process, and the Hadoop JobTracker and NameNode daemons. The info operation has the following syntax. 
  Note that it requires a cluster directory (-d, --hod.clusterdir):</p>
<pre class="code">$ hod info -d cluster_dir</pre>
<p>The <span class="codefrag">cluster_dir</span> should be a valid cluster directory specified in an earlier <em>allocate</em> operation.</p>
<a name="N1015B"></a><a name="Using+a+Tarball+to+Distribute+Hadoop"></a>
<h4> Using a Tarball to Distribute Hadoop </h4>
<a name="Using_a_tarball_to_distribute_Ha" id="Using_a_tarball_to_distribute_Ha"></a>
<p>When provisioning Hadoop, HOD can use either a pre-installed Hadoop on the cluster nodes or distribute and install a Hadoop tarball as part 
  of the provisioning operation. If the tarball option is being used, there is no need to have a pre-installed Hadoop on the cluster nodes, nor a need 
  to use a pre-installed one. This is especially useful in a development / QE environment where individual developers may have different versions of 
  Hadoop to test on a shared cluster. </p>
<p>In order to use a pre-installed Hadoop, you must specify, in the hodrc, the <span class="codefrag">pkgs</span> option in the <span class="codefrag">gridservice-hdfs</span> 
  and <span class="codefrag">gridservice-mapred</span> sections. This must point to the path where Hadoop is installed on all nodes of the cluster.</p>
<p>The syntax for specifying tarball is as follows:</p>
<pre class="code">$ hod allocate -d cluster_dir -n number_of_nodes -t hadoop_tarball_location</pre>
<p>For example, the following command allocates Hadoop provided by the tarball <span class="codefrag">~/share/hadoop.tar.gz</span>:</p>
<pre class="code">$ hod allocate -d ~/hadoop-cluster -n 10 -t ~/share/hadoop.tar.gz</pre>
<p>Similarly, when using hod script, the syntax is as follows:</p>
<pre class="code">$ hod script -d cluster_directory -s script_file -n number_of_nodes -t hadoop_tarball_location</pre>
<p>The hadoop_tarball specified in the syntax above should point to a path on a shared file system that is accessible from all the compute nodes. 
  Currently, HOD only supports NFS mounted file systems.</p>
<p>
<em>Note:</em>
</p>
<ul>
    
<li> For better distribution performance it is recommended that the Hadoop tarball contain only the libraries and binaries, and not the source or documentation.</li>
    
    
<li> When you want to run jobs against a cluster allocated using the tarball, you must use a compatible version of hadoop to submit your jobs. 
    The best would be to untar and use the version that is present in the tarball itself.</li>
    
<li> You need to make sure that there are no Hadoop configuration files, hadoop-env.sh and hadoop-site.xml, present in the conf directory of the
     tarred distribution. The presence of these files with incorrect values could make the cluster allocation to fail.</li>
  
</ul>
<a name="N1019D"></a><a name="Using+an+External+HDFS"></a>
<h4> Using an External HDFS </h4>
<a name="Using_an_external_HDFS" id="Using_an_external_HDFS"></a>
<p>In typical Hadoop clusters provisioned by HOD, HDFS is already set up statically (without using HOD). This allows data to persist in HDFS after 
  the HOD provisioned clusters is deallocated. To use a statically configured HDFS, your hodrc must point to an external HDFS. Specifically, set the 
  following options to the correct values in the section <span class="codefrag">gridservice-hdfs</span> of the hodrc:</p>
<pre class="code">
external = true
host = Hostname of the HDFS NameNode
fs_port = Port number of the HDFS NameNode
info_port = Port number of the HDFS NameNode web UI
</pre>
<p>
<em>Note:</em> You can also enable this option from command line. That is, to use a static HDFS, you will need to say: <br>
    
</p>
<pre class="code">$ hod allocate -d cluster_dir -n number_of_nodes --gridservice-hdfs.external</pre>
<p>HOD can be used to provision an HDFS cluster as well as a MapReduce cluster, if required. To do so, set the following option in the section 
  <span class="codefrag">gridservice-hdfs</span> of the hodrc:</p>
<pre class="code">external = false</pre>
<a name="N101C4"></a><a name="Options+for+Configuring+Hadoop"></a>
<h4> Options for Configuring Hadoop </h4>
<a name="Options_for_Configuring_Hadoop" id="Options_for_Configuring_Hadoop"></a>
<p>HOD provides a very convenient mechanism to configure both the Hadoop daemons that it provisions and also the hadoop-site.xml that 
  it generates on the client side. This is done by specifying Hadoop configuration parameters in either the HOD configuration file, or from the 
  command line when allocating clusters.</p>
<p>
<strong> Configuring Hadoop Daemons </strong>
</p>
<a name="Configuring_Hadoop_Daemons" id="Configuring_Hadoop_Daemons"></a>
<p>For configuring the Hadoop daemons, you can do the following:</p>
<p>For MapReduce, specify the options as a comma separated list of key-value pairs to the <span class="codefrag">server-params</span> option in the 
  <span class="codefrag">gridservice-mapred</span> section. Likewise for a dynamically provisioned HDFS cluster, specify the options in the 
  <span class="codefrag">server-params</span> option in the <span class="codefrag">gridservice-hdfs</span> section. If these parameters should be marked as 
  <em>final</em>, then include these in the <span class="codefrag">final-server-params</span> option of the appropriate section.</p>
<p>For example:</p>
<pre class="code">
server-params = mapred.reduce.parallel.copies=20,io.sort.factor=100,io.sort.mb=128,io.file.buffer.size=131072
final-server-params = mapred.child.java.opts=-Xmx512m,dfs.block.size=134217728,fs.inmemory.size.mb=128   
</pre>
<p>In order to provide the options from command line, you can use the following syntax:</p>
<p>For configuring the MapReduce daemons use:</p>
<pre class="code">$ hod allocate -d cluster_dir -n number_of_nodes -Mmapred.reduce.parallel.copies=20 -Mio.sort.factor=100</pre>
<p>In the example above, the <em>mapred.reduce.parallel.copies</em> parameter and the <em>io.sort.factor</em> 
  parameter will be appended to the other <span class="codefrag">server-params</span> or if they already exist in <span class="codefrag">server-params</span>, 
  will override them. In order to specify these are <em>final</em> parameters, you can use:</p>
<pre class="code">$ hod allocate -d cluster_dir -n number_of_nodes -Fmapred.reduce.parallel.copies=20 -Fio.sort.factor=100</pre>
<p>However, note that final parameters cannot be overwritten from command line. They can only be appended if not already specified.</p>
<p>Similar options exist for configuring dynamically provisioned HDFS daemons. For doing so, replace -M with -H and -F with -S.</p>
<p>
<strong> Configuring Hadoop Job Submission (Client) Programs </strong>
</p>
<a name="Configuring_Hadoop_Job_Submissio" id="Configuring_Hadoop_Job_Submissio"></a>
<p>As mentioned above, if the allocation operation completes successfully then <span class="codefrag">cluster_dir/hadoop-site.xml</span> will be generated 
  and will contain information about the allocated cluster's JobTracker and NameNode. This configuration is used when submitting jobs to the cluster. 
  HOD provides an option to include additional Hadoop configuration parameters into this file. The syntax for doing so is as follows:</p>
<pre class="code">$ hod allocate -d cluster_dir -n number_of_nodes -Cmapred.userlog.limit.kb=200 -Cmapred.child.java.opts=-Xmx512m</pre>
<p>In this example, the <em>mapred.userlog.limit.kb</em> and <em>mapred.child.java.opts</em> options will be included into 
  the hadoop-site.xml that is generated by HOD.</p>
<a name="N10233"></a><a name="Viewing+Hadoop+Web-UIs"></a>
<h4> Viewing Hadoop Web-UIs </h4>
<a name="Viewing_Hadoop_Web_UIs" id="Viewing_Hadoop_Web_UIs"></a>
<p>The HOD allocation operation prints the JobTracker and NameNode web UI URLs. For example:</p>
<pre class="code">
$ hod allocate -d ~/hadoop-cluster -n 10 -c ~/hod-conf-dir/hodrc
INFO - HDFS UI on http://host242.foo.com:55391
INFO - Mapred UI on http://host521.foo.com:54874
</pre>
<p>The same information is also available via the <em>info</em> operation described above.</p>
<a name="N10248"></a><a name="Collecting+and+Viewing+Hadoop+Logs"></a>
<h4> Collecting and Viewing Hadoop Logs </h4>
<a name="Collecting_and_Viewing_Hadoop_Lo" id="Collecting_and_Viewing_Hadoop_Lo"></a>
<p>To get the Hadoop logs of the daemons running on one of the allocated nodes: </p>
<ul>
    
<li> Log into the node of interest. If you want to look at the logs of the JobTracker or NameNode, then you can find the node running these by 
    using the <em>list</em> and <em>info</em> operations mentioned above.</li>
    
<li> Get the process information of the daemon of interest (for example, <span class="codefrag">ps ux | grep TaskTracker</span>)</li>
    
<li> In the process information, search for the value of the variable <span class="codefrag">-Dhadoop.log.dir</span>. Typically this will be a decendent directory 
    of the <span class="codefrag">hodring.temp-dir</span> value from the hod configuration file.</li>
    
<li> Change to the <span class="codefrag">hadoop.log.dir</span> directory to view daemon and user logs.</li>
  
</ul>
<p>HOD also provides a mechanism to collect logs when a cluster is being deallocated and persist them into a file system, or an externally 
  configured HDFS. By doing so, these logs can be viewed after the jobs are completed and the nodes are released. In order to do so, configure 
  the log-destination-uri to a URI as follows:</p>
<pre class="code">
log-destination-uri = hdfs://host123:45678/user/hod/logs
log-destination-uri = file://path/to/store/log/files</pre>
<p>Under the root directory specified above in the path, HOD will create a path user_name/torque_jobid and store gzipped log files for each 
  node that was part of the job.</p>
<p>Note that to store the files to HDFS, you may need to configure the <span class="codefrag">hodring.pkgs</span> option with the Hadoop version that 
  matches the HDFS mentioned. If not, HOD will try to use the Hadoop version that it is using to provision the Hadoop cluster itself.</p>
<a name="N10284"></a><a name="Auto-deallocation+of+Idle+Clusters"></a>
<h4> Auto-deallocation of Idle Clusters </h4>
<a name="Auto_deallocation_of_Idle_Cluste" id="Auto_deallocation_of_Idle_Cluste"></a>
<p>HOD automatically deallocates clusters that are not running Hadoop jobs for a given period of time. Each HOD allocation includes a 
  monitoring facility that constantly checks for running Hadoop jobs. If it detects no running Hadoop jobs for a given period, it will automatically 
  deallocate its own cluster and thus free up nodes which are not being used effectively.</p>
<p>
<em>Note:</em> While the cluster is deallocated, the <em>cluster directory</em> is not cleaned up automatically. The user must 
  deallocate this cluster through the regular <em>deallocate</em> operation to clean this up.</p>
<a name="N1029A"></a><a name="Specifying+Additional+Job+Attributes"></a>
<h4> Specifying Additional Job Attributes </h4>
<a name="Specifying_Additional_Job_Attrib" id="Specifying_Additional_Job_Attrib"></a>
<p>HOD allows the user to specify a wallclock time and a name (or title) for a Torque job. </p>
<p>The wallclock time is the estimated amount of time for which the Torque job will be valid. After this time has expired, Torque will 
  automatically delete the job and free up the nodes. Specifying the wallclock time can also help the job scheduler to better schedule 
  jobs, and help improve utilization of cluster resources.</p>
<p>To specify the wallclock time, use the following syntax:</p>
<pre class="code">$ hod allocate -d cluster_dir -n number_of_nodes -l time_in_seconds</pre>
<p>The name or title of a Torque job helps in user friendly identification of the job. The string specified here will show up in all information 
  where Torque job attributes are displayed, including the <span class="codefrag">qstat</span> command.</p>
<p>To specify the name or title, use the following syntax:</p>
<pre class="code">$ hod allocate -d cluster_dir -n number_of_nodes -N name_of_job</pre>
<p>
<em>Note:</em> Due to restriction in the underlying Torque resource manager, names which do not start with an alphabet character 
  or contain a 'space' will cause the job to fail. The failure message points to the problem being in the specified job name.</p>
<a name="N102C1"></a><a name="Capturing+HOD+Exit+Codes+in+Torque"></a>
<h4> Capturing HOD Exit Codes in Torque </h4>
<a name="Capturing_HOD_exit_codes_in_Torq" id="Capturing_HOD_exit_codes_in_Torq"></a>
<p>HOD exit codes are captured in the Torque exit_status field. This will help users and system administrators to distinguish successful 
  runs from unsuccessful runs of HOD. The exit codes are 0 if allocation succeeded and all hadoop jobs ran on the allocated cluster correctly. 
  They are non-zero if allocation failed or some of the hadoop jobs failed on the allocated cluster. The exit codes that are possible are 
  mentioned in the table below. <em>Note: Hadoop job status is captured only if the version of Hadoop used is 16 or above.</em>
</p>
<table class="ForrestTable" cellspacing="1" cellpadding="4">
    
      
<tr>
        
<th colspan="1" rowspan="1"> Exit Code </th>
        <th colspan="1" rowspan="1"> Meaning </th>
      
</tr>
      
<tr>
        
<td colspan="1" rowspan="1"> 6 </td>
        <td colspan="1" rowspan="1"> Ringmaster failure </td>
      
</tr>
      
<tr>
        
<td colspan="1" rowspan="1"> 7 </td>
        <td colspan="1" rowspan="1"> HDFS failure </td>
      
</tr>
      
<tr>
        
<td colspan="1" rowspan="1"> 8 </td>
        <td colspan="1" rowspan="1"> Job tracker failure </td>
      
</tr>
      
<tr>
        
<td colspan="1" rowspan="1"> 10 </td>
        <td colspan="1" rowspan="1"> Cluster dead </td>
      
</tr>
      
<tr>
        
<td colspan="1" rowspan="1"> 12 </td>
        <td colspan="1" rowspan="1"> Cluster already allocated </td>
      
</tr>
      
<tr>
        
<td colspan="1" rowspan="1"> 13 </td>
        <td colspan="1" rowspan="1"> HDFS dead </td>
      
</tr>
      
<tr>
        
<td colspan="1" rowspan="1"> 14 </td>
        <td colspan="1" rowspan="1"> Mapred dead </td>
      
</tr>
      
<tr>
        
<td colspan="1" rowspan="1"> 16 </td>
        <td colspan="1" rowspan="1"> All MapReduce jobs that ran on the cluster failed. Refer to hadoop logs for more details. </td>
      
</tr>
      
<tr>
        
<td colspan="1" rowspan="1"> 17 </td>
        <td colspan="1" rowspan="1"> Some of the MapReduce jobs that ran on the cluster failed. Refer to hadoop logs for more details. </td>
      
</tr>
    
  
</table>
<a name="N10353"></a><a name="Command+Line"></a>
<h4> Command Line</h4>
<a name="Command_Line" id="Command_Line"></a>
<p>HOD command line has the following general syntax:</p>
<pre class="code">hod &lt;operation&gt; [ARGS] [OPTIONS]</pre>
<p> Allowed operations are 'allocate', 'deallocate', 'info', 'list', 'script' and 'help'. For help with a particular operation do: </p>
<pre class="code">hod help &lt;operation&gt;</pre>
<p>To have a look at possible options do:</p>
<pre class="code">hod help options</pre>
<ul>

      
<li>
<em>allocate</em>
<br>
      
<em>Usage : hod allocate -d cluster_dir -n number_of_nodes [OPTIONS]</em>
<br>
        Allocates a cluster on the given number of cluster nodes, and store the allocation information in cluster_dir for use with subsequent 
        <span class="codefrag">hadoop</span> commands. Note that the <span class="codefrag">cluster_dir</span> must exist before running the command.</li>
        
      
<li>
<em>list</em>
<br>
      
<em>Usage : hod list [OPTIONS]</em>
<br>
       Lists the clusters allocated by this user. Information provided includes the Torque job id corresponding to the cluster, the cluster 
       directory where the allocation information is stored, and whether the MapReduce daemon is still active or not.</li>
       
      
<li>
<em>info</em>
<br>
      
<em>Usage : hod info -d cluster_dir [OPTIONS]</em>
<br>
        Lists information about the cluster whose allocation information is stored in the specified cluster directory.</li>
        
      
<li>
<em>deallocate</em>
<br>
      
<em>Usage : hod deallocate -d cluster_dir [OPTIONS]</em>
<br>
        Deallocates the cluster whose allocation information is stored in the specified cluster directory.</li>
        
      
<li>
<em>script</em>
<br>
      
<em>Usage : hod script -s script_file -d cluster_directory -n number_of_nodes [OPTIONS]</em>
<br>
        Runs a hadoop script using HOD<em>script</em> operation. Provisions Hadoop on a given number of nodes, executes the given 
        script from the submitting node, and deallocates the cluster when the script completes.</li>
        
      
<li>
<em>help</em>
<br>
      
<em>Usage : hod help [operation | 'options']</em>
<br>
       When no argument is specified, <span class="codefrag">hod help</span> gives the usage and basic options, and is equivalent to 
       <span class="codefrag">hod --help</span> (See below). When 'options' is given as argument, hod displays only the basic options 
       that hod takes. When an operation is specified, it displays the usage and description corresponding to that particular 
       operation. For e.g, to know about allocate operation, one can do a <span class="codefrag">hod help allocate</span>
</li>
    
</ul>
<p>Besides the operations, HOD can take the following command line options.</p>
<ul>

      
<li>
<em>--help</em>
<br>
        Prints out the help message to see the usage and basic options.</li>
        
      
<li>
<em>--verbose-help</em>
<br>
        All configuration options provided in the hodrc file can be passed on the command line, using the syntax 
        <span class="codefrag">--section_name.option_name[=value]</span>. When provided this way, the value provided on command line 
        overrides the option provided in hodrc. The verbose-help command lists all the available options in the hodrc file. 
        This is also a nice way to see the meaning of the configuration options. <br>"</li>
        
</ul>
<p>See <a href="#Options_Configuring_HOD">Options Configuring HOD</a> for a description of most important hod configuration options. 
       For basic options do <span class="codefrag">hod help options</span> and for all options possible in hod configuration do <span class="codefrag">hod --verbose-help</span>. 
       See <a href="#HOD+Configuration">HOD Configuration</a> for a description of all options.</p>
<a name="N103E9"></a><a name="Options+Configuring+HOD"></a>
<h4> Options Configuring HOD </h4>
<a name="Options_Configuring_HOD" id="Options_Configuring_HOD"></a>
<p>As described above, HOD is configured using a configuration file that is usually set up by system administrators. 
  This is a INI style configuration file that is divided into sections, and options inside each section. Each section relates 
  to one of the HOD processes: client, ringmaster, hodring, mapreduce or hdfs. The options inside a section comprise 
  of an option name and value. </p>
<p>Users can override the configuration defined in the default configuration in two ways: </p>
<ul>
    
<li> Users can supply their own configuration file to HOD in each of the commands, using the <span class="codefrag">-c</span> option</li>
    
<li> Users can supply specific configuration options to HOD/ Options provided on command line <em>override</em> 
    the values provided in the configuration file being used.</li>
  
</ul>
<p>This section describes some of the most commonly used configuration options. These commonly used options are 
  provided with a <em>short</em> option for convenience of specification. All other options can be specified using 
  a <em>long</em> option that is also described below.</p>
<ul>

  
<li>
<em>-c config_file</em>
<br>
    Provides the configuration file to use. Can be used with all other options of HOD. Alternatively, the 
    <span class="codefrag">HOD_CONF_DIR</span> environment variable can be defined to specify a directory that contains a file 
    named <span class="codefrag">hodrc</span>, alleviating the need to specify the configuration file in each HOD command.</li>
    
  
<li>
<em>-d cluster_dir</em>
<br>
        This is required for most of the hod operations. As described under <a href="#Create_a_Cluster_Directory">Create a Cluster Directory</a>, 
        the <em>cluster directory</em> is a directory on the local file system where <span class="codefrag">hod</span> will generate the Hadoop configuration, 
        <em>hadoop-site.xml</em>, corresponding to the cluster it allocates. Pass it to the <span class="codefrag">hod</span> operations as an argument 
        to -d or --hod.clusterdir. If it doesn't already exist, HOD will automatically try to create it and use it. Once a cluster is allocated, a 
        user can utilize it to run Hadoop jobs by specifying the clusterdirectory as the Hadoop --config option.</li>
        
  
<li>
<em>-n number_of_nodes</em>
<br>
  This is required for the hod 'allocation' operation and for script operation. This denotes the number of nodes to be allocated.</li>
  
  
<li>
<em>-s script-file</em>
<br>
   Required when using script operation, specifies the script file to execute.</li>
   
 
<li>
<em>-b 1|2|3|4</em>
<br>
    Enables the given debug level. Can be used with all other options of HOD. 4 is most verbose.</li>
    
  
<li>
<em>-t hadoop_tarball</em>
<br>
    Provisions Hadoop from the given tar.gz file. This option is only applicable to the <em>allocate</em> operation. For better 
    distribution performance it is strongly recommended that the Hadoop tarball is created <em>after</em> removing the source 
    or documentation.</li>
    
  
<li>
<em>-N job-name</em>
<br>
    The Name to give to the resource manager job that HOD uses underneath. For e.g. in the case of Torque, this translates to 
    the <span class="codefrag">qsub -N</span> option, and can be seen as the job name using the <span class="codefrag">qstat</span> command.</li>
    
  
<li>
<em>-l wall-clock-time</em>
<br>
    The amount of time for which the user expects to have work on the allocated cluster. This is passed to the resource manager 
    underneath HOD, and can be used in more efficient scheduling and utilization of the cluster. Note that in the case of Torque, 
    the cluster is automatically deallocated after this time expires.</li>
    
  
<li>
<em>-j java-home</em>
<br>
    Path to be set to the JAVA_HOME environment variable. This is used in the <em>script</em> operation. HOD sets the 
    JAVA_HOME environment variable tot his value and launches the user script in that.</li>
    
  
<li>
<em>-A account-string</em>
<br>
    Accounting information to pass to underlying resource manager.</li>
    
  
<li>
<em>-Q queue-name</em>
<br>
    Name of the queue in the underlying resource manager to which the job must be submitted.</li>
    
  
<li>
<em>-Mkey1=value1 -Mkey2=value2</em>
<br>
    Provides configuration parameters for the provisioned MapReduce daemons (JobTracker and TaskTrackers). A 
    hadoop-site.xml is generated with these values on the cluster nodes. <br>
    
<em>Note:</em> Values which have the following characters: space, comma, equal-to, semi-colon need to be 
    escaped with a '\' character, and need to be enclosed within quotes. You can escape a '\' with a '\' too. </li>
    
  
<li>
<em>-Hkey1=value1 -Hkey2=value2</em>
<br>
    Provides configuration parameters for the provisioned HDFS daemons (NameNode and DataNodes). A hadoop-site.xml 
    is generated with these values on the cluster nodes <br>
    
<em>Note:</em> Values which have the following characters: space, comma, equal-to, semi-colon need to be 
    escaped with a '\' character, and need to be enclosed within quotes. You can escape a '\' with a '\' too. </li>
    
  
<li>
<em>-Ckey1=value1 -Ckey2=value2</em>
<br>
    Provides configuration parameters for the client from where jobs can be submitted. A hadoop-site.xml is generated 
    with these values on the submit node. <br>
    
<em>Note:</em> Values which have the following characters: space, comma, equal-to, semi-colon need to be 
    escaped with a '\' character, and need to be enclosed within quotes. You can escape a '\' with a '\' too. </li>
    
  
<li>
<em>--section-name.option-name=value</em>
<br>
    This is the method to provide options using the <em>long</em> format. For e.g. you could say <em>--hod.script-wait-time=20</em>
</li>
   
</ul>
<a name="N104A6"></a><a name="Troubleshooting-N104A6"></a>
<h3 class="h4"> Troubleshooting </h3>
<a name="Troubleshooting" id="Troubleshooting"></a>
<p>The following section identifies some of the most likely error conditions users can run into when using HOD and ways to trouble-shoot them</p>
<a name="N104B1"></a><a name="HOD+Hangs+During+Allocation"></a>
<h4>HOD Hangs During Allocation </h4>
<a name="_hod_Hangs_During_Allocation" id="_hod_Hangs_During_Allocation"></a><a name="hod_Hangs_During_Allocation" id="hod_Hangs_During_Allocation"></a>
<p>
<em>Possible Cause:</em> One of the HOD or Hadoop components have failed to come up. In such a case, the 
  <span class="codefrag">hod</span> command will return after a few minutes (typically 2-3 minutes) with an error code of either 7 or 8 
  as defined in the Error Codes section. Refer to that section for further details. </p>
<p>
<em>Possible Cause:</em> A large allocation is fired with a tarball. Sometimes due to load in the network, or on 
  the allocated nodes, the tarball distribution might be significantly slow and take a couple of minutes to come back. 
  Wait for completion. Also check that the tarball does not have the Hadoop sources or documentation.</p>
<p>
<em>Possible Cause:</em> A Torque related problem. If the cause is Torque related, the <span class="codefrag">hod</span> 
  command will not return for more than 5 minutes. Running <span class="codefrag">hod</span> in debug mode may show the 
  <span class="codefrag">qstat</span> command being executed repeatedly. Executing the <span class="codefrag">qstat</span> command from 
  a separate shell may show that the job is in the <span class="codefrag">Q</span> (Queued) state. This usually indicates a 
  problem with Torque. Possible causes could include some nodes being down, or new nodes added that Torque 
  is not aware of. Generally, system administator help is needed to resolve this problem.</p>
<a name="N104DD"></a><a name="HOD+Hangs+During+Deallocation"></a>
<h4>HOD Hangs During Deallocation </h4>
<a name="_hod_Hangs_During_Deallocation" id="_hod_Hangs_During_Deallocation"></a><a name="hod_Hangs_During_Deallocation" id="hod_Hangs_During_Deallocation"></a>
<p>
<em>Possible Cause:</em> A Torque related problem, usually load on the Torque server, or the allocation is very large. 
  Generally, waiting for the command to complete is the only option.</p>
<a name="N104ED"></a><a name="HOD+Fails+With+an+Error+Code+and+Error+Message"></a>
<h4>HOD Fails With an Error Code and Error Message </h4>
<a name="hod_Fails_With_an_error_code_and" id="hod_Fails_With_an_error_code_and"></a><a name="_hod_Fails_With_an_error_code_an" id="_hod_Fails_With_an_error_code_an"></a>
<p>If the exit code of the <span class="codefrag">hod</span> command is not <span class="codefrag">0</span>, then refer to the following table 
  of error exit codes to determine why the code may have occurred and how to debug the situation.</p>
<p>
<strong> Error Codes </strong>
</p>
<a name="Error_Codes" id="Error_Codes"></a>
<table class="ForrestTable" cellspacing="1" cellpadding="4">
    
      
<tr>
        
<th colspan="1" rowspan="1">Error Code</th>
        <th colspan="1" rowspan="1">Meaning</th>
        <th colspan="1" rowspan="1">Possible Causes and Remedial Actions</th>
      
</tr>
      
<tr>
        
<td colspan="1" rowspan="1"> 1 </td>
        <td colspan="1" rowspan="1"> Configuration error </td>
        <td colspan="1" rowspan="1"> Incorrect configuration values specified in hodrc, or other errors related to HOD configuration. 
        The error messages in this case must be sufficient to debug and fix the problem. </td>
      
</tr>
      
<tr>
        
<td colspan="1" rowspan="1"> 2 </td>
        <td colspan="1" rowspan="1"> Invalid operation </td>
        <td colspan="1" rowspan="1"> Do <span class="codefrag">hod help</span> for the list of valid operations. </td>
      
</tr>
      
<tr>
        
<td colspan="1" rowspan="1"> 3 </td>
        <td colspan="1" rowspan="1"> Invalid operation arguments </td>
        <td colspan="1" rowspan="1"> Do <span class="codefrag">hod help operation</span> for listing the usage of a particular operation.</td>
      
</tr>
      
<tr>
        
<td colspan="1" rowspan="1"> 4 </td>
        <td colspan="1" rowspan="1"> Scheduler failure </td>
        <td colspan="1" rowspan="1"> 1. Requested more resources than available. Run <span class="codefrag">checknodes cluster_name</span> to see if enough nodes are available. <br>
          2. Requested resources exceed resource manager limits. <br>
          3. Torque is misconfigured, the path to Torque binaries is misconfigured, or other Torque problems. Contact system administrator. </td>
      
</tr>
      
<tr>
        
<td colspan="1" rowspan="1"> 5 </td>
        <td colspan="1" rowspan="1"> Job execution failure </td>
        <td colspan="1" rowspan="1"> 1. Torque Job was deleted from outside. Execute the Torque <span class="codefrag">qstat</span> command to see if you have any jobs in the 
        <span class="codefrag">R</span> (Running) state. If none exist, try re-executing HOD. <br>
          2. Torque problems such as the server momentarily going down, or becoming unresponsive. Contact system administrator. <br>
          3. The system administrator might have configured account verification, and an invalid account is specified. Contact system administrator.</td>
      
</tr>
      
<tr>
        
<td colspan="1" rowspan="1"> 6 </td>
        <td colspan="1" rowspan="1"> Ringmaster failure </td>
        <td colspan="1" rowspan="1"> HOD prints the message "Cluster could not be allocated because of the following errors on the ringmaster host &lt;hostname&gt;". 
        The actual error message may indicate one of the following:<br>
          1. Invalid configuration on the node running the ringmaster, specified by the hostname in the error message.<br>
          2. Invalid configuration in the <span class="codefrag">ringmaster</span> section,<br>
          3. Invalid <span class="codefrag">pkgs</span> option in <span class="codefrag">gridservice-mapred or gridservice-hdfs</span> section,<br>
          4. An invalid hadoop tarball, or a tarball which has bundled an invalid configuration file in the conf directory,<br>
          5. Mismatched version in Hadoop between the MapReduce and an external HDFS.<br>
          The Torque <span class="codefrag">qstat</span> command will most likely show a job in the <span class="codefrag">C</span> (Completed) state. <br>
          One can login to the ringmaster host as given by HOD failure message and debug the problem with the help of the error message. 
          If the error message doesn't give complete information, ringmaster logs should help finding out the root cause of the problem. 
          Refer to the section <em>Locating Ringmaster Logs</em> below for more information. </td>
      
</tr>
      
<tr>
        
<td colspan="1" rowspan="1"> 7 </td>
        <td colspan="1" rowspan="1"> HDFS failure </td>
        <td colspan="1" rowspan="1"> When HOD fails to allocate due to HDFS failures (or Job tracker failures, error code 8, see below), it prints a failure message 
        "Hodring at &lt;hostname&gt; failed with following errors:" and then gives the actual error message, which may indicate one of the following:<br>
          1. Problem in starting Hadoop clusters. Usually the actual cause in the error message will indicate the problem on the hostname mentioned. 
          Also, review the Hadoop related configuration in the HOD configuration files. Look at the Hadoop logs using information specified in 
          <em>Collecting and Viewing Hadoop Logs</em> section above. <br>
          2. Invalid configuration on the node running the hodring, specified by the hostname in the error message <br>
          3. Invalid configuration in the <span class="codefrag">hodring</span> section of hodrc. <span class="codefrag">ssh</span> to the hostname specified in the 
          error message and grep for <span class="codefrag">ERROR</span> or <span class="codefrag">CRITICAL</span> in hodring logs. Refer to the section 
          <em>Locating Hodring Logs</em> below for more information. <br>
          4. Invalid tarball specified which is not packaged correctly. <br>
          5. Cannot communicate with an externally configured HDFS.<br>
          When such HDFS or Job tracker failure occurs, one can login into the host with hostname mentioned in HOD failure message and debug the problem. 
          While fixing the problem, one should also review other log messages in the ringmaster log to see which other machines also might have had problems 
          bringing up the jobtracker/namenode, apart from the hostname that is reported in the failure message. This possibility of other machines also having problems 
          occurs because HOD continues to try and launch hadoop daemons on multiple machines one after another depending upon the value of the configuration 
          variable <a href="hod_scheduler.html#ringmaster+options">ringmaster.max-master-failures</a>. 
          See <a href="hod_scheduler.html#Locating+Ringmaster+Logs">Locating Ringmaster Logs</a> for more information.</td>
      
</tr>
      
<tr>
        
<td colspan="1" rowspan="1"> 8 </td>
        <td colspan="1" rowspan="1"> Job tracker failure </td>
        <td colspan="1" rowspan="1"> Similar to the causes in <em>DFS failure</em> case. </td>
      
</tr>
      
<tr>
        
<td colspan="1" rowspan="1"> 10 </td>
        <td colspan="1" rowspan="1"> Cluster dead </td>
        <td colspan="1" rowspan="1"> 1. Cluster was auto-deallocated because it was idle for a long time. <br>
          2. Cluster was auto-deallocated because the wallclock time specified by the system administrator or user was exceeded. <br>
          3. Cannot communicate with the JobTracker and HDFS NameNode which were successfully allocated. Deallocate the cluster, and allocate again. </td>
      
</tr>
      
<tr>
        
<td colspan="1" rowspan="1"> 12 </td>
        <td colspan="1" rowspan="1"> Cluster already allocated </td>
        <td colspan="1" rowspan="1"> The cluster directory specified has been used in a previous allocate operation and is not yet deallocated. 
        Specify a different directory, or deallocate the previous allocation first. </td>
      
</tr>
      
<tr>
        
<td colspan="1" rowspan="1"> 13 </td>
        <td colspan="1" rowspan="1"> HDFS dead </td>
        <td colspan="1" rowspan="1"> Cannot communicate with the HDFS NameNode. HDFS NameNode went down. </td>
      
</tr>
      
<tr>
        
<td colspan="1" rowspan="1"> 14 </td>
        <td colspan="1" rowspan="1"> Mapred dead </td>
        <td colspan="1" rowspan="1"> 1. Cluster was auto-deallocated because it was idle for a long time. <br>
          2. Cluster was auto-deallocated because the wallclock time specified by the system administrator or user was exceeded. <br>
          3. Cannot communicate with the MapReduce JobTracker. JobTracker node went down. <br>
          
</td>
      
</tr>
      
<tr>
        
<td colspan="1" rowspan="1"> 15 </td>
        <td colspan="1" rowspan="1"> Cluster not allocated </td>
        <td colspan="1" rowspan="1"> An operation which requires an allocated cluster is given a cluster directory with no state information. </td>
      
</tr>
   
      
<tr>
        
<td colspan="1" rowspan="1"> Any non-zero exit code </td>
        <td colspan="1" rowspan="1"> HOD script error </td>
        <td colspan="1" rowspan="1"> If the hod script option was used, it is likely that the exit code is from the script. Unfortunately, this could clash with the 
        exit codes of the hod command itself. In order to help users differentiate these two, hod writes the script's exit code to a file 
        called script.exitcode in the cluster directory, if the script returned an exit code. You can cat this file to determine the script's 
        exit code. If it does not exist, then it is a hod command exit code.</td> 
      
</tr>
  
</table>
<a name="N10682"></a><a name="Hadoop+DFSClient+Warns+with+a%0A++NotReplicatedYetException"></a>
<h4>Hadoop DFSClient Warns with a
  NotReplicatedYetException</h4>
<p>Sometimes, when you try to upload a file to the HDFS immediately after
  allocating a HOD cluster, DFSClient warns with a NotReplicatedYetException. It
  usually shows a message something like - </p>
<pre class="code">
WARN hdfs.DFSClient: NotReplicatedYetException sleeping  &lt;filename&gt; retries left 3
08/01/25 16:31:40 INFO hdfs.DFSClient: org.apache.hadoop.ipc.RemoteException: java.io.IOException: 
File &lt;filename&gt; could only be replicated to 0 nodes, instead of 1</pre>
<p> This scenario arises when you try to upload a file
  to the HDFS while the DataNodes are still in the process of contacting the
  NameNode. This can be resolved by waiting for some time before uploading a new
  file to the HDFS, so that enough DataNodes start and contact the NameNode.</p>
<a name="N10692"></a><a name="Hadoop+Jobs+Not+Running+on+a+Successfully+Allocated+Cluster"></a>
<h4> Hadoop Jobs Not Running on a Successfully Allocated Cluster </h4>
<a name="Hadoop_Jobs_Not_Running_on_a_Suc" id="Hadoop_Jobs_Not_Running_on_a_Suc"></a>
<p>This scenario generally occurs when a cluster is allocated, and is left inactive for sometime, and then hadoop jobs 
  are attempted to be run on them. Then Hadoop jobs fail with the following exception:</p>
<pre class="code">08/01/25 16:31:40 INFO ipc.Client: Retrying connect to server: foo.bar.com/1.1.1.1:53567. Already tried 1 time(s).</pre>
<p>
<em>Possible Cause:</em> No Hadoop jobs were run for a significant portion of time. Thus the cluster would have got 
  deallocated as described in the section <em>Auto-deallocation of Idle Clusters</em>. Deallocate the cluster and allocate it again.</p>
<p>
<em>Possible Cause:</em> The wallclock limit specified by the Torque administrator or the <span class="codefrag">-l</span> option 
  defined in the section <em>Specifying Additional Job Attributes</em> was exceeded since allocation time. Thus the cluster 
  would have got released. Deallocate the cluster and allocate it again.</p>
<p>
<em>Possible Cause:</em> There is a version mismatch between the version of the hadoop being used in provisioning 
  (typically via the tarball option) and the external HDFS. Ensure compatible versions are being used.</p>
<p>
<em>Possible Cause:</em> There is a version mismatch between the version of the hadoop client being used to submit
   jobs and the hadoop used in provisioning (typically via the tarball option). Ensure compatible versions are being used.</p>
<p>
<em>Possible Cause:</em> You used one of the options for specifying Hadoop configuration <span class="codefrag">-M or -H</span>, 
  which had special characters like space or comma that were not escaped correctly. Refer to the section 
  <em>Options Configuring HOD</em> for checking how to specify such options correctly.</p>
<a name="N106C9"></a><a name="My+Hadoop+Job+Got+Killed"></a>
<h4> My Hadoop Job Got Killed </h4>
<a name="My_Hadoop_Job_Got_Killed" id="My_Hadoop_Job_Got_Killed"></a>
<p>
<em>Possible Cause:</em> The wallclock limit specified by the Torque administrator or the <span class="codefrag">-l</span> 
  option defined in the section <em>Specifying Additional Job Attributes</em> was exceeded since allocation time. 
  Thus the cluster would have got released. Deallocate the cluster and allocate it again, this time with a larger wallclock time.</p>
<p>
<em>Possible Cause:</em> Problems with the JobTracker node. Refer to the section in <em>Collecting and Viewing Hadoop Logs</em> to get more information.</p>
<a name="N106E4"></a><a name="Hadoop+Job+Fails+with+Message%3A+%27Job+tracker+still+initializing%27"></a>
<h4> Hadoop Job Fails with Message: 'Job tracker still initializing' </h4>
<a name="Hadoop_Job_Fails_with_Message_Jo" id="Hadoop_Job_Fails_with_Message_Jo"></a>
<p>
<em>Possible Cause:</em> The hadoop job was being run as part of the HOD script command, and it started before the JobTracker could come up fully. 
  Allocate the cluster using a large value for the configuration option <span class="codefrag">--hod.script-wait-time</span>.
   Typically a value of 120 should work, though it is typically unnecessary to be that large.</p>
<a name="N106F4"></a><a name="The+Exit+Codes+For+HOD+Are+Not+Getting+Into+Torque"></a>
<h4> The Exit Codes For HOD Are Not Getting Into Torque </h4>
<a name="The_Exit_Codes_For_HOD_Are_Not_G" id="The_Exit_Codes_For_HOD_Are_Not_G"></a>
<p>
<em>Possible Cause:</em> Version 0.16 of hadoop is required for this functionality to work. 
  The version of Hadoop used does not match. Use the required version of Hadoop.</p>
<p>
<em>Possible Cause:</em> The deallocation was done without using the <span class="codefrag">hod</span> 
  command; for e.g. directly using <span class="codefrag">qdel</span>. When the cluster is deallocated in this manner, 
  the HOD processes are terminated using signals. This results in the exit code to be based on the 
  signal number, rather than the exit code of the program.</p>
<a name="N1070C"></a><a name="The+Hadoop+Logs+are+Not+Uploaded+to+HDFS"></a>
<h4> The Hadoop Logs are Not Uploaded to HDFS </h4>
<a name="The_Hadoop_Logs_are_Not_Uploaded" id="The_Hadoop_Logs_are_Not_Uploaded"></a>
<p>
<em>Possible Cause:</em> There is a version mismatch between the version of the hadoop being used for uploading the logs 
  and the external HDFS. Ensure that the correct version is specified in the <span class="codefrag">hodring.pkgs</span> option.</p>
<a name="N1071C"></a><a name="Locating+Ringmaster+Logs"></a>
<h4> Locating Ringmaster Logs </h4>
<a name="Locating_Ringmaster_Logs" id="Locating_Ringmaster_Logs"></a>
<p>To locate the ringmaster logs, follow these steps: </p>
<ul>
    
<li> Execute hod in the debug mode using the -b option. This will print the Torque job id for the current run.</li>
    
<li> Execute <span class="codefrag">qstat -f torque_job_id</span> and look up the value of the <span class="codefrag">exec_host</span> parameter in the output. 
    The first host in this list is the ringmaster node.</li>
    
<li> Login to this node.</li>
    
<li> The ringmaster log location is specified by the <span class="codefrag">ringmaster.log-dir</span> option in the hodrc. The name of the log file will be 
    <span class="codefrag">username.torque_job_id/ringmaster-main.log</span>.</li>
    
<li> If you don't get enough information, you may want to set the ringmaster debug level to 4. This can be done by passing 
    <span class="codefrag">--ringmaster.debug 4</span> to the hod command line.</li>
  
</ul>
<a name="N10748"></a><a name="Locating+Hodring+Logs"></a>
<h4> Locating Hodring Logs </h4>
<a name="Locating_Hodring_Logs" id="Locating_Hodring_Logs"></a>
<p>To locate hodring logs, follow the steps below: </p>
<ul>
    
<li> Execute hod in the debug mode using the -b option. This will print the Torque job id for the current run.</li>
    
<li> Execute <span class="codefrag">qstat -f torque_job_id</span> and look up the value of the <span class="codefrag">exec_host</span> parameter in the output. 
    All nodes in this list should have a hodring on them.</li>
    
<li> Login to any of these nodes.</li>
    
<li> The hodring log location is specified by the <span class="codefrag">hodring.log-dir</span> option in the hodrc. The name of the log file will be 
    <span class="codefrag">username.torque_job_id/hodring-main.log</span>.</li>
    
<li> If you don't get enough information, you may want to set the hodring debug level to 4. This can be done by passing 
    <span class="codefrag">--hodring.debug 4</span> to the hod command line.</li>
  
</ul>
</div>
	  
	  
	  
<!-- HOD ADMINISTRATORS -->

  
<a name="N10778"></a><a name="HOD+Administrators"></a>
<h2 class="h3">HOD Administrators</h2>
<div class="section">
<p>This section show administrators how to install, configure and run HOD.</p>
<a name="N10781"></a><a name="Getting+Started-N10781"></a>
<h3 class="h4">Getting Started</h3>
<p>The basic system architecture of HOD includes these components:</p>
<ul>
  
<li>A Resource manager, possibly together with a scheduler (see <a href="hod_scheduler.html#Prerequisites"> Prerequisites</a>) </li>
  
<li>Various HOD components</li>
  
<li>Hadoop MapReduce and HDFS daemons</li>

</ul>
<p>
HOD provisions and maintains Hadoop MapReduce and, optionally, HDFS instances 
through interaction with the above components on a given cluster of nodes. A cluster of
nodes can be thought of as comprising two sets of nodes:</p>
<ul>
  
<li>Submit nodes: Users use the HOD client on these nodes to allocate clusters, and then
use the Hadoop client to submit Hadoop jobs. </li>
  
<li>Compute nodes: Using the resource manager, HOD components are run on these nodes to 
provision the Hadoop daemons. After that Hadoop jobs run on them.</li>

</ul>
<p>
Here is a brief description of the sequence of operations in allocating a cluster and
running jobs on them.
</p>
<ul>
  
<li>The user uses the HOD client on the Submit node to allocate a desired number of
cluster nodes and to provision Hadoop on them.</li>
  
<li>The HOD client uses a resource manager interface (qsub, in Torque) to submit a HOD
process, called the RingMaster, as a Resource Manager job, to request the user's desired number 
of nodes. This job is submitted to the central server of the resource manager (pbs_server, in Torque).</li>
  
<li>On the compute nodes, the resource manager slave daemons (pbs_moms in Torque) accept
and run jobs that they are assigned by the central server (pbs_server in Torque). The RingMaster 
process is started on one of the compute nodes (mother superior, in Torque).</li>
  
<li>The RingMaster then uses another resource manager interface (pbsdsh, in Torque) to run
the second HOD component, HodRing, as distributed tasks on each of the compute
nodes allocated.</li>
  
<li>The HodRings, after initializing, communicate with the RingMaster to get Hadoop commands, 
and run them accordingly. Once the Hadoop commands are started, they register with the RingMaster,
giving information about the daemons.</li>
  
<li>All the configuration files needed for Hadoop instances are generated by HOD itself, 
some obtained from options given by user in its own configuration file.</li>
  
<li>The HOD client keeps communicating with the RingMaster to find out the location of the 
JobTracker and HDFS daemons.</li>

</ul>
<a name="N107C2"></a><a name="Prerequisites"></a>
<h3 class="h4">Prerequisites</h3>
<p>To use HOD, your system should include the following components.</p>
<ul>


<li>Operating System: HOD is currently tested on RHEL4.</li>


<li>Nodes: HOD requires a minimum of three nodes configured through a resource manager.</li>


<li>Software: The following components must be installed on ALL nodes before using HOD:
<ul>
 
<li>
<a href="http://www.clusterresources.com/pages/products/torque-resource-manager.php">Torque: Resource manager</a>
</li>
 
<li>
<a href="http://www.python.org">Python</a> : HOD requires version 2.5.1 of Python.</li>

</ul>
</li>


<li>Software (optional): The following components are optional and can be installed to obtain better
functionality from HOD:
<ul>
 
<li>
<a href="http://twistedmatrix.com/trac/">Twisted Python</a>: This can be
  used for improving the scalability of HOD. If this module is detected to be
  installed, HOD uses it, else it falls back to default modules.</li>
 
<li>
<a href="http://hadoop.apache.org">Hadoop</a>: HOD can automatically
 distribute Hadoop to all nodes in the cluster. However, it can also use a
 pre-installed version of Hadoop, if it is available on all nodes in the cluster.
  HOD currently supports Hadoop 0.15 and above.</li>

</ul>
</li>


</ul>
<p>Note: HOD configuration requires the location of installs of these
components to be the same on all nodes in the cluster. It will also
make the configuration simpler to have the same location on the submit
nodes.
</p>
<a name="N107F9"></a><a name="Resource+Manager"></a>
<h3 class="h4">Resource Manager</h3>
<p>  Currently HOD works with the Torque resource manager, which it uses for its node
  allocation and job submission. Torque is an open source resource manager from
  <a href="http://www.clusterresources.com">Cluster Resources</a>, a community effort
  based on the PBS project. It provides control over batch jobs and distributed compute nodes. Torque is
  freely available for download from <a href="http://www.clusterresources.com/downloads/torque/">here</a>.
  </p>
<p>  All documentation related to torque can be seen under
  the section TORQUE Resource Manager <a href="http://www.clusterresources.com/pages/resources/documentation.php">here</a>. You can
  get wiki documentation from <a href="http://www.clusterresources.com/wiki/doku.php?id=torque:torque_wiki">here</a>.
  Users may wish to subscribe to TORQUE&rsquo;s mailing list or view the archive for questions,
  comments <a href="http://www.clusterresources.com/pages/resources/mailing-lists.php">here</a>.
</p>
<p>To use HOD with Torque:</p>
<ul>
 
<li>Install Torque components: pbs_server on one node (head node), pbs_mom on all
  compute nodes, and PBS client tools on all compute nodes and submit
  nodes. Perform at least a basic configuration so that the Torque system is up and
  running, that is, pbs_server knows which machines to talk to. Look <a href="http://www.clusterresources.com/wiki/doku.php?id=torque:1.2_basic_configuration">here</a>
  for basic configuration.

  For advanced configuration, see <a href="http://www.clusterresources.com/wiki/doku.php?id=torque:1.3_advanced_configuration">here</a>
</li>
 
<li>Create a queue for submitting jobs on the pbs_server. The name of the queue is the
  same as the HOD configuration parameter, resource-manager.queue. The HOD client uses this queue to
  submit the RingMaster process as a Torque job.</li>
 
<li>Specify a cluster name as a property for all nodes in the cluster.
  This can be done by using the qmgr command. For example:
  <span class="codefrag">qmgr -c "set node node properties=cluster-name"</span>. The name of the cluster is the same as
  the HOD configuration parameter, hod.cluster. </li>
 
<li>Make sure that jobs can be submitted to the nodes. This can be done by
  using the qsub command. For example:
  <span class="codefrag">echo "sleep 30" | qsub -l nodes=3</span>
</li>

</ul>
<a name="N10838"></a><a name="Installing+HOD"></a>
<h3 class="h4">Installing HOD</h3>
<p>Once the resource manager is set up, you can obtain and
install HOD.</p>
<ul>
 
<li>If you are getting HOD from the Hadoop tarball, it is available under the 
  'contrib' section of Hadoop, under the root  directory 'hod'.</li>
 
<li>If you are building from source, you can run ant tar from the Hadoop root
  directory to generate the Hadoop tarball, and then get HOD from there,
  as described above.</li>
 
<li>Distribute the files under this directory to all the nodes in the
  cluster. Note that the location where the files are copied should be
  the same on all the nodes.</li>
  
<li>Note that compiling hadoop would build HOD with appropriate permissions 
  set on all the required script files in HOD.</li>

</ul>
<a name="N10851"></a><a name="Configuring+HOD"></a>
<h3 class="h4">Configuring HOD</h3>
<p>You can configure HOD once it is installed. The minimal configuration needed
to run HOD is described below. More advanced configuration options are discussed
in the HOD Configuration.</p>
<a name="N1085A"></a><a name="Minimal+Configuration"></a>
<h4>Minimal Configuration</h4>
<p>To get started using HOD, the following minimal configuration is
  required:</p>
<ul>
 
<li>On the node from where you want to run HOD, edit the file hodrc
  located in the &lt;install dir&gt;/conf directory. This file
  contains the minimal set of values required to run hod.</li>
 
<li>

<p>Specify values suitable to your environment for the following
  variables defined in the configuration file. Note that some of these
  variables are defined at more than one place in the file.</p>

  
<ul>
   
<li>${JAVA_HOME}: Location of Java for Hadoop. Hadoop supports Sun JDK
    1.6.x and above.</li>
   
<li>${CLUSTER_NAME}: Name of the cluster which is specified in the
    'node property' as mentioned in resource manager configuration.</li>
   
<li>${HADOOP_HOME}: Location of Hadoop installation on the compute and
    submit nodes.</li>
   
<li>${RM_QUEUE}: Queue configured for submitting jobs in the resource
    manager configuration.</li>
   
<li>${RM_HOME}: Location of the resource manager installation on the
    compute and submit nodes.</li>
    
</ul>

</li>


<li>

<p>The following environment variables may need to be set depending on
  your environment. These variables must be defined where you run the
  HOD client and must also be specified in the HOD configuration file as the
  value of the key resource_manager.env-vars. Multiple variables can be
  specified as a comma separated list of key=value pairs.</p>

  
<ul>
   
<li>HOD_PYTHON_HOME: If you install python to a non-default location
    of the compute nodes, or submit nodes, then this variable must be
    defined to point to the python executable in the non-standard
    location.</li>
    
</ul>

</li>

</ul>
<a name="N1088E"></a><a name="Advanced+Configuration"></a>
<h4>Advanced Configuration</h4>
<p> You can review and modify other configuration options to suit
 your specific needs. See <a href="#HOD+Configuration">HOD Configuration</a> for more information.</p>
<a name="N1089D"></a><a name="Running+HOD"></a>
<h3 class="h4">Running HOD</h3>
<p>You can run HOD once it is configured. Refer to <a href="#HOD+Users"> HOD Users</a> for more information.</p>
<a name="N108AB"></a><a name="Supporting+Tools+and+Utilities"></a>
<h3 class="h4">Supporting Tools and Utilities</h3>
<p>This section describes supporting tools and utilities that can be used to
    manage HOD deployments.</p>
<a name="N108B4"></a><a name="logcondense.py+-+Manage+Log+Files"></a>
<h4>logcondense.py - Manage Log Files</h4>
<p>As mentioned under 
         <a href="hod_scheduler.html#Collecting+and+Viewing+Hadoop+Logs">Collecting and Viewing Hadoop Logs</a>,
         HOD can be configured to upload
         Hadoop logs to a statically configured HDFS. Over time, the number of logs uploaded
         to HDFS could increase. logcondense.py is a tool that helps
         administrators to remove log files uploaded to HDFS. </p>
<a name="N108C1"></a><a name="Running+logcondense.py"></a>
<h5>Running logcondense.py</h5>
<p>logcondense.py is available under hod_install_location/support folder. You can either
        run it using python, for example, <em>python logcondense.py</em>, or give execute permissions 
        to the file, and directly run it as <em>logcondense.py</em>. logcondense.py needs to be 
        run by a user who has sufficient permissions to remove files from locations where log 
        files are uploaded in the HDFS, if permissions are enabled. For example as mentioned under
        <a href="hod_scheduler.html#hodring+options">hodring options</a>, the logs could
        be configured to come under the user's home directory in HDFS. In that case, the user
        running logcondense.py should have super user privileges to remove the files from under
        all user home directories.</p>
<a name="N108D5"></a><a name="Command+Line+Options+for+logcondense.py"></a>
<h5>Command Line Options for logcondense.py</h5>
<p>The following command line options are supported for logcondense.py.</p>
<table class="ForrestTable" cellspacing="1" cellpadding="4">
            
<tr>
              
<th colspan="1" rowspan="1">Short Option</th>
              <th colspan="1" rowspan="1">Long option</th>
              <th colspan="1" rowspan="1">Meaning</th>
              <th colspan="1" rowspan="1">Example</th>
            
</tr>
            
<tr>
              
<td colspan="1" rowspan="1">-p</td>
              <td colspan="1" rowspan="1">--package</td>
              <td colspan="1" rowspan="1">Complete path to the hadoop script. The version of hadoop must be the same as the 
                  one running HDFS.</td>
              <td colspan="1" rowspan="1">/usr/bin/hadoop</td>
            
</tr>
            
<tr>
              
<td colspan="1" rowspan="1">-d</td>
              <td colspan="1" rowspan="1">--days</td>
              <td colspan="1" rowspan="1">Delete log files older than the specified number of days</td>
              <td colspan="1" rowspan="1">7</td>
            
</tr>
            
<tr>
              
<td colspan="1" rowspan="1">-c</td>
              <td colspan="1" rowspan="1">--config</td>
              <td colspan="1" rowspan="1">Path to the Hadoop configuration directory, under which hadoop-site.xml resides.
              The hadoop-site.xml must point to the HDFS NameNode from where logs are to be removed.</td>
              <td colspan="1" rowspan="1">/home/foo/hadoop/conf</td>
            
</tr>
            
<tr>
              
<td colspan="1" rowspan="1">-l</td>
              <td colspan="1" rowspan="1">--logs</td>
              <td colspan="1" rowspan="1">A HDFS path, this must be the same HDFS path as specified for the log-destination-uri,
              as mentioned under <a href="hod_scheduler.html#hodring+options">hodring options</a>,
              without the hdfs:// URI string</td>
              <td colspan="1" rowspan="1">/user</td>
            
</tr>
            
<tr>
              
<td colspan="1" rowspan="1">-n</td>
              <td colspan="1" rowspan="1">--dynamicdfs</td>
              <td colspan="1" rowspan="1">If true, this will indicate that the logcondense.py script should delete HDFS logs
              in addition to MapReduce logs. Otherwise, it only deletes MapReduce logs, which is also the
              default if this option is not specified. This option is useful if
              dynamic HDFS installations 
              are being provisioned by HOD, and the static HDFS installation is being used only to collect 
              logs - a scenario that may be common in test clusters.</td>
              <td colspan="1" rowspan="1">false</td>
            
</tr>
            
<tr>
              
<td colspan="1" rowspan="1">-r</td>
              <td colspan="1" rowspan="1">--retain-master-logs</td>
              <td colspan="1" rowspan="1">If true, this will keep the JobTracker logs of job in hod-logs inside HDFS and it 
              will delete only the TaskTracker logs. Also, this will keep the Namenode logs along with 
              JobTracker logs and will only delete the Datanode logs if 'dynamicdfs' options is set 
              to true. Otherwise, it will delete the complete job directory from hod-logs inside 
              HDFS. By default it is set to false.</td>
              <td colspan="1" rowspan="1">false</td>
            
</tr>
          
</table>
<p>So, for example, to delete all log files older than 7 days using a hadoop-site.xml stored in
        ~/hadoop-conf, using the hadoop installation under ~/hadoop-0.17.0, you could say:</p>
<p>
<em>python logcondense.py -p ~/hadoop-0.17.0/bin/hadoop -d 7 -c ~/hadoop-conf -l /user</em>
</p>
<a name="N1098F"></a><a name="checklimits.sh+-+Monitor+Resource+Limits"></a>
<h4>checklimits.sh - Monitor Resource Limits</h4>
<p>checklimits.sh is a HOD tool specific to the Torque/Maui environment
      (<a href="http://www.clusterresources.com/pages/products/maui-cluster-scheduler.php">Maui Cluster Scheduler</a> is an open source job
      scheduler for clusters and supercomputers, from clusterresources). The
      checklimits.sh script
      updates the torque comment field when newly submitted job(s) violate or
      exceed
      over user limits set up in Maui scheduler. It uses qstat, does one pass
      over the torque job-list to determine queued or unfinished jobs, runs Maui
      tool checkjob on each job to see if user limits are violated and then
      runs torque's qalter utility to update job attribute 'comment'. Currently
      it updates the comment as <em>User-limits exceeded. Requested:([0-9]*)
      Used:([0-9]*) MaxLimit:([0-9]*)</em> for those jobs that violate limits.
      This comment field is then used by HOD to behave accordingly depending on
      the type of violation.</p>
<a name="N1099F"></a><a name="Running+checklimits.sh"></a>
<h5>Running checklimits.sh</h5>
<p>checklimits.sh is available under the hod_install_location/support
        folder. This shell script can be run directly as <em>sh
        checklimits.sh </em>or as <em>./checklimits.sh</em> after enabling
        execute permissions. Torque and Maui binaries should be available
        on the machine where the tool is run and should be in the path
        of the shell script process. To update the
        comment field of jobs from different users, this tool must be run with
        torque administrative privileges. This tool must be run repeatedly
        after specific intervals of time to frequently update jobs violating
        constraints, for example via cron. Please note that the resource manager
        and scheduler commands used in this script can be expensive and so
        it is better not to run this inside a tight loop without sleeping.</p>
<a name="N109B0"></a><a name="verify-account+Script"></a>
<h4>verify-account Script</h4>
<p>Production systems use accounting packages to charge users for using
      shared compute resources. HOD supports a parameter 
      <em>resource_manager.pbs-account</em> to allow users to identify the
      account under which they would like to submit jobs. It may be necessary
      to verify that this account is a valid one configured in an accounting
      system. The <em>hod-install-dir/bin/verify-account</em> script 
      provides a mechanism to plug-in a custom script that can do this
      verification.</p>
<a name="N109BF"></a><a name="Integrating+the+verify-account+script+with+HOD"></a>
<h5>Integrating the verify-account script with HOD</h5>
<p>HOD runs the <em>verify-account</em> script passing in the
        <em>resource_manager.pbs-account</em> value as argument to the script,
        before allocating a cluster. Sites can write a script that verify this 
        account against their accounting systems. Returning a non-zero exit 
        code from this script will cause HOD to fail allocation. Also, in
        case of an error, HOD will print the output of script to the user.
        Any descriptive error message can be passed to the user from the
        script in this manner.</p>
<p>The default script that comes with the HOD installation does not
        do any validation, and returns a zero exit code.</p>
<p>If the verify-account script is not found, then HOD will treat
        that verification is disabled, and continue allocation as is.</p>
</div>


<!-- HOD CONFIGURATION -->

   
<a name="N109DA"></a><a name="HOD+Configuration"></a>
<h2 class="h3">HOD Configuration</h2>
<div class="section">
<p>This section discusses how to work with the HOD configuration options.</p>
<a name="N109E3"></a><a name="Getting+Started-N109E3"></a>
<h3 class="h4">Getting Started</h3>
<p>Configuration options can be specified in two ways: as a configuration file 
      in the INI format and as command line options to the HOD shell, 
      specified in the format --section.option[=value]. If the same option is 
      specified in both places, the value specified on the command line 
      overrides the value in the configuration file.</p>
<p>To get a simple description of all configuration options use:</p>
<pre class="code">$ hod --verbose-help</pre>
<a name="N109F4"></a><a name="Configuation+Options"></a>
<h3 class="h4">Configuation Options</h3>
<p>HOD organizes configuration options into these sections:</p>
<ul>
        
<li>  common: Options that appear in more than one section. Options defined in a section are used by the
        process for which that section applies. Common options have the same meaning, but can have different values in each section.</li>
        
<li>  hod: Options for the HOD client</li>
        
<li>  resource_manager: Options for specifying which resource manager to use, and other parameters for using that resource manager</li>
        
<li>  ringmaster: Options for the RingMaster process, </li>
        
<li>  hodring: Options for the HodRing processes</li>
        
<li>  gridservice-mapred: Options for the MapReduce daemons</li>
        
<li>  gridservice-hdfs: Options for the HDFS daemons.</li>
      
</ul>
<a name="N10A15"></a><a name="common+options"></a>
<h4>common options</h4>
<ul>
          
<li>temp-dir: Temporary directory for usage by the HOD processes. Make 
                      sure that the users who will run hod have rights to create 
                      directories under the directory specified here. If you
                      wish to make this directory vary across allocations,
                      you can make use of the environmental variables which will
                      be made available by the resource manager to the HOD
                      processes. For example, in a Torque setup, having
                      --ringmaster.temp-dir=/tmp/hod-temp-dir.$PBS_JOBID would
                      let ringmaster use different temp-dir for each
                      allocation; Torque expands this variable before starting
                      the ringmaster.</li>
          
          
<li>debug: Numeric value from 1-4. 4 produces the most log information,
                   and 1 the least.</li>
          
          
<li>log-dir: Directory where log files are stored. By default, this is
                     &lt;install-location&gt;/logs/. The restrictions and notes for the
                     temp-dir variable apply here too.
          </li>
          
          
<li>xrs-port-range: Range of ports, among which an available port shall
                            be picked for use to run an XML-RPC server.</li>
          
          
<li>http-port-range: Range of ports, among which an available port shall
                             be picked for use to run an HTTP server.</li>
          
          
<li>java-home: Location of Java to be used by Hadoop.</li>
          
<li>syslog-address: Address to which a syslog daemon is bound to. The format 
                              of the value is host:port. If configured, HOD log messages
                              will be logged to syslog using this value.</li>
                              
        
</ul>
<a name="N10A34"></a><a name="hod+options"></a>
<h4>hod options</h4>
<ul>
          
<li>cluster: Descriptive name given to the cluster. For Torque, this is specified as a 'Node property' for every node in the cluster. 
          HOD uses this value to compute the number of available nodes.</li>
          
          
<li>client-params: Comma-separated list of hadoop config parameters specified as key-value pairs. 
          These will be used to generate a hadoop-site.xml on the submit node that should be used for running MapReduce jobs.</li>

          
<li>job-feasibility-attr: Regular expression string that specifies whether and how to check job feasibility - resource 
          manager or scheduler limits. The current implementation corresponds to the torque job attribute 'comment' and by default is disabled. 
          When set, HOD uses it to decide what type of limit violation is triggered and either deallocates the cluster or stays in queued state
          according as the request is beyond maximum limits or the cumulative usage has crossed maximum limits. The torque comment attribute may be updated 
          periodically by an external mechanism. For example, comment attribute can be updated by running 
          <a href="hod_scheduler.html#checklimits.sh+-+Monitor+Resource+Limits">checklimits.sh</a> script in hod/support directory, 
          and then setting job-feasibility-attr equal to the value TORQUE_USER_LIMITS_COMMENT_FIELD, "User-limits exceeded. Requested:([0-9]*) 
          Used:([0-9]*) MaxLimit:([0-9]*)", will make HOD behave accordingly.</li>
         
</ul>
<a name="N10A4B"></a><a name="resource_manager+options"></a>
<h4>resource_manager options</h4>
<ul>
          
<li>queue: Name of the queue configured in the resource manager to which
                   jobs are to be submitted.</li>
          
          
<li>batch-home: Install directory to which 'bin' is appended and under 
                        which the executables of the resource manager can be 
                        found.</li> 
          
          
<li>env-vars: Comma-separated list of key-value pairs, 
                      expressed as key=value, which would be passed to the jobs 
                      launched on the compute nodes. 
                      For example, if the python installation is 
                      in a non-standard location, one can set the environment
                      variable 'HOD_PYTHON_HOME' to the path to the python 
                      executable. The HOD processes launched on the compute nodes
                      can then use this variable.</li>
          
<li>options: Comma-separated list of key-value pairs,
                      expressed as
                      &lt;option&gt;:&lt;sub-option&gt;=&lt;value&gt;. When
                      passing to the job submission program, these are expanded
                      as -&lt;option&gt; &lt;sub-option&gt;=&lt;value&gt;. These
                      are generally used for specifying additional resource
                      contraints for scheduling. For instance, with a Torque
                      setup, one can specify
                      --resource_manager.options='l:arch=x86_64' for
                      constraining the nodes being allocated to a particular
                      architecture; this option will be passed to Torque's qsub
                      command as "-l arch=x86_64".</li>
        
</ul>
<a name="N10A61"></a><a name="ringmaster+options"></a>
<h4>ringmaster options</h4>
<ul>
          
<li>work-dirs: Comma-separated list of paths that will serve
                       as the root for directories that HOD generates and passes
                       to Hadoop for use to store DFS and MapReduce data. For
                       example,
                       this is where DFS data blocks will be stored. Typically,
                       as many paths are specified as there are disks available
                       to ensure all disks are being utilized. The restrictions
                       and notes for the temp-dir variable apply here too.</li>
          
<li>max-master-failures: Number of times a hadoop master
                       daemon can fail to launch, beyond which HOD will fail
                       the cluster allocation altogether. In HOD clusters,
                       sometimes there might be a single or few "bad" nodes due
                       to issues like missing java, missing or incorrect version
                       of Hadoop etc. When this configuration variable is set
                       to a positive integer, the RingMaster returns an error
                       to the client only when the number of times a hadoop
                       master (JobTracker or NameNode) fails to start on these
                       bad nodes because of above issues, exceeds the specified
                       value. If the number is not exceeded, the next HodRing
                       which requests for a command to launch is given the same
                       hadoop master again. This way, HOD tries its best for a
                       successful allocation even in the presence of a few bad
                       nodes in the cluster.
                       </li>
          
<li>workers_per_ring: Number of workers per service per HodRing.
                       By default this is set to 1. If this configuration
                       variable is set to a value 'n', the HodRing will run
                       'n' instances of the workers (TaskTrackers or DataNodes)
                       on each node acting as a slave. This can be used to run
                       multiple workers per HodRing, so that the total number of
                       workers  in a HOD cluster is not limited by the total
                       number of nodes requested during allocation. However, note
                       that this will mean each worker should be configured to use
                       only a proportional fraction of the capacity of the 
                       resources on the node. In general, this feature is only
                       useful for testing and simulation purposes, and not for
                       production use.</li>
        
</ul>
<a name="N10A74"></a><a name="gridservice-hdfs+options"></a>
<h4>gridservice-hdfs options</h4>
<ul>
          
<li>external: If false, indicates that a HDFS cluster must be 
                      bought up by the HOD system, on the nodes which it 
                      allocates via the allocate command. Note that in that case,
                      when the cluster is de-allocated, it will bring down the 
                      HDFS cluster, and all the data will be lost.
                      If true, it will try and connect to an externally configured
                      HDFS system.
                      Typically, because input for jobs are placed into HDFS
                      before jobs are run, and also the output from jobs in HDFS 
                      is required to be persistent, an internal HDFS cluster is 
                      of little value in a production system. However, it allows 
                      for quick testing.</li>
          
          
<li>host: Hostname of the externally configured NameNode, if any</li>
          
          
<li>fs_port: Port to which NameNode RPC server is bound.</li>
          
          
<li>info_port: Port to which the NameNode web UI server is bound.</li>
          
          
<li>pkgs: Installation directory, under which bin/hadoop executable is 
                  located. This can be used to use a pre-installed version of
                  Hadoop on the cluster.</li>
          
          
<li>server-params: Comma-separated list of hadoop config parameters
                           specified key-value pairs. These will be used to
                           generate a hadoop-site.xml that will be used by the
                           NameNode and DataNodes.</li>
          
          
<li>final-server-params: Same as above, except they will be marked final.</li>
        
</ul>
<a name="N10A93"></a><a name="gridservice-mapred+options"></a>
<h4>gridservice-mapred options</h4>
<ul>
          
<li>external: If false, indicates that a MapReduce cluster must be
                      bought up by the HOD system on the nodes which it allocates
                      via the allocate command.
                      If true, if will try and connect to an externally 
                      configured MapReduce system.</li>
          
          
<li>host: Hostname of the externally configured JobTracker, if any</li>
          
          
<li>tracker_port: Port to which the JobTracker RPC server is bound</li>
          
          
<li>info_port: Port to which the JobTracker web UI server is bound.</li>
          
          
<li>pkgs: Installation directory, under which bin/hadoop executable is 
                  located</li>
          
          
<li>server-params: Comma-separated list of hadoop config parameters
                           specified key-value pairs. These will be used to
                           generate a hadoop-site.xml that will be used by the
                           JobTracker and TaskTrackers</li>
          
          
<li>final-server-params: Same as above, except they will be marked final.</li>
        
</ul>
<a name="N10AB2"></a><a name="hodring+options"></a>
<h4>hodring options</h4>
<ul>
          
<li>mapred-system-dir-root: Directory in the DFS under which HOD will
                                      generate sub-directory names and pass the full path
                                      as the value of the 'mapred.system.dir' configuration 
                                      parameter to Hadoop daemons. The format of the full 
                                      path will be value-of-this-option/userid/mapredsystem/cluster-id.
                                      Note that the directory specified here should be such
                                      that all users can create directories under this, if
                                      permissions are enabled in HDFS. Setting the value of
                                      this option to /user will make HOD use the user's
                                      home directory to generate the mapred.system.dir value.</li>

          
<li>log-destination-uri: URL describing a path in an external, static DFS or the 
                                   cluster node's local file system where HOD will upload 
                                   Hadoop logs when a cluster is deallocated. To specify a 
                                   DFS path, use the format 'hdfs://path'. To specify a 
                                   cluster node's local file path, use the format 'file://path'.

                                   When clusters are deallocated by HOD, the hadoop logs will
                                   be deleted as part of HOD's cleanup process. To ensure these
                                   logs persist, you can use this configuration option.

                                   The format of the path is 
                                   value-of-this-option/userid/hod-logs/cluster-id

                                   Note that the directory you specify here must be such that all
                                   users can create sub-directories under this. Setting this value
                                   to hdfs://user will make the logs come in the user's home directory
                                   in DFS.</li>

          
<li>pkgs: Installation directory, under which bin/hadoop executable is located. This will
                    be used by HOD to upload logs if a HDFS URL is specified in log-destination-uri
                    option. Note that this is useful if the users are using a tarball whose version
                    may differ from the external, static HDFS version.</li>

          
<li>hadoop-port-range: Range of ports, among which an available port shall
                             be picked for use to run a Hadoop Service, like JobTracker or TaskTracker. </li>
          
                                      
        
</ul>
</div>
   
   

</div>
<!--+
    |end content
    +-->
<div class="clearboth">&nbsp;</div>
</div>
<div id="footer">
<!--+
    |start bottomstrip
    +-->
<div class="lastmodified">
<script type="text/javascript"><!--
document.write("Last Published: " + document.lastModified);
//  --></script>
</div>
<div class="copyright">
        Copyright &copy;
         2008 <a href="http://www.apache.org/licenses/">The Apache Software Foundation.</a>
</div>
<!--+
    |end bottomstrip
    +-->
</div>
</body>
</html>
