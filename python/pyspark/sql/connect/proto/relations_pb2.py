#
# Licensed to the Apache Software Foundation (ASF) under one or more
# contributor license agreements.  See the NOTICE file distributed with
# this work for additional information regarding copyright ownership.
# The ASF licenses this file to You under the Apache License, Version 2.0
# (the "License"); you may not use this file except in compliance with
# the License.  You may obtain a copy of the License at
#
#    http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
#
# -*- coding: utf-8 -*-
# Generated by the protocol buffer compiler.  DO NOT EDIT!
# source: spark/connect/relations.proto
"""Generated protocol buffer code."""
from google.protobuf.internal import builder as _builder
from google.protobuf import descriptor as _descriptor
from google.protobuf import descriptor_pool as _descriptor_pool
from google.protobuf import symbol_database as _symbol_database

# @@protoc_insertion_point(imports)

_sym_db = _symbol_database.Default()


from pyspark.sql.connect.proto import (
    expressions_pb2 as spark_dot_connect_dot_expressions__pb2,
)


DESCRIPTOR = _descriptor_pool.Default().AddSerializedFile(
    b'\n\x1dspark/connect/relations.proto\x12\rspark.connect\x1a\x1fspark/connect/expressions.proto"\xa6\x04\n\x08Relation\x12\x35\n\x06\x63ommon\x18\x01 \x01(\x0b\x32\x1d.spark.connect.RelationCommonR\x06\x63ommon\x12)\n\x04read\x18\x02 \x01(\x0b\x32\x13.spark.connect.ReadH\x00R\x04read\x12\x32\n\x07project\x18\x03 \x01(\x0b\x32\x16.spark.connect.ProjectH\x00R\x07project\x12/\n\x06\x66ilter\x18\x04 \x01(\x0b\x32\x15.spark.connect.FilterH\x00R\x06\x66ilter\x12)\n\x04join\x18\x05 \x01(\x0b\x32\x13.spark.connect.JoinH\x00R\x04join\x12,\n\x05union\x18\x06 \x01(\x0b\x32\x14.spark.connect.UnionH\x00R\x05union\x12)\n\x04sort\x18\x07 \x01(\x0b\x32\x13.spark.connect.SortH\x00R\x04sort\x12,\n\x05\x66\x65tch\x18\x08 \x01(\x0b\x32\x14.spark.connect.FetchH\x00R\x05\x66\x65tch\x12\x38\n\taggregate\x18\t \x01(\x0b\x32\x18.spark.connect.AggregateH\x00R\taggregate\x12&\n\x03sql\x18\n \x01(\x0b\x32\x12.spark.connect.SqlH\x00R\x03sql\x12\x33\n\x07unknown\x18\xe7\x07 \x01(\x0b\x32\x16.spark.connect.UnknownH\x00R\x07unknownB\n\n\x08rel_type"\t\n\x07Unknown"G\n\x0eRelationCommon\x12\x1f\n\x0bsource_info\x18\x01 \x01(\tR\nsourceInfo\x12\x14\n\x05\x61lias\x18\x02 \x01(\tR\x05\x61lias"\x1b\n\x03Sql\x12\x14\n\x05query\x18\x01 \x01(\tR\x05query"z\n\x04Read\x12\x41\n\x0bnamed_table\x18\x01 \x01(\x0b\x32\x1e.spark.connect.Read.NamedTableH\x00R\nnamedTable\x1a"\n\nNamedTable\x12\x14\n\x05parts\x18\x01 \x03(\tR\x05partsB\x0b\n\tread_type"u\n\x07Project\x12-\n\x05input\x18\x01 \x01(\x0b\x32\x17.spark.connect.RelationR\x05input\x12;\n\x0b\x65xpressions\x18\x03 \x03(\x0b\x32\x19.spark.connect.ExpressionR\x0b\x65xpressions"p\n\x06\x46ilter\x12-\n\x05input\x18\x01 \x01(\x0b\x32\x17.spark.connect.RelationR\x05input\x12\x37\n\tcondition\x18\x02 \x01(\x0b\x32\x19.spark.connect.ExpressionR\tcondition"\xd8\x02\n\x04Join\x12+\n\x04left\x18\x01 \x01(\x0b\x32\x17.spark.connect.RelationR\x04left\x12-\n\x05right\x18\x02 \x01(\x0b\x32\x17.spark.connect.RelationR\x05right\x12)\n\x02on\x18\x03 \x01(\x0b\x32\x19.spark.connect.ExpressionR\x02on\x12.\n\x03how\x18\x04 \x01(\x0e\x32\x1c.spark.connect.Join.JoinTypeR\x03how"\x98\x01\n\x08JoinType\x12\x19\n\x15JOIN_TYPE_UNSPECIFIED\x10\x00\x12\x13\n\x0fJOIN_TYPE_INNER\x10\x01\x12\x13\n\x0fJOIN_TYPE_OUTER\x10\x02\x12\x18\n\x14JOIN_TYPE_LEFT_OUTER\x10\x03\x12\x19\n\x15JOIN_TYPE_RIGHT_OUTER\x10\x04\x12\x12\n\x0eJOIN_TYPE_ANTI\x10\x05"\xcd\x01\n\x05Union\x12/\n\x06inputs\x18\x01 \x03(\x0b\x32\x17.spark.connect.RelationR\x06inputs\x12=\n\nunion_type\x18\x02 \x01(\x0e\x32\x1e.spark.connect.Union.UnionTypeR\tunionType"T\n\tUnionType\x12\x1a\n\x16UNION_TYPE_UNSPECIFIED\x10\x00\x12\x17\n\x13UNION_TYPE_DISTINCT\x10\x01\x12\x12\n\x0eUNION_TYPE_ALL\x10\x02"d\n\x05\x46\x65tch\x12-\n\x05input\x18\x01 \x01(\x0b\x32\x17.spark.connect.RelationR\x05input\x12\x14\n\x05limit\x18\x02 \x01(\x05R\x05limit\x12\x16\n\x06offset\x18\x03 \x01(\x05R\x06offset"\x8b\x04\n\tAggregate\x12-\n\x05input\x18\x01 \x01(\x0b\x32\x17.spark.connect.RelationR\x05input\x12I\n\rgrouping_sets\x18\x02 \x03(\x0b\x32$.spark.connect.Aggregate.GroupingSetR\x0cgroupingSets\x12<\n\x08measures\x18\x03 \x03(\x0b\x32 .spark.connect.Aggregate.MeasureR\x08measures\x1a]\n\x0bGroupingSet\x12N\n\x15\x61ggregate_expressions\x18\x01 \x03(\x0b\x32\x19.spark.connect.ExpressionR\x14\x61ggregateExpressions\x1a\x84\x01\n\x07Measure\x12\x46\n\x08\x66unction\x18\x01 \x01(\x0b\x32*.spark.connect.Aggregate.AggregateFunctionR\x08\x66unction\x12\x31\n\x06\x66ilter\x18\x02 \x01(\x0b\x32\x19.spark.connect.ExpressionR\x06\x66ilter\x1a`\n\x11\x41ggregateFunction\x12\x12\n\x04name\x18\x01 \x01(\tR\x04name\x12\x37\n\targuments\x18\x02 \x03(\x0b\x32\x19.spark.connect.ExpressionR\targuments"\xf6\x03\n\x04Sort\x12-\n\x05input\x18\x01 \x01(\x0b\x32\x17.spark.connect.RelationR\x05input\x12>\n\x0bsort_fields\x18\x02 \x03(\x0b\x32\x1d.spark.connect.Sort.SortFieldR\nsortFields\x1a\xbc\x01\n\tSortField\x12\x39\n\nexpression\x18\x01 \x01(\x0b\x32\x19.spark.connect.ExpressionR\nexpression\x12?\n\tdirection\x18\x02 \x01(\x0e\x32!.spark.connect.Sort.SortDirectionR\tdirection\x12\x33\n\x05nulls\x18\x03 \x01(\x0e\x32\x1d.spark.connect.Sort.SortNullsR\x05nulls"l\n\rSortDirection\x12\x1e\n\x1aSORT_DIRECTION_UNSPECIFIED\x10\x00\x12\x1c\n\x18SORT_DIRECTION_ASCENDING\x10\x01\x12\x1d\n\x19SORT_DIRECTION_DESCENDING\x10\x02"R\n\tSortNulls\x12\x1a\n\x16SORT_NULLS_UNSPECIFIED\x10\x00\x12\x14\n\x10SORT_NULLS_FIRST\x10\x01\x12\x13\n\x0fSORT_NULLS_LAST\x10\x02\x42M\n\x1eorg.apache.spark.connect.protoP\x01Z)github.com/databricks/spark-connect/protob\x06proto3'
)

_builder.BuildMessageAndEnumDescriptors(DESCRIPTOR, globals())
_builder.BuildTopDescriptorsAndMessages(DESCRIPTOR, "spark.connect.relations_pb2", globals())
if _descriptor._USE_C_DESCRIPTORS == False:

    DESCRIPTOR._options = None
    DESCRIPTOR._serialized_options = (
        b"\n\036org.apache.spark.connect.protoP\001Z)github.com/databricks/spark-connect/proto"
    )
    _RELATION._serialized_start = 82
    _RELATION._serialized_end = 632
    _UNKNOWN._serialized_start = 634
    _UNKNOWN._serialized_end = 643
    _RELATIONCOMMON._serialized_start = 645
    _RELATIONCOMMON._serialized_end = 716
    _SQL._serialized_start = 718
    _SQL._serialized_end = 745
    _READ._serialized_start = 747
    _READ._serialized_end = 869
    _READ_NAMEDTABLE._serialized_start = 822
    _READ_NAMEDTABLE._serialized_end = 856
    _PROJECT._serialized_start = 871
    _PROJECT._serialized_end = 988
    _FILTER._serialized_start = 990
    _FILTER._serialized_end = 1102
    _JOIN._serialized_start = 1105
    _JOIN._serialized_end = 1449
    _JOIN_JOINTYPE._serialized_start = 1297
    _JOIN_JOINTYPE._serialized_end = 1449
    _UNION._serialized_start = 1452
    _UNION._serialized_end = 1657
    _UNION_UNIONTYPE._serialized_start = 1573
    _UNION_UNIONTYPE._serialized_end = 1657
    _FETCH._serialized_start = 1659
    _FETCH._serialized_end = 1759
    _AGGREGATE._serialized_start = 1762
    _AGGREGATE._serialized_end = 2285
    _AGGREGATE_GROUPINGSET._serialized_start = 1959
    _AGGREGATE_GROUPINGSET._serialized_end = 2052
    _AGGREGATE_MEASURE._serialized_start = 2055
    _AGGREGATE_MEASURE._serialized_end = 2187
    _AGGREGATE_AGGREGATEFUNCTION._serialized_start = 2189
    _AGGREGATE_AGGREGATEFUNCTION._serialized_end = 2285
    _SORT._serialized_start = 2288
    _SORT._serialized_end = 2790
    _SORT_SORTFIELD._serialized_start = 2408
    _SORT_SORTFIELD._serialized_end = 2596
    _SORT_SORTDIRECTION._serialized_start = 2598
    _SORT_SORTDIRECTION._serialized_end = 2706
    _SORT_SORTNULLS._serialized_start = 2708
    _SORT_SORTNULLS._serialized_end = 2790
# @@protoc_insertion_point(module_scope)
