#
# Licensed to the Apache Software Foundation (ASF) under one or more
# contributor license agreements.  See the NOTICE file distributed with
# this work for additional information regarding copyright ownership.
# The ASF licenses this file to You under the Apache License, Version 2.0
# (the "License"); you may not use this file except in compliance with
# the License.  You may obtain a copy of the License at
#
#    http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
#
"""
@generated by mypy-protobuf.  Do not edit manually!
isort:skip_file

Licensed to the Apache Software Foundation (ASF) under one or more
contributor license agreements.  See the NOTICE file distributed with
this work for additional information regarding copyright ownership.
The ASF licenses this file to You under the Apache License, Version 2.0
(the "License"); you may not use this file except in compliance with
the License.  You may obtain a copy of the License at

   http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
"""

from collections import abc as _abc
from google.protobuf import descriptor as _descriptor
from google.protobuf import message as _message
from google.protobuf.internal import containers as _containers
from pyspark.sql.connect.proto import common_pb2 as _common_pb2
from pyspark.sql.connect.proto import types_pb2 as _types_pb2
import builtins as _builtins
import sys
import typing as _typing

if sys.version_info >= (3, 10):
    from typing import TypeAlias as _TypeAlias
else:
    from typing_extensions import TypeAlias as _TypeAlias

DESCRIPTOR: _descriptor.FileDescriptor

@_typing.final
class Catalog(_message.Message):
    """Catalog messages are marked as unstable."""

    DESCRIPTOR: _descriptor.Descriptor

    CURRENT_DATABASE_FIELD_NUMBER: _builtins.int
    SET_CURRENT_DATABASE_FIELD_NUMBER: _builtins.int
    LIST_DATABASES_FIELD_NUMBER: _builtins.int
    LIST_TABLES_FIELD_NUMBER: _builtins.int
    LIST_FUNCTIONS_FIELD_NUMBER: _builtins.int
    LIST_COLUMNS_FIELD_NUMBER: _builtins.int
    GET_DATABASE_FIELD_NUMBER: _builtins.int
    GET_TABLE_FIELD_NUMBER: _builtins.int
    GET_FUNCTION_FIELD_NUMBER: _builtins.int
    DATABASE_EXISTS_FIELD_NUMBER: _builtins.int
    TABLE_EXISTS_FIELD_NUMBER: _builtins.int
    FUNCTION_EXISTS_FIELD_NUMBER: _builtins.int
    CREATE_EXTERNAL_TABLE_FIELD_NUMBER: _builtins.int
    CREATE_TABLE_FIELD_NUMBER: _builtins.int
    DROP_TEMP_VIEW_FIELD_NUMBER: _builtins.int
    DROP_GLOBAL_TEMP_VIEW_FIELD_NUMBER: _builtins.int
    RECOVER_PARTITIONS_FIELD_NUMBER: _builtins.int
    IS_CACHED_FIELD_NUMBER: _builtins.int
    CACHE_TABLE_FIELD_NUMBER: _builtins.int
    UNCACHE_TABLE_FIELD_NUMBER: _builtins.int
    CLEAR_CACHE_FIELD_NUMBER: _builtins.int
    REFRESH_TABLE_FIELD_NUMBER: _builtins.int
    REFRESH_BY_PATH_FIELD_NUMBER: _builtins.int
    CURRENT_CATALOG_FIELD_NUMBER: _builtins.int
    SET_CURRENT_CATALOG_FIELD_NUMBER: _builtins.int
    LIST_CATALOGS_FIELD_NUMBER: _builtins.int
    @_builtins.property
    def current_database(self) -> Global___CurrentDatabase: ...
    @_builtins.property
    def set_current_database(self) -> Global___SetCurrentDatabase: ...
    @_builtins.property
    def list_databases(self) -> Global___ListDatabases: ...
    @_builtins.property
    def list_tables(self) -> Global___ListTables: ...
    @_builtins.property
    def list_functions(self) -> Global___ListFunctions: ...
    @_builtins.property
    def list_columns(self) -> Global___ListColumns: ...
    @_builtins.property
    def get_database(self) -> Global___GetDatabase: ...
    @_builtins.property
    def get_table(self) -> Global___GetTable: ...
    @_builtins.property
    def get_function(self) -> Global___GetFunction: ...
    @_builtins.property
    def database_exists(self) -> Global___DatabaseExists: ...
    @_builtins.property
    def table_exists(self) -> Global___TableExists: ...
    @_builtins.property
    def function_exists(self) -> Global___FunctionExists: ...
    @_builtins.property
    def create_external_table(self) -> Global___CreateExternalTable: ...
    @_builtins.property
    def create_table(self) -> Global___CreateTable: ...
    @_builtins.property
    def drop_temp_view(self) -> Global___DropTempView: ...
    @_builtins.property
    def drop_global_temp_view(self) -> Global___DropGlobalTempView: ...
    @_builtins.property
    def recover_partitions(self) -> Global___RecoverPartitions: ...
    @_builtins.property
    def is_cached(self) -> Global___IsCached: ...
    @_builtins.property
    def cache_table(self) -> Global___CacheTable: ...
    @_builtins.property
    def uncache_table(self) -> Global___UncacheTable: ...
    @_builtins.property
    def clear_cache(self) -> Global___ClearCache: ...
    @_builtins.property
    def refresh_table(self) -> Global___RefreshTable: ...
    @_builtins.property
    def refresh_by_path(self) -> Global___RefreshByPath: ...
    @_builtins.property
    def current_catalog(self) -> Global___CurrentCatalog: ...
    @_builtins.property
    def set_current_catalog(self) -> Global___SetCurrentCatalog: ...
    @_builtins.property
    def list_catalogs(self) -> Global___ListCatalogs: ...
    def __init__(
        self,
        *,
        current_database: Global___CurrentDatabase | None = ...,
        set_current_database: Global___SetCurrentDatabase | None = ...,
        list_databases: Global___ListDatabases | None = ...,
        list_tables: Global___ListTables | None = ...,
        list_functions: Global___ListFunctions | None = ...,
        list_columns: Global___ListColumns | None = ...,
        get_database: Global___GetDatabase | None = ...,
        get_table: Global___GetTable | None = ...,
        get_function: Global___GetFunction | None = ...,
        database_exists: Global___DatabaseExists | None = ...,
        table_exists: Global___TableExists | None = ...,
        function_exists: Global___FunctionExists | None = ...,
        create_external_table: Global___CreateExternalTable | None = ...,
        create_table: Global___CreateTable | None = ...,
        drop_temp_view: Global___DropTempView | None = ...,
        drop_global_temp_view: Global___DropGlobalTempView | None = ...,
        recover_partitions: Global___RecoverPartitions | None = ...,
        is_cached: Global___IsCached | None = ...,
        cache_table: Global___CacheTable | None = ...,
        uncache_table: Global___UncacheTable | None = ...,
        clear_cache: Global___ClearCache | None = ...,
        refresh_table: Global___RefreshTable | None = ...,
        refresh_by_path: Global___RefreshByPath | None = ...,
        current_catalog: Global___CurrentCatalog | None = ...,
        set_current_catalog: Global___SetCurrentCatalog | None = ...,
        list_catalogs: Global___ListCatalogs | None = ...,
    ) -> None: ...
    _HasFieldArgType: _TypeAlias = _typing.Literal[
        "cache_table",
        b"cache_table",
        "cat_type",
        b"cat_type",
        "clear_cache",
        b"clear_cache",
        "create_external_table",
        b"create_external_table",
        "create_table",
        b"create_table",
        "current_catalog",
        b"current_catalog",
        "current_database",
        b"current_database",
        "database_exists",
        b"database_exists",
        "drop_global_temp_view",
        b"drop_global_temp_view",
        "drop_temp_view",
        b"drop_temp_view",
        "function_exists",
        b"function_exists",
        "get_database",
        b"get_database",
        "get_function",
        b"get_function",
        "get_table",
        b"get_table",
        "is_cached",
        b"is_cached",
        "list_catalogs",
        b"list_catalogs",
        "list_columns",
        b"list_columns",
        "list_databases",
        b"list_databases",
        "list_functions",
        b"list_functions",
        "list_tables",
        b"list_tables",
        "recover_partitions",
        b"recover_partitions",
        "refresh_by_path",
        b"refresh_by_path",
        "refresh_table",
        b"refresh_table",
        "set_current_catalog",
        b"set_current_catalog",
        "set_current_database",
        b"set_current_database",
        "table_exists",
        b"table_exists",
        "uncache_table",
        b"uncache_table",
    ]  # noqa: Y015
    def HasField(self, field_name: _HasFieldArgType) -> _builtins.bool: ...
    _ClearFieldArgType: _TypeAlias = _typing.Literal[
        "cache_table",
        b"cache_table",
        "cat_type",
        b"cat_type",
        "clear_cache",
        b"clear_cache",
        "create_external_table",
        b"create_external_table",
        "create_table",
        b"create_table",
        "current_catalog",
        b"current_catalog",
        "current_database",
        b"current_database",
        "database_exists",
        b"database_exists",
        "drop_global_temp_view",
        b"drop_global_temp_view",
        "drop_temp_view",
        b"drop_temp_view",
        "function_exists",
        b"function_exists",
        "get_database",
        b"get_database",
        "get_function",
        b"get_function",
        "get_table",
        b"get_table",
        "is_cached",
        b"is_cached",
        "list_catalogs",
        b"list_catalogs",
        "list_columns",
        b"list_columns",
        "list_databases",
        b"list_databases",
        "list_functions",
        b"list_functions",
        "list_tables",
        b"list_tables",
        "recover_partitions",
        b"recover_partitions",
        "refresh_by_path",
        b"refresh_by_path",
        "refresh_table",
        b"refresh_table",
        "set_current_catalog",
        b"set_current_catalog",
        "set_current_database",
        b"set_current_database",
        "table_exists",
        b"table_exists",
        "uncache_table",
        b"uncache_table",
    ]  # noqa: Y015
    def ClearField(self, field_name: _ClearFieldArgType) -> None: ...
    _WhichOneofReturnType_cat_type: _TypeAlias = _typing.Literal[
        "current_database",
        "set_current_database",
        "list_databases",
        "list_tables",
        "list_functions",
        "list_columns",
        "get_database",
        "get_table",
        "get_function",
        "database_exists",
        "table_exists",
        "function_exists",
        "create_external_table",
        "create_table",
        "drop_temp_view",
        "drop_global_temp_view",
        "recover_partitions",
        "is_cached",
        "cache_table",
        "uncache_table",
        "clear_cache",
        "refresh_table",
        "refresh_by_path",
        "current_catalog",
        "set_current_catalog",
        "list_catalogs",
    ]  # noqa: Y015
    _WhichOneofArgType_cat_type: _TypeAlias = _typing.Literal["cat_type", b"cat_type"]  # noqa: Y015
    def WhichOneof(
        self, oneof_group: _WhichOneofArgType_cat_type
    ) -> _WhichOneofReturnType_cat_type | None: ...

Global___Catalog: _TypeAlias = Catalog  # noqa: Y015

@_typing.final
class CurrentDatabase(_message.Message):
    """See `spark.catalog.currentDatabase`"""

    DESCRIPTOR: _descriptor.Descriptor

    def __init__(
        self,
    ) -> None: ...

Global___CurrentDatabase: _TypeAlias = CurrentDatabase  # noqa: Y015

@_typing.final
class SetCurrentDatabase(_message.Message):
    """See `spark.catalog.setCurrentDatabase`"""

    DESCRIPTOR: _descriptor.Descriptor

    DB_NAME_FIELD_NUMBER: _builtins.int
    db_name: _builtins.str
    """(Required)"""
    def __init__(
        self,
        *,
        db_name: _builtins.str = ...,
    ) -> None: ...
    _ClearFieldArgType: _TypeAlias = _typing.Literal["db_name", b"db_name"]  # noqa: Y015
    def ClearField(self, field_name: _ClearFieldArgType) -> None: ...

Global___SetCurrentDatabase: _TypeAlias = SetCurrentDatabase  # noqa: Y015

@_typing.final
class ListDatabases(_message.Message):
    """See `spark.catalog.listDatabases`"""

    DESCRIPTOR: _descriptor.Descriptor

    PATTERN_FIELD_NUMBER: _builtins.int
    pattern: _builtins.str
    """(Optional) The pattern that the database name needs to match"""
    def __init__(
        self,
        *,
        pattern: _builtins.str | None = ...,
    ) -> None: ...
    _HasFieldArgType: _TypeAlias = _typing.Literal[
        "_pattern", b"_pattern", "pattern", b"pattern"
    ]  # noqa: Y015
    def HasField(self, field_name: _HasFieldArgType) -> _builtins.bool: ...
    _ClearFieldArgType: _TypeAlias = _typing.Literal[
        "_pattern", b"_pattern", "pattern", b"pattern"
    ]  # noqa: Y015
    def ClearField(self, field_name: _ClearFieldArgType) -> None: ...
    _WhichOneofReturnType__pattern: _TypeAlias = _typing.Literal["pattern"]  # noqa: Y015
    _WhichOneofArgType__pattern: _TypeAlias = _typing.Literal["_pattern", b"_pattern"]  # noqa: Y015
    def WhichOneof(
        self, oneof_group: _WhichOneofArgType__pattern
    ) -> _WhichOneofReturnType__pattern | None: ...

Global___ListDatabases: _TypeAlias = ListDatabases  # noqa: Y015

@_typing.final
class ListTables(_message.Message):
    """See `spark.catalog.listTables`"""

    DESCRIPTOR: _descriptor.Descriptor

    DB_NAME_FIELD_NUMBER: _builtins.int
    PATTERN_FIELD_NUMBER: _builtins.int
    db_name: _builtins.str
    """(Optional)"""
    pattern: _builtins.str
    """(Optional) The pattern that the table name needs to match"""
    def __init__(
        self,
        *,
        db_name: _builtins.str | None = ...,
        pattern: _builtins.str | None = ...,
    ) -> None: ...
    _HasFieldArgType: _TypeAlias = _typing.Literal[
        "_db_name",
        b"_db_name",
        "_pattern",
        b"_pattern",
        "db_name",
        b"db_name",
        "pattern",
        b"pattern",
    ]  # noqa: Y015
    def HasField(self, field_name: _HasFieldArgType) -> _builtins.bool: ...
    _ClearFieldArgType: _TypeAlias = _typing.Literal[
        "_db_name",
        b"_db_name",
        "_pattern",
        b"_pattern",
        "db_name",
        b"db_name",
        "pattern",
        b"pattern",
    ]  # noqa: Y015
    def ClearField(self, field_name: _ClearFieldArgType) -> None: ...
    _WhichOneofReturnType__db_name: _TypeAlias = _typing.Literal["db_name"]  # noqa: Y015
    _WhichOneofArgType__db_name: _TypeAlias = _typing.Literal["_db_name", b"_db_name"]  # noqa: Y015
    _WhichOneofReturnType__pattern: _TypeAlias = _typing.Literal["pattern"]  # noqa: Y015
    _WhichOneofArgType__pattern: _TypeAlias = _typing.Literal["_pattern", b"_pattern"]  # noqa: Y015
    @_typing.overload
    def WhichOneof(
        self, oneof_group: _WhichOneofArgType__db_name
    ) -> _WhichOneofReturnType__db_name | None: ...
    @_typing.overload
    def WhichOneof(
        self, oneof_group: _WhichOneofArgType__pattern
    ) -> _WhichOneofReturnType__pattern | None: ...

Global___ListTables: _TypeAlias = ListTables  # noqa: Y015

@_typing.final
class ListFunctions(_message.Message):
    """See `spark.catalog.listFunctions`"""

    DESCRIPTOR: _descriptor.Descriptor

    DB_NAME_FIELD_NUMBER: _builtins.int
    PATTERN_FIELD_NUMBER: _builtins.int
    db_name: _builtins.str
    """(Optional)"""
    pattern: _builtins.str
    """(Optional) The pattern that the function name needs to match"""
    def __init__(
        self,
        *,
        db_name: _builtins.str | None = ...,
        pattern: _builtins.str | None = ...,
    ) -> None: ...
    _HasFieldArgType: _TypeAlias = _typing.Literal[
        "_db_name",
        b"_db_name",
        "_pattern",
        b"_pattern",
        "db_name",
        b"db_name",
        "pattern",
        b"pattern",
    ]  # noqa: Y015
    def HasField(self, field_name: _HasFieldArgType) -> _builtins.bool: ...
    _ClearFieldArgType: _TypeAlias = _typing.Literal[
        "_db_name",
        b"_db_name",
        "_pattern",
        b"_pattern",
        "db_name",
        b"db_name",
        "pattern",
        b"pattern",
    ]  # noqa: Y015
    def ClearField(self, field_name: _ClearFieldArgType) -> None: ...
    _WhichOneofReturnType__db_name: _TypeAlias = _typing.Literal["db_name"]  # noqa: Y015
    _WhichOneofArgType__db_name: _TypeAlias = _typing.Literal["_db_name", b"_db_name"]  # noqa: Y015
    _WhichOneofReturnType__pattern: _TypeAlias = _typing.Literal["pattern"]  # noqa: Y015
    _WhichOneofArgType__pattern: _TypeAlias = _typing.Literal["_pattern", b"_pattern"]  # noqa: Y015
    @_typing.overload
    def WhichOneof(
        self, oneof_group: _WhichOneofArgType__db_name
    ) -> _WhichOneofReturnType__db_name | None: ...
    @_typing.overload
    def WhichOneof(
        self, oneof_group: _WhichOneofArgType__pattern
    ) -> _WhichOneofReturnType__pattern | None: ...

Global___ListFunctions: _TypeAlias = ListFunctions  # noqa: Y015

@_typing.final
class ListColumns(_message.Message):
    """See `spark.catalog.listColumns`"""

    DESCRIPTOR: _descriptor.Descriptor

    TABLE_NAME_FIELD_NUMBER: _builtins.int
    DB_NAME_FIELD_NUMBER: _builtins.int
    table_name: _builtins.str
    """(Required)"""
    db_name: _builtins.str
    """(Optional)"""
    def __init__(
        self,
        *,
        table_name: _builtins.str = ...,
        db_name: _builtins.str | None = ...,
    ) -> None: ...
    _HasFieldArgType: _TypeAlias = _typing.Literal[
        "_db_name", b"_db_name", "db_name", b"db_name"
    ]  # noqa: Y015
    def HasField(self, field_name: _HasFieldArgType) -> _builtins.bool: ...
    _ClearFieldArgType: _TypeAlias = _typing.Literal[
        "_db_name", b"_db_name", "db_name", b"db_name", "table_name", b"table_name"
    ]  # noqa: Y015
    def ClearField(self, field_name: _ClearFieldArgType) -> None: ...
    _WhichOneofReturnType__db_name: _TypeAlias = _typing.Literal["db_name"]  # noqa: Y015
    _WhichOneofArgType__db_name: _TypeAlias = _typing.Literal["_db_name", b"_db_name"]  # noqa: Y015
    def WhichOneof(
        self, oneof_group: _WhichOneofArgType__db_name
    ) -> _WhichOneofReturnType__db_name | None: ...

Global___ListColumns: _TypeAlias = ListColumns  # noqa: Y015

@_typing.final
class GetDatabase(_message.Message):
    """See `spark.catalog.getDatabase`"""

    DESCRIPTOR: _descriptor.Descriptor

    DB_NAME_FIELD_NUMBER: _builtins.int
    db_name: _builtins.str
    """(Required)"""
    def __init__(
        self,
        *,
        db_name: _builtins.str = ...,
    ) -> None: ...
    _ClearFieldArgType: _TypeAlias = _typing.Literal["db_name", b"db_name"]  # noqa: Y015
    def ClearField(self, field_name: _ClearFieldArgType) -> None: ...

Global___GetDatabase: _TypeAlias = GetDatabase  # noqa: Y015

@_typing.final
class GetTable(_message.Message):
    """See `spark.catalog.getTable`"""

    DESCRIPTOR: _descriptor.Descriptor

    TABLE_NAME_FIELD_NUMBER: _builtins.int
    DB_NAME_FIELD_NUMBER: _builtins.int
    table_name: _builtins.str
    """(Required)"""
    db_name: _builtins.str
    """(Optional)"""
    def __init__(
        self,
        *,
        table_name: _builtins.str = ...,
        db_name: _builtins.str | None = ...,
    ) -> None: ...
    _HasFieldArgType: _TypeAlias = _typing.Literal[
        "_db_name", b"_db_name", "db_name", b"db_name"
    ]  # noqa: Y015
    def HasField(self, field_name: _HasFieldArgType) -> _builtins.bool: ...
    _ClearFieldArgType: _TypeAlias = _typing.Literal[
        "_db_name", b"_db_name", "db_name", b"db_name", "table_name", b"table_name"
    ]  # noqa: Y015
    def ClearField(self, field_name: _ClearFieldArgType) -> None: ...
    _WhichOneofReturnType__db_name: _TypeAlias = _typing.Literal["db_name"]  # noqa: Y015
    _WhichOneofArgType__db_name: _TypeAlias = _typing.Literal["_db_name", b"_db_name"]  # noqa: Y015
    def WhichOneof(
        self, oneof_group: _WhichOneofArgType__db_name
    ) -> _WhichOneofReturnType__db_name | None: ...

Global___GetTable: _TypeAlias = GetTable  # noqa: Y015

@_typing.final
class GetFunction(_message.Message):
    """See `spark.catalog.getFunction`"""

    DESCRIPTOR: _descriptor.Descriptor

    FUNCTION_NAME_FIELD_NUMBER: _builtins.int
    DB_NAME_FIELD_NUMBER: _builtins.int
    function_name: _builtins.str
    """(Required)"""
    db_name: _builtins.str
    """(Optional)"""
    def __init__(
        self,
        *,
        function_name: _builtins.str = ...,
        db_name: _builtins.str | None = ...,
    ) -> None: ...
    _HasFieldArgType: _TypeAlias = _typing.Literal[
        "_db_name", b"_db_name", "db_name", b"db_name"
    ]  # noqa: Y015
    def HasField(self, field_name: _HasFieldArgType) -> _builtins.bool: ...
    _ClearFieldArgType: _TypeAlias = _typing.Literal[
        "_db_name", b"_db_name", "db_name", b"db_name", "function_name", b"function_name"
    ]  # noqa: Y015
    def ClearField(self, field_name: _ClearFieldArgType) -> None: ...
    _WhichOneofReturnType__db_name: _TypeAlias = _typing.Literal["db_name"]  # noqa: Y015
    _WhichOneofArgType__db_name: _TypeAlias = _typing.Literal["_db_name", b"_db_name"]  # noqa: Y015
    def WhichOneof(
        self, oneof_group: _WhichOneofArgType__db_name
    ) -> _WhichOneofReturnType__db_name | None: ...

Global___GetFunction: _TypeAlias = GetFunction  # noqa: Y015

@_typing.final
class DatabaseExists(_message.Message):
    """See `spark.catalog.databaseExists`"""

    DESCRIPTOR: _descriptor.Descriptor

    DB_NAME_FIELD_NUMBER: _builtins.int
    db_name: _builtins.str
    """(Required)"""
    def __init__(
        self,
        *,
        db_name: _builtins.str = ...,
    ) -> None: ...
    _ClearFieldArgType: _TypeAlias = _typing.Literal["db_name", b"db_name"]  # noqa: Y015
    def ClearField(self, field_name: _ClearFieldArgType) -> None: ...

Global___DatabaseExists: _TypeAlias = DatabaseExists  # noqa: Y015

@_typing.final
class TableExists(_message.Message):
    """See `spark.catalog.tableExists`"""

    DESCRIPTOR: _descriptor.Descriptor

    TABLE_NAME_FIELD_NUMBER: _builtins.int
    DB_NAME_FIELD_NUMBER: _builtins.int
    table_name: _builtins.str
    """(Required)"""
    db_name: _builtins.str
    """(Optional)"""
    def __init__(
        self,
        *,
        table_name: _builtins.str = ...,
        db_name: _builtins.str | None = ...,
    ) -> None: ...
    _HasFieldArgType: _TypeAlias = _typing.Literal[
        "_db_name", b"_db_name", "db_name", b"db_name"
    ]  # noqa: Y015
    def HasField(self, field_name: _HasFieldArgType) -> _builtins.bool: ...
    _ClearFieldArgType: _TypeAlias = _typing.Literal[
        "_db_name", b"_db_name", "db_name", b"db_name", "table_name", b"table_name"
    ]  # noqa: Y015
    def ClearField(self, field_name: _ClearFieldArgType) -> None: ...
    _WhichOneofReturnType__db_name: _TypeAlias = _typing.Literal["db_name"]  # noqa: Y015
    _WhichOneofArgType__db_name: _TypeAlias = _typing.Literal["_db_name", b"_db_name"]  # noqa: Y015
    def WhichOneof(
        self, oneof_group: _WhichOneofArgType__db_name
    ) -> _WhichOneofReturnType__db_name | None: ...

Global___TableExists: _TypeAlias = TableExists  # noqa: Y015

@_typing.final
class FunctionExists(_message.Message):
    """See `spark.catalog.functionExists`"""

    DESCRIPTOR: _descriptor.Descriptor

    FUNCTION_NAME_FIELD_NUMBER: _builtins.int
    DB_NAME_FIELD_NUMBER: _builtins.int
    function_name: _builtins.str
    """(Required)"""
    db_name: _builtins.str
    """(Optional)"""
    def __init__(
        self,
        *,
        function_name: _builtins.str = ...,
        db_name: _builtins.str | None = ...,
    ) -> None: ...
    _HasFieldArgType: _TypeAlias = _typing.Literal[
        "_db_name", b"_db_name", "db_name", b"db_name"
    ]  # noqa: Y015
    def HasField(self, field_name: _HasFieldArgType) -> _builtins.bool: ...
    _ClearFieldArgType: _TypeAlias = _typing.Literal[
        "_db_name", b"_db_name", "db_name", b"db_name", "function_name", b"function_name"
    ]  # noqa: Y015
    def ClearField(self, field_name: _ClearFieldArgType) -> None: ...
    _WhichOneofReturnType__db_name: _TypeAlias = _typing.Literal["db_name"]  # noqa: Y015
    _WhichOneofArgType__db_name: _TypeAlias = _typing.Literal["_db_name", b"_db_name"]  # noqa: Y015
    def WhichOneof(
        self, oneof_group: _WhichOneofArgType__db_name
    ) -> _WhichOneofReturnType__db_name | None: ...

Global___FunctionExists: _TypeAlias = FunctionExists  # noqa: Y015

@_typing.final
class CreateExternalTable(_message.Message):
    """See `spark.catalog.createExternalTable`"""

    DESCRIPTOR: _descriptor.Descriptor

    @_typing.final
    class OptionsEntry(_message.Message):
        DESCRIPTOR: _descriptor.Descriptor

        KEY_FIELD_NUMBER: _builtins.int
        VALUE_FIELD_NUMBER: _builtins.int
        key: _builtins.str
        value: _builtins.str
        def __init__(
            self,
            *,
            key: _builtins.str = ...,
            value: _builtins.str = ...,
        ) -> None: ...
        _ClearFieldArgType: _TypeAlias = _typing.Literal[
            "key", b"key", "value", b"value"
        ]  # noqa: Y015
        def ClearField(self, field_name: _ClearFieldArgType) -> None: ...

    TABLE_NAME_FIELD_NUMBER: _builtins.int
    PATH_FIELD_NUMBER: _builtins.int
    SOURCE_FIELD_NUMBER: _builtins.int
    SCHEMA_FIELD_NUMBER: _builtins.int
    OPTIONS_FIELD_NUMBER: _builtins.int
    table_name: _builtins.str
    """(Required)"""
    path: _builtins.str
    """(Optional)"""
    source: _builtins.str
    """(Optional)"""
    @_builtins.property
    def schema(self) -> _types_pb2.DataType:
        """(Optional)"""
    @_builtins.property
    def options(self) -> _containers.ScalarMap[_builtins.str, _builtins.str]:
        """Options could be empty for valid data source format.
        The map key is case insensitive.
        """
    def __init__(
        self,
        *,
        table_name: _builtins.str = ...,
        path: _builtins.str | None = ...,
        source: _builtins.str | None = ...,
        schema: _types_pb2.DataType | None = ...,
        options: _abc.Mapping[_builtins.str, _builtins.str] | None = ...,
    ) -> None: ...
    _HasFieldArgType: _TypeAlias = _typing.Literal[
        "_path",
        b"_path",
        "_schema",
        b"_schema",
        "_source",
        b"_source",
        "path",
        b"path",
        "schema",
        b"schema",
        "source",
        b"source",
    ]  # noqa: Y015
    def HasField(self, field_name: _HasFieldArgType) -> _builtins.bool: ...
    _ClearFieldArgType: _TypeAlias = _typing.Literal[
        "_path",
        b"_path",
        "_schema",
        b"_schema",
        "_source",
        b"_source",
        "options",
        b"options",
        "path",
        b"path",
        "schema",
        b"schema",
        "source",
        b"source",
        "table_name",
        b"table_name",
    ]  # noqa: Y015
    def ClearField(self, field_name: _ClearFieldArgType) -> None: ...
    _WhichOneofReturnType__path: _TypeAlias = _typing.Literal["path"]  # noqa: Y015
    _WhichOneofArgType__path: _TypeAlias = _typing.Literal["_path", b"_path"]  # noqa: Y015
    _WhichOneofReturnType__schema: _TypeAlias = _typing.Literal["schema"]  # noqa: Y015
    _WhichOneofArgType__schema: _TypeAlias = _typing.Literal["_schema", b"_schema"]  # noqa: Y015
    _WhichOneofReturnType__source: _TypeAlias = _typing.Literal["source"]  # noqa: Y015
    _WhichOneofArgType__source: _TypeAlias = _typing.Literal["_source", b"_source"]  # noqa: Y015
    @_typing.overload
    def WhichOneof(
        self, oneof_group: _WhichOneofArgType__path
    ) -> _WhichOneofReturnType__path | None: ...
    @_typing.overload
    def WhichOneof(
        self, oneof_group: _WhichOneofArgType__schema
    ) -> _WhichOneofReturnType__schema | None: ...
    @_typing.overload
    def WhichOneof(
        self, oneof_group: _WhichOneofArgType__source
    ) -> _WhichOneofReturnType__source | None: ...

Global___CreateExternalTable: _TypeAlias = CreateExternalTable  # noqa: Y015

@_typing.final
class CreateTable(_message.Message):
    """See `spark.catalog.createTable`"""

    DESCRIPTOR: _descriptor.Descriptor

    @_typing.final
    class OptionsEntry(_message.Message):
        DESCRIPTOR: _descriptor.Descriptor

        KEY_FIELD_NUMBER: _builtins.int
        VALUE_FIELD_NUMBER: _builtins.int
        key: _builtins.str
        value: _builtins.str
        def __init__(
            self,
            *,
            key: _builtins.str = ...,
            value: _builtins.str = ...,
        ) -> None: ...
        _ClearFieldArgType: _TypeAlias = _typing.Literal[
            "key", b"key", "value", b"value"
        ]  # noqa: Y015
        def ClearField(self, field_name: _ClearFieldArgType) -> None: ...

    TABLE_NAME_FIELD_NUMBER: _builtins.int
    PATH_FIELD_NUMBER: _builtins.int
    SOURCE_FIELD_NUMBER: _builtins.int
    DESCRIPTION_FIELD_NUMBER: _builtins.int
    SCHEMA_FIELD_NUMBER: _builtins.int
    OPTIONS_FIELD_NUMBER: _builtins.int
    table_name: _builtins.str
    """(Required)"""
    path: _builtins.str
    """(Optional)"""
    source: _builtins.str
    """(Optional)"""
    description: _builtins.str
    """(Optional)"""
    @_builtins.property
    def schema(self) -> _types_pb2.DataType:
        """(Optional)"""
    @_builtins.property
    def options(self) -> _containers.ScalarMap[_builtins.str, _builtins.str]:
        """Options could be empty for valid data source format.
        The map key is case insensitive.
        """
    def __init__(
        self,
        *,
        table_name: _builtins.str = ...,
        path: _builtins.str | None = ...,
        source: _builtins.str | None = ...,
        description: _builtins.str | None = ...,
        schema: _types_pb2.DataType | None = ...,
        options: _abc.Mapping[_builtins.str, _builtins.str] | None = ...,
    ) -> None: ...
    _HasFieldArgType: _TypeAlias = _typing.Literal[
        "_description",
        b"_description",
        "_path",
        b"_path",
        "_schema",
        b"_schema",
        "_source",
        b"_source",
        "description",
        b"description",
        "path",
        b"path",
        "schema",
        b"schema",
        "source",
        b"source",
    ]  # noqa: Y015
    def HasField(self, field_name: _HasFieldArgType) -> _builtins.bool: ...
    _ClearFieldArgType: _TypeAlias = _typing.Literal[
        "_description",
        b"_description",
        "_path",
        b"_path",
        "_schema",
        b"_schema",
        "_source",
        b"_source",
        "description",
        b"description",
        "options",
        b"options",
        "path",
        b"path",
        "schema",
        b"schema",
        "source",
        b"source",
        "table_name",
        b"table_name",
    ]  # noqa: Y015
    def ClearField(self, field_name: _ClearFieldArgType) -> None: ...
    _WhichOneofReturnType__description: _TypeAlias = _typing.Literal["description"]  # noqa: Y015
    _WhichOneofArgType__description: _TypeAlias = _typing.Literal[
        "_description", b"_description"
    ]  # noqa: Y015
    _WhichOneofReturnType__path: _TypeAlias = _typing.Literal["path"]  # noqa: Y015
    _WhichOneofArgType__path: _TypeAlias = _typing.Literal["_path", b"_path"]  # noqa: Y015
    _WhichOneofReturnType__schema: _TypeAlias = _typing.Literal["schema"]  # noqa: Y015
    _WhichOneofArgType__schema: _TypeAlias = _typing.Literal["_schema", b"_schema"]  # noqa: Y015
    _WhichOneofReturnType__source: _TypeAlias = _typing.Literal["source"]  # noqa: Y015
    _WhichOneofArgType__source: _TypeAlias = _typing.Literal["_source", b"_source"]  # noqa: Y015
    @_typing.overload
    def WhichOneof(
        self, oneof_group: _WhichOneofArgType__description
    ) -> _WhichOneofReturnType__description | None: ...
    @_typing.overload
    def WhichOneof(
        self, oneof_group: _WhichOneofArgType__path
    ) -> _WhichOneofReturnType__path | None: ...
    @_typing.overload
    def WhichOneof(
        self, oneof_group: _WhichOneofArgType__schema
    ) -> _WhichOneofReturnType__schema | None: ...
    @_typing.overload
    def WhichOneof(
        self, oneof_group: _WhichOneofArgType__source
    ) -> _WhichOneofReturnType__source | None: ...

Global___CreateTable: _TypeAlias = CreateTable  # noqa: Y015

@_typing.final
class DropTempView(_message.Message):
    """See `spark.catalog.dropTempView`"""

    DESCRIPTOR: _descriptor.Descriptor

    VIEW_NAME_FIELD_NUMBER: _builtins.int
    view_name: _builtins.str
    """(Required)"""
    def __init__(
        self,
        *,
        view_name: _builtins.str = ...,
    ) -> None: ...
    _ClearFieldArgType: _TypeAlias = _typing.Literal["view_name", b"view_name"]  # noqa: Y015
    def ClearField(self, field_name: _ClearFieldArgType) -> None: ...

Global___DropTempView: _TypeAlias = DropTempView  # noqa: Y015

@_typing.final
class DropGlobalTempView(_message.Message):
    """See `spark.catalog.dropGlobalTempView`"""

    DESCRIPTOR: _descriptor.Descriptor

    VIEW_NAME_FIELD_NUMBER: _builtins.int
    view_name: _builtins.str
    """(Required)"""
    def __init__(
        self,
        *,
        view_name: _builtins.str = ...,
    ) -> None: ...
    _ClearFieldArgType: _TypeAlias = _typing.Literal["view_name", b"view_name"]  # noqa: Y015
    def ClearField(self, field_name: _ClearFieldArgType) -> None: ...

Global___DropGlobalTempView: _TypeAlias = DropGlobalTempView  # noqa: Y015

@_typing.final
class RecoverPartitions(_message.Message):
    """See `spark.catalog.recoverPartitions`"""

    DESCRIPTOR: _descriptor.Descriptor

    TABLE_NAME_FIELD_NUMBER: _builtins.int
    table_name: _builtins.str
    """(Required)"""
    def __init__(
        self,
        *,
        table_name: _builtins.str = ...,
    ) -> None: ...
    _ClearFieldArgType: _TypeAlias = _typing.Literal["table_name", b"table_name"]  # noqa: Y015
    def ClearField(self, field_name: _ClearFieldArgType) -> None: ...

Global___RecoverPartitions: _TypeAlias = RecoverPartitions  # noqa: Y015

@_typing.final
class IsCached(_message.Message):
    """See `spark.catalog.isCached`"""

    DESCRIPTOR: _descriptor.Descriptor

    TABLE_NAME_FIELD_NUMBER: _builtins.int
    table_name: _builtins.str
    """(Required)"""
    def __init__(
        self,
        *,
        table_name: _builtins.str = ...,
    ) -> None: ...
    _ClearFieldArgType: _TypeAlias = _typing.Literal["table_name", b"table_name"]  # noqa: Y015
    def ClearField(self, field_name: _ClearFieldArgType) -> None: ...

Global___IsCached: _TypeAlias = IsCached  # noqa: Y015

@_typing.final
class CacheTable(_message.Message):
    """See `spark.catalog.cacheTable`"""

    DESCRIPTOR: _descriptor.Descriptor

    TABLE_NAME_FIELD_NUMBER: _builtins.int
    STORAGE_LEVEL_FIELD_NUMBER: _builtins.int
    table_name: _builtins.str
    """(Required)"""
    @_builtins.property
    def storage_level(self) -> _common_pb2.StorageLevel:
        """(Optional)"""
    def __init__(
        self,
        *,
        table_name: _builtins.str = ...,
        storage_level: _common_pb2.StorageLevel | None = ...,
    ) -> None: ...
    _HasFieldArgType: _TypeAlias = _typing.Literal[
        "_storage_level", b"_storage_level", "storage_level", b"storage_level"
    ]  # noqa: Y015
    def HasField(self, field_name: _HasFieldArgType) -> _builtins.bool: ...
    _ClearFieldArgType: _TypeAlias = _typing.Literal[
        "_storage_level",
        b"_storage_level",
        "storage_level",
        b"storage_level",
        "table_name",
        b"table_name",
    ]  # noqa: Y015
    def ClearField(self, field_name: _ClearFieldArgType) -> None: ...
    _WhichOneofReturnType__storage_level: _TypeAlias = _typing.Literal[
        "storage_level"
    ]  # noqa: Y015
    _WhichOneofArgType__storage_level: _TypeAlias = _typing.Literal[
        "_storage_level", b"_storage_level"
    ]  # noqa: Y015
    def WhichOneof(
        self, oneof_group: _WhichOneofArgType__storage_level
    ) -> _WhichOneofReturnType__storage_level | None: ...

Global___CacheTable: _TypeAlias = CacheTable  # noqa: Y015

@_typing.final
class UncacheTable(_message.Message):
    """See `spark.catalog.uncacheTable`"""

    DESCRIPTOR: _descriptor.Descriptor

    TABLE_NAME_FIELD_NUMBER: _builtins.int
    table_name: _builtins.str
    """(Required)"""
    def __init__(
        self,
        *,
        table_name: _builtins.str = ...,
    ) -> None: ...
    _ClearFieldArgType: _TypeAlias = _typing.Literal["table_name", b"table_name"]  # noqa: Y015
    def ClearField(self, field_name: _ClearFieldArgType) -> None: ...

Global___UncacheTable: _TypeAlias = UncacheTable  # noqa: Y015

@_typing.final
class ClearCache(_message.Message):
    """See `spark.catalog.clearCache`"""

    DESCRIPTOR: _descriptor.Descriptor

    def __init__(
        self,
    ) -> None: ...

Global___ClearCache: _TypeAlias = ClearCache  # noqa: Y015

@_typing.final
class RefreshTable(_message.Message):
    """See `spark.catalog.refreshTable`"""

    DESCRIPTOR: _descriptor.Descriptor

    TABLE_NAME_FIELD_NUMBER: _builtins.int
    table_name: _builtins.str
    """(Required)"""
    def __init__(
        self,
        *,
        table_name: _builtins.str = ...,
    ) -> None: ...
    _ClearFieldArgType: _TypeAlias = _typing.Literal["table_name", b"table_name"]  # noqa: Y015
    def ClearField(self, field_name: _ClearFieldArgType) -> None: ...

Global___RefreshTable: _TypeAlias = RefreshTable  # noqa: Y015

@_typing.final
class RefreshByPath(_message.Message):
    """See `spark.catalog.refreshByPath`"""

    DESCRIPTOR: _descriptor.Descriptor

    PATH_FIELD_NUMBER: _builtins.int
    path: _builtins.str
    """(Required)"""
    def __init__(
        self,
        *,
        path: _builtins.str = ...,
    ) -> None: ...
    _ClearFieldArgType: _TypeAlias = _typing.Literal["path", b"path"]  # noqa: Y015
    def ClearField(self, field_name: _ClearFieldArgType) -> None: ...

Global___RefreshByPath: _TypeAlias = RefreshByPath  # noqa: Y015

@_typing.final
class CurrentCatalog(_message.Message):
    """See `spark.catalog.currentCatalog`"""

    DESCRIPTOR: _descriptor.Descriptor

    def __init__(
        self,
    ) -> None: ...

Global___CurrentCatalog: _TypeAlias = CurrentCatalog  # noqa: Y015

@_typing.final
class SetCurrentCatalog(_message.Message):
    """See `spark.catalog.setCurrentCatalog`"""

    DESCRIPTOR: _descriptor.Descriptor

    CATALOG_NAME_FIELD_NUMBER: _builtins.int
    catalog_name: _builtins.str
    """(Required)"""
    def __init__(
        self,
        *,
        catalog_name: _builtins.str = ...,
    ) -> None: ...
    _ClearFieldArgType: _TypeAlias = _typing.Literal["catalog_name", b"catalog_name"]  # noqa: Y015
    def ClearField(self, field_name: _ClearFieldArgType) -> None: ...

Global___SetCurrentCatalog: _TypeAlias = SetCurrentCatalog  # noqa: Y015

@_typing.final
class ListCatalogs(_message.Message):
    """See `spark.catalog.listCatalogs`"""

    DESCRIPTOR: _descriptor.Descriptor

    PATTERN_FIELD_NUMBER: _builtins.int
    pattern: _builtins.str
    """(Optional) The pattern that the catalog name needs to match"""
    def __init__(
        self,
        *,
        pattern: _builtins.str | None = ...,
    ) -> None: ...
    _HasFieldArgType: _TypeAlias = _typing.Literal[
        "_pattern", b"_pattern", "pattern", b"pattern"
    ]  # noqa: Y015
    def HasField(self, field_name: _HasFieldArgType) -> _builtins.bool: ...
    _ClearFieldArgType: _TypeAlias = _typing.Literal[
        "_pattern", b"_pattern", "pattern", b"pattern"
    ]  # noqa: Y015
    def ClearField(self, field_name: _ClearFieldArgType) -> None: ...
    _WhichOneofReturnType__pattern: _TypeAlias = _typing.Literal["pattern"]  # noqa: Y015
    _WhichOneofArgType__pattern: _TypeAlias = _typing.Literal["_pattern", b"_pattern"]  # noqa: Y015
    def WhichOneof(
        self, oneof_group: _WhichOneofArgType__pattern
    ) -> _WhichOneofReturnType__pattern | None: ...

Global___ListCatalogs: _TypeAlias = ListCatalogs  # noqa: Y015
