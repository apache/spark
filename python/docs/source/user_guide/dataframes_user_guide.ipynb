{
    "cells": [
        {
            "attachments": {},
            "cell_type": "markdown",
            "id": "0b37fb0a",
            "metadata": {},
            "source": [
                "# Chapter 4: DataFrames - A view into your structured data"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 1,
            "id": "edfe9cd6",
            "metadata": {
                "tags": [
                    "remove-cell"
                ]
            },
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Requirement already satisfied: pyspark in /Users/amanda.liu/anaconda3/lib/python3.10/site-packages (3.5.0)\n",
                        "Requirement already satisfied: py4j==0.10.9.7 in /Users/amanda.liu/anaconda3/lib/python3.10/site-packages (from pyspark) (0.10.9.7)\n",
                        "Note: you may need to restart the kernel to use updated packages.\n"
                    ]
                }
            ],
            "source": [
                "pip install pyspark"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "id": "43b0e61f",
            "metadata": {
                "tags": [
                    "remove-cell"
                ]
            },
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "Setting default log level to \"WARN\".\n",
                        "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
                        "24/08/19 07:41:14 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
                    ]
                }
            ],
            "source": [
                "from pyspark.sql import SparkSession\n",
                "\n",
                "spark = SparkSession \\\n",
                "    .builder \\\n",
                "    .appName(\"Python Spark SQL basic example\") \\\n",
                "    .config(\"spark.some.config.option\", \"some-value\") \\\n",
                "    .getOrCreate()"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "id": "ae367125",
            "metadata": {},
            "source": [
                "This section introduces the most fundamental data structure in PySpark: the DataFrame.\n",
                "\n",
                "A DataFrame is a two-dimensional labeled data structure with columns \n",
                "of potentially different types. You can think of a DataFrame like a spreadsheet, a SQL table, or a dictionary of series objects. \n",
                "Apache Spark DataFrames support a rich set of APIs (select columns, filter, join, aggregate, etc.) \n",
                "that allow you to solve common data analysis problems efficiently.\n",
                "\n",
                "Compared to traditional relational databases, Spark DataFrames offer several key advantages for big data processing and analytics:\n",
                "\n",
                "- **Distributed computing**: Spark distributes data across multiple nodes in a cluster, allowing for parallel processing of big data\n",
                "- **In-memory processing**: Spark performs computations in memory, which can be significantly faster than disk-based processing\n",
                "- **Schema flexibility**: Unlike traditional databases, PySpark DataFrames support schema evolution and dynamic typing.\n",
                "- **Fault tolerance**: PySpark DataFrames are built on top of Resilient Distributed Dataset (RDDs), which are inherently fault-tolerant. \n",
                "Spark automatically handles node failures and data replication, ensuring data reliability and integrity.\n",
                "\n",
                "A note on RDDs: \n",
                "Direct use of RDDs are no longer supported on Spark Connect as of Spark 4.0.\n",
                "Interacting directly with Spark DataFrames uses a unified planning and optimization engine, \n",
                "allowing us to get nearly identical performance across all supported languages on Databricks (Python, SQL, Scala, and R)."
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "id": "443ebbb1",
            "metadata": {},
            "source": [
                "## Create a DataFrame\n",
                "\n",
                "There are several ways to create a DataFrame in PySpark."
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "id": "33ebd507",
            "metadata": {},
            "source": [
                "### From a list of dictionaries\n",
                "\n",
                "The simplest way is to use the createDataFrame() method like so:"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "id": "b26403e5",
            "metadata": {},
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "                                                                                \r"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "+---+--------+\n",
                        "|age|    name|\n",
                        "+---+--------+\n",
                        "| 30| John D.|\n",
                        "| 25|Alice G.|\n",
                        "| 35|  Bob T.|\n",
                        "| 28|  Eve A.|\n",
                        "+---+--------+\n",
                        "\n"
                    ]
                }
            ],
            "source": [
                "employees = [{\"name\": \"John D.\", \"age\": 30},\n",
                "  {\"name\": \"Alice G.\", \"age\": 25},\n",
                "  {\"name\": \"Bob T.\", \"age\": 35},\n",
                "  {\"name\": \"Eve A.\", \"age\": 28}]\n",
                "\n",
                "# Create a DataFrame containing the employees data\n",
                "df = spark.createDataFrame(employees)\n",
                "df.show()"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "id": "fe7cd086",
            "metadata": {},
            "source": [
                "### From a local file\n",
                "\n",
                "We can also create a DataFrame from a local CSV file:"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "id": "b421b87d",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "+-----------+-----------------+-----------------+\n",
                        "|Employee ID|             Role|         Location|\n",
                        "+-----------+-----------------+-----------------+\n",
                        "|      19238|     Data Analyst|      Seattle, WA|\n",
                        "|      19239|Software Engineer|      Seattle, WA|\n",
                        "|      19240|    IT Specialist|      Seattle, WA|\n",
                        "|      19241|     Data Analyst|     New York, NY|\n",
                        "|      19242|        Recruiter|San Francisco, CA|\n",
                        "|      19243|  Product Manager|     New York, NY|\n",
                        "+-----------+-----------------+-----------------+\n",
                        "\n"
                    ]
                }
            ],
            "source": [
                "df = spark.read.csv(\"../data/employees.csv\", header=True, inferSchema=True)\n",
                "df.show()"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "id": "9e8a5246",
            "metadata": {},
            "source": [
                "Or from a local JSON file:"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 5,
            "id": "4a2d7fe9",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "+-----------+-----------------+-----------------+\n",
                        "|Employee ID|         Location|             Role|\n",
                        "+-----------+-----------------+-----------------+\n",
                        "|      19238|      Seattle, WA|     Data Analyst|\n",
                        "|      19239|      Seattle, WA|Software Engineer|\n",
                        "|      19240|      Seattle, WA|    IT Specialist|\n",
                        "|      19241|     New York, NY|     Data Analyst|\n",
                        "|      19242|San Francisco, CA|        Recruiter|\n",
                        "|      19243|     New York, NY|  Product Manager|\n",
                        "+-----------+-----------------+-----------------+\n",
                        "\n"
                    ]
                }
            ],
            "source": [
                "df = spark.read.option(\"multiline\",\"true\").json(\"../data/employees.json\")\n",
                "df.show()"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "id": "da022789",
            "metadata": {},
            "source": [
                "### From an existing DataFrame\n",
                "\n",
                "We can even create a DataFrame from another existing DataFrame, by selecting certain columns:"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 6,
            "id": "d494632d",
            "metadata": {},
            "outputs": [],
            "source": [
                "employees = [\n",
                "  {\"name\": \"John D.\", \"age\": 30, \"department\": \"HR\"},\n",
                "  {\"name\": \"Alice G.\", \"age\": 25, \"department\": \"Finance\"},\n",
                "  {\"name\": \"Bob T.\", \"age\": 35, \"department\": \"IT\"},\n",
                "  {\"name\": \"Eve A.\", \"age\": 28, \"department\": \"Marketing\"}\n",
                "]\n",
                "df = spark.createDataFrame(employees)\n",
                "\n",
                "# Select only the name and age columns\n",
                "new_df = df.select(\"name\", \"age\")"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "id": "a330bfa9",
            "metadata": {},
            "source": [
                "### From a table\n",
                "\n",
                "If you have an existing table `table_name` in your Spark environment, you can create a DataFrame like this:"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 7,
            "id": "14ad6034",
            "metadata": {
                "tags": [
                    "remove-output"
                ]
            },
            "outputs": [
                {
                    "ename": "AnalysisException",
                    "evalue": "[TABLE_OR_VIEW_NOT_FOUND] The table or view `table_name` cannot be found. Verify the spelling and correctness of the schema and catalog.\nIf you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.\nTo tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.;\n'UnresolvedRelation [table_name], [], false\n",
                    "output_type": "error",
                    "traceback": [
                        "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
                        "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
                        "Cell \u001b[0;32mIn[7], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mspark\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtable\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtable_name\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
                        "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/pyspark/sql/readwriter.py:484\u001b[0m, in \u001b[0;36mDataFrameReader.table\u001b[0;34m(self, tableName)\u001b[0m\n\u001b[1;32m    450\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtable\u001b[39m(\u001b[38;5;28mself\u001b[39m, tableName: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataFrame\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    451\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Returns the specified table as a :class:`DataFrame`.\u001b[39;00m\n\u001b[1;32m    452\u001b[0m \n\u001b[1;32m    453\u001b[0m \u001b[38;5;124;03m    .. versionadded:: 1.4.0\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    482\u001b[0m \u001b[38;5;124;03m    >>> _ = spark.sql(\"DROP TABLE tblA\")\u001b[39;00m\n\u001b[1;32m    483\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 484\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_df(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jreader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtable\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtableName\u001b[49m\u001b[43m)\u001b[49m)\n",
                        "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
                        "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/pyspark/errors/exceptions/captured.py:185\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    181\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[1;32m    182\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    183\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    184\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[0;32m--> 185\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    186\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    187\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
                        "\u001b[0;31mAnalysisException\u001b[0m: [TABLE_OR_VIEW_NOT_FOUND] The table or view `table_name` cannot be found. Verify the spelling and correctness of the schema and catalog.\nIf you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.\nTo tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.;\n'UnresolvedRelation [table_name], [], false\n"
                    ]
                }
            ],
            "source": [
                "df = spark.read.table(\"table_name\")"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "id": "892871a6",
            "metadata": {},
            "source": [
                "### From a database\n",
                "\n",
                "If your table is in a database, you can use JDBC to read the table into a DataFrame.\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 9,
            "id": "a40a5d54",
            "metadata": {
                "tags": [
                    "remove-output"
                ]
            },
            "outputs": [
                {
                    "ename": "Py4JJavaError",
                    "evalue": "An error occurred while calling o85.jdbc.\n: java.sql.SQLException: No suitable driver\n\tat java.sql/java.sql.DriverManager.getDriver(DriverManager.java:300)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions.$anonfun$driverClass$2(JDBCOptions.scala:109)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions.<init>(JDBCOptions.scala:109)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions.<init>(JDBCOptions.scala:41)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcRelationProvider.createRelation(JdbcRelationProvider.scala:34)\n\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:346)\n\tat org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:229)\n\tat org.apache.spark.sql.DataFrameReader.$anonfun$load$2(DataFrameReader.scala:211)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:211)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:172)\n\tat org.apache.spark.sql.DataFrameReader.jdbc(DataFrameReader.scala:249)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:75)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:52)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:580)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:1583)\n",
                    "output_type": "error",
                    "traceback": [
                        "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
                        "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
                        "Cell \u001b[0;32mIn[9], line 9\u001b[0m\n\u001b[1;32m      3\u001b[0m properties \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m      4\u001b[0m   \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muser\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124musername\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      5\u001b[0m   \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpassword\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpassword\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      6\u001b[0m }\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# Read table into DataFrame\u001b[39;00m\n\u001b[0;32m----> 9\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mspark\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjdbc\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtable\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mproperties\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproperties\u001b[49m\u001b[43m)\u001b[49m\n",
                        "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/pyspark/sql/readwriter.py:946\u001b[0m, in \u001b[0;36mDataFrameReader.jdbc\u001b[0;34m(self, url, table, column, lowerBound, upperBound, numPartitions, predicates, properties)\u001b[0m\n\u001b[1;32m    944\u001b[0m     jpredicates \u001b[38;5;241m=\u001b[39m utils\u001b[38;5;241m.\u001b[39mtoJArray(gateway, gateway\u001b[38;5;241m.\u001b[39mjvm\u001b[38;5;241m.\u001b[39mjava\u001b[38;5;241m.\u001b[39mlang\u001b[38;5;241m.\u001b[39mString, predicates)\n\u001b[1;32m    945\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_df(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jreader\u001b[38;5;241m.\u001b[39mjdbc(url, table, jpredicates, jprop))\n\u001b[0;32m--> 946\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_df(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jreader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjdbc\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtable\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mjprop\u001b[49m\u001b[43m)\u001b[49m)\n",
                        "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
                        "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/pyspark/errors/exceptions/captured.py:179\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    177\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m    178\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 179\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    180\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    181\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
                        "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/py4j/protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[0;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[1;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[1;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
                        "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o85.jdbc.\n: java.sql.SQLException: No suitable driver\n\tat java.sql/java.sql.DriverManager.getDriver(DriverManager.java:300)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions.$anonfun$driverClass$2(JDBCOptions.scala:109)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions.<init>(JDBCOptions.scala:109)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions.<init>(JDBCOptions.scala:41)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcRelationProvider.createRelation(JdbcRelationProvider.scala:34)\n\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:346)\n\tat org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:229)\n\tat org.apache.spark.sql.DataFrameReader.$anonfun$load$2(DataFrameReader.scala:211)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:211)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:172)\n\tat org.apache.spark.sql.DataFrameReader.jdbc(DataFrameReader.scala:249)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:75)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:52)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:580)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:1583)\n"
                    ]
                }
            ],
            "source": [
                "url = \"jdbc:mysql://localhost:3306/mydatabase\"\n",
                "table = \"employees\"\n",
                "properties = {\n",
                "  \"user\": \"username\",\n",
                "  \"password\": \"password\"\n",
                "}\n",
                "\n",
                "# Read table into DataFrame\n",
                "df = spark.read.jdbc(url=url, table=table, properties=properties)"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "id": "91c58617",
            "metadata": {},
            "source": [
                "## View the DataFrame\n",
                "\n",
                "We can use PySpark to view and interact with our DataFrame."
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "id": "25faacd7",
            "metadata": {},
            "source": [
                "### Display the DataFrame\n",
                "\n",
                "`df.show()` displays a basic visualization of the DataFrame's contents. From our above `createDataFrame()` example:"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 10,
            "id": "6a91ef12",
            "metadata": {},
            "outputs": [],
            "source": [
                "employees = [{\"name\": \"John D.\", \"age\": 30},\n",
                "  {\"name\": \"Alice G.\", \"age\": 25},\n",
                "  {\"name\": \"Bob T.\", \"age\": 35},\n",
                "  {\"name\": \"Eve A.\", \"age\": 28}]\n",
                "\n",
                "# Create a DataFrame containing the employees data\n",
                "df = spark.createDataFrame(employees)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 11,
            "id": "c2ce1c82",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "+---+--------+\n",
                        "|age|    name|\n",
                        "+---+--------+\n",
                        "| 30| John D.|\n",
                        "| 25|Alice G.|\n",
                        "| 35|  Bob T.|\n",
                        "| 28|  Eve A.|\n",
                        "+---+--------+\n",
                        "\n"
                    ]
                }
            ],
            "source": [
                "df.show()"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "id": "c6ee7c70",
            "metadata": {},
            "source": [
                "`df.show()` has 3 optional arguments: `n`, `truncate`, and `vertical`.\n",
                "\n",
                "By default, `df.show()` displays up to the first 20 rows of the DataFrame. \n",
                "We can control the number of rows displayed by passing an argument to the show() method:"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 12,
            "id": "01417e41",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "+---+--------+\n",
                        "|age|    name|\n",
                        "+---+--------+\n",
                        "| 30| John D.|\n",
                        "| 25|Alice G.|\n",
                        "+---+--------+\n",
                        "only showing top 2 rows\n",
                        "\n"
                    ]
                }
            ],
            "source": [
                "df.show(n=2)"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "id": "605cd26c",
            "metadata": {},
            "source": [
                "The truncate argument controls the length of displayed column values (default value is 20):"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 13,
            "id": "b01d5223",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "+---+----+\n",
                        "|age|name|\n",
                        "+---+----+\n",
                        "| 30| Joh|\n",
                        "| 25| Ali|\n",
                        "| 35| Bob|\n",
                        "| 28| Eve|\n",
                        "+---+----+\n",
                        "\n"
                    ]
                }
            ],
            "source": [
                "df.show(truncate=3)"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "id": "e9bedaa6",
            "metadata": {},
            "source": [
                "If we set `vertical` to True, the DataFrame will be displayed vertically with one line per value:"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 14,
            "id": "267facfc",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "-RECORD 0--------\n",
                        " age  | 30       \n",
                        " name | John D.  \n",
                        "-RECORD 1--------\n",
                        " age  | 25       \n",
                        " name | Alice G. \n",
                        "-RECORD 2--------\n",
                        " age  | 35       \n",
                        " name | Bob T.   \n",
                        "-RECORD 3--------\n",
                        " age  | 28       \n",
                        " name | Eve A.   \n",
                        "\n"
                    ]
                }
            ],
            "source": [
                "df.show(vertical=True)"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "id": "4de10f68",
            "metadata": {},
            "source": [
                "### Print the DataFrame schema\n",
                "\n",
                "We can view information about the DataFrame schema using the `printSchema()` method:"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 15,
            "id": "27481fa9",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "root\n",
                        " |-- age: long (nullable = true)\n",
                        " |-- name: string (nullable = true)\n",
                        "\n"
                    ]
                }
            ],
            "source": [
                "df.printSchema()"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "id": "8e90d081",
            "metadata": {},
            "source": [
                "## DataFrame Manipulation\n",
                "\n",
                "Let's look at some ways we can transform our DataFrames.\n",
                "\n",
                "For more detailed information, please see the section about data manipulation, [Chapter 6: Function Junction - Data manipulation with PySpark](https://databricks-eng.github.io/pyspark-cookbook/07-dataprep.html).\n",
                "\n",
                "### Rename columns\n",
                "\n",
                "We can rename DataFrame columns using the `withColumnRenamed()` method:"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 16,
            "id": "65d6dfcb",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "+---+--------+\n",
                        "|age|    name|\n",
                        "+---+--------+\n",
                        "| 30| John D.|\n",
                        "| 25|Alice G.|\n",
                        "| 35|  Bob T.|\n",
                        "| 28|  Eve A.|\n",
                        "+---+--------+\n",
                        "\n",
                        "+---+---------+\n",
                        "|age|full_name|\n",
                        "+---+---------+\n",
                        "| 30|  John D.|\n",
                        "| 25| Alice G.|\n",
                        "| 35|   Bob T.|\n",
                        "| 28|   Eve A.|\n",
                        "+---+---------+\n",
                        "\n"
                    ]
                }
            ],
            "source": [
                "df.show()\n",
                "df2 = df.withColumnRenamed(\"name\", \"full_name\")\n",
                "df2.show()"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "id": "10f3c03f",
            "metadata": {},
            "source": [
                "### Filter rows\n",
                "\n",
                "We can filter for employees within a certain age range.\n",
                "The following `df.filter` will create a new DataFrame with rows that match our age condition:"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 17,
            "id": "af133309",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "+---+-------+\n",
                        "|age|   name|\n",
                        "+---+-------+\n",
                        "| 30|John D.|\n",
                        "| 28| Eve A.|\n",
                        "+---+-------+\n",
                        "\n"
                    ]
                }
            ],
            "source": [
                "filtered_df = df.filter((df[\"age\"] > 26) & (df[\"age\"] < 32))\n",
                "filtered_df.show()"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "id": "c49ea696",
            "metadata": {},
            "source": [
                "We can also use `df.where` to get the same result:"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 18,
            "id": "a29a0719",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "+---+-------+\n",
                        "|age|   name|\n",
                        "+---+-------+\n",
                        "| 30|John D.|\n",
                        "| 28| Eve A.|\n",
                        "+---+-------+\n",
                        "\n"
                    ]
                }
            ],
            "source": [
                "where_df = df.where((df[\"age\"] > 26) & (df[\"age\"] < 32))\n",
                "where_df.show()"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "id": "b6d1026a",
            "metadata": {},
            "source": [
                "## DataFrames vs. Tables\n",
                "A DataFrame is an immutable distributed collection of data, only available in the current Spark session.\n",
                "\n",
                "A table is a persistent data structure that can be accessed across multiple Spark sessions.\n",
                "\n",
                "If you wish to promote a DataFrame to a table, you can use the `createOrReplaceTempView()` method:"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 19,
            "id": "778ad9c5",
            "metadata": {},
            "outputs": [],
            "source": [
                "df.createOrReplaceTempView(\"employees\")"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "id": "0a448326",
            "metadata": {},
            "source": [
                "Note that the lifetime of this temporary table is tied to the SparkSession that was used to create this DataFrame.\n",
                "To persist the table beyond this Spark session, you will need to save it to persistent storage."
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "id": "bd253f06",
            "metadata": {},
            "source": [
                "## Save DataFrame to Persistent Storage\n",
                "\n",
                "There are several ways to save a DataFrame to persistent storage in PySpark.\n",
                "For more detailed information about saving data to your local environment,\n",
                "please see the section about Data Loading (TODO: add link)."
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "id": "b72663f5",
            "metadata": {},
            "source": [
                "### Save to file-based data source\n",
                "\n",
                "For file-based data source (text, parquet, json, etc.), you can specify a custom table path like so: "
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 20,
            "id": "714d52bd",
            "metadata": {
                "tags": [
                    "remove-output"
                ]
            },
            "outputs": [],
            "source": [
                "df.write.option(\"path\", \"../dataout\").saveAsTable(\"dataframes_savetable_example\")"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "id": "fcc3fc81",
            "metadata": {},
            "source": [
                "Even if the table is dropped, the custom table path and table data will still be there. \n",
                "\n",
                "If no custom table path is specified, Spark will write data to a default table path under the warehouse directory. \n",
                "When the table is dropped, the default table path will be removed too."
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "id": "c98e0afb",
            "metadata": {},
            "source": [
                "### Save to Hive metastore\n",
                "To save to Hive metastore, you can use the following:"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 21,
            "id": "00c35126",
            "metadata": {
                "tags": [
                    "remove-output"
                ]
            },
            "outputs": [
                {
                    "ename": "TypeError",
                    "evalue": "'DataFrameWriter' object is not callable",
                    "output_type": "error",
                    "traceback": [
                        "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
                        "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
                        "Cell \u001b[0;32mIn[21], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mmode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moverwrite\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39msaveAsTable(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mschemaName.tableName\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
                        "\u001b[0;31mTypeError\u001b[0m: 'DataFrameWriter' object is not callable"
                    ]
                }
            ],
            "source": [
                "df.write().mode(\"overwrite\").saveAsTable(\"schemaName.tableName\")"
            ]
        }
    ],
    "metadata": {
        "celltoolbar": "Tags",
        "kernelspec": {
            "display_name": "",
            "language": "python",
            "name": ""
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.11.4"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}
