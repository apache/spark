{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4fa81d13",
   "metadata": {},
   "source": [
    "# ANSI Migration Guide - Pandas API on Spark\n",
    "ANSI mode is now on by default for Pandas API on Spark. This guide helps you understand the key behavior differences you’ll see.\n",
    "In short, with ANSI mode on, Pandas API on Spark behavior matches native pandas in cases where Pandas API on Spark with ANSI off did not."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e1c7952",
   "metadata": {},
   "source": [
    "## Behavior Change\n",
    "### String Number Comparison\n",
    "**ANSI off:** Spark implicitly casts numbers and strings, so `1` and `'1'` are considered equal.\n",
    "**ANSI on:** behaves like pandas, `1 == '1'` is False."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "76524036",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pyspark'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpd\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpyspark\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mps\u001b[39;00m\n\u001b[32m      4\u001b[39m pdf = pd.DataFrame({\u001b[33m\"\u001b[39m\u001b[33mint\u001b[39m\u001b[33m\"\u001b[39m: [\u001b[32m1\u001b[39m, \u001b[32m2\u001b[39m], \u001b[33m\"\u001b[39m\u001b[33mstr\u001b[39m\u001b[33m\"\u001b[39m: [\u001b[33m\"\u001b[39m\u001b[33m1\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33m2\u001b[39m\u001b[33m\"\u001b[39m]})\n\u001b[32m      5\u001b[39m psdf = ps.from_pandas(pdf)\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'pyspark'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import pyspark.pandas as ps\n",
    "\n",
    "pdf = pd.DataFrame({\"int\": [1, 2], \"str\": [\"1\", \"2\"]})\n",
    "psdf = ps.from_pandas(pdf)\n",
    "\n",
    "# ANSI on\n",
    "print(psdf[\"int\"] == psdf[\"str\"])\n",
    "print(pdf[\"int\"] == pdf[\"str\"])\n",
    "\n",
    "# ANSI off\n",
    "spark.conf.set(\"spark.sql.ansi.enabled\", False)\n",
    "print(psdf[\"int\"] == psdf[\"str\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90a4ea8d",
   "metadata": {},
   "source": [
    "### Strict Casting\n",
    "**ANSI off:** invalid casts (e.g., `'a' → int`) quietly became NULL.\n",
    "**ANSI on:** the same casts raise errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b172839",
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf = pd.DataFrame({\"str\": [\"a\"]})\n",
    "psdf = ps.from_pandas(pdf)\n",
    "\n",
    "# ANSI on\n",
    "try:\n",
    "    print(psdf[\"str\"].astype(int))\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "\n",
    "try:\n",
    "    print(pdf[\"str\"].astype(int))\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "\n",
    "# ANSI off\n",
    "spark.conf.set(\"spark.sql.ansi.enabled\", False)\n",
    "print(psdf[\"str\"].astype(int))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e11583e2",
   "metadata": {},
   "source": [
    "### MultiIndex.to_series Return\n",
    "**ANSI off:** returns each row as a list ([1, red]).\n",
    "**ANSI on:** returns each row as a tuple ((1, red)), with the Runtime SQL Configuration `spark.sql.execution.pandas.structHandlingMode` set to `'row'`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e7d2458",
   "metadata": {},
   "outputs": [],
   "source": [
    "arrays = [[1,  2], [\"red\", \"blue\"]]\n",
    "pidx = pd.MultiIndex.from_arrays(arrays, names=(\"number\", \"color\"))\n",
    "psidx = ps.from_pandas(pidx)\n",
    "\n",
    "spark.conf.set(\"spark.sql.execution.pandas.structHandlingMode\", \"row\")\n",
    "print(psidx.to_series())\n",
    "print(pidx.to_series())\n",
    "\n",
    "# ANSI off\n",
    "spark.conf.set(\"spark.sql.ansi.enabled\", False)\n",
    "print(psidx.to_series())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe146afd",
   "metadata": {},
   "source": [
    "## Related Configurations\n",
    "1. **`compute.fail_on_ansi_mode` (Pandas API on Spark option)**\n",
    "   - Controls whether Pandas API on Spark fails immediately when ANSI mode is enabled.\n",
    "   - Now overridden by `compute.ansi_mode_support`.\n",
    "\n",
    "2. **`compute.ansi_mode_support` (Pandas API on Spark option)**\n",
    "   - Indicates whether ANSI mode is fully supported.\n",
    "\n",
    "3. **`spark.sql.ansi.enabled` (Spark config)**\n",
    "   - Native Spark setting that controls ANSI mode."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
