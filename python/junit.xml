<?xml version='1.0' encoding='UTF-8'?>
<testsuites tests="277" failures="0" errors="3" skipped="9" time="103.061" name="pyspark-error"><testsuite name="pyspark.sql.tests.connect.test_parity_python_datasource.PythonDataSourceParityTests-20260102163327" tests="1" file="pyspark/sql/tests/connect/test_parity_python_datasource.py" time="6.534" timestamp="2026-01-02T16:33:37" failures="0" errors="0" skipped="0">
	<testcase classname="pyspark.sql.tests.connect.test_parity_python_datasource.PythonDataSourceParityTests" name="test_data_source_writer_with_logging" time="6.534" timestamp="2026-01-02T16:33:37" file="python/pyspark/sql/tests/connect/test_parity_python_datasource.py" line="1122">
		<system-err>TestJsonDataSource.name
</system-err>
	</testcase>
</testsuite><testsuite name="pyspark.sql.tests.test_udf.UDFInitializationTests-20260102111617" tests="2" file="pyspark/sql/tests/test_udf.py" time="0.001" timestamp="2026-01-02T11:16:21" failures="0" errors="0" skipped="0">
	<testcase classname="pyspark.sql.tests.test_udf.UDFInitializationTests" name="test_err_parse_type_when_no_sc" time="0.001" timestamp="2026-01-02T11:16:21" file="python/pyspark/sql/tests/test_udf.py" line="1773"/>
	<testcase classname="pyspark.sql.tests.test_udf.UDFInitializationTests" name="test_udf_init_should_not_initialize_context" time="0.000" timestamp="2026-01-02T11:16:21" file="python/pyspark/sql/tests/test_udf.py" line="1759"/>
</testsuite><testsuite name="pyspark.sql.tests.test_udf.UDFInitializationTests-20260102112122" tests="2" file="pyspark/sql/tests/test_udf.py" time="0.001" timestamp="2026-01-02T11:21:25" failures="0" errors="0" skipped="0">
	<testcase classname="pyspark.sql.tests.test_udf.UDFInitializationTests" name="test_err_parse_type_when_no_sc" time="0.001" timestamp="2026-01-02T11:21:25" file="python/pyspark/sql/tests/test_udf.py" line="1773"/>
	<testcase classname="pyspark.sql.tests.test_udf.UDFInitializationTests" name="test_udf_init_should_not_initialize_context" time="0.000" timestamp="2026-01-02T11:21:25" file="python/pyspark/sql/tests/test_udf.py" line="1759"/>
</testsuite><testsuite name="pyspark.sql.tests.test_udf.UDFInitializationTests-20260102125329" tests="2" file="pyspark/sql/tests/test_udf.py" time="0.001" timestamp="2026-01-02T12:53:32" failures="0" errors="0" skipped="0">
	<testcase classname="pyspark.sql.tests.test_udf.UDFInitializationTests" name="test_err_parse_type_when_no_sc" time="0.001" timestamp="2026-01-02T12:53:32" file="python/pyspark/sql/tests/test_udf.py" line="1773"/>
	<testcase classname="pyspark.sql.tests.test_udf.UDFInitializationTests" name="test_udf_init_should_not_initialize_context" time="0.000" timestamp="2026-01-02T12:53:32" file="python/pyspark/sql/tests/test_udf.py" line="1759"/>
</testsuite><testsuite name="pyspark.sql.tests.test_udf.UDFTests-20260102111617" tests="89" file="pyspark/sql/tests/test_udf.py" time="31.792" timestamp="2026-01-02T11:16:53" failures="0" errors="3" skipped="3">
	<testcase classname="pyspark.sql.tests.test_udf.UDFTests" name="test_assert_classic_mode" time="0.002" timestamp="2026-01-02T11:16:21" file="python/pyspark/sql/tests/test_udf.py" line="324"/>
	<testcase classname="pyspark.sql.tests.test_udf.UDFTests" name="test_broadcast_in_udf" time="1.639" timestamp="2026-01-02T11:16:23" file="python/pyspark/sql/tests/test_udf.py" line="341">
		<system-err>/home/gaogaotiantian/programs/spark/python/pyspark/sql/catalog.py:957: FutureWarning: Deprecated in 2.3.0. Use spark.udf.register instead.
  warnings.warn("Deprecated in 2.3.0. Use spark.udf.register instead.", FutureWarning)
</system-err>
	</testcase>
	<testcase classname="pyspark.sql.tests.test_udf.UDFTests" name="test_chained_udf" time="0.358" timestamp="2026-01-02T11:16:23" file="python/pyspark/sql/tests/test_udf.py" line="205"/>
	<testcase classname="pyspark.sql.tests.test_udf.UDFTests" name="test_chained_udfs_with_variant" time="0.554" timestamp="2026-01-02T11:16:23" file="python/pyspark/sql/tests/test_udf.py" line="432"/>
	<testcase classname="pyspark.sql.tests.test_udf.UDFTests" name="test_complex_return_types" time="0.246" timestamp="2026-01-02T11:16:24" file="python/pyspark/sql/tests/test_udf.py" line="1070"/>
	<testcase classname="pyspark.sql.tests.test_udf.UDFTests" name="test_err_udf_init" time="0.012" timestamp="2026-01-02T11:16:24" file="python/pyspark/sql/tests/test_udf.py" line="1261"/>
	<testcase classname="pyspark.sql.tests.test_udf.UDFTests" name="test_err_udf_registration" time="0.010" timestamp="2026-01-02T11:16:24" file="python/pyspark/sql/tests/test_udf.py" line="596"/>
	<testcase classname="pyspark.sql.tests.test_udf.UDFTests" name="test_kwargs" time="0.835" timestamp="2026-01-02T11:16:25" file="python/pyspark/sql/tests/test_udf.py" line="1134">
		<system-err>{"ts": "2026-01-02 11:16:24.981", "level": "ERROR", "logger": "SQLQueryContextLogger", "msg": "[DUPLICATE_ROUTINE_PARAMETER_ASSIGNMENT.DOUBLE_NAMED_ARGUMENT_REFERENCE] Call to routine `test_udf` is invalid because it includes multiple argument assignments to the same parameter name `a`. More than one named argument referred to the same parameter. Please assign a value only once. SQLSTATE: 4274K", "context": {"errorClass": "DUPLICATE_ROUTINE_PARAMETER_ASSIGNMENT.DOUBLE_NAMED_ARGUMENT_REFERENCE"}, "exception": {"class": "Py4JJavaError", "msg": "An error occurred while calling o85.sql.\n: org.apache.spark.sql.AnalysisException: [DUPLICATE_ROUTINE_PARAMETER_ASSIGNMENT.DOUBLE_NAMED_ARGUMENT_REFERENCE] Call to routine `test_udf` is invalid because it includes multiple argument assignments to the same parameter name `a`. More than one named argument referred to the same parameter. Please assign a value only once. SQLSTATE: 4274K; line 1 pos 7\n\tat org.apache.spark.sql.errors.QueryCompilationErrors$.doubleNamedArgumentReference(QueryCompilationErrors.scala:90)\n\tat org.apache.spark.sql.catalyst.plans.logical.NamedParametersSupport$.$anonfun$splitAndCheckNamedArguments$2(FunctionBuilderBase.scala:108)\n\tat scala.collection.immutable.Vector1.map(Vector.scala:2155)\n\tat scala.collection.immutable.Vector1.map(Vector.scala:386)\n\tat org.apache.spark.sql.catalyst.plans.logical.NamedParametersSupport$.splitAndCheckNamedArguments(FunctionBuilderBase.scala:104)\n\tat org.apache.spark.sql.execution.python.UserDefinedPythonFunction.builder(UserDefinedPythonFunction.scala:62)\n\tat org.apache.spark.sql.classic.UDFRegistration.$anonfun$registerPython$1(UDFRegistration.scala:64)\n\tat org.apache.spark.sql.catalyst.analysis.SimpleFunctionRegistryBase.lookupFunction(FunctionRegistry.scala:246)\n\tat org.apache.spark.sql.catalyst.analysis.SimpleFunctionRegistryBase.lookupFunction$(FunctionRegistry.scala:240)\n\tat org.apache.spark.sql.catalyst.analysis.SimpleFunctionRegistry.lookupFunction(FunctionRegistry.scala:305)\n\tat org.apache.spark.sql.catalyst.catalog.SessionCatalog.$anonfun$resolveBuiltinOrTempFunctionInternal$1(SessionCatalog.scala:2121)\n\tat org.apache.spark.sql.catalyst.catalog.SessionCatalog.lookupTempFuncWithViewContext(SessionCatalog.scala:2143)\n\tat org.apache.spark.sql.catalyst.catalog.SessionCatalog.resolveBuiltinOrTempFunctionInternal(SessionCatalog.scala:2121)\n\tat org.apache.spark.sql.catalyst.catalog.SessionCatalog.resolveBuiltinOrTempFunction(SessionCatalog.scala:2098)\n\tat org.apache.spark.sql.catalyst.analysis.FunctionResolution.resolveBuiltinOrTempFunction(FunctionResolution.scala:102)\n\tat org.apache.spark.sql.catalyst.analysis.FunctionResolution.$anonfun$resolveFunction$1(FunctionResolution.scala:51)\n\tat org.apache.spark.sql.catalyst.analysis.package$.withPosition(package.scala:104)\n\tat org.apache.spark.sql.catalyst.analysis.FunctionResolution.resolveFunction(FunctionResolution.scala:51)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveFunctions$$anonfun$apply$19$$anonfun$applyOrElse$141.applyOrElse(Analyzer.scala:2256)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveFunctions$$anonfun$apply$19$$anonfun$applyOrElse$141.applyOrElse(Analyzer.scala:2226)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformUpWithPruning$2(TreeNode.scala:549)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:107)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformUpWithPruning(TreeNode.scala:549)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformUpWithPruning$1(TreeNode.scala:546)\n\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1268)\n\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1267)\n\tat org.apache.spark.sql.catalyst.expressions.UnaryExpression.mapChildren(Expression.scala:587)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformUpWithPruning(TreeNode.scala:546)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.$anonfun$transformExpressionsUpWithPruning$1(QueryPlan.scala:245)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.$anonfun$mapExpressions$1(QueryPlan.scala:257)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:107)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpression$1(QueryPlan.scala:257)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.recursiveTransform$1(QueryPlan.scala:269)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.$anonfun$mapExpressions$4(QueryPlan.scala:275)\n\tat scala.collection.immutable.List.map(List.scala:236)\n\tat scala.collection.immutable.List.map(List.scala:79)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.recursiveTransform$1(QueryPlan.scala:275)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.$anonfun$mapExpressions$5(QueryPlan.scala:280)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:337)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.mapExpressions(QueryPlan.scala:280)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpressionsUpWithPruning(QueryPlan.scala:245)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveFunctions$$anonfun$apply$19.applyOrElse(Analyzer.scala:2226)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveFunctions$$anonfun$apply$19.applyOrElse(Analyzer.scala:2096)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$3(AnalysisHelper.scala:139)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:107)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:139)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:416)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:135)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:131)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:37)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveFunctions$.apply(Analyzer.scala:2096)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveFunctions$.apply(Analyzer.scala:2093)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:248)\n\tat scala.collection.LinearSeqOps.foldLeft(LinearSeq.scala:183)\n\tat scala.collection.LinearSeqOps.foldLeft$(LinearSeq.scala:179)\n\tat scala.collection.immutable.List.foldLeft(List.scala:79)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:245)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:237)\n\tat scala.collection.immutable.List.foreach(List.scala:323)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:237)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext(Analyzer.scala:344)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:340)\n\tat org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:224)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:340)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:290)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:207)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:207)\n\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.resolveInFixedPoint(HybridAnalyzer.scala:237)\n\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.$anonfun$apply$1(HybridAnalyzer.scala:92)\n\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.withTrackedAnalyzerBridgeState(HybridAnalyzer.scala:123)\n\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.apply(HybridAnalyzer.scala:85)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:323)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:423)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:323)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$2(QueryExecution.scala:139)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:148)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:325)\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:712)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:325)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:810)\n\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:324)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$1(QueryExecution.scala:139)\n\tat scala.util.Try$.apply(Try.scala:217)\n\tat org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1392)\n\tat org.apache.spark.util.Utils$.getTryWithCallerStacktrace(Utils.scala:1453)\n\tat org.apache.spark.util.LazyTry.get(LazyTry.scala:58)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:150)\n\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:90)\n\tat org.apache.spark.sql.classic.Dataset$.$anonfun$ofRows$5(Dataset.scala:138)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:810)\n\tat org.apache.spark.sql.classic.Dataset$.ofRows(Dataset.scala:135)\n\tat org.apache.spark.sql.classic.SparkSession.$anonfun$sql$1(SparkSession.scala:534)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:810)\n\tat org.apache.spark.sql.classic.SparkSession.sql(SparkSession.scala:504)\n\tat org.apache.spark.sql.classic.SparkSession.sql(SparkSession.scala:539)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:75)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:52)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:580)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:184)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:108)\n\tat java.base/java.lang.Thread.run(Thread.java:1583)\n\tSuppressed: org.apache.spark.util.Utils$OriginalTryStackTraceException: Full stacktrace of original doTryWithCallerStacktrace caller\n\t\tat org.apache.spark.sql.errors.QueryCompilationErrors$.doubleNamedArgumentReference(QueryCompilationErrors.scala:90)\n\t\tat org.apache.spark.sql.catalyst.plans.logical.NamedParametersSupport$.$anonfun$splitAndCheckNamedArguments$2(FunctionBuilderBase.scala:108)\n\t\tat scala.collection.immutable.Vector1.map(Vector.scala:2155)\n\t\tat scala.collection.immutable.Vector1.map(Vector.scala:386)\n\t\tat org.apache.spark.sql.catalyst.plans.logical.NamedParametersSupport$.splitAndCheckNamedArguments(FunctionBuilderBase.scala:104)\n\t\tat org.apache.spark.sql.execution.python.UserDefinedPythonFunction.builder(UserDefinedPythonFunction.scala:62)\n\t\tat org.apache.spark.sql.classic.UDFRegistration.$anonfun$registerPython$1(UDFRegistration.scala:64)\n\t\tat org.apache.spark.sql.catalyst.analysis.SimpleFunctionRegistryBase.lookupFunction(FunctionRegistry.scala:246)\n\t\tat org.apache.spark.sql.catalyst.analysis.SimpleFunctionRegistryBase.lookupFunction$(FunctionRegistry.scala:240)\n\t\tat org.apache.spark.sql.catalyst.analysis.SimpleFunctionRegistry.lookupFunction(FunctionRegistry.scala:305)\n\t\tat org.apache.spark.sql.catalyst.catalog.SessionCatalog.$anonfun$resolveBuiltinOrTempFunctionInternal$1(SessionCatalog.scala:2121)\n\t\tat org.apache.spark.sql.catalyst.catalog.SessionCatalog.lookupTempFuncWithViewContext(SessionCatalog.scala:2143)\n\t\tat org.apache.spark.sql.catalyst.catalog.SessionCatalog.resolveBuiltinOrTempFunctionInternal(SessionCatalog.scala:2121)\n\t\tat org.apache.spark.sql.catalyst.catalog.SessionCatalog.resolveBuiltinOrTempFunction(SessionCatalog.scala:2098)\n\t\tat org.apache.spark.sql.catalyst.analysis.FunctionResolution.resolveBuiltinOrTempFunction(FunctionResolution.scala:102)\n\t\tat org.apache.spark.sql.catalyst.analysis.FunctionResolution.$anonfun$resolveFunction$1(FunctionResolution.scala:51)\n\t\tat org.apache.spark.sql.catalyst.analysis.package$.withPosition(package.scala:104)\n\t\tat org.apache.spark.sql.catalyst.analysis.FunctionResolution.resolveFunction(FunctionResolution.scala:51)\n\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveFunctions$$anonfun$apply$19$$anonfun$applyOrElse$141.applyOrElse(Analyzer.scala:2256)\n\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveFunctions$$anonfun$apply$19$$anonfun$applyOrElse$141.applyOrElse(Analyzer.scala:2226)\n\t\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformUpWithPruning$2(TreeNode.scala:549)\n\t\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:107)\n\t\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformUpWithPruning(TreeNode.scala:549)\n\t\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformUpWithPruning$1(TreeNode.scala:546)\n\t\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1268)\n\t\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1267)\n\t\tat org.apache.spark.sql.catalyst.expressions.UnaryExpression.mapChildren(Expression.scala:587)\n\t\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformUpWithPruning(TreeNode.scala:546)\n\t\tat org.apache.spark.sql.catalyst.plans.QueryPlan.$anonfun$transformExpressionsUpWithPruning$1(QueryPlan.scala:245)\n\t\tat org.apache.spark.sql.catalyst.plans.QueryPlan.$anonfun$mapExpressions$1(QueryPlan.scala:257)\n\t\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:107)\n\t\tat org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpression$1(QueryPlan.scala:257)\n\t\tat org.apache.spark.sql.catalyst.plans.QueryPlan.recursiveTransform$1(QueryPlan.scala:269)\n\t\tat org.apache.spark.sql.catalyst.plans.QueryPlan.$anonfun$mapExpressions$4(QueryPlan.scala:275)\n\t\tat scala.collection.immutable.List.map(List.scala:236)\n\t\tat scala.collection.immutable.List.map(List.scala:79)\n\t\tat org.apache.spark.sql.catalyst.plans.QueryPlan.recursiveTransform$1(QueryPlan.scala:275)\n\t\tat org.apache.spark.sql.catalyst.plans.QueryPlan.$anonfun$mapExpressions$5(QueryPlan.scala:280)\n\t\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:337)\n\t\tat org.apache.spark.sql.catalyst.plans.QueryPlan.mapExpressions(QueryPlan.scala:280)\n\t\tat org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpressionsUpWithPruning(QueryPlan.scala:245)\n\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveFunctions$$anonfun$apply$19.applyOrElse(Analyzer.scala:2226)\n\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveFunctions$$anonfun$apply$19.applyOrElse(Analyzer.scala:2096)\n\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$3(AnalysisHelper.scala:139)\n\t\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:107)\n\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:139)\n\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:416)\n\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:135)\n\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:131)\n\t\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:37)\n\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveFunctions$.apply(Analyzer.scala:2096)\n\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveFunctions$.apply(Analyzer.scala:2093)\n\t\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:248)\n\t\tat scala.collection.LinearSeqOps.foldLeft(LinearSeq.scala:183)\n\t\tat scala.collection.LinearSeqOps.foldLeft$(LinearSeq.scala:179)\n\t\tat scala.collection.immutable.List.foldLeft(List.scala:79)\n\t\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:245)\n\t\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:237)\n\t\tat scala.collection.immutable.List.foreach(List.scala:323)\n\t\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:237)\n\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext(Analyzer.scala:344)\n\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:340)\n\t\tat org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:224)\n\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:340)\n\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:290)\n\t\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:207)\n\t\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)\n\t\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:207)\n\t\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.resolveInFixedPoint(HybridAnalyzer.scala:237)\n\t\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.$anonfun$apply$1(HybridAnalyzer.scala:92)\n\t\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.withTrackedAnalyzerBridgeState(HybridAnalyzer.scala:123)\n\t\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.apply(HybridAnalyzer.scala:85)\n\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:323)\n\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:423)\n\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:323)\n\t\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$2(QueryExecution.scala:139)\n\t\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:148)\n\t\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:325)\n\t\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:712)\n\t\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:325)\n\t\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:810)\n\t\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:324)\n\t\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$1(QueryExecution.scala:139)\n\t\tat scala.util.Try$.apply(Try.scala:217)\n\t\tat org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1392)\n\t\tat org.apache.spark.util.LazyTry.tryT$lzycompute(LazyTry.scala:46)\n\t\tat org.apache.spark.util.LazyTry.tryT(LazyTry.scala:46)\n\t\t... 22 more\n", "stacktrace": [{"class": null, "method": "deco", "file": "/home/gaogaotiantian/programs/spark/python/pyspark/errors/exceptions/captured.py", "line": "263"}, {"class": null, "method": "get_return_value", "file": "/home/gaogaotiantian/programs/spark/python/lib/py4j-0.10.9.9-src.zip/py4j/protocol.py", "line": "327"}]}}
{"ts": "2026-01-02 11:16:25.095", "level": "ERROR", "logger": "SQLQueryContextLogger", "msg": "[UNEXPECTED_POSITIONAL_ARGUMENT] Cannot invoke routine `test_udf` because it contains positional argument(s) following the named argument assigned to `a`; please rearrange them so the positional arguments come first and then retry the query again. SQLSTATE: 4274K", "context": {"errorClass": "UNEXPECTED_POSITIONAL_ARGUMENT"}, "exception": {"class": "Py4JJavaError", "msg": "An error occurred while calling o85.sql.\n: org.apache.spark.sql.AnalysisException: [UNEXPECTED_POSITIONAL_ARGUMENT] Cannot invoke routine `test_udf` because it contains positional argument(s) following the named argument assigned to `a`; please rearrange them so the positional arguments come first and then retry the query again. SQLSTATE: 4274K; line 1 pos 7\n\tat org.apache.spark.sql.errors.QueryCompilationErrors$.unexpectedPositionalArgument(QueryCompilationErrors.scala:128)\n\tat org.apache.spark.sql.catalyst.plans.logical.NamedParametersSupport$.$anonfun$splitAndCheckNamedArguments$2(FunctionBuilderBase.scala:114)\n\tat scala.collection.immutable.Vector1.map(Vector.scala:2155)\n\tat scala.collection.immutable.Vector1.map(Vector.scala:386)\n\tat org.apache.spark.sql.catalyst.plans.logical.NamedParametersSupport$.splitAndCheckNamedArguments(FunctionBuilderBase.scala:104)\n\tat org.apache.spark.sql.execution.python.UserDefinedPythonFunction.builder(UserDefinedPythonFunction.scala:62)\n\tat org.apache.spark.sql.classic.UDFRegistration.$anonfun$registerPython$1(UDFRegistration.scala:64)\n\tat org.apache.spark.sql.catalyst.analysis.SimpleFunctionRegistryBase.lookupFunction(FunctionRegistry.scala:246)\n\tat org.apache.spark.sql.catalyst.analysis.SimpleFunctionRegistryBase.lookupFunction$(FunctionRegistry.scala:240)\n\tat org.apache.spark.sql.catalyst.analysis.SimpleFunctionRegistry.lookupFunction(FunctionRegistry.scala:305)\n\tat org.apache.spark.sql.catalyst.catalog.SessionCatalog.$anonfun$resolveBuiltinOrTempFunctionInternal$1(SessionCatalog.scala:2121)\n\tat org.apache.spark.sql.catalyst.catalog.SessionCatalog.lookupTempFuncWithViewContext(SessionCatalog.scala:2143)\n\tat org.apache.spark.sql.catalyst.catalog.SessionCatalog.resolveBuiltinOrTempFunctionInternal(SessionCatalog.scala:2121)\n\tat org.apache.spark.sql.catalyst.catalog.SessionCatalog.resolveBuiltinOrTempFunction(SessionCatalog.scala:2098)\n\tat org.apache.spark.sql.catalyst.analysis.FunctionResolution.resolveBuiltinOrTempFunction(FunctionResolution.scala:102)\n\tat org.apache.spark.sql.catalyst.analysis.FunctionResolution.$anonfun$resolveFunction$1(FunctionResolution.scala:51)\n\tat org.apache.spark.sql.catalyst.analysis.package$.withPosition(package.scala:104)\n\tat org.apache.spark.sql.catalyst.analysis.FunctionResolution.resolveFunction(FunctionResolution.scala:51)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveFunctions$$anonfun$apply$19$$anonfun$applyOrElse$141.applyOrElse(Analyzer.scala:2256)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveFunctions$$anonfun$apply$19$$anonfun$applyOrElse$141.applyOrElse(Analyzer.scala:2226)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformUpWithPruning$2(TreeNode.scala:549)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:107)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformUpWithPruning(TreeNode.scala:549)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformUpWithPruning$1(TreeNode.scala:546)\n\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1268)\n\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1267)\n\tat org.apache.spark.sql.catalyst.expressions.UnaryExpression.mapChildren(Expression.scala:587)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformUpWithPruning(TreeNode.scala:546)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.$anonfun$transformExpressionsUpWithPruning$1(QueryPlan.scala:245)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.$anonfun$mapExpressions$1(QueryPlan.scala:257)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:107)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpression$1(QueryPlan.scala:257)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.recursiveTransform$1(QueryPlan.scala:269)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.$anonfun$mapExpressions$4(QueryPlan.scala:275)\n\tat scala.collection.immutable.List.map(List.scala:236)\n\tat scala.collection.immutable.List.map(List.scala:79)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.recursiveTransform$1(QueryPlan.scala:275)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.$anonfun$mapExpressions$5(QueryPlan.scala:280)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:337)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.mapExpressions(QueryPlan.scala:280)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpressionsUpWithPruning(QueryPlan.scala:245)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveFunctions$$anonfun$apply$19.applyOrElse(Analyzer.scala:2226)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveFunctions$$anonfun$apply$19.applyOrElse(Analyzer.scala:2096)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$3(AnalysisHelper.scala:139)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:107)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:139)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:416)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:135)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:131)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:37)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveFunctions$.apply(Analyzer.scala:2096)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveFunctions$.apply(Analyzer.scala:2093)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:248)\n\tat scala.collection.LinearSeqOps.foldLeft(LinearSeq.scala:183)\n\tat scala.collection.LinearSeqOps.foldLeft$(LinearSeq.scala:179)\n\tat scala.collection.immutable.List.foldLeft(List.scala:79)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:245)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:237)\n\tat scala.collection.immutable.List.foreach(List.scala:323)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:237)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext(Analyzer.scala:344)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:340)\n\tat org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:224)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:340)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:290)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:207)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:207)\n\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.resolveInFixedPoint(HybridAnalyzer.scala:237)\n\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.$anonfun$apply$1(HybridAnalyzer.scala:92)\n\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.withTrackedAnalyzerBridgeState(HybridAnalyzer.scala:123)\n\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.apply(HybridAnalyzer.scala:85)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:323)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:423)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:323)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$2(QueryExecution.scala:139)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:148)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:325)\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:712)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:325)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:810)\n\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:324)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$1(QueryExecution.scala:139)\n\tat scala.util.Try$.apply(Try.scala:217)\n\tat org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1392)\n\tat org.apache.spark.util.Utils$.getTryWithCallerStacktrace(Utils.scala:1453)\n\tat org.apache.spark.util.LazyTry.get(LazyTry.scala:58)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:150)\n\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:90)\n\tat org.apache.spark.sql.classic.Dataset$.$anonfun$ofRows$5(Dataset.scala:138)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:810)\n\tat org.apache.spark.sql.classic.Dataset$.ofRows(Dataset.scala:135)\n\tat org.apache.spark.sql.classic.SparkSession.$anonfun$sql$1(SparkSession.scala:534)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:810)\n\tat org.apache.spark.sql.classic.SparkSession.sql(SparkSession.scala:504)\n\tat org.apache.spark.sql.classic.SparkSession.sql(SparkSession.scala:539)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:75)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:52)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:580)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:184)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:108)\n\tat java.base/java.lang.Thread.run(Thread.java:1583)\n\tSuppressed: org.apache.spark.util.Utils$OriginalTryStackTraceException: Full stacktrace of original doTryWithCallerStacktrace caller\n\t\tat org.apache.spark.sql.errors.QueryCompilationErrors$.unexpectedPositionalArgument(QueryCompilationErrors.scala:128)\n\t\tat org.apache.spark.sql.catalyst.plans.logical.NamedParametersSupport$.$anonfun$splitAndCheckNamedArguments$2(FunctionBuilderBase.scala:114)\n\t\tat scala.collection.immutable.Vector1.map(Vector.scala:2155)\n\t\tat scala.collection.immutable.Vector1.map(Vector.scala:386)\n\t\tat org.apache.spark.sql.catalyst.plans.logical.NamedParametersSupport$.splitAndCheckNamedArguments(FunctionBuilderBase.scala:104)\n\t\tat org.apache.spark.sql.execution.python.UserDefinedPythonFunction.builder(UserDefinedPythonFunction.scala:62)\n\t\tat org.apache.spark.sql.classic.UDFRegistration.$anonfun$registerPython$1(UDFRegistration.scala:64)\n\t\tat org.apache.spark.sql.catalyst.analysis.SimpleFunctionRegistryBase.lookupFunction(FunctionRegistry.scala:246)\n\t\tat org.apache.spark.sql.catalyst.analysis.SimpleFunctionRegistryBase.lookupFunction$(FunctionRegistry.scala:240)\n\t\tat org.apache.spark.sql.catalyst.analysis.SimpleFunctionRegistry.lookupFunction(FunctionRegistry.scala:305)\n\t\tat org.apache.spark.sql.catalyst.catalog.SessionCatalog.$anonfun$resolveBuiltinOrTempFunctionInternal$1(SessionCatalog.scala:2121)\n\t\tat org.apache.spark.sql.catalyst.catalog.SessionCatalog.lookupTempFuncWithViewContext(SessionCatalog.scala:2143)\n\t\tat org.apache.spark.sql.catalyst.catalog.SessionCatalog.resolveBuiltinOrTempFunctionInternal(SessionCatalog.scala:2121)\n\t\tat org.apache.spark.sql.catalyst.catalog.SessionCatalog.resolveBuiltinOrTempFunction(SessionCatalog.scala:2098)\n\t\tat org.apache.spark.sql.catalyst.analysis.FunctionResolution.resolveBuiltinOrTempFunction(FunctionResolution.scala:102)\n\t\tat org.apache.spark.sql.catalyst.analysis.FunctionResolution.$anonfun$resolveFunction$1(FunctionResolution.scala:51)\n\t\tat org.apache.spark.sql.catalyst.analysis.package$.withPosition(package.scala:104)\n\t\tat org.apache.spark.sql.catalyst.analysis.FunctionResolution.resolveFunction(FunctionResolution.scala:51)\n\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveFunctions$$anonfun$apply$19$$anonfun$applyOrElse$141.applyOrElse(Analyzer.scala:2256)\n\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveFunctions$$anonfun$apply$19$$anonfun$applyOrElse$141.applyOrElse(Analyzer.scala:2226)\n\t\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformUpWithPruning$2(TreeNode.scala:549)\n\t\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:107)\n\t\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformUpWithPruning(TreeNode.scala:549)\n\t\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformUpWithPruning$1(TreeNode.scala:546)\n\t\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1268)\n\t\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1267)\n\t\tat org.apache.spark.sql.catalyst.expressions.UnaryExpression.mapChildren(Expression.scala:587)\n\t\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformUpWithPruning(TreeNode.scala:546)\n\t\tat org.apache.spark.sql.catalyst.plans.QueryPlan.$anonfun$transformExpressionsUpWithPruning$1(QueryPlan.scala:245)\n\t\tat org.apache.spark.sql.catalyst.plans.QueryPlan.$anonfun$mapExpressions$1(QueryPlan.scala:257)\n\t\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:107)\n\t\tat org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpression$1(QueryPlan.scala:257)\n\t\tat org.apache.spark.sql.catalyst.plans.QueryPlan.recursiveTransform$1(QueryPlan.scala:269)\n\t\tat org.apache.spark.sql.catalyst.plans.QueryPlan.$anonfun$mapExpressions$4(QueryPlan.scala:275)\n\t\tat scala.collection.immutable.List.map(List.scala:236)\n\t\tat scala.collection.immutable.List.map(List.scala:79)\n\t\tat org.apache.spark.sql.catalyst.plans.QueryPlan.recursiveTransform$1(QueryPlan.scala:275)\n\t\tat org.apache.spark.sql.catalyst.plans.QueryPlan.$anonfun$mapExpressions$5(QueryPlan.scala:280)\n\t\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:337)\n\t\tat org.apache.spark.sql.catalyst.plans.QueryPlan.mapExpressions(QueryPlan.scala:280)\n\t\tat org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpressionsUpWithPruning(QueryPlan.scala:245)\n\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveFunctions$$anonfun$apply$19.applyOrElse(Analyzer.scala:2226)\n\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveFunctions$$anonfun$apply$19.applyOrElse(Analyzer.scala:2096)\n\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$3(AnalysisHelper.scala:139)\n\t\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:107)\n\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:139)\n\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:416)\n\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:135)\n\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:131)\n\t\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:37)\n\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveFunctions$.apply(Analyzer.scala:2096)\n\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveFunctions$.apply(Analyzer.scala:2093)\n\t\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:248)\n\t\tat scala.collection.LinearSeqOps.foldLeft(LinearSeq.scala:183)\n\t\tat scala.collection.LinearSeqOps.foldLeft$(LinearSeq.scala:179)\n\t\tat scala.collection.immutable.List.foldLeft(List.scala:79)\n\t\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:245)\n\t\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:237)\n\t\tat scala.collection.immutable.List.foreach(List.scala:323)\n\t\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:237)\n\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext(Analyzer.scala:344)\n\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:340)\n\t\tat org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:224)\n\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:340)\n\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:290)\n\t\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:207)\n\t\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)\n\t\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:207)\n\t\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.resolveInFixedPoint(HybridAnalyzer.scala:237)\n\t\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.$anonfun$apply$1(HybridAnalyzer.scala:92)\n\t\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.withTrackedAnalyzerBridgeState(HybridAnalyzer.scala:123)\n\t\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.apply(HybridAnalyzer.scala:85)\n\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:323)\n\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:423)\n\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:323)\n\t\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$2(QueryExecution.scala:139)\n\t\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:148)\n\t\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:325)\n\t\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:712)\n\t\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:325)\n\t\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:810)\n\t\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:324)\n\t\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$1(QueryExecution.scala:139)\n\t\tat scala.util.Try$.apply(Try.scala:217)\n\t\tat org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1392)\n\t\tat org.apache.spark.util.LazyTry.tryT$lzycompute(LazyTry.scala:46)\n\t\tat org.apache.spark.util.LazyTry.tryT(LazyTry.scala:46)\n\t\t... 22 more\n", "stacktrace": [{"class": null, "method": "deco", "file": "/home/gaogaotiantian/programs/spark/python/pyspark/errors/exceptions/captured.py", "line": "263"}, {"class": null, "method": "get_return_value", "file": "/home/gaogaotiantian/programs/spark/python/lib/py4j-0.10.9.9-src.zip/py4j/protocol.py", "line": "327"}]}}
</system-err>
	</testcase>
	<testcase classname="pyspark.sql.tests.test_udf.UDFTests" name="test_multiple_udfs" time="0.289" timestamp="2026-01-02T11:16:25" file="python/pyspark/sql/tests/test_udf.py" line="222"/>
	<testcase classname="pyspark.sql.tests.test_udf.UDFTests" name="test_multiple_udfs_with_logging" time="0.811" timestamp="2026-01-02T11:16:26" file="python/pyspark/sql/tests/test_udf.py" line="1637"/>
	<testcase classname="pyspark.sql.tests.test_udf.UDFTests" name="test_named_arguments" time="0.525" timestamp="2026-01-02T11:16:26" file="python/pyspark/sql/tests/test_udf.py" line="1086"/>
	<testcase classname="pyspark.sql.tests.test_udf.UDFTests" name="test_named_arguments_and_defaults" time="0.843" timestamp="2026-01-02T11:16:27" file="python/pyspark/sql/tests/test_udf.py" line="1163"/>
	<testcase classname="pyspark.sql.tests.test_udf.UDFTests" name="test_named_arguments_negative" time="0.563" timestamp="2026-01-02T11:16:28" file="python/pyspark/sql/tests/test_udf.py" line="1107">
		<system-err>{"ts": "2026-01-02 11:16:27.683", "level": "ERROR", "logger": "SQLQueryContextLogger", "msg": "[DUPLICATE_ROUTINE_PARAMETER_ASSIGNMENT.DOUBLE_NAMED_ARGUMENT_REFERENCE] Call to routine `test_udf` is invalid because it includes multiple argument assignments to the same parameter name `a`. More than one named argument referred to the same parameter. Please assign a value only once. SQLSTATE: 4274K", "context": {"errorClass": "DUPLICATE_ROUTINE_PARAMETER_ASSIGNMENT.DOUBLE_NAMED_ARGUMENT_REFERENCE"}, "exception": {"class": "Py4JJavaError", "msg": "An error occurred while calling o85.sql.\n: org.apache.spark.sql.AnalysisException: [DUPLICATE_ROUTINE_PARAMETER_ASSIGNMENT.DOUBLE_NAMED_ARGUMENT_REFERENCE] Call to routine `test_udf` is invalid because it includes multiple argument assignments to the same parameter name `a`. More than one named argument referred to the same parameter. Please assign a value only once. SQLSTATE: 4274K; line 1 pos 7\n\tat org.apache.spark.sql.errors.QueryCompilationErrors$.doubleNamedArgumentReference(QueryCompilationErrors.scala:90)\n\tat org.apache.spark.sql.catalyst.plans.logical.NamedParametersSupport$.$anonfun$splitAndCheckNamedArguments$2(FunctionBuilderBase.scala:108)\n\tat scala.collection.immutable.Vector1.map(Vector.scala:2155)\n\tat scala.collection.immutable.Vector1.map(Vector.scala:386)\n\tat org.apache.spark.sql.catalyst.plans.logical.NamedParametersSupport$.splitAndCheckNamedArguments(FunctionBuilderBase.scala:104)\n\tat org.apache.spark.sql.execution.python.UserDefinedPythonFunction.builder(UserDefinedPythonFunction.scala:62)\n\tat org.apache.spark.sql.classic.UDFRegistration.$anonfun$registerPython$1(UDFRegistration.scala:64)\n\tat org.apache.spark.sql.catalyst.analysis.SimpleFunctionRegistryBase.lookupFunction(FunctionRegistry.scala:246)\n\tat org.apache.spark.sql.catalyst.analysis.SimpleFunctionRegistryBase.lookupFunction$(FunctionRegistry.scala:240)\n\tat org.apache.spark.sql.catalyst.analysis.SimpleFunctionRegistry.lookupFunction(FunctionRegistry.scala:305)\n\tat org.apache.spark.sql.catalyst.catalog.SessionCatalog.$anonfun$resolveBuiltinOrTempFunctionInternal$1(SessionCatalog.scala:2121)\n\tat org.apache.spark.sql.catalyst.catalog.SessionCatalog.lookupTempFuncWithViewContext(SessionCatalog.scala:2143)\n\tat org.apache.spark.sql.catalyst.catalog.SessionCatalog.resolveBuiltinOrTempFunctionInternal(SessionCatalog.scala:2121)\n\tat org.apache.spark.sql.catalyst.catalog.SessionCatalog.resolveBuiltinOrTempFunction(SessionCatalog.scala:2098)\n\tat org.apache.spark.sql.catalyst.analysis.FunctionResolution.resolveBuiltinOrTempFunction(FunctionResolution.scala:102)\n\tat org.apache.spark.sql.catalyst.analysis.FunctionResolution.$anonfun$resolveFunction$1(FunctionResolution.scala:51)\n\tat org.apache.spark.sql.catalyst.analysis.package$.withPosition(package.scala:104)\n\tat org.apache.spark.sql.catalyst.analysis.FunctionResolution.resolveFunction(FunctionResolution.scala:51)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveFunctions$$anonfun$apply$19$$anonfun$applyOrElse$141.applyOrElse(Analyzer.scala:2256)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveFunctions$$anonfun$apply$19$$anonfun$applyOrElse$141.applyOrElse(Analyzer.scala:2226)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformUpWithPruning$2(TreeNode.scala:549)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:107)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformUpWithPruning(TreeNode.scala:549)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformUpWithPruning$1(TreeNode.scala:546)\n\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1268)\n\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1267)\n\tat org.apache.spark.sql.catalyst.expressions.UnaryExpression.mapChildren(Expression.scala:587)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformUpWithPruning(TreeNode.scala:546)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.$anonfun$transformExpressionsUpWithPruning$1(QueryPlan.scala:245)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.$anonfun$mapExpressions$1(QueryPlan.scala:257)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:107)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpression$1(QueryPlan.scala:257)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.recursiveTransform$1(QueryPlan.scala:269)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.$anonfun$mapExpressions$4(QueryPlan.scala:275)\n\tat scala.collection.immutable.List.map(List.scala:236)\n\tat scala.collection.immutable.List.map(List.scala:79)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.recursiveTransform$1(QueryPlan.scala:275)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.$anonfun$mapExpressions$5(QueryPlan.scala:280)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:337)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.mapExpressions(QueryPlan.scala:280)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpressionsUpWithPruning(QueryPlan.scala:245)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveFunctions$$anonfun$apply$19.applyOrElse(Analyzer.scala:2226)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveFunctions$$anonfun$apply$19.applyOrElse(Analyzer.scala:2096)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$3(AnalysisHelper.scala:139)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:107)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:139)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:416)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:135)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:131)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:37)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveFunctions$.apply(Analyzer.scala:2096)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveFunctions$.apply(Analyzer.scala:2093)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:248)\n\tat scala.collection.LinearSeqOps.foldLeft(LinearSeq.scala:183)\n\tat scala.collection.LinearSeqOps.foldLeft$(LinearSeq.scala:179)\n\tat scala.collection.immutable.List.foldLeft(List.scala:79)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:245)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:237)\n\tat scala.collection.immutable.List.foreach(List.scala:323)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:237)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext(Analyzer.scala:344)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:340)\n\tat org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:224)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:340)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:290)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:207)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:207)\n\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.resolveInFixedPoint(HybridAnalyzer.scala:237)\n\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.$anonfun$apply$1(HybridAnalyzer.scala:92)\n\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.withTrackedAnalyzerBridgeState(HybridAnalyzer.scala:123)\n\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.apply(HybridAnalyzer.scala:85)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:323)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:423)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:323)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$2(QueryExecution.scala:139)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:148)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:325)\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:712)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:325)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:810)\n\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:324)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$1(QueryExecution.scala:139)\n\tat scala.util.Try$.apply(Try.scala:217)\n\tat org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1392)\n\tat org.apache.spark.util.Utils$.getTryWithCallerStacktrace(Utils.scala:1453)\n\tat org.apache.spark.util.LazyTry.get(LazyTry.scala:58)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:150)\n\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:90)\n\tat org.apache.spark.sql.classic.Dataset$.$anonfun$ofRows$5(Dataset.scala:138)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:810)\n\tat org.apache.spark.sql.classic.Dataset$.ofRows(Dataset.scala:135)\n\tat org.apache.spark.sql.classic.SparkSession.$anonfun$sql$1(SparkSession.scala:534)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:810)\n\tat org.apache.spark.sql.classic.SparkSession.sql(SparkSession.scala:504)\n\tat org.apache.spark.sql.classic.SparkSession.sql(SparkSession.scala:539)\n\tat jdk.internal.reflect.GeneratedMethodAccessor77.invoke(Unknown Source)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:52)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:580)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:184)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:108)\n\tat java.base/java.lang.Thread.run(Thread.java:1583)\n\tSuppressed: org.apache.spark.util.Utils$OriginalTryStackTraceException: Full stacktrace of original doTryWithCallerStacktrace caller\n\t\tat org.apache.spark.sql.errors.QueryCompilationErrors$.doubleNamedArgumentReference(QueryCompilationErrors.scala:90)\n\t\tat org.apache.spark.sql.catalyst.plans.logical.NamedParametersSupport$.$anonfun$splitAndCheckNamedArguments$2(FunctionBuilderBase.scala:108)\n\t\tat scala.collection.immutable.Vector1.map(Vector.scala:2155)\n\t\tat scala.collection.immutable.Vector1.map(Vector.scala:386)\n\t\tat org.apache.spark.sql.catalyst.plans.logical.NamedParametersSupport$.splitAndCheckNamedArguments(FunctionBuilderBase.scala:104)\n\t\tat org.apache.spark.sql.execution.python.UserDefinedPythonFunction.builder(UserDefinedPythonFunction.scala:62)\n\t\tat org.apache.spark.sql.classic.UDFRegistration.$anonfun$registerPython$1(UDFRegistration.scala:64)\n\t\tat org.apache.spark.sql.catalyst.analysis.SimpleFunctionRegistryBase.lookupFunction(FunctionRegistry.scala:246)\n\t\tat org.apache.spark.sql.catalyst.analysis.SimpleFunctionRegistryBase.lookupFunction$(FunctionRegistry.scala:240)\n\t\tat org.apache.spark.sql.catalyst.analysis.SimpleFunctionRegistry.lookupFunction(FunctionRegistry.scala:305)\n\t\tat org.apache.spark.sql.catalyst.catalog.SessionCatalog.$anonfun$resolveBuiltinOrTempFunctionInternal$1(SessionCatalog.scala:2121)\n\t\tat org.apache.spark.sql.catalyst.catalog.SessionCatalog.lookupTempFuncWithViewContext(SessionCatalog.scala:2143)\n\t\tat org.apache.spark.sql.catalyst.catalog.SessionCatalog.resolveBuiltinOrTempFunctionInternal(SessionCatalog.scala:2121)\n\t\tat org.apache.spark.sql.catalyst.catalog.SessionCatalog.resolveBuiltinOrTempFunction(SessionCatalog.scala:2098)\n\t\tat org.apache.spark.sql.catalyst.analysis.FunctionResolution.resolveBuiltinOrTempFunction(FunctionResolution.scala:102)\n\t\tat org.apache.spark.sql.catalyst.analysis.FunctionResolution.$anonfun$resolveFunction$1(FunctionResolution.scala:51)\n\t\tat org.apache.spark.sql.catalyst.analysis.package$.withPosition(package.scala:104)\n\t\tat org.apache.spark.sql.catalyst.analysis.FunctionResolution.resolveFunction(FunctionResolution.scala:51)\n\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveFunctions$$anonfun$apply$19$$anonfun$applyOrElse$141.applyOrElse(Analyzer.scala:2256)\n\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveFunctions$$anonfun$apply$19$$anonfun$applyOrElse$141.applyOrElse(Analyzer.scala:2226)\n\t\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformUpWithPruning$2(TreeNode.scala:549)\n\t\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:107)\n\t\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformUpWithPruning(TreeNode.scala:549)\n\t\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformUpWithPruning$1(TreeNode.scala:546)\n\t\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1268)\n\t\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1267)\n\t\tat org.apache.spark.sql.catalyst.expressions.UnaryExpression.mapChildren(Expression.scala:587)\n\t\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformUpWithPruning(TreeNode.scala:546)\n\t\tat org.apache.spark.sql.catalyst.plans.QueryPlan.$anonfun$transformExpressionsUpWithPruning$1(QueryPlan.scala:245)\n\t\tat org.apache.spark.sql.catalyst.plans.QueryPlan.$anonfun$mapExpressions$1(QueryPlan.scala:257)\n\t\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:107)\n\t\tat org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpression$1(QueryPlan.scala:257)\n\t\tat org.apache.spark.sql.catalyst.plans.QueryPlan.recursiveTransform$1(QueryPlan.scala:269)\n\t\tat org.apache.spark.sql.catalyst.plans.QueryPlan.$anonfun$mapExpressions$4(QueryPlan.scala:275)\n\t\tat scala.collection.immutable.List.map(List.scala:236)\n\t\tat scala.collection.immutable.List.map(List.scala:79)\n\t\tat org.apache.spark.sql.catalyst.plans.QueryPlan.recursiveTransform$1(QueryPlan.scala:275)\n\t\tat org.apache.spark.sql.catalyst.plans.QueryPlan.$anonfun$mapExpressions$5(QueryPlan.scala:280)\n\t\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:337)\n\t\tat org.apache.spark.sql.catalyst.plans.QueryPlan.mapExpressions(QueryPlan.scala:280)\n\t\tat org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpressionsUpWithPruning(QueryPlan.scala:245)\n\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveFunctions$$anonfun$apply$19.applyOrElse(Analyzer.scala:2226)\n\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveFunctions$$anonfun$apply$19.applyOrElse(Analyzer.scala:2096)\n\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$3(AnalysisHelper.scala:139)\n\t\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:107)\n\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:139)\n\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:416)\n\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:135)\n\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:131)\n\t\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:37)\n\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveFunctions$.apply(Analyzer.scala:2096)\n\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveFunctions$.apply(Analyzer.scala:2093)\n\t\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:248)\n\t\tat scala.collection.LinearSeqOps.foldLeft(LinearSeq.scala:183)\n\t\tat scala.collection.LinearSeqOps.foldLeft$(LinearSeq.scala:179)\n\t\tat scala.collection.immutable.List.foldLeft(List.scala:79)\n\t\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:245)\n\t\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:237)\n\t\tat scala.collection.immutable.List.foreach(List.scala:323)\n\t\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:237)\n\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext(Analyzer.scala:344)\n\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:340)\n\t\tat org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:224)\n\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:340)\n\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:290)\n\t\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:207)\n\t\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)\n\t\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:207)\n\t\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.resolveInFixedPoint(HybridAnalyzer.scala:237)\n\t\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.$anonfun$apply$1(HybridAnalyzer.scala:92)\n\t\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.withTrackedAnalyzerBridgeState(HybridAnalyzer.scala:123)\n\t\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.apply(HybridAnalyzer.scala:85)\n\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:323)\n\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:423)\n\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:323)\n\t\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$2(QueryExecution.scala:139)\n\t\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:148)\n\t\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:325)\n\t\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:712)\n\t\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:325)\n\t\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:810)\n\t\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:324)\n\t\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$1(QueryExecution.scala:139)\n\t\tat scala.util.Try$.apply(Try.scala:217)\n\t\tat org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1392)\n\t\tat org.apache.spark.util.LazyTry.tryT$lzycompute(LazyTry.scala:46)\n\t\tat org.apache.spark.util.LazyTry.tryT(LazyTry.scala:46)\n\t\t... 21 more\n", "stacktrace": [{"class": null, "method": "deco", "file": "/home/gaogaotiantian/programs/spark/python/pyspark/errors/exceptions/captured.py", "line": "263"}, {"class": null, "method": "get_return_value", "file": "/home/gaogaotiantian/programs/spark/python/lib/py4j-0.10.9.9-src.zip/py4j/protocol.py", "line": "327"}]}}
{"ts": "2026-01-02 11:16:27.801", "level": "ERROR", "logger": "SQLQueryContextLogger", "msg": "[UNEXPECTED_POSITIONAL_ARGUMENT] Cannot invoke routine `test_udf` because it contains positional argument(s) following the named argument assigned to `a`; please rearrange them so the positional arguments come first and then retry the query again. SQLSTATE: 4274K", "context": {"errorClass": "UNEXPECTED_POSITIONAL_ARGUMENT"}, "exception": {"class": "Py4JJavaError", "msg": "An error occurred while calling o85.sql.\n: org.apache.spark.sql.AnalysisException: [UNEXPECTED_POSITIONAL_ARGUMENT] Cannot invoke routine `test_udf` because it contains positional argument(s) following the named argument assigned to `a`; please rearrange them so the positional arguments come first and then retry the query again. SQLSTATE: 4274K; line 1 pos 7\n\tat org.apache.spark.sql.errors.QueryCompilationErrors$.unexpectedPositionalArgument(QueryCompilationErrors.scala:128)\n\tat org.apache.spark.sql.catalyst.plans.logical.NamedParametersSupport$.$anonfun$splitAndCheckNamedArguments$2(FunctionBuilderBase.scala:114)\n\tat scala.collection.immutable.Vector1.map(Vector.scala:2155)\n\tat scala.collection.immutable.Vector1.map(Vector.scala:386)\n\tat org.apache.spark.sql.catalyst.plans.logical.NamedParametersSupport$.splitAndCheckNamedArguments(FunctionBuilderBase.scala:104)\n\tat org.apache.spark.sql.execution.python.UserDefinedPythonFunction.builder(UserDefinedPythonFunction.scala:62)\n\tat org.apache.spark.sql.classic.UDFRegistration.$anonfun$registerPython$1(UDFRegistration.scala:64)\n\tat org.apache.spark.sql.catalyst.analysis.SimpleFunctionRegistryBase.lookupFunction(FunctionRegistry.scala:246)\n\tat org.apache.spark.sql.catalyst.analysis.SimpleFunctionRegistryBase.lookupFunction$(FunctionRegistry.scala:240)\n\tat org.apache.spark.sql.catalyst.analysis.SimpleFunctionRegistry.lookupFunction(FunctionRegistry.scala:305)\n\tat org.apache.spark.sql.catalyst.catalog.SessionCatalog.$anonfun$resolveBuiltinOrTempFunctionInternal$1(SessionCatalog.scala:2121)\n\tat org.apache.spark.sql.catalyst.catalog.SessionCatalog.lookupTempFuncWithViewContext(SessionCatalog.scala:2143)\n\tat org.apache.spark.sql.catalyst.catalog.SessionCatalog.resolveBuiltinOrTempFunctionInternal(SessionCatalog.scala:2121)\n\tat org.apache.spark.sql.catalyst.catalog.SessionCatalog.resolveBuiltinOrTempFunction(SessionCatalog.scala:2098)\n\tat org.apache.spark.sql.catalyst.analysis.FunctionResolution.resolveBuiltinOrTempFunction(FunctionResolution.scala:102)\n\tat org.apache.spark.sql.catalyst.analysis.FunctionResolution.$anonfun$resolveFunction$1(FunctionResolution.scala:51)\n\tat org.apache.spark.sql.catalyst.analysis.package$.withPosition(package.scala:104)\n\tat org.apache.spark.sql.catalyst.analysis.FunctionResolution.resolveFunction(FunctionResolution.scala:51)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveFunctions$$anonfun$apply$19$$anonfun$applyOrElse$141.applyOrElse(Analyzer.scala:2256)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveFunctions$$anonfun$apply$19$$anonfun$applyOrElse$141.applyOrElse(Analyzer.scala:2226)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformUpWithPruning$2(TreeNode.scala:549)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:107)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformUpWithPruning(TreeNode.scala:549)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformUpWithPruning$1(TreeNode.scala:546)\n\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1268)\n\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1267)\n\tat org.apache.spark.sql.catalyst.expressions.UnaryExpression.mapChildren(Expression.scala:587)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformUpWithPruning(TreeNode.scala:546)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.$anonfun$transformExpressionsUpWithPruning$1(QueryPlan.scala:245)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.$anonfun$mapExpressions$1(QueryPlan.scala:257)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:107)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpression$1(QueryPlan.scala:257)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.recursiveTransform$1(QueryPlan.scala:269)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.$anonfun$mapExpressions$4(QueryPlan.scala:275)\n\tat scala.collection.immutable.List.map(List.scala:236)\n\tat scala.collection.immutable.List.map(List.scala:79)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.recursiveTransform$1(QueryPlan.scala:275)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.$anonfun$mapExpressions$5(QueryPlan.scala:280)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:337)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.mapExpressions(QueryPlan.scala:280)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpressionsUpWithPruning(QueryPlan.scala:245)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveFunctions$$anonfun$apply$19.applyOrElse(Analyzer.scala:2226)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveFunctions$$anonfun$apply$19.applyOrElse(Analyzer.scala:2096)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$3(AnalysisHelper.scala:139)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:107)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:139)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:416)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:135)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:131)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:37)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveFunctions$.apply(Analyzer.scala:2096)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveFunctions$.apply(Analyzer.scala:2093)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:248)\n\tat scala.collection.LinearSeqOps.foldLeft(LinearSeq.scala:183)\n\tat scala.collection.LinearSeqOps.foldLeft$(LinearSeq.scala:179)\n\tat scala.collection.immutable.List.foldLeft(List.scala:79)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:245)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:237)\n\tat scala.collection.immutable.List.foreach(List.scala:323)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:237)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext(Analyzer.scala:344)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:340)\n\tat org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:224)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:340)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:290)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:207)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:207)\n\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.resolveInFixedPoint(HybridAnalyzer.scala:237)\n\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.$anonfun$apply$1(HybridAnalyzer.scala:92)\n\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.withTrackedAnalyzerBridgeState(HybridAnalyzer.scala:123)\n\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.apply(HybridAnalyzer.scala:85)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:323)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:423)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:323)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$2(QueryExecution.scala:139)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:148)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:325)\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:712)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:325)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:810)\n\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:324)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$1(QueryExecution.scala:139)\n\tat scala.util.Try$.apply(Try.scala:217)\n\tat org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1392)\n\tat org.apache.spark.util.Utils$.getTryWithCallerStacktrace(Utils.scala:1453)\n\tat org.apache.spark.util.LazyTry.get(LazyTry.scala:58)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:150)\n\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:90)\n\tat org.apache.spark.sql.classic.Dataset$.$anonfun$ofRows$5(Dataset.scala:138)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:810)\n\tat org.apache.spark.sql.classic.Dataset$.ofRows(Dataset.scala:135)\n\tat org.apache.spark.sql.classic.SparkSession.$anonfun$sql$1(SparkSession.scala:534)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:810)\n\tat org.apache.spark.sql.classic.SparkSession.sql(SparkSession.scala:504)\n\tat org.apache.spark.sql.classic.SparkSession.sql(SparkSession.scala:539)\n\tat jdk.internal.reflect.GeneratedMethodAccessor77.invoke(Unknown Source)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:52)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:580)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:184)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:108)\n\tat java.base/java.lang.Thread.run(Thread.java:1583)\n\tSuppressed: org.apache.spark.util.Utils$OriginalTryStackTraceException: Full stacktrace of original doTryWithCallerStacktrace caller\n\t\tat org.apache.spark.sql.errors.QueryCompilationErrors$.unexpectedPositionalArgument(QueryCompilationErrors.scala:128)\n\t\tat org.apache.spark.sql.catalyst.plans.logical.NamedParametersSupport$.$anonfun$splitAndCheckNamedArguments$2(FunctionBuilderBase.scala:114)\n\t\tat scala.collection.immutable.Vector1.map(Vector.scala:2155)\n\t\tat scala.collection.immutable.Vector1.map(Vector.scala:386)\n\t\tat org.apache.spark.sql.catalyst.plans.logical.NamedParametersSupport$.splitAndCheckNamedArguments(FunctionBuilderBase.scala:104)\n\t\tat org.apache.spark.sql.execution.python.UserDefinedPythonFunction.builder(UserDefinedPythonFunction.scala:62)\n\t\tat org.apache.spark.sql.classic.UDFRegistration.$anonfun$registerPython$1(UDFRegistration.scala:64)\n\t\tat org.apache.spark.sql.catalyst.analysis.SimpleFunctionRegistryBase.lookupFunction(FunctionRegistry.scala:246)\n\t\tat org.apache.spark.sql.catalyst.analysis.SimpleFunctionRegistryBase.lookupFunction$(FunctionRegistry.scala:240)\n\t\tat org.apache.spark.sql.catalyst.analysis.SimpleFunctionRegistry.lookupFunction(FunctionRegistry.scala:305)\n\t\tat org.apache.spark.sql.catalyst.catalog.SessionCatalog.$anonfun$resolveBuiltinOrTempFunctionInternal$1(SessionCatalog.scala:2121)\n\t\tat org.apache.spark.sql.catalyst.catalog.SessionCatalog.lookupTempFuncWithViewContext(SessionCatalog.scala:2143)\n\t\tat org.apache.spark.sql.catalyst.catalog.SessionCatalog.resolveBuiltinOrTempFunctionInternal(SessionCatalog.scala:2121)\n\t\tat org.apache.spark.sql.catalyst.catalog.SessionCatalog.resolveBuiltinOrTempFunction(SessionCatalog.scala:2098)\n\t\tat org.apache.spark.sql.catalyst.analysis.FunctionResolution.resolveBuiltinOrTempFunction(FunctionResolution.scala:102)\n\t\tat org.apache.spark.sql.catalyst.analysis.FunctionResolution.$anonfun$resolveFunction$1(FunctionResolution.scala:51)\n\t\tat org.apache.spark.sql.catalyst.analysis.package$.withPosition(package.scala:104)\n\t\tat org.apache.spark.sql.catalyst.analysis.FunctionResolution.resolveFunction(FunctionResolution.scala:51)\n\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveFunctions$$anonfun$apply$19$$anonfun$applyOrElse$141.applyOrElse(Analyzer.scala:2256)\n\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveFunctions$$anonfun$apply$19$$anonfun$applyOrElse$141.applyOrElse(Analyzer.scala:2226)\n\t\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformUpWithPruning$2(TreeNode.scala:549)\n\t\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:107)\n\t\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformUpWithPruning(TreeNode.scala:549)\n\t\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformUpWithPruning$1(TreeNode.scala:546)\n\t\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1268)\n\t\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1267)\n\t\tat org.apache.spark.sql.catalyst.expressions.UnaryExpression.mapChildren(Expression.scala:587)\n\t\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformUpWithPruning(TreeNode.scala:546)\n\t\tat org.apache.spark.sql.catalyst.plans.QueryPlan.$anonfun$transformExpressionsUpWithPruning$1(QueryPlan.scala:245)\n\t\tat org.apache.spark.sql.catalyst.plans.QueryPlan.$anonfun$mapExpressions$1(QueryPlan.scala:257)\n\t\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:107)\n\t\tat org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpression$1(QueryPlan.scala:257)\n\t\tat org.apache.spark.sql.catalyst.plans.QueryPlan.recursiveTransform$1(QueryPlan.scala:269)\n\t\tat org.apache.spark.sql.catalyst.plans.QueryPlan.$anonfun$mapExpressions$4(QueryPlan.scala:275)\n\t\tat scala.collection.immutable.List.map(List.scala:236)\n\t\tat scala.collection.immutable.List.map(List.scala:79)\n\t\tat org.apache.spark.sql.catalyst.plans.QueryPlan.recursiveTransform$1(QueryPlan.scala:275)\n\t\tat org.apache.spark.sql.catalyst.plans.QueryPlan.$anonfun$mapExpressions$5(QueryPlan.scala:280)\n\t\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:337)\n\t\tat org.apache.spark.sql.catalyst.plans.QueryPlan.mapExpressions(QueryPlan.scala:280)\n\t\tat org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpressionsUpWithPruning(QueryPlan.scala:245)\n\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveFunctions$$anonfun$apply$19.applyOrElse(Analyzer.scala:2226)\n\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveFunctions$$anonfun$apply$19.applyOrElse(Analyzer.scala:2096)\n\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$3(AnalysisHelper.scala:139)\n\t\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:107)\n\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:139)\n\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:416)\n\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:135)\n\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:131)\n\t\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:37)\n\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveFunctions$.apply(Analyzer.scala:2096)\n\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveFunctions$.apply(Analyzer.scala:2093)\n\t\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:248)\n\t\tat scala.collection.LinearSeqOps.foldLeft(LinearSeq.scala:183)\n\t\tat scala.collection.LinearSeqOps.foldLeft$(LinearSeq.scala:179)\n\t\tat scala.collection.immutable.List.foldLeft(List.scala:79)\n\t\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:245)\n\t\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:237)\n\t\tat scala.collection.immutable.List.foreach(List.scala:323)\n\t\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:237)\n\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext(Analyzer.scala:344)\n\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:340)\n\t\tat org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:224)\n\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:340)\n\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:290)\n\t\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:207)\n\t\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)\n\t\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:207)\n\t\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.resolveInFixedPoint(HybridAnalyzer.scala:237)\n\t\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.$anonfun$apply$1(HybridAnalyzer.scala:92)\n\t\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.withTrackedAnalyzerBridgeState(HybridAnalyzer.scala:123)\n\t\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.apply(HybridAnalyzer.scala:85)\n\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:323)\n\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:423)\n\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:323)\n\t\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$2(QueryExecution.scala:139)\n\t\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:148)\n\t\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:325)\n\t\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:712)\n\t\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:325)\n\t\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:810)\n\t\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:324)\n\t\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$1(QueryExecution.scala:139)\n\t\tat scala.util.Try$.apply(Try.scala:217)\n\t\tat org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1392)\n\t\tat org.apache.spark.util.LazyTry.tryT$lzycompute(LazyTry.scala:46)\n\t\tat org.apache.spark.util.LazyTry.tryT(LazyTry.scala:46)\n\t\t... 21 more\n", "stacktrace": [{"class": null, "method": "deco", "file": "/home/gaogaotiantian/programs/spark/python/pyspark/errors/exceptions/captured.py", "line": "263"}, {"class": null, "method": "get_return_value", "file": "/home/gaogaotiantian/programs/spark/python/lib/py4j-0.10.9.9-src.zip/py4j/protocol.py", "line": "327"}]}}
</system-err>
	</testcase>
	<testcase classname="pyspark.sql.tests.test_udf.UDFTests" name="test_nested_array" time="0.212" timestamp="2026-01-02T11:16:28" file="python/pyspark/sql/tests/test_udf.py" line="1053"/>
	<testcase classname="pyspark.sql.tests.test_udf.UDFTests" name="test_nested_map" time="0.187" timestamp="2026-01-02T11:16:28" file="python/pyspark/sql/tests/test_udf.py" line="1038"/>
	<testcase classname="pyspark.sql.tests.test_udf.UDFTests" name="test_nested_struct" time="0.195" timestamp="2026-01-02T11:16:28" file="python/pyspark/sql/tests/test_udf.py" line="1023"/>
	<testcase classname="pyspark.sql.tests.test_udf.UDFTests" name="test_non_existed_udaf" time="0.017" timestamp="2026-01-02T11:16:28" file="python/pyspark/sql/tests/test_udf.py" line="626"/>
	<testcase classname="pyspark.sql.tests.test_udf.UDFTests" name="test_non_existed_udf" time="0.006" timestamp="2026-01-02T11:16:28" file="python/pyspark/sql/tests/test_udf.py" line="607"/>
	<testcase classname="pyspark.sql.tests.test_udf.UDFTests" name="test_non_existed_udf_with_sql_context" time="0.007" timestamp="2026-01-02T11:16:28" file="python/pyspark/sql/tests/test_udf.py" line="615">
		<system-err>/home/gaogaotiantian/programs/spark/python/pyspark/sql/context.py:157: FutureWarning: Deprecated in 3.0.0. Use SparkSession.builder.getOrCreate() instead.
  warnings.warn(
/home/gaogaotiantian/programs/spark/python/pyspark/sql/context.py:308: FutureWarning: Deprecated in 2.3.0. Use spark.udf.registerJavaFunction instead.
  warnings.warn(
</system-err>
	</testcase>
	<testcase classname="pyspark.sql.tests.test_udf.UDFTests" name="test_nondeterministic_udf" time="0.193" timestamp="2026-01-02T11:16:29" file="python/pyspark/sql/tests/test_udf.py" line="146"/>
	<testcase classname="pyspark.sql.tests.test_udf.UDFTests" name="test_nondeterministic_udf2" time="0.240" timestamp="2026-01-02T11:16:29" file="python/pyspark/sql/tests/test_udf.py" line="157"/>
	<testcase classname="pyspark.sql.tests.test_udf.UDFTests" name="test_nondeterministic_udf3" time="0.014" timestamp="2026-01-02T11:16:29" file="python/pyspark/sql/tests/test_udf.py" line="177"/>
	<testcase classname="pyspark.sql.tests.test_udf.UDFTests" name="test_nondeterministic_udf_in_aggregate" time="0.131" timestamp="2026-01-02T11:16:29" file="python/pyspark/sql/tests/test_udf.py" line="189"/>
	<testcase classname="pyspark.sql.tests.test_udf.UDFTests" name="test_nonparam_udf_with_aggregate" time="0.394" timestamp="2026-01-02T11:16:29" file="python/pyspark/sql/tests/test_udf.py" line="825"/>
	<testcase classname="pyspark.sql.tests.test_udf.UDFTests" name="test_num_arguments" time="0.272" timestamp="2026-01-02T11:16:30" file="python/pyspark/sql/tests/test_udf.py" line="1197"/>
	<testcase classname="pyspark.sql.tests.test_udf.UDFTests" name="test_python_udf_segfault" time="1.675" timestamp="2026-01-02T11:16:31" file="python/pyspark/sql/tests/test_udf.py" line="1232"/>
	<testcase classname="pyspark.sql.tests.test_udf.UDFTests" name="test_raise_stop_iteration" time="0.238" timestamp="2026-01-02T11:16:31" file="python/pyspark/sql/tests/test_udf.py" line="1217"/>
	<testcase classname="pyspark.sql.tests.test_udf.UDFTests" name="test_same_accumulator_in_udfs" time="0.107" timestamp="2026-01-02T11:16:32" file="python/pyspark/sql/tests/test_udf.py" line="899"/>
	<testcase classname="pyspark.sql.tests.test_udf.UDFTests" name="test_single_udf_with_repeated_argument" time="0.083" timestamp="2026-01-02T11:16:32" file="python/pyspark/sql/tests/test_udf.py" line="215"/>
	<testcase classname="pyspark.sql.tests.test_udf.UDFTests" name="test_timeout_util_with_udf" time="1.016" timestamp="2026-01-02T11:16:33" file="python/pyspark/sql/tests/test_udf.py" line="1293">
		<system-err>ERROR:root:Exception while sending command.
Traceback (most recent call last):
  File "/home/gaogaotiantian/programs/spark/python/lib/py4j-0.10.9.9-src.zip/py4j/clientserver.py", line 535, in send_command
    answer = smart_decode(self.stream.readline()[:-1])
                          ^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.12/socket.py", line 707, in readinto
    return self._sock.recv_into(b)
           ^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/gaogaotiantian/programs/spark/python/pyspark/testing/utils.py", line 166, in handler
    raise TimeoutError(f"Function {func.__name__} timed out after {timeout} seconds")
TimeoutError: Function timeout_func timed out after 1 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/gaogaotiantian/programs/spark/python/lib/py4j-0.10.9.9-src.zip/py4j/java_gateway.py", line 1038, in send_command
    response = connection.send_command(command)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/gaogaotiantian/programs/spark/python/lib/py4j-0.10.9.9-src.zip/py4j/clientserver.py", line 566, in send_command
    raise Py4JNetworkError(
py4j.protocol.Py4JNetworkError: Error while sending or receiving
</system-err>
	</testcase>
	<testcase classname="pyspark.sql.tests.test_udf.UDFTests" name="test_udf" time="0.091" timestamp="2026-01-02T11:16:33" file="python/pyspark/sql/tests/test_udf.py" line="88"/>
	<testcase classname="pyspark.sql.tests.test_udf.UDFTests" name="test_udf2" time="0.236" timestamp="2026-01-02T11:16:33" file="python/pyspark/sql/tests/test_udf.py" line="104"/>
	<testcase classname="pyspark.sql.tests.test_udf.UDFTests" name="test_udf3" time="0.082" timestamp="2026-01-02T11:16:33" file="python/pyspark/sql/tests/test_udf.py" line="111"/>
	<testcase classname="pyspark.sql.tests.test_udf.UDFTests" name="test_udf_and_common_filter_in_join_condition" time="0.397" timestamp="2026-01-02T11:16:33" file="python/pyspark/sql/tests/test_udf.py" line="281"/>
	<testcase classname="pyspark.sql.tests.test_udf.UDFTests" name="test_udf_as_join_condition" time="0.285" timestamp="2026-01-02T11:16:34" file="python/pyspark/sql/tests/test_udf.py" line="311"/>
	<testcase classname="pyspark.sql.tests.test_udf.UDFTests" name="test_udf_binary_type" time="0.533" timestamp="2026-01-02T11:16:34" file="python/pyspark/sql/tests/test_udf.py" line="1479"/>
	<testcase classname="pyspark.sql.tests.test_udf.UDFTests" name="test_udf_binary_type_in_nested_structures" time="1.477" timestamp="2026-01-02T11:16:36" file="python/pyspark/sql/tests/test_udf.py" line="1496">
		<!--Test that binary type in arrays, maps, and structs respects binaryAsBytes config-->
	</testcase>
	<testcase classname="pyspark.sql.tests.test_udf.UDFTests" name="test_udf_cache" time="0.032" timestamp="2026-01-02T11:16:36" file="python/pyspark/sql/tests/test_udf.py" line="965"/>
	<testcase classname="pyspark.sql.tests.test_udf.UDFTests" name="test_udf_daytime_interval" time="0.317" timestamp="2026-01-02T11:16:36" file="python/pyspark/sql/tests/test_udf.py" line="808"/>
	<testcase classname="pyspark.sql.tests.test_udf.UDFTests" name="test_udf_defers_judf_initialization" time="0.010" timestamp="2026-01-02T11:16:36" file="python/pyspark/sql/tests/test_udf.py" line="666"/>
	<testcase classname="pyspark.sql.tests.test_udf.UDFTests" name="test_udf_empty_frame" time="0.142" timestamp="2026-01-02T11:16:36" file="python/pyspark/sql/tests/test_udf.py" line="1402"/>
	<testcase classname="pyspark.sql.tests.test_udf.UDFTests" name="test_udf_globals_not_overwritten" time="0.125" timestamp="2026-01-02T11:16:36" file="python/pyspark/sql/tests/test_udf.py" line="932"/>
	<testcase classname="pyspark.sql.tests.test_udf.UDFTests" name="test_udf_in_filter_on_top_of_join" time="0.643" timestamp="2026-01-02T11:16:37" file="python/pyspark/sql/tests/test_udf.py" line="245"/>
	<testcase classname="pyspark.sql.tests.test_udf.UDFTests" name="test_udf_in_filter_on_top_of_outer_join" time="0.382" timestamp="2026-01-02T11:16:37" file="python/pyspark/sql/tests/test_udf.py" line="238"/>
	<testcase classname="pyspark.sql.tests.test_udf.UDFTests" name="test_udf_in_generate" time="0.444" timestamp="2026-01-02T11:16:38" file="python/pyspark/sql/tests/test_udf.py" line="498"/>
	<testcase classname="pyspark.sql.tests.test_udf.UDFTests" name="test_udf_in_join_condition" time="0.169" timestamp="2026-01-02T11:16:38" file="python/pyspark/sql/tests/test_udf.py" line="253"/>
	<testcase classname="pyspark.sql.tests.test_udf.UDFTests" name="test_udf_in_left_outer_join_condition" time="0.241" timestamp="2026-01-02T11:16:38" file="python/pyspark/sql/tests/test_udf.py" line="268"/>
	<testcase classname="pyspark.sql.tests.test_udf.UDFTests" name="test_udf_in_subquery" time="0.198" timestamp="2026-01-02T11:16:38" file="python/pyspark/sql/tests/test_udf.py" line="923"/>
	<testcase classname="pyspark.sql.tests.test_udf.UDFTests" name="test_udf_input_serialization_valuecompare_disabled" time="0.193" timestamp="2026-01-02T11:16:39" file="python/pyspark/sql/tests/test_udf.py" line="984"/>
	<testcase classname="pyspark.sql.tests.test_udf.UDFTests" name="test_udf_kill_on_timeout" time="1.216" timestamp="2026-01-02T11:16:40" file="python/pyspark/sql/tests/test_udf.py" line="1242"/>
	<testcase classname="pyspark.sql.tests.test_udf.UDFTests" name="test_udf_not_supported_in_join_condition" time="0.342" timestamp="2026-01-02T11:16:40" file="python/pyspark/sql/tests/test_udf.py" line="291"/>
	<testcase classname="pyspark.sql.tests.test_udf.UDFTests" name="test_udf_on_sql_context" time="0.084" timestamp="2026-01-02T11:16:40" file="python/pyspark/sql/tests/test_udf.py" line="94">
		<system-err>/home/gaogaotiantian/programs/spark/python/pyspark/sql/context.py:294: FutureWarning: Deprecated in 2.3.0. Use spark.udf.register instead.
  warnings.warn("Deprecated in 2.3.0. Use spark.udf.register instead.", FutureWarning)
</system-err>
	</testcase>
	<testcase classname="pyspark.sql.tests.test_udf.UDFTests" name="test_udf_registration_return_type_none" time="0.074" timestamp="2026-01-02T11:16:40" file="python/pyspark/sql/tests/test_udf.py" line="120"/>
	<testcase classname="pyspark.sql.tests.test_udf.UDFTests" name="test_udf_registration_return_type_not_none" time="0.004" timestamp="2026-01-02T11:16:40" file="python/pyspark/sql/tests/test_udf.py" line="129"/>
	<testcase classname="pyspark.sql.tests.test_udf.UDFTests" name="test_udf_registration_returns_udf" time="0.486" timestamp="2026-01-02T11:16:41" file="python/pyspark/sql/tests/test_udf.py" line="532"/>
	<testcase classname="pyspark.sql.tests.test_udf.UDFTests" name="test_udf_registration_returns_udf_on_sql_context" time="0.243" timestamp="2026-01-02T11:16:41" file="python/pyspark/sql/tests/test_udf.py" line="549"/>
	<testcase classname="pyspark.sql.tests.test_udf.UDFTests" name="test_udf_should_not_accept_noncallable_object" time="0.001" timestamp="2026-01-02T11:16:41" file="python/pyspark/sql/tests/test_udf.py" line="699"/>
	<testcase classname="pyspark.sql.tests.test_udf.UDFTests" name="test_udf_timestamp_ntz" time="0.285" timestamp="2026-01-02T11:16:41" file="python/pyspark/sql/tests/test_udf.py" line="792"/>
	<testcase classname="pyspark.sql.tests.test_udf.UDFTests" name="test_udf_with_256_args" time="0.324" timestamp="2026-01-02T11:16:42" file="python/pyspark/sql/tests/test_udf.py" line="952"/>
	<testcase classname="pyspark.sql.tests.test_udf.UDFTests" name="test_udf_with_aggregate_function" time="0.452" timestamp="2026-01-02T11:16:42" file="python/pyspark/sql/tests/test_udf.py" line="480"/>
	<testcase classname="pyspark.sql.tests.test_udf.UDFTests" name="test_udf_with_array_type" time="0.125" timestamp="2026-01-02T11:16:42" file="python/pyspark/sql/tests/test_udf.py" line="325"/>
	<testcase classname="pyspark.sql.tests.test_udf.UDFTests" name="test_udf_with_callable" time="0.134" timestamp="2026-01-02T11:16:42" file="python/pyspark/sql/tests/test_udf.py" line="63"/>
	<testcase classname="pyspark.sql.tests.test_udf.UDFTests" name="test_udf_with_char_varchar_return_type" time="0.136" timestamp="2026-01-02T11:16:43" file="python/pyspark/sql/tests/test_udf.py" line="1435"/>
	<testcase classname="pyspark.sql.tests.test_udf.UDFTests" name="test_udf_with_collated_string_types" time="0.418" timestamp="2026-01-02T11:16:43" file="python/pyspark/sql/tests/test_udf.py" line="1412"/>
	<testcase classname="pyspark.sql.tests.test_udf.UDFTests" name="test_udf_with_complex_variant_input" time="0.296" timestamp="2026-01-02T11:16:43" file="python/pyspark/sql/tests/test_udf.py" line="365"/>
	<testcase classname="pyspark.sql.tests.test_udf.UDFTests" name="test_udf_with_complex_variant_output" time="0.256" timestamp="2026-01-02T11:16:44" file="python/pyspark/sql/tests/test_udf.py" line="400"/>
	<testcase classname="pyspark.sql.tests.test_udf.UDFTests" name="test_udf_with_decorator" time="0.177" timestamp="2026-01-02T11:16:44" file="python/pyspark/sql/tests/test_udf.py" line="703"/>
	<testcase classname="pyspark.sql.tests.test_udf.UDFTests" name="test_udf_with_filter_function" time="0.111" timestamp="2026-01-02T11:16:44" file="python/pyspark/sql/tests/test_udf.py" line="351"/>
	<testcase classname="pyspark.sql.tests.test_udf.UDFTests" name="test_udf_with_input_file_name_for_hadooprdd" time="0.486" timestamp="2026-01-02T11:16:45" file="python/pyspark/sql/tests/test_udf.py" line="642"/>
	<testcase classname="pyspark.sql.tests.test_udf.UDFTests" name="test_udf_with_logging" time="0.393" timestamp="2026-01-02T11:16:45" file="python/pyspark/sql/tests/test_udf.py" line="1562"/>
	<testcase classname="pyspark.sql.tests.test_udf.UDFTests" name="test_udf_with_order_by_and_limit" time="0.091" timestamp="2026-01-02T11:16:45" file="python/pyspark/sql/tests/test_udf.py" line="526"/>
	<testcase classname="pyspark.sql.tests.test_udf.UDFTests" name="test_udf_with_partial_function" time="0.125" timestamp="2026-01-02T11:16:45" file="python/pyspark/sql/tests/test_udf.py" line="76"/>
	<testcase classname="pyspark.sql.tests.test_udf.UDFTests" name="test_udf_with_pyspark_logger" time="0.106" timestamp="2026-01-02T11:16:45" file="python/pyspark/sql/tests/test_udf.py" line="1677"/>
	<testcase classname="pyspark.sql.tests.test_udf.UDFTests" name="test_udf_with_rand" time="0.087" timestamp="2026-01-02T11:16:46" file="python/pyspark/sql/tests/test_udf.py" line="1017"/>
	<testcase classname="pyspark.sql.tests.test_udf.UDFTests" name="test_udf_with_string_return_type" time="0.099" timestamp="2026-01-02T11:16:46" file="python/pyspark/sql/tests/test_udf.py" line="682"/>
	<testcase classname="pyspark.sql.tests.test_udf.UDFTests" name="test_udf_with_udt" time="1.501" timestamp="2026-01-02T11:16:47" file="python/pyspark/sql/tests/test_udf.py" line="1308"/>
	<testcase classname="pyspark.sql.tests.test_udf.UDFTests" name="test_udf_with_variant_input" time="0.101" timestamp="2026-01-02T11:16:47" file="python/pyspark/sql/tests/test_udf.py" line="358"/>
	<testcase classname="pyspark.sql.tests.test_udf.UDFTests" name="test_udf_with_variant_output" time="0.090" timestamp="2026-01-02T11:16:47" file="python/pyspark/sql/tests/test_udf.py" line="389"/>
	<testcase classname="pyspark.sql.tests.test_udf.UDFTests" name="test_udf_with_various_batch_size" time="2.986" timestamp="2026-01-02T11:16:50" file="python/pyspark/sql/tests/test_udf.py" line="1716"/>
	<testcase classname="pyspark.sql.tests.test_udf.UDFTests" name="test_udf_with_various_buffer_size" time="2.305" timestamp="2026-01-02T11:16:53" file="python/pyspark/sql/tests/test_udf.py" line="1734"/>
	<testcase classname="pyspark.sql.tests.test_udf.UDFTests" name="test_udf_without_arguments" time="0.080" timestamp="2026-01-02T11:16:53" file="python/pyspark/sql/tests/test_udf.py" line="319"/>
	<testcase classname="pyspark.sql.tests.test_udf.UDFTests" name="test_udf_wrapper" time="0.002" timestamp="2026-01-02T11:16:53" file="python/pyspark/sql/tests/test_udf.py" line="758"/>
	<testcase classname="pyspark.sql.tests.test_udf.UDFTests" name="test_worker_original_stdin_closed" time="0.061" timestamp="2026-01-02T11:16:53" file="python/pyspark/sql/tests/test_udf.py" line="939"/>
	<testcase classname="pyspark.sql.tests.test_udf.UDFTests" name="test_file_dsv2_with_udf_filter" time="0.068" timestamp="2026-01-02T11:16:24" file="python/pyspark/sql/tests/test_udf.py" line="881">
		<error type="FileNotFoundError" message="[Errno 2] No such file or directory: '/home/gaogaotiantian/programs/spark/python/target/139210dc-2321-48bd-9934-1a419bf714ae/tmpzvymd_8o'">Traceback (most recent call last):
  File "/home/gaogaotiantian/programs/spark/python/pyspark/sql/tests/test_udf.py", line 889, in test_file_dsv2_with_udf_filter
    self.spark.range(1).write.mode("overwrite").format("parquet").save(path)
  File "/home/gaogaotiantian/programs/spark/python/pyspark/sql/readwriter.py", line 1745, in save
    self._jwrite.save(path)
  File "/home/gaogaotiantian/programs/spark/python/lib/py4j-0.10.9.9-src.zip/py4j/java_gateway.py", line 1362, in __call__
    return_value = get_return_value(
                   ^^^^^^^^^^^^^^^^^
  File "/home/gaogaotiantian/programs/spark/python/pyspark/errors/exceptions/captured.py", line 263, in deco
    return f(*a, **kw)
           ^^^^^^^^^^^
  File "/home/gaogaotiantian/programs/spark/python/lib/py4j-0.10.9.9-src.zip/py4j/protocol.py", line 327, in get_return_value
    raise Py4JJavaError(
py4j.protocol.Py4JJavaError: An error occurred while calling o333.save.
: java.util.ServiceConfigurationError: org.apache.spark.sql.sources.DataSourceRegister: org.apache.spark.sql.hive.execution.HiveFileFormat Unable to get public no-arg constructor
	at java.base/java.util.ServiceLoader.fail(ServiceLoader.java:586)
	at java.base/java.util.ServiceLoader.getConstructor(ServiceLoader.java:679)
	at java.base/java.util.ServiceLoader$LazyClassPathLookupIterator.hasNextService(ServiceLoader.java:1240)
	at java.base/java.util.ServiceLoader$LazyClassPathLookupIterator.hasNext(ServiceLoader.java:1273)
	at java.base/java.util.ServiceLoader$2.hasNext(ServiceLoader.java:1309)
	at java.base/java.util.ServiceLoader$3.hasNext(ServiceLoader.java:1393)
	at scala.collection.convert.JavaCollectionWrappers$JIteratorWrapper.hasNext(JavaCollectionWrappers.scala:46)
	at scala.collection.StrictOptimizedIterableOps.filterImpl(StrictOptimizedIterableOps.scala:225)
	at scala.collection.StrictOptimizedIterableOps.filterImpl$(StrictOptimizedIterableOps.scala:222)
	at scala.collection.convert.JavaCollectionWrappers$JIterableWrapper.filterImpl(JavaCollectionWrappers.scala:83)
	at scala.collection.StrictOptimizedIterableOps.filter(StrictOptimizedIterableOps.scala:218)
	at scala.collection.StrictOptimizedIterableOps.filter$(StrictOptimizedIterableOps.scala:218)
	at scala.collection.convert.JavaCollectionWrappers$JIterableWrapper.filter(JavaCollectionWrappers.scala:83)
	at org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSource(DataSource.scala:666)
	at org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSourceV2(DataSource.scala:745)
	at org.apache.spark.sql.classic.DataFrameWriter.lookupV2Provider(DataFrameWriter.scala:596)
	at org.apache.spark.sql.classic.DataFrameWriter.saveCommand(DataFrameWriter.scala:141)
	at org.apache.spark.sql.classic.DataFrameWriter.save(DataFrameWriter.scala:115)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:75)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:52)
	at java.base/java.lang.reflect.Method.invoke(Method.java:580)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:184)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:108)
	at java.base/java.lang.Thread.run(Thread.java:1583)
Caused by: java.lang.NoClassDefFoundError: org/apache/hadoop/hive/ql/plan/FileSinkDesc
	at java.base/java.lang.Class.getDeclaredConstructors0(Native Method)
	at java.base/java.lang.Class.privateGetDeclaredConstructors(Class.java:3551)
	at java.base/java.lang.Class.getConstructor0(Class.java:3756)
	at java.base/java.lang.Class.getConstructor(Class.java:2444)
	at java.base/java.util.ServiceLoader$1.run(ServiceLoader.java:666)
	at java.base/java.util.ServiceLoader$1.run(ServiceLoader.java:663)
	at java.base/java.security.AccessController.doPrivileged(AccessController.java:571)
	at java.base/java.util.ServiceLoader.getConstructor(ServiceLoader.java:674)
	... 28 more
Caused by: java.lang.ClassNotFoundException: org.apache.hadoop.hive.ql.plan.FileSinkDesc
	at java.base/jdk.internal.loader.BuiltinClassLoader.loadClass(BuiltinClassLoader.java:641)
	at java.base/jdk.internal.loader.ClassLoaders$AppClassLoader.loadClass(ClassLoaders.java:188)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:526)
	... 36 more


During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/gaogaotiantian/programs/spark/python/pyspark/sql/tests/test_udf.py", line 896, in test_file_dsv2_with_udf_filter
    shutil.rmtree(path)
  File "/usr/lib/python3.12/shutil.py", line 775, in rmtree
    onexc(os.lstat, path, err)
  File "/usr/lib/python3.12/shutil.py", line 773, in rmtree
    orig_st = os.lstat(path, dir_fd=dir_fd)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
FileNotFoundError: [Errno 2] No such file or directory: '/home/gaogaotiantian/programs/spark/python/target/139210dc-2321-48bd-9934-1a419bf714ae/tmpzvymd_8o'
</error>
	</testcase>
	<testcase classname="pyspark.sql.tests.test_udf.UDFTests" name="test_udf_with_column_vector" time="0.040" timestamp="2026-01-02T11:16:43" file="python/pyspark/sql/tests/test_udf.py" line="997">
		<error type="FileNotFoundError" message="[Errno 2] No such file or directory: '/home/gaogaotiantian/programs/spark/python/target/139210dc-2321-48bd-9934-1a419bf714ae/tmpji0uwf9c'">Traceback (most recent call last):
  File "/home/gaogaotiantian/programs/spark/python/pyspark/sql/tests/test_udf.py", line 1002, in test_udf_with_column_vector
    self.spark.range(0, 100000, 1, 1).write.parquet(path)
  File "/home/gaogaotiantian/programs/spark/python/pyspark/sql/readwriter.py", line 2003, in parquet
    self._jwrite.parquet(path)
  File "/home/gaogaotiantian/programs/spark/python/lib/py4j-0.10.9.9-src.zip/py4j/java_gateway.py", line 1362, in __call__
    return_value = get_return_value(
                   ^^^^^^^^^^^^^^^^^
  File "/home/gaogaotiantian/programs/spark/python/pyspark/errors/exceptions/captured.py", line 263, in deco
    return f(*a, **kw)
           ^^^^^^^^^^^
  File "/home/gaogaotiantian/programs/spark/python/lib/py4j-0.10.9.9-src.zip/py4j/protocol.py", line 327, in get_return_value
    raise Py4JJavaError(
py4j.protocol.Py4JJavaError: An error occurred while calling o3304.parquet.
: java.util.ServiceConfigurationError: org.apache.spark.sql.sources.DataSourceRegister: org.apache.spark.sql.hive.execution.HiveFileFormat Unable to get public no-arg constructor
	at java.base/java.util.ServiceLoader.fail(ServiceLoader.java:586)
	at java.base/java.util.ServiceLoader.getConstructor(ServiceLoader.java:679)
	at java.base/java.util.ServiceLoader$LazyClassPathLookupIterator.hasNextService(ServiceLoader.java:1240)
	at java.base/java.util.ServiceLoader$LazyClassPathLookupIterator.hasNext(ServiceLoader.java:1273)
	at java.base/java.util.ServiceLoader$2.hasNext(ServiceLoader.java:1309)
	at java.base/java.util.ServiceLoader$3.hasNext(ServiceLoader.java:1393)
	at scala.collection.convert.JavaCollectionWrappers$JIteratorWrapper.hasNext(JavaCollectionWrappers.scala:46)
	at scala.collection.StrictOptimizedIterableOps.filterImpl(StrictOptimizedIterableOps.scala:225)
	at scala.collection.StrictOptimizedIterableOps.filterImpl$(StrictOptimizedIterableOps.scala:222)
	at scala.collection.convert.JavaCollectionWrappers$JIterableWrapper.filterImpl(JavaCollectionWrappers.scala:83)
	at scala.collection.StrictOptimizedIterableOps.filter(StrictOptimizedIterableOps.scala:218)
	at scala.collection.StrictOptimizedIterableOps.filter$(StrictOptimizedIterableOps.scala:218)
	at scala.collection.convert.JavaCollectionWrappers$JIterableWrapper.filter(JavaCollectionWrappers.scala:83)
	at org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSource(DataSource.scala:666)
	at org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSourceV2(DataSource.scala:745)
	at org.apache.spark.sql.classic.DataFrameWriter.lookupV2Provider(DataFrameWriter.scala:596)
	at org.apache.spark.sql.classic.DataFrameWriter.saveCommand(DataFrameWriter.scala:141)
	at org.apache.spark.sql.classic.DataFrameWriter.save(DataFrameWriter.scala:115)
	at org.apache.spark.sql.DataFrameWriter.parquet(DataFrameWriter.scala:369)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:75)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:52)
	at java.base/java.lang.reflect.Method.invoke(Method.java:580)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:184)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:108)
	at java.base/java.lang.Thread.run(Thread.java:1583)
Caused by: java.lang.NoClassDefFoundError: org/apache/hadoop/hive/ql/plan/FileSinkDesc
	at java.base/java.lang.Class.getDeclaredConstructors0(Native Method)
	at java.base/java.lang.Class.privateGetDeclaredConstructors(Class.java:3551)
	at java.base/java.lang.Class.getConstructor0(Class.java:3756)
	at java.base/java.lang.Class.getConstructor(Class.java:2444)
	at java.base/java.util.ServiceLoader$1.run(ServiceLoader.java:666)
	at java.base/java.util.ServiceLoader$1.run(ServiceLoader.java:663)
	at java.base/java.security.AccessController.doPrivileged(AccessController.java:571)
	at java.base/java.util.ServiceLoader.getConstructor(ServiceLoader.java:674)
	... 29 more
Caused by: java.lang.ClassNotFoundException: org.apache.hadoop.hive.ql.plan.FileSinkDesc
	at java.base/jdk.internal.loader.BuiltinClassLoader.loadClass(BuiltinClassLoader.java:641)
	at java.base/jdk.internal.loader.ClassLoaders$AppClassLoader.loadClass(ClassLoaders.java:188)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:526)
	... 37 more


During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/gaogaotiantian/programs/spark/python/pyspark/sql/tests/test_udf.py", line 1015, in test_udf_with_column_vector
    shutil.rmtree(path)
  File "/usr/lib/python3.12/shutil.py", line 775, in rmtree
    onexc(os.lstat, path, err)
  File "/usr/lib/python3.12/shutil.py", line 773, in rmtree
    orig_st = os.lstat(path, dir_fd=dir_fd)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
FileNotFoundError: [Errno 2] No such file or directory: '/home/gaogaotiantian/programs/spark/python/target/139210dc-2321-48bd-9934-1a419bf714ae/tmpji0uwf9c'
</error>
	</testcase>
	<testcase classname="pyspark.sql.tests.test_udf.UDFTests" name="test_udf_with_input_file_name" time="0.335" timestamp="2026-01-02T11:16:44" file="python/pyspark/sql/tests/test_udf.py" line="634">
		<error type="Py4JJavaError" message="An error occurred while calling o3588.json. : java.util.ServiceConfigurationError: org.apache.spark.sql.sources.DataSourceRegister: org.apache.spark.sql.hive.execution.HiveFileFormat Unable to get public no-arg constructor  at java.base/java.util.ServiceLoader.fail(ServiceLoader.java:586)  at java.base/java.util.ServiceLoader.getConstructor(ServiceLoader.java:679)  at java.base/java.util.ServiceLoader$LazyClassPathLookupIterator.hasNextService(ServiceLoader.java:1240)  at java.base/java.util.ServiceLoader$LazyClassPathLookupIterator.hasNext(ServiceLoader.java:1273)  at java.base/java.util.ServiceLoader$2.hasNext(ServiceLoader.java:1309)  at java.base/java.util.ServiceLoader$3.hasNext(ServiceLoader.java:1393)  at scala.collection.convert.JavaCollectionWrappers$JIteratorWrapper.hasNext(JavaCollectionWrappers.scala:46)  at scala.collection.StrictOptimizedIterableOps.filterImpl(StrictOptimizedIterableOps.scala:225)  at scala.collection.StrictOptimizedIterableOps.filterImpl$(StrictOptimizedIterableOps.scala:222)  at scala.collection.convert.JavaCollectionWrappers$JIterableWrapper.filterImpl(JavaCollectionWrappers.scala:83)  at scala.collection.StrictOptimizedIterableOps.filter(StrictOptimizedIterableOps.scala:218)  at scala.collection.StrictOptimizedIterableOps.filter$(StrictOptimizedIterableOps.scala:218)  at scala.collection.convert.JavaCollectionWrappers$JIterableWrapper.filter(JavaCollectionWrappers.scala:83)  at org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSource(DataSource.scala:666)  at org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSourceV2(DataSource.scala:745)  at org.apache.spark.sql.catalyst.analysis.ResolveDataSource$$anonfun$apply$1.applyOrElse(ResolveDataSource.scala:58)  at org.apache.spark.sql.catalyst.analysis.ResolveDataSource$$anonfun$apply$1.applyOrElse(ResolveDataSource.scala:45)  at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$3(AnalysisHelper.scala:139)  at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:107)  at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:139)  at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:416)  at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:135)  at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:131)  at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:37)  at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp(AnalysisHelper.scala:112)  at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp$(AnalysisHelper.scala:111)  at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUp(LogicalPlan.scala:37)  at org.apache.spark.sql.catalyst.analysis.ResolveDataSource.apply(ResolveDataSource.scala:45)  at org.apache.spark.sql.catalyst.analysis.ResolveDataSource.apply(ResolveDataSource.scala:43)  at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:248)  at scala.collection.LinearSeqOps.foldLeft(LinearSeq.scala:183)  at scala.collection.LinearSeqOps.foldLeft$(LinearSeq.scala:179)  at scala.collection.immutable.List.foldLeft(List.scala:79)  at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:245)  at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:237)  at scala.collection.immutable.List.foreach(List.scala:323)  at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:237)  at org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext(Analyzer.scala:344)  at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:340)  at org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:224)  at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:340)  at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:290)  at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:207)  at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)  at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:207)  at org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.resolveInFixedPoint(HybridAnalyzer.scala:237)  at org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.$anonfun$apply$1(HybridAnalyzer.scala:92)  at org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.withTrackedAnalyzerBridgeState(HybridAnalyzer.scala:123)  at org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.apply(HybridAnalyzer.scala:85)  at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:323)  at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:423)  at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:323)  at org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$2(QueryExecution.scala:139)  at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:148)  at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:325)  at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:712)  at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:325)  at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:810)  at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:324)  at org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$1(QueryExecution.scala:139)  at scala.util.Try$.apply(Try.scala:217)  at org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1392)  at org.apache.spark.util.Utils$.getTryWithCallerStacktrace(Utils.scala:1453)  at org.apache.spark.util.LazyTry.get(LazyTry.scala:58)  at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:150)  at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:90)  at org.apache.spark.sql.classic.Dataset$.$anonfun$ofRows$1(Dataset.scala:114)  at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:810)  at org.apache.spark.sql.classic.Dataset$.ofRows(Dataset.scala:112)  at org.apache.spark.sql.classic.DataFrameReader.load(DataFrameReader.scala:108)  at org.apache.spark.sql.classic.DataFrameReader.load(DataFrameReader.scala:57)  at org.apache.spark.sql.DataFrameReader.json(DataFrameReader.scala:296)  at org.apache.spark.sql.classic.DataFrameReader.json(DataFrameReader.scala:149)  at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)  at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:75)  at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:52)  at java.base/java.lang.reflect.Method.invoke(Method.java:580)  at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)  at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)  at py4j.Gateway.invoke(Gateway.java:282)  at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)  at py4j.commands.CallCommand.execute(CallCommand.java:79)  at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:184)  at py4j.ClientServerConnection.run(ClientServerConnection.java:108)  at java.base/java.lang.Thread.run(Thread.java:1583)  Suppressed: org.apache.spark.util.Utils$OriginalTryStackTraceException: Full stacktrace of original doTryWithCallerStacktrace caller   at java.base/java.util.ServiceLoader.fail(ServiceLoader.java:586)   at java.base/java.util.ServiceLoader.getConstructor(ServiceLoader.java:679)   at java.base/java.util.ServiceLoader$LazyClassPathLookupIterator.hasNextService(ServiceLoader.java:1240)   at java.base/java.util.ServiceLoader$LazyClassPathLookupIterator.hasNext(ServiceLoader.java:1273)   at java.base/java.util.ServiceLoader$2.hasNext(ServiceLoader.java:1309)   at java.base/java.util.ServiceLoader$3.hasNext(ServiceLoader.java:1393)   at scala.collection.convert.JavaCollectionWrappers$JIteratorWrapper.hasNext(JavaCollectionWrappers.scala:46)   at scala.collection.StrictOptimizedIterableOps.filterImpl(StrictOptimizedIterableOps.scala:225)   at scala.collection.StrictOptimizedIterableOps.filterImpl$(StrictOptimizedIterableOps.scala:222)   at scala.collection.convert.JavaCollectionWrappers$JIterableWrapper.filterImpl(JavaCollectionWrappers.scala:83)   at scala.collection.StrictOptimizedIterableOps.filter(StrictOptimizedIterableOps.scala:218)   at scala.collection.StrictOptimizedIterableOps.filter$(StrictOptimizedIterableOps.scala:218)   at scala.collection.convert.JavaCollectionWrappers$JIterableWrapper.filter(JavaCollectionWrappers.scala:83)   at org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSource(DataSource.scala:666)   at org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSourceV2(DataSource.scala:745)   at org.apache.spark.sql.catalyst.analysis.ResolveDataSource$$anonfun$apply$1.applyOrElse(ResolveDataSource.scala:58)   at org.apache.spark.sql.catalyst.analysis.ResolveDataSource$$anonfun$apply$1.applyOrElse(ResolveDataSource.scala:45)   at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$3(AnalysisHelper.scala:139)   at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:107)   at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:139)   at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:416)   at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:135)   at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:131)   at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:37)   at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp(AnalysisHelper.scala:112)   at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp$(AnalysisHelper.scala:111)   at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUp(LogicalPlan.scala:37)   at org.apache.spark.sql.catalyst.analysis.ResolveDataSource.apply(ResolveDataSource.scala:45)   at org.apache.spark.sql.catalyst.analysis.ResolveDataSource.apply(ResolveDataSource.scala:43)   at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:248)   at scala.collection.LinearSeqOps.foldLeft(LinearSeq.scala:183)   at scala.collection.LinearSeqOps.foldLeft$(LinearSeq.scala:179)   at scala.collection.immutable.List.foldLeft(List.scala:79)   at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:245)   at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:237)   at scala.collection.immutable.List.foreach(List.scala:323)   at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:237)   at org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext(Analyzer.scala:344)   at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:340)   at org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:224)   at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:340)   at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:290)   at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:207)   at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)   at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:207)   at org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.resolveInFixedPoint(HybridAnalyzer.scala:237)   at org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.$anonfun$apply$1(HybridAnalyzer.scala:92)   at org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.withTrackedAnalyzerBridgeState(HybridAnalyzer.scala:123)   at org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.apply(HybridAnalyzer.scala:85)   at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:323)   at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:423)   at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:323)   at org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$2(QueryExecution.scala:139)   at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:148)   at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:325)   at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:712)   at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:325)   at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:810)   at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:324)   at org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$1(QueryExecution.scala:139)   at scala.util.Try$.apply(Try.scala:217)   at org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1392)   at org.apache.spark.util.LazyTry.tryT$lzycompute(LazyTry.scala:46)   at org.apache.spark.util.LazyTry.tryT(LazyTry.scala:46)   ... 22 more Caused by: java.lang.NoClassDefFoundError: org/apache/hadoop/hive/ql/plan/FileSinkDesc  at java.base/java.lang.Class.getDeclaredConstructors0(Native Method)  at java.base/java.lang.Class.privateGetDeclaredConstructors(Class.java:3551)  at java.base/java.lang.Class.getConstructor0(Class.java:3756)  at java.base/java.lang.Class.getConstructor(Class.java:2444)  at java.base/java.util.ServiceLoader$1.run(ServiceLoader.java:666)  at java.base/java.util.ServiceLoader$1.run(ServiceLoader.java:663)  at java.base/java.security.AccessController.doPrivileged(AccessController.java:571)  at java.base/java.util.ServiceLoader.getConstructor(ServiceLoader.java:674)  at java.base/java.util.ServiceLoader$LazyClassPathLookupIterator.hasNextService(ServiceLoader.java:1240)  at java.base/java.util.ServiceLoader$LazyClassPathLookupIterator.hasNext(ServiceLoader.java:1273)  at java.base/java.util.ServiceLoader$2.hasNext(ServiceLoader.java:1309)  at java.base/java.util.ServiceLoader$3.hasNext(ServiceLoader.java:1393)  at scala.collection.convert.JavaCollectionWrappers$JIteratorWrapper.hasNext(JavaCollectionWrappers.scala:46)  at scala.collection.StrictOptimizedIterableOps.filterImpl(StrictOptimizedIterableOps.scala:225)  at scala.collection.StrictOptimizedIterableOps.filterImpl$(StrictOptimizedIterableOps.scala:222)  at scala.collection.convert.JavaCollectionWrappers$JIterableWrapper.filterImpl(JavaCollectionWrappers.scala:83)  at scala.collection.StrictOptimizedIterableOps.filter(StrictOptimizedIterableOps.scala:218)  at scala.collection.StrictOptimizedIterableOps.filter$(StrictOptimizedIterableOps.scala:218)  at scala.collection.convert.JavaCollectionWrappers$JIterableWrapper.filter(JavaCollectionWrappers.scala:83)  at org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSource(DataSource.scala:666)  at org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSourceV2(DataSource.scala:745)  at org.apache.spark.sql.catalyst.analysis.ResolveDataSource$$anonfun$apply$1.applyOrElse(ResolveDataSource.scala:58)  at org.apache.spark.sql.catalyst.analysis.ResolveDataSource$$anonfun$apply$1.applyOrElse(ResolveDataSource.scala:45)  at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$3(AnalysisHelper.scala:139)  at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:107)  at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:139)  at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:416)  at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:135)  at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:131)  at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:37)  at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp(AnalysisHelper.scala:112)  at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp$(AnalysisHelper.scala:111)  at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUp(LogicalPlan.scala:37)  at org.apache.spark.sql.catalyst.analysis.ResolveDataSource.apply(ResolveDataSource.scala:45)  at org.apache.spark.sql.catalyst.analysis.ResolveDataSource.apply(ResolveDataSource.scala:43)  at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:248)  at scala.collection.LinearSeqOps.foldLeft(LinearSeq.scala:183)  at scala.collection.LinearSeqOps.foldLeft$(LinearSeq.scala:179)  at scala.collection.immutable.List.foldLeft(List.scala:79)  at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:245)  at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:237)  at scala.collection.immutable.List.foreach(List.scala:323)  at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:237)  at org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext(Analyzer.scala:344)  at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:340)  at org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:224)  at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:340)  at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:290)  at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:207)  at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)  at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:207)  at org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.resolveInFixedPoint(HybridAnalyzer.scala:237)  at org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.$anonfun$apply$1(HybridAnalyzer.scala:92)  at org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.withTrackedAnalyzerBridgeState(HybridAnalyzer.scala:123)  at org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.apply(HybridAnalyzer.scala:85)  at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:323)  at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:423)  at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:323)  at org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$2(QueryExecution.scala:139)  at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:148)  at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:325)  at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:712)  at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:325)  at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:810)  at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:324)  at org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$1(QueryExecution.scala:139)  at scala.util.Try$.apply(Try.scala:217)  at org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1392)  at org.apache.spark.util.LazyTry.tryT$lzycompute(LazyTry.scala:46)  at org.apache.spark.util.LazyTry.tryT(LazyTry.scala:46)  ... 22 more Caused by: java.lang.ClassNotFoundException: org.apache.hadoop.hive.ql.plan.FileSinkDesc  at java.base/jdk.internal.loader.BuiltinClassLoader.loadClass(BuiltinClassLoader.java:641)  at java.base/jdk.internal.loader.ClassLoaders$AppClassLoader.loadClass(ClassLoaders.java:188)  at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:526)  ... 92 more ">Traceback (most recent call last):
  File "/home/gaogaotiantian/programs/spark/python/pyspark/sql/tests/test_udf.py", line 639, in test_udf_with_input_file_name
    row = self.spark.read.json(filePath).select(sourceFile(input_file_name())).first()
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/gaogaotiantian/programs/spark/python/pyspark/sql/readwriter.py", line 468, in json
    return self._df(self._jreader.json(self._spark._sc._jvm.PythonUtils.toSeq(path)))
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/gaogaotiantian/programs/spark/python/lib/py4j-0.10.9.9-src.zip/py4j/java_gateway.py", line 1362, in __call__
    return_value = get_return_value(
                   ^^^^^^^^^^^^^^^^^
  File "/home/gaogaotiantian/programs/spark/python/pyspark/errors/exceptions/captured.py", line 263, in deco
    return f(*a, **kw)
           ^^^^^^^^^^^
  File "/home/gaogaotiantian/programs/spark/python/lib/py4j-0.10.9.9-src.zip/py4j/protocol.py", line 327, in get_return_value
    raise Py4JJavaError(
py4j.protocol.Py4JJavaError: An error occurred while calling o3588.json.
: java.util.ServiceConfigurationError: org.apache.spark.sql.sources.DataSourceRegister: org.apache.spark.sql.hive.execution.HiveFileFormat Unable to get public no-arg constructor
	at java.base/java.util.ServiceLoader.fail(ServiceLoader.java:586)
	at java.base/java.util.ServiceLoader.getConstructor(ServiceLoader.java:679)
	at java.base/java.util.ServiceLoader$LazyClassPathLookupIterator.hasNextService(ServiceLoader.java:1240)
	at java.base/java.util.ServiceLoader$LazyClassPathLookupIterator.hasNext(ServiceLoader.java:1273)
	at java.base/java.util.ServiceLoader$2.hasNext(ServiceLoader.java:1309)
	at java.base/java.util.ServiceLoader$3.hasNext(ServiceLoader.java:1393)
	at scala.collection.convert.JavaCollectionWrappers$JIteratorWrapper.hasNext(JavaCollectionWrappers.scala:46)
	at scala.collection.StrictOptimizedIterableOps.filterImpl(StrictOptimizedIterableOps.scala:225)
	at scala.collection.StrictOptimizedIterableOps.filterImpl$(StrictOptimizedIterableOps.scala:222)
	at scala.collection.convert.JavaCollectionWrappers$JIterableWrapper.filterImpl(JavaCollectionWrappers.scala:83)
	at scala.collection.StrictOptimizedIterableOps.filter(StrictOptimizedIterableOps.scala:218)
	at scala.collection.StrictOptimizedIterableOps.filter$(StrictOptimizedIterableOps.scala:218)
	at scala.collection.convert.JavaCollectionWrappers$JIterableWrapper.filter(JavaCollectionWrappers.scala:83)
	at org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSource(DataSource.scala:666)
	at org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSourceV2(DataSource.scala:745)
	at org.apache.spark.sql.catalyst.analysis.ResolveDataSource$$anonfun$apply$1.applyOrElse(ResolveDataSource.scala:58)
	at org.apache.spark.sql.catalyst.analysis.ResolveDataSource$$anonfun$apply$1.applyOrElse(ResolveDataSource.scala:45)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$3(AnalysisHelper.scala:139)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:107)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:139)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:416)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:135)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:131)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:37)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp(AnalysisHelper.scala:112)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp$(AnalysisHelper.scala:111)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUp(LogicalPlan.scala:37)
	at org.apache.spark.sql.catalyst.analysis.ResolveDataSource.apply(ResolveDataSource.scala:45)
	at org.apache.spark.sql.catalyst.analysis.ResolveDataSource.apply(ResolveDataSource.scala:43)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:248)
	at scala.collection.LinearSeqOps.foldLeft(LinearSeq.scala:183)
	at scala.collection.LinearSeqOps.foldLeft$(LinearSeq.scala:179)
	at scala.collection.immutable.List.foldLeft(List.scala:79)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:245)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:237)
	at scala.collection.immutable.List.foreach(List.scala:323)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:237)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext(Analyzer.scala:344)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:340)
	at org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:224)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:340)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:290)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:207)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:207)
	at org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.resolveInFixedPoint(HybridAnalyzer.scala:237)
	at org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.$anonfun$apply$1(HybridAnalyzer.scala:92)
	at org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.withTrackedAnalyzerBridgeState(HybridAnalyzer.scala:123)
	at org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.apply(HybridAnalyzer.scala:85)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:323)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:423)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:323)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$2(QueryExecution.scala:139)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:148)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:325)
	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:712)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:325)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:810)
	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:324)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$1(QueryExecution.scala:139)
	at scala.util.Try$.apply(Try.scala:217)
	at org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1392)
	at org.apache.spark.util.Utils$.getTryWithCallerStacktrace(Utils.scala:1453)
	at org.apache.spark.util.LazyTry.get(LazyTry.scala:58)
	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:150)
	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:90)
	at org.apache.spark.sql.classic.Dataset$.$anonfun$ofRows$1(Dataset.scala:114)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:810)
	at org.apache.spark.sql.classic.Dataset$.ofRows(Dataset.scala:112)
	at org.apache.spark.sql.classic.DataFrameReader.load(DataFrameReader.scala:108)
	at org.apache.spark.sql.classic.DataFrameReader.load(DataFrameReader.scala:57)
	at org.apache.spark.sql.DataFrameReader.json(DataFrameReader.scala:296)
	at org.apache.spark.sql.classic.DataFrameReader.json(DataFrameReader.scala:149)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:75)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:52)
	at java.base/java.lang.reflect.Method.invoke(Method.java:580)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:184)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:108)
	at java.base/java.lang.Thread.run(Thread.java:1583)
	Suppressed: org.apache.spark.util.Utils$OriginalTryStackTraceException: Full stacktrace of original doTryWithCallerStacktrace caller
		at java.base/java.util.ServiceLoader.fail(ServiceLoader.java:586)
		at java.base/java.util.ServiceLoader.getConstructor(ServiceLoader.java:679)
		at java.base/java.util.ServiceLoader$LazyClassPathLookupIterator.hasNextService(ServiceLoader.java:1240)
		at java.base/java.util.ServiceLoader$LazyClassPathLookupIterator.hasNext(ServiceLoader.java:1273)
		at java.base/java.util.ServiceLoader$2.hasNext(ServiceLoader.java:1309)
		at java.base/java.util.ServiceLoader$3.hasNext(ServiceLoader.java:1393)
		at scala.collection.convert.JavaCollectionWrappers$JIteratorWrapper.hasNext(JavaCollectionWrappers.scala:46)
		at scala.collection.StrictOptimizedIterableOps.filterImpl(StrictOptimizedIterableOps.scala:225)
		at scala.collection.StrictOptimizedIterableOps.filterImpl$(StrictOptimizedIterableOps.scala:222)
		at scala.collection.convert.JavaCollectionWrappers$JIterableWrapper.filterImpl(JavaCollectionWrappers.scala:83)
		at scala.collection.StrictOptimizedIterableOps.filter(StrictOptimizedIterableOps.scala:218)
		at scala.collection.StrictOptimizedIterableOps.filter$(StrictOptimizedIterableOps.scala:218)
		at scala.collection.convert.JavaCollectionWrappers$JIterableWrapper.filter(JavaCollectionWrappers.scala:83)
		at org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSource(DataSource.scala:666)
		at org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSourceV2(DataSource.scala:745)
		at org.apache.spark.sql.catalyst.analysis.ResolveDataSource$$anonfun$apply$1.applyOrElse(ResolveDataSource.scala:58)
		at org.apache.spark.sql.catalyst.analysis.ResolveDataSource$$anonfun$apply$1.applyOrElse(ResolveDataSource.scala:45)
		at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$3(AnalysisHelper.scala:139)
		at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:107)
		at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:139)
		at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:416)
		at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:135)
		at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:131)
		at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:37)
		at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp(AnalysisHelper.scala:112)
		at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp$(AnalysisHelper.scala:111)
		at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUp(LogicalPlan.scala:37)
		at org.apache.spark.sql.catalyst.analysis.ResolveDataSource.apply(ResolveDataSource.scala:45)
		at org.apache.spark.sql.catalyst.analysis.ResolveDataSource.apply(ResolveDataSource.scala:43)
		at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:248)
		at scala.collection.LinearSeqOps.foldLeft(LinearSeq.scala:183)
		at scala.collection.LinearSeqOps.foldLeft$(LinearSeq.scala:179)
		at scala.collection.immutable.List.foldLeft(List.scala:79)
		at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:245)
		at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:237)
		at scala.collection.immutable.List.foreach(List.scala:323)
		at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:237)
		at org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext(Analyzer.scala:344)
		at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:340)
		at org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:224)
		at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:340)
		at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:290)
		at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:207)
		at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)
		at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:207)
		at org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.resolveInFixedPoint(HybridAnalyzer.scala:237)
		at org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.$anonfun$apply$1(HybridAnalyzer.scala:92)
		at org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.withTrackedAnalyzerBridgeState(HybridAnalyzer.scala:123)
		at org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.apply(HybridAnalyzer.scala:85)
		at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:323)
		at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:423)
		at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:323)
		at org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$2(QueryExecution.scala:139)
		at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:148)
		at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:325)
		at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:712)
		at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:325)
		at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:810)
		at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:324)
		at org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$1(QueryExecution.scala:139)
		at scala.util.Try$.apply(Try.scala:217)
		at org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1392)
		at org.apache.spark.util.LazyTry.tryT$lzycompute(LazyTry.scala:46)
		at org.apache.spark.util.LazyTry.tryT(LazyTry.scala:46)
		... 22 more
Caused by: java.lang.NoClassDefFoundError: org/apache/hadoop/hive/ql/plan/FileSinkDesc
	at java.base/java.lang.Class.getDeclaredConstructors0(Native Method)
	at java.base/java.lang.Class.privateGetDeclaredConstructors(Class.java:3551)
	at java.base/java.lang.Class.getConstructor0(Class.java:3756)
	at java.base/java.lang.Class.getConstructor(Class.java:2444)
	at java.base/java.util.ServiceLoader$1.run(ServiceLoader.java:666)
	at java.base/java.util.ServiceLoader$1.run(ServiceLoader.java:663)
	at java.base/java.security.AccessController.doPrivileged(AccessController.java:571)
	at java.base/java.util.ServiceLoader.getConstructor(ServiceLoader.java:674)
	at java.base/java.util.ServiceLoader$LazyClassPathLookupIterator.hasNextService(ServiceLoader.java:1240)
	at java.base/java.util.ServiceLoader$LazyClassPathLookupIterator.hasNext(ServiceLoader.java:1273)
	at java.base/java.util.ServiceLoader$2.hasNext(ServiceLoader.java:1309)
	at java.base/java.util.ServiceLoader$3.hasNext(ServiceLoader.java:1393)
	at scala.collection.convert.JavaCollectionWrappers$JIteratorWrapper.hasNext(JavaCollectionWrappers.scala:46)
	at scala.collection.StrictOptimizedIterableOps.filterImpl(StrictOptimizedIterableOps.scala:225)
	at scala.collection.StrictOptimizedIterableOps.filterImpl$(StrictOptimizedIterableOps.scala:222)
	at scala.collection.convert.JavaCollectionWrappers$JIterableWrapper.filterImpl(JavaCollectionWrappers.scala:83)
	at scala.collection.StrictOptimizedIterableOps.filter(StrictOptimizedIterableOps.scala:218)
	at scala.collection.StrictOptimizedIterableOps.filter$(StrictOptimizedIterableOps.scala:218)
	at scala.collection.convert.JavaCollectionWrappers$JIterableWrapper.filter(JavaCollectionWrappers.scala:83)
	at org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSource(DataSource.scala:666)
	at org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSourceV2(DataSource.scala:745)
	at org.apache.spark.sql.catalyst.analysis.ResolveDataSource$$anonfun$apply$1.applyOrElse(ResolveDataSource.scala:58)
	at org.apache.spark.sql.catalyst.analysis.ResolveDataSource$$anonfun$apply$1.applyOrElse(ResolveDataSource.scala:45)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$3(AnalysisHelper.scala:139)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:107)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:139)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:416)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:135)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:131)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:37)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp(AnalysisHelper.scala:112)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp$(AnalysisHelper.scala:111)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUp(LogicalPlan.scala:37)
	at org.apache.spark.sql.catalyst.analysis.ResolveDataSource.apply(ResolveDataSource.scala:45)
	at org.apache.spark.sql.catalyst.analysis.ResolveDataSource.apply(ResolveDataSource.scala:43)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:248)
	at scala.collection.LinearSeqOps.foldLeft(LinearSeq.scala:183)
	at scala.collection.LinearSeqOps.foldLeft$(LinearSeq.scala:179)
	at scala.collection.immutable.List.foldLeft(List.scala:79)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:245)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:237)
	at scala.collection.immutable.List.foreach(List.scala:323)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:237)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext(Analyzer.scala:344)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:340)
	at org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:224)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:340)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:290)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:207)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:207)
	at org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.resolveInFixedPoint(HybridAnalyzer.scala:237)
	at org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.$anonfun$apply$1(HybridAnalyzer.scala:92)
	at org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.withTrackedAnalyzerBridgeState(HybridAnalyzer.scala:123)
	at org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.apply(HybridAnalyzer.scala:85)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:323)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:423)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:323)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$2(QueryExecution.scala:139)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:148)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:325)
	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:712)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:325)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:810)
	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:324)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$1(QueryExecution.scala:139)
	at scala.util.Try$.apply(Try.scala:217)
	at org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1392)
	at org.apache.spark.util.LazyTry.tryT$lzycompute(LazyTry.scala:46)
	at org.apache.spark.util.LazyTry.tryT(LazyTry.scala:46)
	... 22 more
Caused by: java.lang.ClassNotFoundException: org.apache.hadoop.hive.ql.plan.FileSinkDesc
	at java.base/jdk.internal.loader.BuiltinClassLoader.loadClass(BuiltinClassLoader.java:641)
	at java.base/jdk.internal.loader.ClassLoaders$AppClassLoader.loadClass(ClassLoaders.java:188)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:526)
	... 92 more

</error>
	</testcase>
	<testcase classname="pyspark.sql.tests.test_udf.UDFTests" name="test_datasource_with_udf" time="0.000" timestamp="2026-01-02T11:16:24" file="python/pyspark/sql/tests/test_udf.py" line="834">
		<skipped type="skip" message="[TEST_CLASS_NOT_COMPILED] /home/gaogaotiantian/programs/spark/sql/core/target/*/test-classes doesn't exist. Spark sql test classes are not compiled."/>
	</testcase>
	<testcase classname="pyspark.sql.tests.test_udf.UDFTests" name="test_register_java_function" time="0.001" timestamp="2026-01-02T11:16:31" file="python/pyspark/sql/tests/test_udf.py" line="564">
		<skipped type="skip" message="[TEST_CLASS_NOT_COMPILED] /home/gaogaotiantian/programs/spark/sql/core/target/*/test-classes doesn't exist. Spark sql test classes are not compiled."/>
	</testcase>
	<testcase classname="pyspark.sql.tests.test_udf.UDFTests" name="test_register_java_udaf" time="0.000" timestamp="2026-01-02T11:16:31" file="python/pyspark/sql/tests/test_udf.py" line="585">
		<skipped type="skip" message="[TEST_CLASS_NOT_COMPILED] /home/gaogaotiantian/programs/spark/sql/core/target/*/test-classes doesn't exist. Spark sql test classes are not compiled."/>
	</testcase>
</testsuite><testsuite name="pyspark.sql.tests.test_udf.UDFTests-20260102112122" tests="89" file="pyspark/sql/tests/test_udf.py" time="32.525" timestamp="2026-01-02T11:21:58" failures="0" errors="0" skipped="3">
	<testcase classname="pyspark.sql.tests.test_udf.UDFTests" name="test_assert_classic_mode" time="0.002" timestamp="2026-01-02T11:21:25" file="python/pyspark/sql/tests/test_udf.py" line="324"/>
	<testcase classname="pyspark.sql.tests.test_udf.UDFTests" name="test_broadcast_in_udf" time="1.566" timestamp="2026-01-02T11:21:27" file="python/pyspark/sql/tests/test_udf.py" line="341">
		<system-err>/home/gaogaotiantian/programs/spark/python/pyspark/sql/catalog.py:957: FutureWarning: Deprecated in 2.3.0. Use spark.udf.register instead.
  warnings.warn("Deprecated in 2.3.0. Use spark.udf.register instead.", FutureWarning)
</system-err>
	</testcase>
	<testcase classname="pyspark.sql.tests.test_udf.UDFTests" name="test_chained_udf" time="0.373" timestamp="2026-01-02T11:21:27" file="python/pyspark/sql/tests/test_udf.py" line="205"/>
	<testcase classname="pyspark.sql.tests.test_udf.UDFTests" name="test_chained_udfs_with_variant" time="0.551" timestamp="2026-01-02T11:21:28" file="python/pyspark/sql/tests/test_udf.py" line="432"/>
	<testcase classname="pyspark.sql.tests.test_udf.UDFTests" name="test_complex_return_types" time="0.241" timestamp="2026-01-02T11:21:28" file="python/pyspark/sql/tests/test_udf.py" line="1070"/>
	<testcase classname="pyspark.sql.tests.test_udf.UDFTests" name="test_err_udf_init" time="0.017" timestamp="2026-01-02T11:21:28" file="python/pyspark/sql/tests/test_udf.py" line="1261"/>
	<testcase classname="pyspark.sql.tests.test_udf.UDFTests" name="test_err_udf_registration" time="0.011" timestamp="2026-01-02T11:21:28" file="python/pyspark/sql/tests/test_udf.py" line="596"/>
	<testcase classname="pyspark.sql.tests.test_udf.UDFTests" name="test_file_dsv2_with_udf_filter" time="1.024" timestamp="2026-01-02T11:21:29" file="python/pyspark/sql/tests/test_udf.py" line="881"/>
	<testcase classname="pyspark.sql.tests.test_udf.UDFTests" name="test_kwargs" time="0.678" timestamp="2026-01-02T11:21:29" file="python/pyspark/sql/tests/test_udf.py" line="1134">
		<system-err>{"ts": "2026-01-02 11:21:29.837", "level": "ERROR", "logger": "SQLQueryContextLogger", "msg": "[DUPLICATE_ROUTINE_PARAMETER_ASSIGNMENT.DOUBLE_NAMED_ARGUMENT_REFERENCE] Call to routine `test_udf` is invalid because it includes multiple argument assignments to the same parameter name `a`. More than one named argument referred to the same parameter. Please assign a value only once. SQLSTATE: 4274K", "context": {"errorClass": "DUPLICATE_ROUTINE_PARAMETER_ASSIGNMENT.DOUBLE_NAMED_ARGUMENT_REFERENCE"}, "exception": {"class": "Py4JJavaError", "msg": "An error occurred while calling o85.sql.\n: org.apache.spark.sql.AnalysisException: [DUPLICATE_ROUTINE_PARAMETER_ASSIGNMENT.DOUBLE_NAMED_ARGUMENT_REFERENCE] Call to routine `test_udf` is invalid because it includes multiple argument assignments to the same parameter name `a`. More than one named argument referred to the same parameter. Please assign a value only once. SQLSTATE: 4274K; line 1 pos 7\n\tat org.apache.spark.sql.errors.QueryCompilationErrors$.doubleNamedArgumentReference(QueryCompilationErrors.scala:90)\n\tat org.apache.spark.sql.catalyst.plans.logical.NamedParametersSupport$.$anonfun$splitAndCheckNamedArguments$2(FunctionBuilderBase.scala:108)\n\tat scala.collection.immutable.Vector1.map(Vector.scala:2155)\n\tat scala.collection.immutable.Vector1.map(Vector.scala:386)\n\tat org.apache.spark.sql.catalyst.plans.logical.NamedParametersSupport$.splitAndCheckNamedArguments(FunctionBuilderBase.scala:104)\n\tat org.apache.spark.sql.execution.python.UserDefinedPythonFunction.builder(UserDefinedPythonFunction.scala:62)\n\tat org.apache.spark.sql.classic.UDFRegistration.$anonfun$registerPython$1(UDFRegistration.scala:64)\n\tat org.apache.spark.sql.catalyst.analysis.SimpleFunctionRegistryBase.lookupFunction(FunctionRegistry.scala:246)\n\tat org.apache.spark.sql.catalyst.analysis.SimpleFunctionRegistryBase.lookupFunction$(FunctionRegistry.scala:240)\n\tat org.apache.spark.sql.catalyst.analysis.SimpleFunctionRegistry.lookupFunction(FunctionRegistry.scala:305)\n\tat org.apache.spark.sql.catalyst.catalog.SessionCatalog.$anonfun$resolveBuiltinOrTempFunctionInternal$1(SessionCatalog.scala:2121)\n\tat org.apache.spark.sql.catalyst.catalog.SessionCatalog.lookupTempFuncWithViewContext(SessionCatalog.scala:2143)\n\tat org.apache.spark.sql.catalyst.catalog.SessionCatalog.resolveBuiltinOrTempFunctionInternal(SessionCatalog.scala:2121)\n\tat org.apache.spark.sql.catalyst.catalog.SessionCatalog.resolveBuiltinOrTempFunction(SessionCatalog.scala:2098)\n\tat org.apache.spark.sql.catalyst.analysis.FunctionResolution.resolveBuiltinOrTempFunction(FunctionResolution.scala:102)\n\tat org.apache.spark.sql.catalyst.analysis.FunctionResolution.$anonfun$resolveFunction$1(FunctionResolution.scala:51)\n\tat org.apache.spark.sql.catalyst.analysis.package$.withPosition(package.scala:104)\n\tat org.apache.spark.sql.catalyst.analysis.FunctionResolution.resolveFunction(FunctionResolution.scala:51)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveFunctions$$anonfun$apply$19$$anonfun$applyOrElse$141.applyOrElse(Analyzer.scala:2256)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveFunctions$$anonfun$apply$19$$anonfun$applyOrElse$141.applyOrElse(Analyzer.scala:2226)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformUpWithPruning$2(TreeNode.scala:549)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:107)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformUpWithPruning(TreeNode.scala:549)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformUpWithPruning$1(TreeNode.scala:546)\n\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1268)\n\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1267)\n\tat org.apache.spark.sql.catalyst.expressions.UnaryExpression.mapChildren(Expression.scala:587)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformUpWithPruning(TreeNode.scala:546)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.$anonfun$transformExpressionsUpWithPruning$1(QueryPlan.scala:245)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.$anonfun$mapExpressions$1(QueryPlan.scala:257)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:107)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpression$1(QueryPlan.scala:257)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.recursiveTransform$1(QueryPlan.scala:269)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.$anonfun$mapExpressions$4(QueryPlan.scala:275)\n\tat scala.collection.immutable.List.map(List.scala:236)\n\tat scala.collection.immutable.List.map(List.scala:79)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.recursiveTransform$1(QueryPlan.scala:275)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.$anonfun$mapExpressions$5(QueryPlan.scala:280)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:337)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.mapExpressions(QueryPlan.scala:280)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpressionsUpWithPruning(QueryPlan.scala:245)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveFunctions$$anonfun$apply$19.applyOrElse(Analyzer.scala:2226)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveFunctions$$anonfun$apply$19.applyOrElse(Analyzer.scala:2096)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$3(AnalysisHelper.scala:139)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:107)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:139)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:416)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:135)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:131)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:37)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveFunctions$.apply(Analyzer.scala:2096)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveFunctions$.apply(Analyzer.scala:2093)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:248)\n\tat scala.collection.LinearSeqOps.foldLeft(LinearSeq.scala:183)\n\tat scala.collection.LinearSeqOps.foldLeft$(LinearSeq.scala:179)\n\tat scala.collection.immutable.List.foldLeft(List.scala:79)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:245)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:237)\n\tat scala.collection.immutable.List.foreach(List.scala:323)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:237)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext(Analyzer.scala:344)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:340)\n\tat org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:224)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:340)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:290)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:207)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:207)\n\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.resolveInFixedPoint(HybridAnalyzer.scala:237)\n\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.$anonfun$apply$1(HybridAnalyzer.scala:92)\n\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.withTrackedAnalyzerBridgeState(HybridAnalyzer.scala:123)\n\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.apply(HybridAnalyzer.scala:85)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:323)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:423)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:323)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$2(QueryExecution.scala:139)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:148)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:325)\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:712)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:325)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:810)\n\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:324)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$1(QueryExecution.scala:139)\n\tat scala.util.Try$.apply(Try.scala:217)\n\tat org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1392)\n\tat org.apache.spark.util.Utils$.getTryWithCallerStacktrace(Utils.scala:1453)\n\tat org.apache.spark.util.LazyTry.get(LazyTry.scala:58)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:150)\n\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:90)\n\tat org.apache.spark.sql.classic.Dataset$.$anonfun$ofRows$5(Dataset.scala:138)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:810)\n\tat org.apache.spark.sql.classic.Dataset$.ofRows(Dataset.scala:135)\n\tat org.apache.spark.sql.classic.SparkSession.$anonfun$sql$1(SparkSession.scala:534)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:810)\n\tat org.apache.spark.sql.classic.SparkSession.sql(SparkSession.scala:504)\n\tat org.apache.spark.sql.classic.SparkSession.sql(SparkSession.scala:539)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:75)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:52)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:580)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:184)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:108)\n\tat java.base/java.lang.Thread.run(Thread.java:1583)\n\tSuppressed: org.apache.spark.util.Utils$OriginalTryStackTraceException: Full stacktrace of original doTryWithCallerStacktrace caller\n\t\tat org.apache.spark.sql.errors.QueryCompilationErrors$.doubleNamedArgumentReference(QueryCompilationErrors.scala:90)\n\t\tat org.apache.spark.sql.catalyst.plans.logical.NamedParametersSupport$.$anonfun$splitAndCheckNamedArguments$2(FunctionBuilderBase.scala:108)\n\t\tat scala.collection.immutable.Vector1.map(Vector.scala:2155)\n\t\tat scala.collection.immutable.Vector1.map(Vector.scala:386)\n\t\tat org.apache.spark.sql.catalyst.plans.logical.NamedParametersSupport$.splitAndCheckNamedArguments(FunctionBuilderBase.scala:104)\n\t\tat org.apache.spark.sql.execution.python.UserDefinedPythonFunction.builder(UserDefinedPythonFunction.scala:62)\n\t\tat org.apache.spark.sql.classic.UDFRegistration.$anonfun$registerPython$1(UDFRegistration.scala:64)\n\t\tat org.apache.spark.sql.catalyst.analysis.SimpleFunctionRegistryBase.lookupFunction(FunctionRegistry.scala:246)\n\t\tat org.apache.spark.sql.catalyst.analysis.SimpleFunctionRegistryBase.lookupFunction$(FunctionRegistry.scala:240)\n\t\tat org.apache.spark.sql.catalyst.analysis.SimpleFunctionRegistry.lookupFunction(FunctionRegistry.scala:305)\n\t\tat org.apache.spark.sql.catalyst.catalog.SessionCatalog.$anonfun$resolveBuiltinOrTempFunctionInternal$1(SessionCatalog.scala:2121)\n\t\tat org.apache.spark.sql.catalyst.catalog.SessionCatalog.lookupTempFuncWithViewContext(SessionCatalog.scala:2143)\n\t\tat org.apache.spark.sql.catalyst.catalog.SessionCatalog.resolveBuiltinOrTempFunctionInternal(SessionCatalog.scala:2121)\n\t\tat org.apache.spark.sql.catalyst.catalog.SessionCatalog.resolveBuiltinOrTempFunction(SessionCatalog.scala:2098)\n\t\tat org.apache.spark.sql.catalyst.analysis.FunctionResolution.resolveBuiltinOrTempFunction(FunctionResolution.scala:102)\n\t\tat org.apache.spark.sql.catalyst.analysis.FunctionResolution.$anonfun$resolveFunction$1(FunctionResolution.scala:51)\n\t\tat org.apache.spark.sql.catalyst.analysis.package$.withPosition(package.scala:104)\n\t\tat org.apache.spark.sql.catalyst.analysis.FunctionResolution.resolveFunction(FunctionResolution.scala:51)\n\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveFunctions$$anonfun$apply$19$$anonfun$applyOrElse$141.applyOrElse(Analyzer.scala:2256)\n\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveFunctions$$anonfun$apply$19$$anonfun$applyOrElse$141.applyOrElse(Analyzer.scala:2226)\n\t\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformUpWithPruning$2(TreeNode.scala:549)\n\t\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:107)\n\t\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformUpWithPruning(TreeNode.scala:549)\n\t\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformUpWithPruning$1(TreeNode.scala:546)\n\t\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1268)\n\t\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1267)\n\t\tat org.apache.spark.sql.catalyst.expressions.UnaryExpression.mapChildren(Expression.scala:587)\n\t\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformUpWithPruning(TreeNode.scala:546)\n\t\tat org.apache.spark.sql.catalyst.plans.QueryPlan.$anonfun$transformExpressionsUpWithPruning$1(QueryPlan.scala:245)\n\t\tat org.apache.spark.sql.catalyst.plans.QueryPlan.$anonfun$mapExpressions$1(QueryPlan.scala:257)\n\t\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:107)\n\t\tat org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpression$1(QueryPlan.scala:257)\n\t\tat org.apache.spark.sql.catalyst.plans.QueryPlan.recursiveTransform$1(QueryPlan.scala:269)\n\t\tat org.apache.spark.sql.catalyst.plans.QueryPlan.$anonfun$mapExpressions$4(QueryPlan.scala:275)\n\t\tat scala.collection.immutable.List.map(List.scala:236)\n\t\tat scala.collection.immutable.List.map(List.scala:79)\n\t\tat org.apache.spark.sql.catalyst.plans.QueryPlan.recursiveTransform$1(QueryPlan.scala:275)\n\t\tat org.apache.spark.sql.catalyst.plans.QueryPlan.$anonfun$mapExpressions$5(QueryPlan.scala:280)\n\t\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:337)\n\t\tat org.apache.spark.sql.catalyst.plans.QueryPlan.mapExpressions(QueryPlan.scala:280)\n\t\tat org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpressionsUpWithPruning(QueryPlan.scala:245)\n\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveFunctions$$anonfun$apply$19.applyOrElse(Analyzer.scala:2226)\n\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveFunctions$$anonfun$apply$19.applyOrElse(Analyzer.scala:2096)\n\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$3(AnalysisHelper.scala:139)\n\t\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:107)\n\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:139)\n\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:416)\n\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:135)\n\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:131)\n\t\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:37)\n\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveFunctions$.apply(Analyzer.scala:2096)\n\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveFunctions$.apply(Analyzer.scala:2093)\n\t\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:248)\n\t\tat scala.collection.LinearSeqOps.foldLeft(LinearSeq.scala:183)\n\t\tat scala.collection.LinearSeqOps.foldLeft$(LinearSeq.scala:179)\n\t\tat scala.collection.immutable.List.foldLeft(List.scala:79)\n\t\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:245)\n\t\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:237)\n\t\tat scala.collection.immutable.List.foreach(List.scala:323)\n\t\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:237)\n\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext(Analyzer.scala:344)\n\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:340)\n\t\tat org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:224)\n\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:340)\n\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:290)\n\t\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:207)\n\t\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)\n\t\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:207)\n\t\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.resolveInFixedPoint(HybridAnalyzer.scala:237)\n\t\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.$anonfun$apply$1(HybridAnalyzer.scala:92)\n\t\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.withTrackedAnalyzerBridgeState(HybridAnalyzer.scala:123)\n\t\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.apply(HybridAnalyzer.scala:85)\n\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:323)\n\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:423)\n\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:323)\n\t\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$2(QueryExecution.scala:139)\n\t\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:148)\n\t\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:325)\n\t\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:712)\n\t\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:325)\n\t\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:810)\n\t\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:324)\n\t\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$1(QueryExecution.scala:139)\n\t\tat scala.util.Try$.apply(Try.scala:217)\n\t\tat org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1392)\n\t\tat org.apache.spark.util.LazyTry.tryT$lzycompute(LazyTry.scala:46)\n\t\tat org.apache.spark.util.LazyTry.tryT(LazyTry.scala:46)\n\t\t... 22 more\n", "stacktrace": [{"class": null, "method": "deco", "file": "/home/gaogaotiantian/programs/spark/python/pyspark/errors/exceptions/captured.py", "line": "263"}, {"class": null, "method": "get_return_value", "file": "/home/gaogaotiantian/programs/spark/python/lib/py4j-0.10.9.9-src.zip/py4j/protocol.py", "line": "327"}]}}
{"ts": "2026-01-02 11:21:29.941", "level": "ERROR", "logger": "SQLQueryContextLogger", "msg": "[UNEXPECTED_POSITIONAL_ARGUMENT] Cannot invoke routine `test_udf` because it contains positional argument(s) following the named argument assigned to `a`; please rearrange them so the positional arguments come first and then retry the query again. SQLSTATE: 4274K", "context": {"errorClass": "UNEXPECTED_POSITIONAL_ARGUMENT"}, "exception": {"class": "Py4JJavaError", "msg": "An error occurred while calling o85.sql.\n: org.apache.spark.sql.AnalysisException: [UNEXPECTED_POSITIONAL_ARGUMENT] Cannot invoke routine `test_udf` because it contains positional argument(s) following the named argument assigned to `a`; please rearrange them so the positional arguments come first and then retry the query again. SQLSTATE: 4274K; line 1 pos 7\n\tat org.apache.spark.sql.errors.QueryCompilationErrors$.unexpectedPositionalArgument(QueryCompilationErrors.scala:128)\n\tat org.apache.spark.sql.catalyst.plans.logical.NamedParametersSupport$.$anonfun$splitAndCheckNamedArguments$2(FunctionBuilderBase.scala:114)\n\tat scala.collection.immutable.Vector1.map(Vector.scala:2155)\n\tat scala.collection.immutable.Vector1.map(Vector.scala:386)\n\tat org.apache.spark.sql.catalyst.plans.logical.NamedParametersSupport$.splitAndCheckNamedArguments(FunctionBuilderBase.scala:104)\n\tat org.apache.spark.sql.execution.python.UserDefinedPythonFunction.builder(UserDefinedPythonFunction.scala:62)\n\tat org.apache.spark.sql.classic.UDFRegistration.$anonfun$registerPython$1(UDFRegistration.scala:64)\n\tat org.apache.spark.sql.catalyst.analysis.SimpleFunctionRegistryBase.lookupFunction(FunctionRegistry.scala:246)\n\tat org.apache.spark.sql.catalyst.analysis.SimpleFunctionRegistryBase.lookupFunction$(FunctionRegistry.scala:240)\n\tat org.apache.spark.sql.catalyst.analysis.SimpleFunctionRegistry.lookupFunction(FunctionRegistry.scala:305)\n\tat org.apache.spark.sql.catalyst.catalog.SessionCatalog.$anonfun$resolveBuiltinOrTempFunctionInternal$1(SessionCatalog.scala:2121)\n\tat org.apache.spark.sql.catalyst.catalog.SessionCatalog.lookupTempFuncWithViewContext(SessionCatalog.scala:2143)\n\tat org.apache.spark.sql.catalyst.catalog.SessionCatalog.resolveBuiltinOrTempFunctionInternal(SessionCatalog.scala:2121)\n\tat org.apache.spark.sql.catalyst.catalog.SessionCatalog.resolveBuiltinOrTempFunction(SessionCatalog.scala:2098)\n\tat org.apache.spark.sql.catalyst.analysis.FunctionResolution.resolveBuiltinOrTempFunction(FunctionResolution.scala:102)\n\tat org.apache.spark.sql.catalyst.analysis.FunctionResolution.$anonfun$resolveFunction$1(FunctionResolution.scala:51)\n\tat org.apache.spark.sql.catalyst.analysis.package$.withPosition(package.scala:104)\n\tat org.apache.spark.sql.catalyst.analysis.FunctionResolution.resolveFunction(FunctionResolution.scala:51)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveFunctions$$anonfun$apply$19$$anonfun$applyOrElse$141.applyOrElse(Analyzer.scala:2256)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveFunctions$$anonfun$apply$19$$anonfun$applyOrElse$141.applyOrElse(Analyzer.scala:2226)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformUpWithPruning$2(TreeNode.scala:549)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:107)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformUpWithPruning(TreeNode.scala:549)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformUpWithPruning$1(TreeNode.scala:546)\n\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1268)\n\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1267)\n\tat org.apache.spark.sql.catalyst.expressions.UnaryExpression.mapChildren(Expression.scala:587)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformUpWithPruning(TreeNode.scala:546)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.$anonfun$transformExpressionsUpWithPruning$1(QueryPlan.scala:245)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.$anonfun$mapExpressions$1(QueryPlan.scala:257)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:107)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpression$1(QueryPlan.scala:257)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.recursiveTransform$1(QueryPlan.scala:269)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.$anonfun$mapExpressions$4(QueryPlan.scala:275)\n\tat scala.collection.immutable.List.map(List.scala:236)\n\tat scala.collection.immutable.List.map(List.scala:79)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.recursiveTransform$1(QueryPlan.scala:275)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.$anonfun$mapExpressions$5(QueryPlan.scala:280)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:337)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.mapExpressions(QueryPlan.scala:280)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpressionsUpWithPruning(QueryPlan.scala:245)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveFunctions$$anonfun$apply$19.applyOrElse(Analyzer.scala:2226)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveFunctions$$anonfun$apply$19.applyOrElse(Analyzer.scala:2096)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$3(AnalysisHelper.scala:139)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:107)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:139)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:416)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:135)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:131)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:37)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveFunctions$.apply(Analyzer.scala:2096)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveFunctions$.apply(Analyzer.scala:2093)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:248)\n\tat scala.collection.LinearSeqOps.foldLeft(LinearSeq.scala:183)\n\tat scala.collection.LinearSeqOps.foldLeft$(LinearSeq.scala:179)\n\tat scala.collection.immutable.List.foldLeft(List.scala:79)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:245)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:237)\n\tat scala.collection.immutable.List.foreach(List.scala:323)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:237)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext(Analyzer.scala:344)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:340)\n\tat org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:224)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:340)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:290)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:207)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:207)\n\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.resolveInFixedPoint(HybridAnalyzer.scala:237)\n\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.$anonfun$apply$1(HybridAnalyzer.scala:92)\n\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.withTrackedAnalyzerBridgeState(HybridAnalyzer.scala:123)\n\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.apply(HybridAnalyzer.scala:85)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:323)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:423)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:323)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$2(QueryExecution.scala:139)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:148)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:325)\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:712)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:325)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:810)\n\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:324)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$1(QueryExecution.scala:139)\n\tat scala.util.Try$.apply(Try.scala:217)\n\tat org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1392)\n\tat org.apache.spark.util.Utils$.getTryWithCallerStacktrace(Utils.scala:1453)\n\tat org.apache.spark.util.LazyTry.get(LazyTry.scala:58)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:150)\n\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:90)\n\tat org.apache.spark.sql.classic.Dataset$.$anonfun$ofRows$5(Dataset.scala:138)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:810)\n\tat org.apache.spark.sql.classic.Dataset$.ofRows(Dataset.scala:135)\n\tat org.apache.spark.sql.classic.SparkSession.$anonfun$sql$1(SparkSession.scala:534)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:810)\n\tat org.apache.spark.sql.classic.SparkSession.sql(SparkSession.scala:504)\n\tat org.apache.spark.sql.classic.SparkSession.sql(SparkSession.scala:539)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:75)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:52)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:580)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:184)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:108)\n\tat java.base/java.lang.Thread.run(Thread.java:1583)\n\tSuppressed: org.apache.spark.util.Utils$OriginalTryStackTraceException: Full stacktrace of original doTryWithCallerStacktrace caller\n\t\tat org.apache.spark.sql.errors.QueryCompilationErrors$.unexpectedPositionalArgument(QueryCompilationErrors.scala:128)\n\t\tat org.apache.spark.sql.catalyst.plans.logical.NamedParametersSupport$.$anonfun$splitAndCheckNamedArguments$2(FunctionBuilderBase.scala:114)\n\t\tat scala.collection.immutable.Vector1.map(Vector.scala:2155)\n\t\tat scala.collection.immutable.Vector1.map(Vector.scala:386)\n\t\tat org.apache.spark.sql.catalyst.plans.logical.NamedParametersSupport$.splitAndCheckNamedArguments(FunctionBuilderBase.scala:104)\n\t\tat org.apache.spark.sql.execution.python.UserDefinedPythonFunction.builder(UserDefinedPythonFunction.scala:62)\n\t\tat org.apache.spark.sql.classic.UDFRegistration.$anonfun$registerPython$1(UDFRegistration.scala:64)\n\t\tat org.apache.spark.sql.catalyst.analysis.SimpleFunctionRegistryBase.lookupFunction(FunctionRegistry.scala:246)\n\t\tat org.apache.spark.sql.catalyst.analysis.SimpleFunctionRegistryBase.lookupFunction$(FunctionRegistry.scala:240)\n\t\tat org.apache.spark.sql.catalyst.analysis.SimpleFunctionRegistry.lookupFunction(FunctionRegistry.scala:305)\n\t\tat org.apache.spark.sql.catalyst.catalog.SessionCatalog.$anonfun$resolveBuiltinOrTempFunctionInternal$1(SessionCatalog.scala:2121)\n\t\tat org.apache.spark.sql.catalyst.catalog.SessionCatalog.lookupTempFuncWithViewContext(SessionCatalog.scala:2143)\n\t\tat org.apache.spark.sql.catalyst.catalog.SessionCatalog.resolveBuiltinOrTempFunctionInternal(SessionCatalog.scala:2121)\n\t\tat org.apache.spark.sql.catalyst.catalog.SessionCatalog.resolveBuiltinOrTempFunction(SessionCatalog.scala:2098)\n\t\tat org.apache.spark.sql.catalyst.analysis.FunctionResolution.resolveBuiltinOrTempFunction(FunctionResolution.scala:102)\n\t\tat org.apache.spark.sql.catalyst.analysis.FunctionResolution.$anonfun$resolveFunction$1(FunctionResolution.scala:51)\n\t\tat org.apache.spark.sql.catalyst.analysis.package$.withPosition(package.scala:104)\n\t\tat org.apache.spark.sql.catalyst.analysis.FunctionResolution.resolveFunction(FunctionResolution.scala:51)\n\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveFunctions$$anonfun$apply$19$$anonfun$applyOrElse$141.applyOrElse(Analyzer.scala:2256)\n\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveFunctions$$anonfun$apply$19$$anonfun$applyOrElse$141.applyOrElse(Analyzer.scala:2226)\n\t\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformUpWithPruning$2(TreeNode.scala:549)\n\t\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:107)\n\t\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformUpWithPruning(TreeNode.scala:549)\n\t\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformUpWithPruning$1(TreeNode.scala:546)\n\t\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1268)\n\t\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1267)\n\t\tat org.apache.spark.sql.catalyst.expressions.UnaryExpression.mapChildren(Expression.scala:587)\n\t\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformUpWithPruning(TreeNode.scala:546)\n\t\tat org.apache.spark.sql.catalyst.plans.QueryPlan.$anonfun$transformExpressionsUpWithPruning$1(QueryPlan.scala:245)\n\t\tat org.apache.spark.sql.catalyst.plans.QueryPlan.$anonfun$mapExpressions$1(QueryPlan.scala:257)\n\t\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:107)\n\t\tat org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpression$1(QueryPlan.scala:257)\n\t\tat org.apache.spark.sql.catalyst.plans.QueryPlan.recursiveTransform$1(QueryPlan.scala:269)\n\t\tat org.apache.spark.sql.catalyst.plans.QueryPlan.$anonfun$mapExpressions$4(QueryPlan.scala:275)\n\t\tat scala.collection.immutable.List.map(List.scala:236)\n\t\tat scala.collection.immutable.List.map(List.scala:79)\n\t\tat org.apache.spark.sql.catalyst.plans.QueryPlan.recursiveTransform$1(QueryPlan.scala:275)\n\t\tat org.apache.spark.sql.catalyst.plans.QueryPlan.$anonfun$mapExpressions$5(QueryPlan.scala:280)\n\t\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:337)\n\t\tat org.apache.spark.sql.catalyst.plans.QueryPlan.mapExpressions(QueryPlan.scala:280)\n\t\tat org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpressionsUpWithPruning(QueryPlan.scala:245)\n\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveFunctions$$anonfun$apply$19.applyOrElse(Analyzer.scala:2226)\n\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveFunctions$$anonfun$apply$19.applyOrElse(Analyzer.scala:2096)\n\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$3(AnalysisHelper.scala:139)\n\t\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:107)\n\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:139)\n\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:416)\n\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:135)\n\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:131)\n\t\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:37)\n\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveFunctions$.apply(Analyzer.scala:2096)\n\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveFunctions$.apply(Analyzer.scala:2093)\n\t\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:248)\n\t\tat scala.collection.LinearSeqOps.foldLeft(LinearSeq.scala:183)\n\t\tat scala.collection.LinearSeqOps.foldLeft$(LinearSeq.scala:179)\n\t\tat scala.collection.immutable.List.foldLeft(List.scala:79)\n\t\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:245)\n\t\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:237)\n\t\tat scala.collection.immutable.List.foreach(List.scala:323)\n\t\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:237)\n\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext(Analyzer.scala:344)\n\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:340)\n\t\tat org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:224)\n\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:340)\n\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:290)\n\t\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:207)\n\t\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)\n\t\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:207)\n\t\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.resolveInFixedPoint(HybridAnalyzer.scala:237)\n\t\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.$anonfun$apply$1(HybridAnalyzer.scala:92)\n\t\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.withTrackedAnalyzerBridgeState(HybridAnalyzer.scala:123)\n\t\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.apply(HybridAnalyzer.scala:85)\n\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:323)\n\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:423)\n\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:323)\n\t\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$2(QueryExecution.scala:139)\n\t\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:148)\n\t\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:325)\n\t\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:712)\n\t\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:325)\n\t\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:810)\n\t\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:324)\n\t\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$1(QueryExecution.scala:139)\n\t\tat scala.util.Try$.apply(Try.scala:217)\n\t\tat org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1392)\n\t\tat org.apache.spark.util.LazyTry.tryT$lzycompute(LazyTry.scala:46)\n\t\tat org.apache.spark.util.LazyTry.tryT(LazyTry.scala:46)\n\t\t... 22 more\n", "stacktrace": [{"class": null, "method": "deco", "file": "/home/gaogaotiantian/programs/spark/python/pyspark/errors/exceptions/captured.py", "line": "263"}, {"class": null, "method": "get_return_value", "file": "/home/gaogaotiantian/programs/spark/python/lib/py4j-0.10.9.9-src.zip/py4j/protocol.py", "line": "327"}]}}
</system-err>
	</testcase>
	<testcase classname="pyspark.sql.tests.test_udf.UDFTests" name="test_multiple_udfs" time="0.268" timestamp="2026-01-02T11:21:30" file="python/pyspark/sql/tests/test_udf.py" line="222"/>
	<testcase classname="pyspark.sql.tests.test_udf.UDFTests" name="test_multiple_udfs_with_logging" time="0.845" timestamp="2026-01-02T11:21:31" file="python/pyspark/sql/tests/test_udf.py" line="1637"/>
	<testcase classname="pyspark.sql.tests.test_udf.UDFTests" name="test_named_arguments" time="0.525" timestamp="2026-01-02T11:21:31" file="python/pyspark/sql/tests/test_udf.py" line="1086"/>
	<testcase classname="pyspark.sql.tests.test_udf.UDFTests" name="test_named_arguments_and_defaults" time="0.828" timestamp="2026-01-02T11:21:32" file="python/pyspark/sql/tests/test_udf.py" line="1163"/>
	<testcase classname="pyspark.sql.tests.test_udf.UDFTests" name="test_named_arguments_negative" time="0.570" timestamp="2026-01-02T11:21:33" file="python/pyspark/sql/tests/test_udf.py" line="1107">
		<system-err>{"ts": "2026-01-02 11:21:32.528", "level": "ERROR", "logger": "SQLQueryContextLogger", "msg": "[DUPLICATE_ROUTINE_PARAMETER_ASSIGNMENT.DOUBLE_NAMED_ARGUMENT_REFERENCE] Call to routine `test_udf` is invalid because it includes multiple argument assignments to the same parameter name `a`. More than one named argument referred to the same parameter. Please assign a value only once. SQLSTATE: 4274K", "context": {"errorClass": "DUPLICATE_ROUTINE_PARAMETER_ASSIGNMENT.DOUBLE_NAMED_ARGUMENT_REFERENCE"}, "exception": {"class": "Py4JJavaError", "msg": "An error occurred while calling o85.sql.\n: org.apache.spark.sql.AnalysisException: [DUPLICATE_ROUTINE_PARAMETER_ASSIGNMENT.DOUBLE_NAMED_ARGUMENT_REFERENCE] Call to routine `test_udf` is invalid because it includes multiple argument assignments to the same parameter name `a`. More than one named argument referred to the same parameter. Please assign a value only once. SQLSTATE: 4274K; line 1 pos 7\n\tat org.apache.spark.sql.errors.QueryCompilationErrors$.doubleNamedArgumentReference(QueryCompilationErrors.scala:90)\n\tat org.apache.spark.sql.catalyst.plans.logical.NamedParametersSupport$.$anonfun$splitAndCheckNamedArguments$2(FunctionBuilderBase.scala:108)\n\tat scala.collection.immutable.Vector1.map(Vector.scala:2155)\n\tat scala.collection.immutable.Vector1.map(Vector.scala:386)\n\tat org.apache.spark.sql.catalyst.plans.logical.NamedParametersSupport$.splitAndCheckNamedArguments(FunctionBuilderBase.scala:104)\n\tat org.apache.spark.sql.execution.python.UserDefinedPythonFunction.builder(UserDefinedPythonFunction.scala:62)\n\tat org.apache.spark.sql.classic.UDFRegistration.$anonfun$registerPython$1(UDFRegistration.scala:64)\n\tat org.apache.spark.sql.catalyst.analysis.SimpleFunctionRegistryBase.lookupFunction(FunctionRegistry.scala:246)\n\tat org.apache.spark.sql.catalyst.analysis.SimpleFunctionRegistryBase.lookupFunction$(FunctionRegistry.scala:240)\n\tat org.apache.spark.sql.catalyst.analysis.SimpleFunctionRegistry.lookupFunction(FunctionRegistry.scala:305)\n\tat org.apache.spark.sql.catalyst.catalog.SessionCatalog.$anonfun$resolveBuiltinOrTempFunctionInternal$1(SessionCatalog.scala:2121)\n\tat org.apache.spark.sql.catalyst.catalog.SessionCatalog.lookupTempFuncWithViewContext(SessionCatalog.scala:2143)\n\tat org.apache.spark.sql.catalyst.catalog.SessionCatalog.resolveBuiltinOrTempFunctionInternal(SessionCatalog.scala:2121)\n\tat org.apache.spark.sql.catalyst.catalog.SessionCatalog.resolveBuiltinOrTempFunction(SessionCatalog.scala:2098)\n\tat org.apache.spark.sql.catalyst.analysis.FunctionResolution.resolveBuiltinOrTempFunction(FunctionResolution.scala:102)\n\tat org.apache.spark.sql.catalyst.analysis.FunctionResolution.$anonfun$resolveFunction$1(FunctionResolution.scala:51)\n\tat org.apache.spark.sql.catalyst.analysis.package$.withPosition(package.scala:104)\n\tat org.apache.spark.sql.catalyst.analysis.FunctionResolution.resolveFunction(FunctionResolution.scala:51)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveFunctions$$anonfun$apply$19$$anonfun$applyOrElse$141.applyOrElse(Analyzer.scala:2256)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveFunctions$$anonfun$apply$19$$anonfun$applyOrElse$141.applyOrElse(Analyzer.scala:2226)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformUpWithPruning$2(TreeNode.scala:549)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:107)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformUpWithPruning(TreeNode.scala:549)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformUpWithPruning$1(TreeNode.scala:546)\n\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1268)\n\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1267)\n\tat org.apache.spark.sql.catalyst.expressions.UnaryExpression.mapChildren(Expression.scala:587)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformUpWithPruning(TreeNode.scala:546)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.$anonfun$transformExpressionsUpWithPruning$1(QueryPlan.scala:245)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.$anonfun$mapExpressions$1(QueryPlan.scala:257)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:107)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpression$1(QueryPlan.scala:257)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.recursiveTransform$1(QueryPlan.scala:269)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.$anonfun$mapExpressions$4(QueryPlan.scala:275)\n\tat scala.collection.immutable.List.map(List.scala:236)\n\tat scala.collection.immutable.List.map(List.scala:79)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.recursiveTransform$1(QueryPlan.scala:275)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.$anonfun$mapExpressions$5(QueryPlan.scala:280)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:337)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.mapExpressions(QueryPlan.scala:280)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpressionsUpWithPruning(QueryPlan.scala:245)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveFunctions$$anonfun$apply$19.applyOrElse(Analyzer.scala:2226)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveFunctions$$anonfun$apply$19.applyOrElse(Analyzer.scala:2096)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$3(AnalysisHelper.scala:139)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:107)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:139)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:416)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:135)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:131)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:37)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveFunctions$.apply(Analyzer.scala:2096)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveFunctions$.apply(Analyzer.scala:2093)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:248)\n\tat scala.collection.LinearSeqOps.foldLeft(LinearSeq.scala:183)\n\tat scala.collection.LinearSeqOps.foldLeft$(LinearSeq.scala:179)\n\tat scala.collection.immutable.List.foldLeft(List.scala:79)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:245)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:237)\n\tat scala.collection.immutable.List.foreach(List.scala:323)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:237)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext(Analyzer.scala:344)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:340)\n\tat org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:224)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:340)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:290)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:207)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:207)\n\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.resolveInFixedPoint(HybridAnalyzer.scala:237)\n\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.$anonfun$apply$1(HybridAnalyzer.scala:92)\n\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.withTrackedAnalyzerBridgeState(HybridAnalyzer.scala:123)\n\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.apply(HybridAnalyzer.scala:85)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:323)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:423)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:323)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$2(QueryExecution.scala:139)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:148)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:325)\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:712)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:325)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:810)\n\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:324)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$1(QueryExecution.scala:139)\n\tat scala.util.Try$.apply(Try.scala:217)\n\tat org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1392)\n\tat org.apache.spark.util.Utils$.getTryWithCallerStacktrace(Utils.scala:1453)\n\tat org.apache.spark.util.LazyTry.get(LazyTry.scala:58)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:150)\n\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:90)\n\tat org.apache.spark.sql.classic.Dataset$.$anonfun$ofRows$5(Dataset.scala:138)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:810)\n\tat org.apache.spark.sql.classic.Dataset$.ofRows(Dataset.scala:135)\n\tat org.apache.spark.sql.classic.SparkSession.$anonfun$sql$1(SparkSession.scala:534)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:810)\n\tat org.apache.spark.sql.classic.SparkSession.sql(SparkSession.scala:504)\n\tat org.apache.spark.sql.classic.SparkSession.sql(SparkSession.scala:539)\n\tat jdk.internal.reflect.GeneratedMethodAccessor77.invoke(Unknown Source)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:52)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:580)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:184)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:108)\n\tat java.base/java.lang.Thread.run(Thread.java:1583)\n\tSuppressed: org.apache.spark.util.Utils$OriginalTryStackTraceException: Full stacktrace of original doTryWithCallerStacktrace caller\n\t\tat org.apache.spark.sql.errors.QueryCompilationErrors$.doubleNamedArgumentReference(QueryCompilationErrors.scala:90)\n\t\tat org.apache.spark.sql.catalyst.plans.logical.NamedParametersSupport$.$anonfun$splitAndCheckNamedArguments$2(FunctionBuilderBase.scala:108)\n\t\tat scala.collection.immutable.Vector1.map(Vector.scala:2155)\n\t\tat scala.collection.immutable.Vector1.map(Vector.scala:386)\n\t\tat org.apache.spark.sql.catalyst.plans.logical.NamedParametersSupport$.splitAndCheckNamedArguments(FunctionBuilderBase.scala:104)\n\t\tat org.apache.spark.sql.execution.python.UserDefinedPythonFunction.builder(UserDefinedPythonFunction.scala:62)\n\t\tat org.apache.spark.sql.classic.UDFRegistration.$anonfun$registerPython$1(UDFRegistration.scala:64)\n\t\tat org.apache.spark.sql.catalyst.analysis.SimpleFunctionRegistryBase.lookupFunction(FunctionRegistry.scala:246)\n\t\tat org.apache.spark.sql.catalyst.analysis.SimpleFunctionRegistryBase.lookupFunction$(FunctionRegistry.scala:240)\n\t\tat org.apache.spark.sql.catalyst.analysis.SimpleFunctionRegistry.lookupFunction(FunctionRegistry.scala:305)\n\t\tat org.apache.spark.sql.catalyst.catalog.SessionCatalog.$anonfun$resolveBuiltinOrTempFunctionInternal$1(SessionCatalog.scala:2121)\n\t\tat org.apache.spark.sql.catalyst.catalog.SessionCatalog.lookupTempFuncWithViewContext(SessionCatalog.scala:2143)\n\t\tat org.apache.spark.sql.catalyst.catalog.SessionCatalog.resolveBuiltinOrTempFunctionInternal(SessionCatalog.scala:2121)\n\t\tat org.apache.spark.sql.catalyst.catalog.SessionCatalog.resolveBuiltinOrTempFunction(SessionCatalog.scala:2098)\n\t\tat org.apache.spark.sql.catalyst.analysis.FunctionResolution.resolveBuiltinOrTempFunction(FunctionResolution.scala:102)\n\t\tat org.apache.spark.sql.catalyst.analysis.FunctionResolution.$anonfun$resolveFunction$1(FunctionResolution.scala:51)\n\t\tat org.apache.spark.sql.catalyst.analysis.package$.withPosition(package.scala:104)\n\t\tat org.apache.spark.sql.catalyst.analysis.FunctionResolution.resolveFunction(FunctionResolution.scala:51)\n\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveFunctions$$anonfun$apply$19$$anonfun$applyOrElse$141.applyOrElse(Analyzer.scala:2256)\n\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveFunctions$$anonfun$apply$19$$anonfun$applyOrElse$141.applyOrElse(Analyzer.scala:2226)\n\t\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformUpWithPruning$2(TreeNode.scala:549)\n\t\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:107)\n\t\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformUpWithPruning(TreeNode.scala:549)\n\t\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformUpWithPruning$1(TreeNode.scala:546)\n\t\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1268)\n\t\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1267)\n\t\tat org.apache.spark.sql.catalyst.expressions.UnaryExpression.mapChildren(Expression.scala:587)\n\t\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformUpWithPruning(TreeNode.scala:546)\n\t\tat org.apache.spark.sql.catalyst.plans.QueryPlan.$anonfun$transformExpressionsUpWithPruning$1(QueryPlan.scala:245)\n\t\tat org.apache.spark.sql.catalyst.plans.QueryPlan.$anonfun$mapExpressions$1(QueryPlan.scala:257)\n\t\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:107)\n\t\tat org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpression$1(QueryPlan.scala:257)\n\t\tat org.apache.spark.sql.catalyst.plans.QueryPlan.recursiveTransform$1(QueryPlan.scala:269)\n\t\tat org.apache.spark.sql.catalyst.plans.QueryPlan.$anonfun$mapExpressions$4(QueryPlan.scala:275)\n\t\tat scala.collection.immutable.List.map(List.scala:236)\n\t\tat scala.collection.immutable.List.map(List.scala:79)\n\t\tat org.apache.spark.sql.catalyst.plans.QueryPlan.recursiveTransform$1(QueryPlan.scala:275)\n\t\tat org.apache.spark.sql.catalyst.plans.QueryPlan.$anonfun$mapExpressions$5(QueryPlan.scala:280)\n\t\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:337)\n\t\tat org.apache.spark.sql.catalyst.plans.QueryPlan.mapExpressions(QueryPlan.scala:280)\n\t\tat org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpressionsUpWithPruning(QueryPlan.scala:245)\n\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveFunctions$$anonfun$apply$19.applyOrElse(Analyzer.scala:2226)\n\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveFunctions$$anonfun$apply$19.applyOrElse(Analyzer.scala:2096)\n\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$3(AnalysisHelper.scala:139)\n\t\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:107)\n\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:139)\n\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:416)\n\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:135)\n\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:131)\n\t\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:37)\n\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveFunctions$.apply(Analyzer.scala:2096)\n\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveFunctions$.apply(Analyzer.scala:2093)\n\t\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:248)\n\t\tat scala.collection.LinearSeqOps.foldLeft(LinearSeq.scala:183)\n\t\tat scala.collection.LinearSeqOps.foldLeft$(LinearSeq.scala:179)\n\t\tat scala.collection.immutable.List.foldLeft(List.scala:79)\n\t\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:245)\n\t\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:237)\n\t\tat scala.collection.immutable.List.foreach(List.scala:323)\n\t\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:237)\n\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext(Analyzer.scala:344)\n\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:340)\n\t\tat org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:224)\n\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:340)\n\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:290)\n\t\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:207)\n\t\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)\n\t\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:207)\n\t\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.resolveInFixedPoint(HybridAnalyzer.scala:237)\n\t\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.$anonfun$apply$1(HybridAnalyzer.scala:92)\n\t\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.withTrackedAnalyzerBridgeState(HybridAnalyzer.scala:123)\n\t\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.apply(HybridAnalyzer.scala:85)\n\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:323)\n\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:423)\n\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:323)\n\t\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$2(QueryExecution.scala:139)\n\t\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:148)\n\t\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:325)\n\t\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:712)\n\t\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:325)\n\t\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:810)\n\t\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:324)\n\t\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$1(QueryExecution.scala:139)\n\t\tat scala.util.Try$.apply(Try.scala:217)\n\t\tat org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1392)\n\t\tat org.apache.spark.util.LazyTry.tryT$lzycompute(LazyTry.scala:46)\n\t\tat org.apache.spark.util.LazyTry.tryT(LazyTry.scala:46)\n\t\t... 21 more\n", "stacktrace": [{"class": null, "method": "deco", "file": "/home/gaogaotiantian/programs/spark/python/pyspark/errors/exceptions/captured.py", "line": "263"}, {"class": null, "method": "get_return_value", "file": "/home/gaogaotiantian/programs/spark/python/lib/py4j-0.10.9.9-src.zip/py4j/protocol.py", "line": "327"}]}}
{"ts": "2026-01-02 11:21:32.648", "level": "ERROR", "logger": "SQLQueryContextLogger", "msg": "[UNEXPECTED_POSITIONAL_ARGUMENT] Cannot invoke routine `test_udf` because it contains positional argument(s) following the named argument assigned to `a`; please rearrange them so the positional arguments come first and then retry the query again. SQLSTATE: 4274K", "context": {"errorClass": "UNEXPECTED_POSITIONAL_ARGUMENT"}, "exception": {"class": "Py4JJavaError", "msg": "An error occurred while calling o85.sql.\n: org.apache.spark.sql.AnalysisException: [UNEXPECTED_POSITIONAL_ARGUMENT] Cannot invoke routine `test_udf` because it contains positional argument(s) following the named argument assigned to `a`; please rearrange them so the positional arguments come first and then retry the query again. SQLSTATE: 4274K; line 1 pos 7\n\tat org.apache.spark.sql.errors.QueryCompilationErrors$.unexpectedPositionalArgument(QueryCompilationErrors.scala:128)\n\tat org.apache.spark.sql.catalyst.plans.logical.NamedParametersSupport$.$anonfun$splitAndCheckNamedArguments$2(FunctionBuilderBase.scala:114)\n\tat scala.collection.immutable.Vector1.map(Vector.scala:2155)\n\tat scala.collection.immutable.Vector1.map(Vector.scala:386)\n\tat org.apache.spark.sql.catalyst.plans.logical.NamedParametersSupport$.splitAndCheckNamedArguments(FunctionBuilderBase.scala:104)\n\tat org.apache.spark.sql.execution.python.UserDefinedPythonFunction.builder(UserDefinedPythonFunction.scala:62)\n\tat org.apache.spark.sql.classic.UDFRegistration.$anonfun$registerPython$1(UDFRegistration.scala:64)\n\tat org.apache.spark.sql.catalyst.analysis.SimpleFunctionRegistryBase.lookupFunction(FunctionRegistry.scala:246)\n\tat org.apache.spark.sql.catalyst.analysis.SimpleFunctionRegistryBase.lookupFunction$(FunctionRegistry.scala:240)\n\tat org.apache.spark.sql.catalyst.analysis.SimpleFunctionRegistry.lookupFunction(FunctionRegistry.scala:305)\n\tat org.apache.spark.sql.catalyst.catalog.SessionCatalog.$anonfun$resolveBuiltinOrTempFunctionInternal$1(SessionCatalog.scala:2121)\n\tat org.apache.spark.sql.catalyst.catalog.SessionCatalog.lookupTempFuncWithViewContext(SessionCatalog.scala:2143)\n\tat org.apache.spark.sql.catalyst.catalog.SessionCatalog.resolveBuiltinOrTempFunctionInternal(SessionCatalog.scala:2121)\n\tat org.apache.spark.sql.catalyst.catalog.SessionCatalog.resolveBuiltinOrTempFunction(SessionCatalog.scala:2098)\n\tat org.apache.spark.sql.catalyst.analysis.FunctionResolution.resolveBuiltinOrTempFunction(FunctionResolution.scala:102)\n\tat org.apache.spark.sql.catalyst.analysis.FunctionResolution.$anonfun$resolveFunction$1(FunctionResolution.scala:51)\n\tat org.apache.spark.sql.catalyst.analysis.package$.withPosition(package.scala:104)\n\tat org.apache.spark.sql.catalyst.analysis.FunctionResolution.resolveFunction(FunctionResolution.scala:51)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveFunctions$$anonfun$apply$19$$anonfun$applyOrElse$141.applyOrElse(Analyzer.scala:2256)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveFunctions$$anonfun$apply$19$$anonfun$applyOrElse$141.applyOrElse(Analyzer.scala:2226)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformUpWithPruning$2(TreeNode.scala:549)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:107)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformUpWithPruning(TreeNode.scala:549)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformUpWithPruning$1(TreeNode.scala:546)\n\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1268)\n\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1267)\n\tat org.apache.spark.sql.catalyst.expressions.UnaryExpression.mapChildren(Expression.scala:587)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformUpWithPruning(TreeNode.scala:546)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.$anonfun$transformExpressionsUpWithPruning$1(QueryPlan.scala:245)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.$anonfun$mapExpressions$1(QueryPlan.scala:257)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:107)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpression$1(QueryPlan.scala:257)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.recursiveTransform$1(QueryPlan.scala:269)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.$anonfun$mapExpressions$4(QueryPlan.scala:275)\n\tat scala.collection.immutable.List.map(List.scala:236)\n\tat scala.collection.immutable.List.map(List.scala:79)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.recursiveTransform$1(QueryPlan.scala:275)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.$anonfun$mapExpressions$5(QueryPlan.scala:280)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:337)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.mapExpressions(QueryPlan.scala:280)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpressionsUpWithPruning(QueryPlan.scala:245)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveFunctions$$anonfun$apply$19.applyOrElse(Analyzer.scala:2226)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveFunctions$$anonfun$apply$19.applyOrElse(Analyzer.scala:2096)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$3(AnalysisHelper.scala:139)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:107)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:139)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:416)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:135)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:131)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:37)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveFunctions$.apply(Analyzer.scala:2096)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveFunctions$.apply(Analyzer.scala:2093)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:248)\n\tat scala.collection.LinearSeqOps.foldLeft(LinearSeq.scala:183)\n\tat scala.collection.LinearSeqOps.foldLeft$(LinearSeq.scala:179)\n\tat scala.collection.immutable.List.foldLeft(List.scala:79)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:245)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:237)\n\tat scala.collection.immutable.List.foreach(List.scala:323)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:237)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext(Analyzer.scala:344)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:340)\n\tat org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:224)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:340)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:290)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:207)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:207)\n\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.resolveInFixedPoint(HybridAnalyzer.scala:237)\n\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.$anonfun$apply$1(HybridAnalyzer.scala:92)\n\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.withTrackedAnalyzerBridgeState(HybridAnalyzer.scala:123)\n\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.apply(HybridAnalyzer.scala:85)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:323)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:423)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:323)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$2(QueryExecution.scala:139)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:148)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:325)\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:712)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:325)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:810)\n\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:324)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$1(QueryExecution.scala:139)\n\tat scala.util.Try$.apply(Try.scala:217)\n\tat org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1392)\n\tat org.apache.spark.util.Utils$.getTryWithCallerStacktrace(Utils.scala:1453)\n\tat org.apache.spark.util.LazyTry.get(LazyTry.scala:58)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:150)\n\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:90)\n\tat org.apache.spark.sql.classic.Dataset$.$anonfun$ofRows$5(Dataset.scala:138)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:810)\n\tat org.apache.spark.sql.classic.Dataset$.ofRows(Dataset.scala:135)\n\tat org.apache.spark.sql.classic.SparkSession.$anonfun$sql$1(SparkSession.scala:534)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:810)\n\tat org.apache.spark.sql.classic.SparkSession.sql(SparkSession.scala:504)\n\tat org.apache.spark.sql.classic.SparkSession.sql(SparkSession.scala:539)\n\tat jdk.internal.reflect.GeneratedMethodAccessor77.invoke(Unknown Source)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:52)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:580)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:184)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:108)\n\tat java.base/java.lang.Thread.run(Thread.java:1583)\n\tSuppressed: org.apache.spark.util.Utils$OriginalTryStackTraceException: Full stacktrace of original doTryWithCallerStacktrace caller\n\t\tat org.apache.spark.sql.errors.QueryCompilationErrors$.unexpectedPositionalArgument(QueryCompilationErrors.scala:128)\n\t\tat org.apache.spark.sql.catalyst.plans.logical.NamedParametersSupport$.$anonfun$splitAndCheckNamedArguments$2(FunctionBuilderBase.scala:114)\n\t\tat scala.collection.immutable.Vector1.map(Vector.scala:2155)\n\t\tat scala.collection.immutable.Vector1.map(Vector.scala:386)\n\t\tat org.apache.spark.sql.catalyst.plans.logical.NamedParametersSupport$.splitAndCheckNamedArguments(FunctionBuilderBase.scala:104)\n\t\tat org.apache.spark.sql.execution.python.UserDefinedPythonFunction.builder(UserDefinedPythonFunction.scala:62)\n\t\tat org.apache.spark.sql.classic.UDFRegistration.$anonfun$registerPython$1(UDFRegistration.scala:64)\n\t\tat org.apache.spark.sql.catalyst.analysis.SimpleFunctionRegistryBase.lookupFunction(FunctionRegistry.scala:246)\n\t\tat org.apache.spark.sql.catalyst.analysis.SimpleFunctionRegistryBase.lookupFunction$(FunctionRegistry.scala:240)\n\t\tat org.apache.spark.sql.catalyst.analysis.SimpleFunctionRegistry.lookupFunction(FunctionRegistry.scala:305)\n\t\tat org.apache.spark.sql.catalyst.catalog.SessionCatalog.$anonfun$resolveBuiltinOrTempFunctionInternal$1(SessionCatalog.scala:2121)\n\t\tat org.apache.spark.sql.catalyst.catalog.SessionCatalog.lookupTempFuncWithViewContext(SessionCatalog.scala:2143)\n\t\tat org.apache.spark.sql.catalyst.catalog.SessionCatalog.resolveBuiltinOrTempFunctionInternal(SessionCatalog.scala:2121)\n\t\tat org.apache.spark.sql.catalyst.catalog.SessionCatalog.resolveBuiltinOrTempFunction(SessionCatalog.scala:2098)\n\t\tat org.apache.spark.sql.catalyst.analysis.FunctionResolution.resolveBuiltinOrTempFunction(FunctionResolution.scala:102)\n\t\tat org.apache.spark.sql.catalyst.analysis.FunctionResolution.$anonfun$resolveFunction$1(FunctionResolution.scala:51)\n\t\tat org.apache.spark.sql.catalyst.analysis.package$.withPosition(package.scala:104)\n\t\tat org.apache.spark.sql.catalyst.analysis.FunctionResolution.resolveFunction(FunctionResolution.scala:51)\n\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveFunctions$$anonfun$apply$19$$anonfun$applyOrElse$141.applyOrElse(Analyzer.scala:2256)\n\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveFunctions$$anonfun$apply$19$$anonfun$applyOrElse$141.applyOrElse(Analyzer.scala:2226)\n\t\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformUpWithPruning$2(TreeNode.scala:549)\n\t\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:107)\n\t\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformUpWithPruning(TreeNode.scala:549)\n\t\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformUpWithPruning$1(TreeNode.scala:546)\n\t\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1268)\n\t\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1267)\n\t\tat org.apache.spark.sql.catalyst.expressions.UnaryExpression.mapChildren(Expression.scala:587)\n\t\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformUpWithPruning(TreeNode.scala:546)\n\t\tat org.apache.spark.sql.catalyst.plans.QueryPlan.$anonfun$transformExpressionsUpWithPruning$1(QueryPlan.scala:245)\n\t\tat org.apache.spark.sql.catalyst.plans.QueryPlan.$anonfun$mapExpressions$1(QueryPlan.scala:257)\n\t\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:107)\n\t\tat org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpression$1(QueryPlan.scala:257)\n\t\tat org.apache.spark.sql.catalyst.plans.QueryPlan.recursiveTransform$1(QueryPlan.scala:269)\n\t\tat org.apache.spark.sql.catalyst.plans.QueryPlan.$anonfun$mapExpressions$4(QueryPlan.scala:275)\n\t\tat scala.collection.immutable.List.map(List.scala:236)\n\t\tat scala.collection.immutable.List.map(List.scala:79)\n\t\tat org.apache.spark.sql.catalyst.plans.QueryPlan.recursiveTransform$1(QueryPlan.scala:275)\n\t\tat org.apache.spark.sql.catalyst.plans.QueryPlan.$anonfun$mapExpressions$5(QueryPlan.scala:280)\n\t\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:337)\n\t\tat org.apache.spark.sql.catalyst.plans.QueryPlan.mapExpressions(QueryPlan.scala:280)\n\t\tat org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpressionsUpWithPruning(QueryPlan.scala:245)\n\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveFunctions$$anonfun$apply$19.applyOrElse(Analyzer.scala:2226)\n\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveFunctions$$anonfun$apply$19.applyOrElse(Analyzer.scala:2096)\n\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$3(AnalysisHelper.scala:139)\n\t\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:107)\n\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:139)\n\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:416)\n\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:135)\n\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:131)\n\t\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:37)\n\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveFunctions$.apply(Analyzer.scala:2096)\n\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveFunctions$.apply(Analyzer.scala:2093)\n\t\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:248)\n\t\tat scala.collection.LinearSeqOps.foldLeft(LinearSeq.scala:183)\n\t\tat scala.collection.LinearSeqOps.foldLeft$(LinearSeq.scala:179)\n\t\tat scala.collection.immutable.List.foldLeft(List.scala:79)\n\t\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:245)\n\t\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:237)\n\t\tat scala.collection.immutable.List.foreach(List.scala:323)\n\t\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:237)\n\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext(Analyzer.scala:344)\n\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:340)\n\t\tat org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:224)\n\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:340)\n\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:290)\n\t\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:207)\n\t\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)\n\t\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:207)\n\t\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.resolveInFixedPoint(HybridAnalyzer.scala:237)\n\t\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.$anonfun$apply$1(HybridAnalyzer.scala:92)\n\t\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.withTrackedAnalyzerBridgeState(HybridAnalyzer.scala:123)\n\t\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.apply(HybridAnalyzer.scala:85)\n\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:323)\n\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:423)\n\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:323)\n\t\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$2(QueryExecution.scala:139)\n\t\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:148)\n\t\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:325)\n\t\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:712)\n\t\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:325)\n\t\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:810)\n\t\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:324)\n\t\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$1(QueryExecution.scala:139)\n\t\tat scala.util.Try$.apply(Try.scala:217)\n\t\tat org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1392)\n\t\tat org.apache.spark.util.LazyTry.tryT$lzycompute(LazyTry.scala:46)\n\t\tat org.apache.spark.util.LazyTry.tryT(LazyTry.scala:46)\n\t\t... 21 more\n", "stacktrace": [{"class": null, "method": "deco", "file": "/home/gaogaotiantian/programs/spark/python/pyspark/errors/exceptions/captured.py", "line": "263"}, {"class": null, "method": "get_return_value", "file": "/home/gaogaotiantian/programs/spark/python/lib/py4j-0.10.9.9-src.zip/py4j/protocol.py", "line": "327"}]}}
</system-err>
	</testcase>
	<testcase classname="pyspark.sql.tests.test_udf.UDFTests" name="test_nested_array" time="0.206" timestamp="2026-01-02T11:21:33" file="python/pyspark/sql/tests/test_udf.py" line="1053"/>
	<testcase classname="pyspark.sql.tests.test_udf.UDFTests" name="test_nested_map" time="0.195" timestamp="2026-01-02T11:21:33" file="python/pyspark/sql/tests/test_udf.py" line="1038"/>
	<testcase classname="pyspark.sql.tests.test_udf.UDFTests" name="test_nested_struct" time="0.194" timestamp="2026-01-02T11:21:33" file="python/pyspark/sql/tests/test_udf.py" line="1023"/>
	<testcase classname="pyspark.sql.tests.test_udf.UDFTests" name="test_non_existed_udaf" time="0.016" timestamp="2026-01-02T11:21:33" file="python/pyspark/sql/tests/test_udf.py" line="626"/>
	<testcase classname="pyspark.sql.tests.test_udf.UDFTests" name="test_non_existed_udf" time="0.006" timestamp="2026-01-02T11:21:33" file="python/pyspark/sql/tests/test_udf.py" line="607"/>
	<testcase classname="pyspark.sql.tests.test_udf.UDFTests" name="test_non_existed_udf_with_sql_context" time="0.006" timestamp="2026-01-02T11:21:33" file="python/pyspark/sql/tests/test_udf.py" line="615">
		<system-err>/home/gaogaotiantian/programs/spark/python/pyspark/sql/context.py:157: FutureWarning: Deprecated in 3.0.0. Use SparkSession.builder.getOrCreate() instead.
  warnings.warn(
/home/gaogaotiantian/programs/spark/python/pyspark/sql/context.py:308: FutureWarning: Deprecated in 2.3.0. Use spark.udf.registerJavaFunction instead.
  warnings.warn(
</system-err>
	</testcase>
	<testcase classname="pyspark.sql.tests.test_udf.UDFTests" name="test_nondeterministic_udf" time="0.175" timestamp="2026-01-02T11:21:33" file="python/pyspark/sql/tests/test_udf.py" line="146"/>
	<testcase classname="pyspark.sql.tests.test_udf.UDFTests" name="test_nondeterministic_udf2" time="0.256" timestamp="2026-01-02T11:21:34" file="python/pyspark/sql/tests/test_udf.py" line="157"/>
	<testcase classname="pyspark.sql.tests.test_udf.UDFTests" name="test_nondeterministic_udf3" time="0.019" timestamp="2026-01-02T11:21:34" file="python/pyspark/sql/tests/test_udf.py" line="177"/>
	<testcase classname="pyspark.sql.tests.test_udf.UDFTests" name="test_nondeterministic_udf_in_aggregate" time="0.140" timestamp="2026-01-02T11:21:34" file="python/pyspark/sql/tests/test_udf.py" line="189"/>
	<testcase classname="pyspark.sql.tests.test_udf.UDFTests" name="test_nonparam_udf_with_aggregate" time="0.286" timestamp="2026-01-02T11:21:34" file="python/pyspark/sql/tests/test_udf.py" line="825"/>
	<testcase classname="pyspark.sql.tests.test_udf.UDFTests" name="test_num_arguments" time="0.265" timestamp="2026-01-02T11:21:34" file="python/pyspark/sql/tests/test_udf.py" line="1197"/>
	<testcase classname="pyspark.sql.tests.test_udf.UDFTests" name="test_python_udf_segfault" time="1.817" timestamp="2026-01-02T11:21:36" file="python/pyspark/sql/tests/test_udf.py" line="1232"/>
	<testcase classname="pyspark.sql.tests.test_udf.UDFTests" name="test_raise_stop_iteration" time="0.231" timestamp="2026-01-02T11:21:36" file="python/pyspark/sql/tests/test_udf.py" line="1217"/>
	<testcase classname="pyspark.sql.tests.test_udf.UDFTests" name="test_same_accumulator_in_udfs" time="0.103" timestamp="2026-01-02T11:21:36" file="python/pyspark/sql/tests/test_udf.py" line="899"/>
	<testcase classname="pyspark.sql.tests.test_udf.UDFTests" name="test_single_udf_with_repeated_argument" time="0.086" timestamp="2026-01-02T11:21:37" file="python/pyspark/sql/tests/test_udf.py" line="215"/>
	<testcase classname="pyspark.sql.tests.test_udf.UDFTests" name="test_timeout_util_with_udf" time="1.007" timestamp="2026-01-02T11:21:38" file="python/pyspark/sql/tests/test_udf.py" line="1293">
		<system-err>ERROR:root:Exception while sending command.
Traceback (most recent call last):
  File "/home/gaogaotiantian/programs/spark/python/lib/py4j-0.10.9.9-src.zip/py4j/clientserver.py", line 535, in send_command
    answer = smart_decode(self.stream.readline()[:-1])
                          ^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.12/socket.py", line 707, in readinto
    return self._sock.recv_into(b)
           ^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/gaogaotiantian/programs/spark/python/pyspark/testing/utils.py", line 166, in handler
    raise TimeoutError(f"Function {func.__name__} timed out after {timeout} seconds")
TimeoutError: Function timeout_func timed out after 1 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/gaogaotiantian/programs/spark/python/lib/py4j-0.10.9.9-src.zip/py4j/java_gateway.py", line 1038, in send_command
    response = connection.send_command(command)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/gaogaotiantian/programs/spark/python/lib/py4j-0.10.9.9-src.zip/py4j/clientserver.py", line 566, in send_command
    raise Py4JNetworkError(
py4j.protocol.Py4JNetworkError: Error while sending or receiving
</system-err>
	</testcase>
	<testcase classname="pyspark.sql.tests.test_udf.UDFTests" name="test_udf" time="0.080" timestamp="2026-01-02T11:21:38" file="python/pyspark/sql/tests/test_udf.py" line="88"/>
	<testcase classname="pyspark.sql.tests.test_udf.UDFTests" name="test_udf2" time="0.234" timestamp="2026-01-02T11:21:38" file="python/pyspark/sql/tests/test_udf.py" line="104"/>
	<testcase classname="pyspark.sql.tests.test_udf.UDFTests" name="test_udf3" time="0.081" timestamp="2026-01-02T11:21:38" file="python/pyspark/sql/tests/test_udf.py" line="111"/>
	<testcase classname="pyspark.sql.tests.test_udf.UDFTests" name="test_udf_and_common_filter_in_join_condition" time="0.377" timestamp="2026-01-02T11:21:38" file="python/pyspark/sql/tests/test_udf.py" line="281"/>
	<testcase classname="pyspark.sql.tests.test_udf.UDFTests" name="test_udf_as_join_condition" time="0.261" timestamp="2026-01-02T11:21:39" file="python/pyspark/sql/tests/test_udf.py" line="311"/>
	<testcase classname="pyspark.sql.tests.test_udf.UDFTests" name="test_udf_binary_type" time="0.517" timestamp="2026-01-02T11:21:39" file="python/pyspark/sql/tests/test_udf.py" line="1479"/>
	<testcase classname="pyspark.sql.tests.test_udf.UDFTests" name="test_udf_binary_type_in_nested_structures" time="1.449" timestamp="2026-01-02T11:21:41" file="python/pyspark/sql/tests/test_udf.py" line="1496">
		<!--Test that binary type in arrays, maps, and structs respects binaryAsBytes config-->
	</testcase>
	<testcase classname="pyspark.sql.tests.test_udf.UDFTests" name="test_udf_cache" time="0.032" timestamp="2026-01-02T11:21:41" file="python/pyspark/sql/tests/test_udf.py" line="965"/>
	<testcase classname="pyspark.sql.tests.test_udf.UDFTests" name="test_udf_daytime_interval" time="0.316" timestamp="2026-01-02T11:21:41" file="python/pyspark/sql/tests/test_udf.py" line="808"/>
	<testcase classname="pyspark.sql.tests.test_udf.UDFTests" name="test_udf_defers_judf_initialization" time="0.006" timestamp="2026-01-02T11:21:41" file="python/pyspark/sql/tests/test_udf.py" line="666"/>
	<testcase classname="pyspark.sql.tests.test_udf.UDFTests" name="test_udf_empty_frame" time="0.131" timestamp="2026-01-02T11:21:41" file="python/pyspark/sql/tests/test_udf.py" line="1402"/>
	<testcase classname="pyspark.sql.tests.test_udf.UDFTests" name="test_udf_globals_not_overwritten" time="0.124" timestamp="2026-01-02T11:21:41" file="python/pyspark/sql/tests/test_udf.py" line="932"/>
	<testcase classname="pyspark.sql.tests.test_udf.UDFTests" name="test_udf_in_filter_on_top_of_join" time="0.676" timestamp="2026-01-02T11:21:42" file="python/pyspark/sql/tests/test_udf.py" line="245"/>
	<testcase classname="pyspark.sql.tests.test_udf.UDFTests" name="test_udf_in_filter_on_top_of_outer_join" time="0.383" timestamp="2026-01-02T11:21:42" file="python/pyspark/sql/tests/test_udf.py" line="238"/>
	<testcase classname="pyspark.sql.tests.test_udf.UDFTests" name="test_udf_in_generate" time="0.437" timestamp="2026-01-02T11:21:43" file="python/pyspark/sql/tests/test_udf.py" line="498"/>
	<testcase classname="pyspark.sql.tests.test_udf.UDFTests" name="test_udf_in_join_condition" time="0.710" timestamp="2026-01-02T11:21:43" file="python/pyspark/sql/tests/test_udf.py" line="253"/>
	<testcase classname="pyspark.sql.tests.test_udf.UDFTests" name="test_udf_in_left_outer_join_condition" time="0.246" timestamp="2026-01-02T11:21:44" file="python/pyspark/sql/tests/test_udf.py" line="268"/>
	<testcase classname="pyspark.sql.tests.test_udf.UDFTests" name="test_udf_in_subquery" time="0.236" timestamp="2026-01-02T11:21:44" file="python/pyspark/sql/tests/test_udf.py" line="923"/>
	<testcase classname="pyspark.sql.tests.test_udf.UDFTests" name="test_udf_input_serialization_valuecompare_disabled" time="0.162" timestamp="2026-01-02T11:21:44" file="python/pyspark/sql/tests/test_udf.py" line="984"/>
	<testcase classname="pyspark.sql.tests.test_udf.UDFTests" name="test_udf_kill_on_timeout" time="1.220" timestamp="2026-01-02T11:21:45" file="python/pyspark/sql/tests/test_udf.py" line="1242"/>
	<testcase classname="pyspark.sql.tests.test_udf.UDFTests" name="test_udf_not_supported_in_join_condition" time="0.357" timestamp="2026-01-02T11:21:46" file="python/pyspark/sql/tests/test_udf.py" line="291"/>
	<testcase classname="pyspark.sql.tests.test_udf.UDFTests" name="test_udf_on_sql_context" time="0.087" timestamp="2026-01-02T11:21:46" file="python/pyspark/sql/tests/test_udf.py" line="94">
		<system-err>/home/gaogaotiantian/programs/spark/python/pyspark/sql/context.py:294: FutureWarning: Deprecated in 2.3.0. Use spark.udf.register instead.
  warnings.warn("Deprecated in 2.3.0. Use spark.udf.register instead.", FutureWarning)
</system-err>
	</testcase>
	<testcase classname="pyspark.sql.tests.test_udf.UDFTests" name="test_udf_registration_return_type_none" time="0.081" timestamp="2026-01-02T11:21:46" file="python/pyspark/sql/tests/test_udf.py" line="120"/>
	<testcase classname="pyspark.sql.tests.test_udf.UDFTests" name="test_udf_registration_return_type_not_none" time="0.004" timestamp="2026-01-02T11:21:46" file="python/pyspark/sql/tests/test_udf.py" line="129"/>
	<testcase classname="pyspark.sql.tests.test_udf.UDFTests" name="test_udf_registration_returns_udf" time="0.516" timestamp="2026-01-02T11:21:46" file="python/pyspark/sql/tests/test_udf.py" line="532"/>
	<testcase classname="pyspark.sql.tests.test_udf.UDFTests" name="test_udf_registration_returns_udf_on_sql_context" time="0.256" timestamp="2026-01-02T11:21:47" file="python/pyspark/sql/tests/test_udf.py" line="549"/>
	<testcase classname="pyspark.sql.tests.test_udf.UDFTests" name="test_udf_should_not_accept_noncallable_object" time="0.001" timestamp="2026-01-02T11:21:47" file="python/pyspark/sql/tests/test_udf.py" line="699"/>
	<testcase classname="pyspark.sql.tests.test_udf.UDFTests" name="test_udf_timestamp_ntz" time="0.279" timestamp="2026-01-02T11:21:47" file="python/pyspark/sql/tests/test_udf.py" line="792"/>
	<testcase classname="pyspark.sql.tests.test_udf.UDFTests" name="test_udf_with_256_args" time="0.314" timestamp="2026-01-02T11:21:47" file="python/pyspark/sql/tests/test_udf.py" line="952"/>
	<testcase classname="pyspark.sql.tests.test_udf.UDFTests" name="test_udf_with_aggregate_function" time="0.468" timestamp="2026-01-02T11:21:48" file="python/pyspark/sql/tests/test_udf.py" line="480"/>
	<testcase classname="pyspark.sql.tests.test_udf.UDFTests" name="test_udf_with_array_type" time="0.130" timestamp="2026-01-02T11:21:48" file="python/pyspark/sql/tests/test_udf.py" line="325"/>
	<testcase classname="pyspark.sql.tests.test_udf.UDFTests" name="test_udf_with_callable" time="0.126" timestamp="2026-01-02T11:21:48" file="python/pyspark/sql/tests/test_udf.py" line="63"/>
	<testcase classname="pyspark.sql.tests.test_udf.UDFTests" name="test_udf_with_char_varchar_return_type" time="0.145" timestamp="2026-01-02T11:21:48" file="python/pyspark/sql/tests/test_udf.py" line="1435"/>
	<testcase classname="pyspark.sql.tests.test_udf.UDFTests" name="test_udf_with_collated_string_types" time="0.386" timestamp="2026-01-02T11:21:48" file="python/pyspark/sql/tests/test_udf.py" line="1412"/>
	<testcase classname="pyspark.sql.tests.test_udf.UDFTests" name="test_udf_with_column_vector" time="0.418" timestamp="2026-01-02T11:21:49" file="python/pyspark/sql/tests/test_udf.py" line="997"/>
	<testcase classname="pyspark.sql.tests.test_udf.UDFTests" name="test_udf_with_complex_variant_input" time="0.260" timestamp="2026-01-02T11:21:49" file="python/pyspark/sql/tests/test_udf.py" line="365"/>
	<testcase classname="pyspark.sql.tests.test_udf.UDFTests" name="test_udf_with_complex_variant_output" time="0.267" timestamp="2026-01-02T11:21:49" file="python/pyspark/sql/tests/test_udf.py" line="400"/>
	<testcase classname="pyspark.sql.tests.test_udf.UDFTests" name="test_udf_with_decorator" time="0.178" timestamp="2026-01-02T11:21:50" file="python/pyspark/sql/tests/test_udf.py" line="703"/>
	<testcase classname="pyspark.sql.tests.test_udf.UDFTests" name="test_udf_with_filter_function" time="0.107" timestamp="2026-01-02T11:21:50" file="python/pyspark/sql/tests/test_udf.py" line="351"/>
	<testcase classname="pyspark.sql.tests.test_udf.UDFTests" name="test_udf_with_input_file_name" time="0.149" timestamp="2026-01-02T11:21:50" file="python/pyspark/sql/tests/test_udf.py" line="634"/>
	<testcase classname="pyspark.sql.tests.test_udf.UDFTests" name="test_udf_with_input_file_name_for_hadooprdd" time="0.416" timestamp="2026-01-02T11:21:50" file="python/pyspark/sql/tests/test_udf.py" line="642"/>
	<testcase classname="pyspark.sql.tests.test_udf.UDFTests" name="test_udf_with_logging" time="0.388" timestamp="2026-01-02T11:21:51" file="python/pyspark/sql/tests/test_udf.py" line="1562"/>
	<testcase classname="pyspark.sql.tests.test_udf.UDFTests" name="test_udf_with_order_by_and_limit" time="0.090" timestamp="2026-01-02T11:21:51" file="python/pyspark/sql/tests/test_udf.py" line="526"/>
	<testcase classname="pyspark.sql.tests.test_udf.UDFTests" name="test_udf_with_partial_function" time="0.132" timestamp="2026-01-02T11:21:51" file="python/pyspark/sql/tests/test_udf.py" line="76"/>
	<testcase classname="pyspark.sql.tests.test_udf.UDFTests" name="test_udf_with_pyspark_logger" time="0.110" timestamp="2026-01-02T11:21:51" file="python/pyspark/sql/tests/test_udf.py" line="1677"/>
	<testcase classname="pyspark.sql.tests.test_udf.UDFTests" name="test_udf_with_rand" time="0.087" timestamp="2026-01-02T11:21:51" file="python/pyspark/sql/tests/test_udf.py" line="1017"/>
	<testcase classname="pyspark.sql.tests.test_udf.UDFTests" name="test_udf_with_string_return_type" time="0.117" timestamp="2026-01-02T11:21:51" file="python/pyspark/sql/tests/test_udf.py" line="682"/>
	<testcase classname="pyspark.sql.tests.test_udf.UDFTests" name="test_udf_with_udt" time="1.372" timestamp="2026-01-02T11:21:52" file="python/pyspark/sql/tests/test_udf.py" line="1308"/>
	<testcase classname="pyspark.sql.tests.test_udf.UDFTests" name="test_udf_with_variant_input" time="0.091" timestamp="2026-01-02T11:21:53" file="python/pyspark/sql/tests/test_udf.py" line="358"/>
	<testcase classname="pyspark.sql.tests.test_udf.UDFTests" name="test_udf_with_variant_output" time="0.097" timestamp="2026-01-02T11:21:53" file="python/pyspark/sql/tests/test_udf.py" line="389"/>
	<testcase classname="pyspark.sql.tests.test_udf.UDFTests" name="test_udf_with_various_batch_size" time="3.017" timestamp="2026-01-02T11:21:56" file="python/pyspark/sql/tests/test_udf.py" line="1716"/>
	<testcase classname="pyspark.sql.tests.test_udf.UDFTests" name="test_udf_with_various_buffer_size" time="1.752" timestamp="2026-01-02T11:21:57" file="python/pyspark/sql/tests/test_udf.py" line="1734"/>
	<testcase classname="pyspark.sql.tests.test_udf.UDFTests" name="test_udf_without_arguments" time="0.079" timestamp="2026-01-02T11:21:58" file="python/pyspark/sql/tests/test_udf.py" line="319"/>
	<testcase classname="pyspark.sql.tests.test_udf.UDFTests" name="test_udf_wrapper" time="0.002" timestamp="2026-01-02T11:21:58" file="python/pyspark/sql/tests/test_udf.py" line="758"/>
	<testcase classname="pyspark.sql.tests.test_udf.UDFTests" name="test_worker_original_stdin_closed" time="0.061" timestamp="2026-01-02T11:21:58" file="python/pyspark/sql/tests/test_udf.py" line="939"/>
	<testcase classname="pyspark.sql.tests.test_udf.UDFTests" name="test_datasource_with_udf" time="0.000" timestamp="2026-01-02T11:21:28" file="python/pyspark/sql/tests/test_udf.py" line="834">
		<skipped type="skip" message="[TEST_CLASS_NOT_COMPILED] /home/gaogaotiantian/programs/spark/sql/core/target/*/test-classes doesn't exist. Spark sql test classes are not compiled."/>
	</testcase>
	<testcase classname="pyspark.sql.tests.test_udf.UDFTests" name="test_register_java_function" time="0.000" timestamp="2026-01-02T11:21:36" file="python/pyspark/sql/tests/test_udf.py" line="564">
		<skipped type="skip" message="[TEST_CLASS_NOT_COMPILED] /home/gaogaotiantian/programs/spark/sql/core/target/*/test-classes doesn't exist. Spark sql test classes are not compiled."/>
	</testcase>
	<testcase classname="pyspark.sql.tests.test_udf.UDFTests" name="test_register_java_udaf" time="0.000" timestamp="2026-01-02T11:21:36" file="python/pyspark/sql/tests/test_udf.py" line="585">
		<skipped type="skip" message="[TEST_CLASS_NOT_COMPILED] /home/gaogaotiantian/programs/spark/sql/core/target/*/test-classes doesn't exist. Spark sql test classes are not compiled."/>
	</testcase>
</testsuite><testsuite name="pyspark.sql.tests.test_udf.UDFTests-20260102125329" tests="89" file="pyspark/sql/tests/test_udf.py" time="32.177" timestamp="2026-01-02T12:54:05" failures="0" errors="0" skipped="3">
	<testcase classname="pyspark.sql.tests.test_udf.UDFTests" name="test_assert_classic_mode" time="0.001" timestamp="2026-01-02T12:53:32" file="python/pyspark/sql/tests/test_udf.py" line="324"/>
	<testcase classname="pyspark.sql.tests.test_udf.UDFTests" name="test_broadcast_in_udf" time="1.610" timestamp="2026-01-02T12:53:34" file="python/pyspark/sql/tests/test_udf.py" line="341">
		<system-err>/home/gaogaotiantian/programs/spark/python/pyspark/sql/catalog.py:957: FutureWarning: Deprecated in 2.3.0. Use spark.udf.register instead.
  warnings.warn("Deprecated in 2.3.0. Use spark.udf.register instead.", FutureWarning)
</system-err>
	</testcase>
	<testcase classname="pyspark.sql.tests.test_udf.UDFTests" name="test_chained_udf" time="0.382" timestamp="2026-01-02T12:53:34" file="python/pyspark/sql/tests/test_udf.py" line="205"/>
	<testcase classname="pyspark.sql.tests.test_udf.UDFTests" name="test_chained_udfs_with_variant" time="0.557" timestamp="2026-01-02T12:53:35" file="python/pyspark/sql/tests/test_udf.py" line="432"/>
	<testcase classname="pyspark.sql.tests.test_udf.UDFTests" name="test_complex_return_types" time="0.237" timestamp="2026-01-02T12:53:35" file="python/pyspark/sql/tests/test_udf.py" line="1070"/>
	<testcase classname="pyspark.sql.tests.test_udf.UDFTests" name="test_err_udf_init" time="0.014" timestamp="2026-01-02T12:53:35" file="python/pyspark/sql/tests/test_udf.py" line="1261"/>
	<testcase classname="pyspark.sql.tests.test_udf.UDFTests" name="test_err_udf_registration" time="0.012" timestamp="2026-01-02T12:53:35" file="python/pyspark/sql/tests/test_udf.py" line="596"/>
	<testcase classname="pyspark.sql.tests.test_udf.UDFTests" name="test_file_dsv2_with_udf_filter" time="0.976" timestamp="2026-01-02T12:53:36" file="python/pyspark/sql/tests/test_udf.py" line="881"/>
	<testcase classname="pyspark.sql.tests.test_udf.UDFTests" name="test_kwargs" time="0.709" timestamp="2026-01-02T12:53:37" file="python/pyspark/sql/tests/test_udf.py" line="1134">
		<system-err>{"ts": "2026-01-02 12:53:37.246", "level": "ERROR", "logger": "SQLQueryContextLogger", "msg": "[DUPLICATE_ROUTINE_PARAMETER_ASSIGNMENT.DOUBLE_NAMED_ARGUMENT_REFERENCE] Call to routine `test_udf` is invalid because it includes multiple argument assignments to the same parameter name `a`. More than one named argument referred to the same parameter. Please assign a value only once. SQLSTATE: 4274K", "context": {"errorClass": "DUPLICATE_ROUTINE_PARAMETER_ASSIGNMENT.DOUBLE_NAMED_ARGUMENT_REFERENCE"}, "exception": {"class": "Py4JJavaError", "msg": "An error occurred while calling o85.sql.\n: org.apache.spark.sql.AnalysisException: [DUPLICATE_ROUTINE_PARAMETER_ASSIGNMENT.DOUBLE_NAMED_ARGUMENT_REFERENCE] Call to routine `test_udf` is invalid because it includes multiple argument assignments to the same parameter name `a`. More than one named argument referred to the same parameter. Please assign a value only once. SQLSTATE: 4274K; line 1 pos 7\n\tat org.apache.spark.sql.errors.QueryCompilationErrors$.doubleNamedArgumentReference(QueryCompilationErrors.scala:90)\n\tat org.apache.spark.sql.catalyst.plans.logical.NamedParametersSupport$.$anonfun$splitAndCheckNamedArguments$2(FunctionBuilderBase.scala:108)\n\tat scala.collection.immutable.Vector1.map(Vector.scala:2155)\n\tat scala.collection.immutable.Vector1.map(Vector.scala:386)\n\tat org.apache.spark.sql.catalyst.plans.logical.NamedParametersSupport$.splitAndCheckNamedArguments(FunctionBuilderBase.scala:104)\n\tat org.apache.spark.sql.execution.python.UserDefinedPythonFunction.builder(UserDefinedPythonFunction.scala:62)\n\tat org.apache.spark.sql.classic.UDFRegistration.$anonfun$registerPython$1(UDFRegistration.scala:64)\n\tat org.apache.spark.sql.catalyst.analysis.SimpleFunctionRegistryBase.lookupFunction(FunctionRegistry.scala:246)\n\tat org.apache.spark.sql.catalyst.analysis.SimpleFunctionRegistryBase.lookupFunction$(FunctionRegistry.scala:240)\n\tat org.apache.spark.sql.catalyst.analysis.SimpleFunctionRegistry.lookupFunction(FunctionRegistry.scala:305)\n\tat org.apache.spark.sql.catalyst.catalog.SessionCatalog.$anonfun$resolveBuiltinOrTempFunctionInternal$1(SessionCatalog.scala:2121)\n\tat org.apache.spark.sql.catalyst.catalog.SessionCatalog.lookupTempFuncWithViewContext(SessionCatalog.scala:2143)\n\tat org.apache.spark.sql.catalyst.catalog.SessionCatalog.resolveBuiltinOrTempFunctionInternal(SessionCatalog.scala:2121)\n\tat org.apache.spark.sql.catalyst.catalog.SessionCatalog.resolveBuiltinOrTempFunction(SessionCatalog.scala:2098)\n\tat org.apache.spark.sql.catalyst.analysis.FunctionResolution.resolveBuiltinOrTempFunction(FunctionResolution.scala:102)\n\tat org.apache.spark.sql.catalyst.analysis.FunctionResolution.$anonfun$resolveFunction$1(FunctionResolution.scala:51)\n\tat org.apache.spark.sql.catalyst.analysis.package$.withPosition(package.scala:104)\n\tat org.apache.spark.sql.catalyst.analysis.FunctionResolution.resolveFunction(FunctionResolution.scala:51)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveFunctions$$anonfun$apply$19$$anonfun$applyOrElse$141.applyOrElse(Analyzer.scala:2256)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveFunctions$$anonfun$apply$19$$anonfun$applyOrElse$141.applyOrElse(Analyzer.scala:2226)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformUpWithPruning$2(TreeNode.scala:549)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:107)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformUpWithPruning(TreeNode.scala:549)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformUpWithPruning$1(TreeNode.scala:546)\n\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1268)\n\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1267)\n\tat org.apache.spark.sql.catalyst.expressions.UnaryExpression.mapChildren(Expression.scala:587)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformUpWithPruning(TreeNode.scala:546)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.$anonfun$transformExpressionsUpWithPruning$1(QueryPlan.scala:245)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.$anonfun$mapExpressions$1(QueryPlan.scala:257)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:107)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpression$1(QueryPlan.scala:257)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.recursiveTransform$1(QueryPlan.scala:269)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.$anonfun$mapExpressions$4(QueryPlan.scala:275)\n\tat scala.collection.immutable.List.map(List.scala:236)\n\tat scala.collection.immutable.List.map(List.scala:79)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.recursiveTransform$1(QueryPlan.scala:275)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.$anonfun$mapExpressions$5(QueryPlan.scala:280)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:337)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.mapExpressions(QueryPlan.scala:280)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpressionsUpWithPruning(QueryPlan.scala:245)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveFunctions$$anonfun$apply$19.applyOrElse(Analyzer.scala:2226)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveFunctions$$anonfun$apply$19.applyOrElse(Analyzer.scala:2096)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$3(AnalysisHelper.scala:139)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:107)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:139)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:416)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:135)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:131)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:37)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveFunctions$.apply(Analyzer.scala:2096)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveFunctions$.apply(Analyzer.scala:2093)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:248)\n\tat scala.collection.LinearSeqOps.foldLeft(LinearSeq.scala:183)\n\tat scala.collection.LinearSeqOps.foldLeft$(LinearSeq.scala:179)\n\tat scala.collection.immutable.List.foldLeft(List.scala:79)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:245)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:237)\n\tat scala.collection.immutable.List.foreach(List.scala:323)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:237)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext(Analyzer.scala:344)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:340)\n\tat org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:224)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:340)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:290)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:207)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:207)\n\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.resolveInFixedPoint(HybridAnalyzer.scala:237)\n\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.$anonfun$apply$1(HybridAnalyzer.scala:92)\n\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.withTrackedAnalyzerBridgeState(HybridAnalyzer.scala:123)\n\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.apply(HybridAnalyzer.scala:85)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:323)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:423)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:323)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$2(QueryExecution.scala:139)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:148)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:325)\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:712)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:325)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:810)\n\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:324)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$1(QueryExecution.scala:139)\n\tat scala.util.Try$.apply(Try.scala:217)\n\tat org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1392)\n\tat org.apache.spark.util.Utils$.getTryWithCallerStacktrace(Utils.scala:1453)\n\tat org.apache.spark.util.LazyTry.get(LazyTry.scala:58)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:150)\n\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:90)\n\tat org.apache.spark.sql.classic.Dataset$.$anonfun$ofRows$5(Dataset.scala:138)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:810)\n\tat org.apache.spark.sql.classic.Dataset$.ofRows(Dataset.scala:135)\n\tat org.apache.spark.sql.classic.SparkSession.$anonfun$sql$1(SparkSession.scala:534)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:810)\n\tat org.apache.spark.sql.classic.SparkSession.sql(SparkSession.scala:504)\n\tat org.apache.spark.sql.classic.SparkSession.sql(SparkSession.scala:539)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:75)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:52)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:580)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:184)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:108)\n\tat java.base/java.lang.Thread.run(Thread.java:1583)\n\tSuppressed: org.apache.spark.util.Utils$OriginalTryStackTraceException: Full stacktrace of original doTryWithCallerStacktrace caller\n\t\tat org.apache.spark.sql.errors.QueryCompilationErrors$.doubleNamedArgumentReference(QueryCompilationErrors.scala:90)\n\t\tat org.apache.spark.sql.catalyst.plans.logical.NamedParametersSupport$.$anonfun$splitAndCheckNamedArguments$2(FunctionBuilderBase.scala:108)\n\t\tat scala.collection.immutable.Vector1.map(Vector.scala:2155)\n\t\tat scala.collection.immutable.Vector1.map(Vector.scala:386)\n\t\tat org.apache.spark.sql.catalyst.plans.logical.NamedParametersSupport$.splitAndCheckNamedArguments(FunctionBuilderBase.scala:104)\n\t\tat org.apache.spark.sql.execution.python.UserDefinedPythonFunction.builder(UserDefinedPythonFunction.scala:62)\n\t\tat org.apache.spark.sql.classic.UDFRegistration.$anonfun$registerPython$1(UDFRegistration.scala:64)\n\t\tat org.apache.spark.sql.catalyst.analysis.SimpleFunctionRegistryBase.lookupFunction(FunctionRegistry.scala:246)\n\t\tat org.apache.spark.sql.catalyst.analysis.SimpleFunctionRegistryBase.lookupFunction$(FunctionRegistry.scala:240)\n\t\tat org.apache.spark.sql.catalyst.analysis.SimpleFunctionRegistry.lookupFunction(FunctionRegistry.scala:305)\n\t\tat org.apache.spark.sql.catalyst.catalog.SessionCatalog.$anonfun$resolveBuiltinOrTempFunctionInternal$1(SessionCatalog.scala:2121)\n\t\tat org.apache.spark.sql.catalyst.catalog.SessionCatalog.lookupTempFuncWithViewContext(SessionCatalog.scala:2143)\n\t\tat org.apache.spark.sql.catalyst.catalog.SessionCatalog.resolveBuiltinOrTempFunctionInternal(SessionCatalog.scala:2121)\n\t\tat org.apache.spark.sql.catalyst.catalog.SessionCatalog.resolveBuiltinOrTempFunction(SessionCatalog.scala:2098)\n\t\tat org.apache.spark.sql.catalyst.analysis.FunctionResolution.resolveBuiltinOrTempFunction(FunctionResolution.scala:102)\n\t\tat org.apache.spark.sql.catalyst.analysis.FunctionResolution.$anonfun$resolveFunction$1(FunctionResolution.scala:51)\n\t\tat org.apache.spark.sql.catalyst.analysis.package$.withPosition(package.scala:104)\n\t\tat org.apache.spark.sql.catalyst.analysis.FunctionResolution.resolveFunction(FunctionResolution.scala:51)\n\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveFunctions$$anonfun$apply$19$$anonfun$applyOrElse$141.applyOrElse(Analyzer.scala:2256)\n\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveFunctions$$anonfun$apply$19$$anonfun$applyOrElse$141.applyOrElse(Analyzer.scala:2226)\n\t\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformUpWithPruning$2(TreeNode.scala:549)\n\t\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:107)\n\t\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformUpWithPruning(TreeNode.scala:549)\n\t\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformUpWithPruning$1(TreeNode.scala:546)\n\t\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1268)\n\t\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1267)\n\t\tat org.apache.spark.sql.catalyst.expressions.UnaryExpression.mapChildren(Expression.scala:587)\n\t\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformUpWithPruning(TreeNode.scala:546)\n\t\tat org.apache.spark.sql.catalyst.plans.QueryPlan.$anonfun$transformExpressionsUpWithPruning$1(QueryPlan.scala:245)\n\t\tat org.apache.spark.sql.catalyst.plans.QueryPlan.$anonfun$mapExpressions$1(QueryPlan.scala:257)\n\t\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:107)\n\t\tat org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpression$1(QueryPlan.scala:257)\n\t\tat org.apache.spark.sql.catalyst.plans.QueryPlan.recursiveTransform$1(QueryPlan.scala:269)\n\t\tat org.apache.spark.sql.catalyst.plans.QueryPlan.$anonfun$mapExpressions$4(QueryPlan.scala:275)\n\t\tat scala.collection.immutable.List.map(List.scala:236)\n\t\tat scala.collection.immutable.List.map(List.scala:79)\n\t\tat org.apache.spark.sql.catalyst.plans.QueryPlan.recursiveTransform$1(QueryPlan.scala:275)\n\t\tat org.apache.spark.sql.catalyst.plans.QueryPlan.$anonfun$mapExpressions$5(QueryPlan.scala:280)\n\t\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:337)\n\t\tat org.apache.spark.sql.catalyst.plans.QueryPlan.mapExpressions(QueryPlan.scala:280)\n\t\tat org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpressionsUpWithPruning(QueryPlan.scala:245)\n\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveFunctions$$anonfun$apply$19.applyOrElse(Analyzer.scala:2226)\n\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveFunctions$$anonfun$apply$19.applyOrElse(Analyzer.scala:2096)\n\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$3(AnalysisHelper.scala:139)\n\t\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:107)\n\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:139)\n\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:416)\n\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:135)\n\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:131)\n\t\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:37)\n\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveFunctions$.apply(Analyzer.scala:2096)\n\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveFunctions$.apply(Analyzer.scala:2093)\n\t\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:248)\n\t\tat scala.collection.LinearSeqOps.foldLeft(LinearSeq.scala:183)\n\t\tat scala.collection.LinearSeqOps.foldLeft$(LinearSeq.scala:179)\n\t\tat scala.collection.immutable.List.foldLeft(List.scala:79)\n\t\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:245)\n\t\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:237)\n\t\tat scala.collection.immutable.List.foreach(List.scala:323)\n\t\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:237)\n\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext(Analyzer.scala:344)\n\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:340)\n\t\tat org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:224)\n\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:340)\n\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:290)\n\t\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:207)\n\t\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)\n\t\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:207)\n\t\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.resolveInFixedPoint(HybridAnalyzer.scala:237)\n\t\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.$anonfun$apply$1(HybridAnalyzer.scala:92)\n\t\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.withTrackedAnalyzerBridgeState(HybridAnalyzer.scala:123)\n\t\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.apply(HybridAnalyzer.scala:85)\n\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:323)\n\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:423)\n\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:323)\n\t\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$2(QueryExecution.scala:139)\n\t\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:148)\n\t\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:325)\n\t\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:712)\n\t\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:325)\n\t\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:810)\n\t\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:324)\n\t\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$1(QueryExecution.scala:139)\n\t\tat scala.util.Try$.apply(Try.scala:217)\n\t\tat org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1392)\n\t\tat org.apache.spark.util.LazyTry.tryT$lzycompute(LazyTry.scala:46)\n\t\tat org.apache.spark.util.LazyTry.tryT(LazyTry.scala:46)\n\t\t... 22 more\n", "stacktrace": [{"class": null, "method": "deco", "file": "/home/gaogaotiantian/programs/spark/python/pyspark/errors/exceptions/captured.py", "line": "263"}, {"class": null, "method": "get_return_value", "file": "/home/gaogaotiantian/programs/spark/python/lib/py4j-0.10.9.9-src.zip/py4j/protocol.py", "line": "327"}]}}
{"ts": "2026-01-02 12:53:37.361", "level": "ERROR", "logger": "SQLQueryContextLogger", "msg": "[UNEXPECTED_POSITIONAL_ARGUMENT] Cannot invoke routine `test_udf` because it contains positional argument(s) following the named argument assigned to `a`; please rearrange them so the positional arguments come first and then retry the query again. SQLSTATE: 4274K", "context": {"errorClass": "UNEXPECTED_POSITIONAL_ARGUMENT"}, "exception": {"class": "Py4JJavaError", "msg": "An error occurred while calling o85.sql.\n: org.apache.spark.sql.AnalysisException: [UNEXPECTED_POSITIONAL_ARGUMENT] Cannot invoke routine `test_udf` because it contains positional argument(s) following the named argument assigned to `a`; please rearrange them so the positional arguments come first and then retry the query again. SQLSTATE: 4274K; line 1 pos 7\n\tat org.apache.spark.sql.errors.QueryCompilationErrors$.unexpectedPositionalArgument(QueryCompilationErrors.scala:128)\n\tat org.apache.spark.sql.catalyst.plans.logical.NamedParametersSupport$.$anonfun$splitAndCheckNamedArguments$2(FunctionBuilderBase.scala:114)\n\tat scala.collection.immutable.Vector1.map(Vector.scala:2155)\n\tat scala.collection.immutable.Vector1.map(Vector.scala:386)\n\tat org.apache.spark.sql.catalyst.plans.logical.NamedParametersSupport$.splitAndCheckNamedArguments(FunctionBuilderBase.scala:104)\n\tat org.apache.spark.sql.execution.python.UserDefinedPythonFunction.builder(UserDefinedPythonFunction.scala:62)\n\tat org.apache.spark.sql.classic.UDFRegistration.$anonfun$registerPython$1(UDFRegistration.scala:64)\n\tat org.apache.spark.sql.catalyst.analysis.SimpleFunctionRegistryBase.lookupFunction(FunctionRegistry.scala:246)\n\tat org.apache.spark.sql.catalyst.analysis.SimpleFunctionRegistryBase.lookupFunction$(FunctionRegistry.scala:240)\n\tat org.apache.spark.sql.catalyst.analysis.SimpleFunctionRegistry.lookupFunction(FunctionRegistry.scala:305)\n\tat org.apache.spark.sql.catalyst.catalog.SessionCatalog.$anonfun$resolveBuiltinOrTempFunctionInternal$1(SessionCatalog.scala:2121)\n\tat org.apache.spark.sql.catalyst.catalog.SessionCatalog.lookupTempFuncWithViewContext(SessionCatalog.scala:2143)\n\tat org.apache.spark.sql.catalyst.catalog.SessionCatalog.resolveBuiltinOrTempFunctionInternal(SessionCatalog.scala:2121)\n\tat org.apache.spark.sql.catalyst.catalog.SessionCatalog.resolveBuiltinOrTempFunction(SessionCatalog.scala:2098)\n\tat org.apache.spark.sql.catalyst.analysis.FunctionResolution.resolveBuiltinOrTempFunction(FunctionResolution.scala:102)\n\tat org.apache.spark.sql.catalyst.analysis.FunctionResolution.$anonfun$resolveFunction$1(FunctionResolution.scala:51)\n\tat org.apache.spark.sql.catalyst.analysis.package$.withPosition(package.scala:104)\n\tat org.apache.spark.sql.catalyst.analysis.FunctionResolution.resolveFunction(FunctionResolution.scala:51)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveFunctions$$anonfun$apply$19$$anonfun$applyOrElse$141.applyOrElse(Analyzer.scala:2256)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveFunctions$$anonfun$apply$19$$anonfun$applyOrElse$141.applyOrElse(Analyzer.scala:2226)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformUpWithPruning$2(TreeNode.scala:549)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:107)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformUpWithPruning(TreeNode.scala:549)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformUpWithPruning$1(TreeNode.scala:546)\n\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1268)\n\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1267)\n\tat org.apache.spark.sql.catalyst.expressions.UnaryExpression.mapChildren(Expression.scala:587)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformUpWithPruning(TreeNode.scala:546)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.$anonfun$transformExpressionsUpWithPruning$1(QueryPlan.scala:245)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.$anonfun$mapExpressions$1(QueryPlan.scala:257)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:107)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpression$1(QueryPlan.scala:257)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.recursiveTransform$1(QueryPlan.scala:269)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.$anonfun$mapExpressions$4(QueryPlan.scala:275)\n\tat scala.collection.immutable.List.map(List.scala:236)\n\tat scala.collection.immutable.List.map(List.scala:79)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.recursiveTransform$1(QueryPlan.scala:275)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.$anonfun$mapExpressions$5(QueryPlan.scala:280)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:337)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.mapExpressions(QueryPlan.scala:280)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpressionsUpWithPruning(QueryPlan.scala:245)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveFunctions$$anonfun$apply$19.applyOrElse(Analyzer.scala:2226)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveFunctions$$anonfun$apply$19.applyOrElse(Analyzer.scala:2096)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$3(AnalysisHelper.scala:139)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:107)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:139)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:416)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:135)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:131)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:37)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveFunctions$.apply(Analyzer.scala:2096)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveFunctions$.apply(Analyzer.scala:2093)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:248)\n\tat scala.collection.LinearSeqOps.foldLeft(LinearSeq.scala:183)\n\tat scala.collection.LinearSeqOps.foldLeft$(LinearSeq.scala:179)\n\tat scala.collection.immutable.List.foldLeft(List.scala:79)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:245)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:237)\n\tat scala.collection.immutable.List.foreach(List.scala:323)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:237)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext(Analyzer.scala:344)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:340)\n\tat org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:224)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:340)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:290)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:207)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:207)\n\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.resolveInFixedPoint(HybridAnalyzer.scala:237)\n\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.$anonfun$apply$1(HybridAnalyzer.scala:92)\n\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.withTrackedAnalyzerBridgeState(HybridAnalyzer.scala:123)\n\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.apply(HybridAnalyzer.scala:85)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:323)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:423)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:323)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$2(QueryExecution.scala:139)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:148)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:325)\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:712)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:325)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:810)\n\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:324)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$1(QueryExecution.scala:139)\n\tat scala.util.Try$.apply(Try.scala:217)\n\tat org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1392)\n\tat org.apache.spark.util.Utils$.getTryWithCallerStacktrace(Utils.scala:1453)\n\tat org.apache.spark.util.LazyTry.get(LazyTry.scala:58)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:150)\n\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:90)\n\tat org.apache.spark.sql.classic.Dataset$.$anonfun$ofRows$5(Dataset.scala:138)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:810)\n\tat org.apache.spark.sql.classic.Dataset$.ofRows(Dataset.scala:135)\n\tat org.apache.spark.sql.classic.SparkSession.$anonfun$sql$1(SparkSession.scala:534)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:810)\n\tat org.apache.spark.sql.classic.SparkSession.sql(SparkSession.scala:504)\n\tat org.apache.spark.sql.classic.SparkSession.sql(SparkSession.scala:539)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:75)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:52)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:580)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:184)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:108)\n\tat java.base/java.lang.Thread.run(Thread.java:1583)\n\tSuppressed: org.apache.spark.util.Utils$OriginalTryStackTraceException: Full stacktrace of original doTryWithCallerStacktrace caller\n\t\tat org.apache.spark.sql.errors.QueryCompilationErrors$.unexpectedPositionalArgument(QueryCompilationErrors.scala:128)\n\t\tat org.apache.spark.sql.catalyst.plans.logical.NamedParametersSupport$.$anonfun$splitAndCheckNamedArguments$2(FunctionBuilderBase.scala:114)\n\t\tat scala.collection.immutable.Vector1.map(Vector.scala:2155)\n\t\tat scala.collection.immutable.Vector1.map(Vector.scala:386)\n\t\tat org.apache.spark.sql.catalyst.plans.logical.NamedParametersSupport$.splitAndCheckNamedArguments(FunctionBuilderBase.scala:104)\n\t\tat org.apache.spark.sql.execution.python.UserDefinedPythonFunction.builder(UserDefinedPythonFunction.scala:62)\n\t\tat org.apache.spark.sql.classic.UDFRegistration.$anonfun$registerPython$1(UDFRegistration.scala:64)\n\t\tat org.apache.spark.sql.catalyst.analysis.SimpleFunctionRegistryBase.lookupFunction(FunctionRegistry.scala:246)\n\t\tat org.apache.spark.sql.catalyst.analysis.SimpleFunctionRegistryBase.lookupFunction$(FunctionRegistry.scala:240)\n\t\tat org.apache.spark.sql.catalyst.analysis.SimpleFunctionRegistry.lookupFunction(FunctionRegistry.scala:305)\n\t\tat org.apache.spark.sql.catalyst.catalog.SessionCatalog.$anonfun$resolveBuiltinOrTempFunctionInternal$1(SessionCatalog.scala:2121)\n\t\tat org.apache.spark.sql.catalyst.catalog.SessionCatalog.lookupTempFuncWithViewContext(SessionCatalog.scala:2143)\n\t\tat org.apache.spark.sql.catalyst.catalog.SessionCatalog.resolveBuiltinOrTempFunctionInternal(SessionCatalog.scala:2121)\n\t\tat org.apache.spark.sql.catalyst.catalog.SessionCatalog.resolveBuiltinOrTempFunction(SessionCatalog.scala:2098)\n\t\tat org.apache.spark.sql.catalyst.analysis.FunctionResolution.resolveBuiltinOrTempFunction(FunctionResolution.scala:102)\n\t\tat org.apache.spark.sql.catalyst.analysis.FunctionResolution.$anonfun$resolveFunction$1(FunctionResolution.scala:51)\n\t\tat org.apache.spark.sql.catalyst.analysis.package$.withPosition(package.scala:104)\n\t\tat org.apache.spark.sql.catalyst.analysis.FunctionResolution.resolveFunction(FunctionResolution.scala:51)\n\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveFunctions$$anonfun$apply$19$$anonfun$applyOrElse$141.applyOrElse(Analyzer.scala:2256)\n\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveFunctions$$anonfun$apply$19$$anonfun$applyOrElse$141.applyOrElse(Analyzer.scala:2226)\n\t\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformUpWithPruning$2(TreeNode.scala:549)\n\t\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:107)\n\t\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformUpWithPruning(TreeNode.scala:549)\n\t\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformUpWithPruning$1(TreeNode.scala:546)\n\t\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1268)\n\t\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1267)\n\t\tat org.apache.spark.sql.catalyst.expressions.UnaryExpression.mapChildren(Expression.scala:587)\n\t\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformUpWithPruning(TreeNode.scala:546)\n\t\tat org.apache.spark.sql.catalyst.plans.QueryPlan.$anonfun$transformExpressionsUpWithPruning$1(QueryPlan.scala:245)\n\t\tat org.apache.spark.sql.catalyst.plans.QueryPlan.$anonfun$mapExpressions$1(QueryPlan.scala:257)\n\t\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:107)\n\t\tat org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpression$1(QueryPlan.scala:257)\n\t\tat org.apache.spark.sql.catalyst.plans.QueryPlan.recursiveTransform$1(QueryPlan.scala:269)\n\t\tat org.apache.spark.sql.catalyst.plans.QueryPlan.$anonfun$mapExpressions$4(QueryPlan.scala:275)\n\t\tat scala.collection.immutable.List.map(List.scala:236)\n\t\tat scala.collection.immutable.List.map(List.scala:79)\n\t\tat org.apache.spark.sql.catalyst.plans.QueryPlan.recursiveTransform$1(QueryPlan.scala:275)\n\t\tat org.apache.spark.sql.catalyst.plans.QueryPlan.$anonfun$mapExpressions$5(QueryPlan.scala:280)\n\t\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:337)\n\t\tat org.apache.spark.sql.catalyst.plans.QueryPlan.mapExpressions(QueryPlan.scala:280)\n\t\tat org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpressionsUpWithPruning(QueryPlan.scala:245)\n\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveFunctions$$anonfun$apply$19.applyOrElse(Analyzer.scala:2226)\n\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveFunctions$$anonfun$apply$19.applyOrElse(Analyzer.scala:2096)\n\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$3(AnalysisHelper.scala:139)\n\t\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:107)\n\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:139)\n\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:416)\n\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:135)\n\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:131)\n\t\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:37)\n\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveFunctions$.apply(Analyzer.scala:2096)\n\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveFunctions$.apply(Analyzer.scala:2093)\n\t\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:248)\n\t\tat scala.collection.LinearSeqOps.foldLeft(LinearSeq.scala:183)\n\t\tat scala.collection.LinearSeqOps.foldLeft$(LinearSeq.scala:179)\n\t\tat scala.collection.immutable.List.foldLeft(List.scala:79)\n\t\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:245)\n\t\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:237)\n\t\tat scala.collection.immutable.List.foreach(List.scala:323)\n\t\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:237)\n\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext(Analyzer.scala:344)\n\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:340)\n\t\tat org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:224)\n\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:340)\n\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:290)\n\t\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:207)\n\t\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)\n\t\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:207)\n\t\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.resolveInFixedPoint(HybridAnalyzer.scala:237)\n\t\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.$anonfun$apply$1(HybridAnalyzer.scala:92)\n\t\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.withTrackedAnalyzerBridgeState(HybridAnalyzer.scala:123)\n\t\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.apply(HybridAnalyzer.scala:85)\n\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:323)\n\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:423)\n\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:323)\n\t\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$2(QueryExecution.scala:139)\n\t\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:148)\n\t\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:325)\n\t\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:712)\n\t\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:325)\n\t\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:810)\n\t\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:324)\n\t\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$1(QueryExecution.scala:139)\n\t\tat scala.util.Try$.apply(Try.scala:217)\n\t\tat org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1392)\n\t\tat org.apache.spark.util.LazyTry.tryT$lzycompute(LazyTry.scala:46)\n\t\tat org.apache.spark.util.LazyTry.tryT(LazyTry.scala:46)\n\t\t... 22 more\n", "stacktrace": [{"class": null, "method": "deco", "file": "/home/gaogaotiantian/programs/spark/python/pyspark/errors/exceptions/captured.py", "line": "263"}, {"class": null, "method": "get_return_value", "file": "/home/gaogaotiantian/programs/spark/python/lib/py4j-0.10.9.9-src.zip/py4j/protocol.py", "line": "327"}]}}
</system-err>
	</testcase>
	<testcase classname="pyspark.sql.tests.test_udf.UDFTests" name="test_multiple_udfs" time="0.278" timestamp="2026-01-02T12:53:37" file="python/pyspark/sql/tests/test_udf.py" line="222"/>
	<testcase classname="pyspark.sql.tests.test_udf.UDFTests" name="test_multiple_udfs_with_logging" time="0.828" timestamp="2026-01-02T12:53:38" file="python/pyspark/sql/tests/test_udf.py" line="1637"/>
	<testcase classname="pyspark.sql.tests.test_udf.UDFTests" name="test_named_arguments" time="0.533" timestamp="2026-01-02T12:53:39" file="python/pyspark/sql/tests/test_udf.py" line="1086"/>
	<testcase classname="pyspark.sql.tests.test_udf.UDFTests" name="test_named_arguments_and_defaults" time="0.841" timestamp="2026-01-02T12:53:39" file="python/pyspark/sql/tests/test_udf.py" line="1163"/>
	<testcase classname="pyspark.sql.tests.test_udf.UDFTests" name="test_named_arguments_negative" time="0.566" timestamp="2026-01-02T12:53:40" file="python/pyspark/sql/tests/test_udf.py" line="1107">
		<system-err>{"ts": "2026-01-02 12:53:39.966", "level": "ERROR", "logger": "SQLQueryContextLogger", "msg": "[DUPLICATE_ROUTINE_PARAMETER_ASSIGNMENT.DOUBLE_NAMED_ARGUMENT_REFERENCE] Call to routine `test_udf` is invalid because it includes multiple argument assignments to the same parameter name `a`. More than one named argument referred to the same parameter. Please assign a value only once. SQLSTATE: 4274K", "context": {"errorClass": "DUPLICATE_ROUTINE_PARAMETER_ASSIGNMENT.DOUBLE_NAMED_ARGUMENT_REFERENCE"}, "exception": {"class": "Py4JJavaError", "msg": "An error occurred while calling o85.sql.\n: org.apache.spark.sql.AnalysisException: [DUPLICATE_ROUTINE_PARAMETER_ASSIGNMENT.DOUBLE_NAMED_ARGUMENT_REFERENCE] Call to routine `test_udf` is invalid because it includes multiple argument assignments to the same parameter name `a`. More than one named argument referred to the same parameter. Please assign a value only once. SQLSTATE: 4274K; line 1 pos 7\n\tat org.apache.spark.sql.errors.QueryCompilationErrors$.doubleNamedArgumentReference(QueryCompilationErrors.scala:90)\n\tat org.apache.spark.sql.catalyst.plans.logical.NamedParametersSupport$.$anonfun$splitAndCheckNamedArguments$2(FunctionBuilderBase.scala:108)\n\tat scala.collection.immutable.Vector1.map(Vector.scala:2155)\n\tat scala.collection.immutable.Vector1.map(Vector.scala:386)\n\tat org.apache.spark.sql.catalyst.plans.logical.NamedParametersSupport$.splitAndCheckNamedArguments(FunctionBuilderBase.scala:104)\n\tat org.apache.spark.sql.execution.python.UserDefinedPythonFunction.builder(UserDefinedPythonFunction.scala:62)\n\tat org.apache.spark.sql.classic.UDFRegistration.$anonfun$registerPython$1(UDFRegistration.scala:64)\n\tat org.apache.spark.sql.catalyst.analysis.SimpleFunctionRegistryBase.lookupFunction(FunctionRegistry.scala:246)\n\tat org.apache.spark.sql.catalyst.analysis.SimpleFunctionRegistryBase.lookupFunction$(FunctionRegistry.scala:240)\n\tat org.apache.spark.sql.catalyst.analysis.SimpleFunctionRegistry.lookupFunction(FunctionRegistry.scala:305)\n\tat org.apache.spark.sql.catalyst.catalog.SessionCatalog.$anonfun$resolveBuiltinOrTempFunctionInternal$1(SessionCatalog.scala:2121)\n\tat org.apache.spark.sql.catalyst.catalog.SessionCatalog.lookupTempFuncWithViewContext(SessionCatalog.scala:2143)\n\tat org.apache.spark.sql.catalyst.catalog.SessionCatalog.resolveBuiltinOrTempFunctionInternal(SessionCatalog.scala:2121)\n\tat org.apache.spark.sql.catalyst.catalog.SessionCatalog.resolveBuiltinOrTempFunction(SessionCatalog.scala:2098)\n\tat org.apache.spark.sql.catalyst.analysis.FunctionResolution.resolveBuiltinOrTempFunction(FunctionResolution.scala:102)\n\tat org.apache.spark.sql.catalyst.analysis.FunctionResolution.$anonfun$resolveFunction$1(FunctionResolution.scala:51)\n\tat org.apache.spark.sql.catalyst.analysis.package$.withPosition(package.scala:104)\n\tat org.apache.spark.sql.catalyst.analysis.FunctionResolution.resolveFunction(FunctionResolution.scala:51)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveFunctions$$anonfun$apply$19$$anonfun$applyOrElse$141.applyOrElse(Analyzer.scala:2256)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveFunctions$$anonfun$apply$19$$anonfun$applyOrElse$141.applyOrElse(Analyzer.scala:2226)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformUpWithPruning$2(TreeNode.scala:549)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:107)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformUpWithPruning(TreeNode.scala:549)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformUpWithPruning$1(TreeNode.scala:546)\n\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1268)\n\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1267)\n\tat org.apache.spark.sql.catalyst.expressions.UnaryExpression.mapChildren(Expression.scala:587)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformUpWithPruning(TreeNode.scala:546)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.$anonfun$transformExpressionsUpWithPruning$1(QueryPlan.scala:245)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.$anonfun$mapExpressions$1(QueryPlan.scala:257)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:107)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpression$1(QueryPlan.scala:257)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.recursiveTransform$1(QueryPlan.scala:269)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.$anonfun$mapExpressions$4(QueryPlan.scala:275)\n\tat scala.collection.immutable.List.map(List.scala:236)\n\tat scala.collection.immutable.List.map(List.scala:79)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.recursiveTransform$1(QueryPlan.scala:275)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.$anonfun$mapExpressions$5(QueryPlan.scala:280)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:337)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.mapExpressions(QueryPlan.scala:280)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpressionsUpWithPruning(QueryPlan.scala:245)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveFunctions$$anonfun$apply$19.applyOrElse(Analyzer.scala:2226)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveFunctions$$anonfun$apply$19.applyOrElse(Analyzer.scala:2096)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$3(AnalysisHelper.scala:139)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:107)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:139)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:416)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:135)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:131)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:37)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveFunctions$.apply(Analyzer.scala:2096)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveFunctions$.apply(Analyzer.scala:2093)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:248)\n\tat scala.collection.LinearSeqOps.foldLeft(LinearSeq.scala:183)\n\tat scala.collection.LinearSeqOps.foldLeft$(LinearSeq.scala:179)\n\tat scala.collection.immutable.List.foldLeft(List.scala:79)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:245)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:237)\n\tat scala.collection.immutable.List.foreach(List.scala:323)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:237)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext(Analyzer.scala:344)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:340)\n\tat org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:224)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:340)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:290)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:207)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:207)\n\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.resolveInFixedPoint(HybridAnalyzer.scala:237)\n\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.$anonfun$apply$1(HybridAnalyzer.scala:92)\n\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.withTrackedAnalyzerBridgeState(HybridAnalyzer.scala:123)\n\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.apply(HybridAnalyzer.scala:85)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:323)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:423)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:323)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$2(QueryExecution.scala:139)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:148)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:325)\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:712)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:325)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:810)\n\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:324)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$1(QueryExecution.scala:139)\n\tat scala.util.Try$.apply(Try.scala:217)\n\tat org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1392)\n\tat org.apache.spark.util.Utils$.getTryWithCallerStacktrace(Utils.scala:1453)\n\tat org.apache.spark.util.LazyTry.get(LazyTry.scala:58)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:150)\n\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:90)\n\tat org.apache.spark.sql.classic.Dataset$.$anonfun$ofRows$5(Dataset.scala:138)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:810)\n\tat org.apache.spark.sql.classic.Dataset$.ofRows(Dataset.scala:135)\n\tat org.apache.spark.sql.classic.SparkSession.$anonfun$sql$1(SparkSession.scala:534)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:810)\n\tat org.apache.spark.sql.classic.SparkSession.sql(SparkSession.scala:504)\n\tat org.apache.spark.sql.classic.SparkSession.sql(SparkSession.scala:539)\n\tat jdk.internal.reflect.GeneratedMethodAccessor77.invoke(Unknown Source)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:52)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:580)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:184)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:108)\n\tat java.base/java.lang.Thread.run(Thread.java:1583)\n\tSuppressed: org.apache.spark.util.Utils$OriginalTryStackTraceException: Full stacktrace of original doTryWithCallerStacktrace caller\n\t\tat org.apache.spark.sql.errors.QueryCompilationErrors$.doubleNamedArgumentReference(QueryCompilationErrors.scala:90)\n\t\tat org.apache.spark.sql.catalyst.plans.logical.NamedParametersSupport$.$anonfun$splitAndCheckNamedArguments$2(FunctionBuilderBase.scala:108)\n\t\tat scala.collection.immutable.Vector1.map(Vector.scala:2155)\n\t\tat scala.collection.immutable.Vector1.map(Vector.scala:386)\n\t\tat org.apache.spark.sql.catalyst.plans.logical.NamedParametersSupport$.splitAndCheckNamedArguments(FunctionBuilderBase.scala:104)\n\t\tat org.apache.spark.sql.execution.python.UserDefinedPythonFunction.builder(UserDefinedPythonFunction.scala:62)\n\t\tat org.apache.spark.sql.classic.UDFRegistration.$anonfun$registerPython$1(UDFRegistration.scala:64)\n\t\tat org.apache.spark.sql.catalyst.analysis.SimpleFunctionRegistryBase.lookupFunction(FunctionRegistry.scala:246)\n\t\tat org.apache.spark.sql.catalyst.analysis.SimpleFunctionRegistryBase.lookupFunction$(FunctionRegistry.scala:240)\n\t\tat org.apache.spark.sql.catalyst.analysis.SimpleFunctionRegistry.lookupFunction(FunctionRegistry.scala:305)\n\t\tat org.apache.spark.sql.catalyst.catalog.SessionCatalog.$anonfun$resolveBuiltinOrTempFunctionInternal$1(SessionCatalog.scala:2121)\n\t\tat org.apache.spark.sql.catalyst.catalog.SessionCatalog.lookupTempFuncWithViewContext(SessionCatalog.scala:2143)\n\t\tat org.apache.spark.sql.catalyst.catalog.SessionCatalog.resolveBuiltinOrTempFunctionInternal(SessionCatalog.scala:2121)\n\t\tat org.apache.spark.sql.catalyst.catalog.SessionCatalog.resolveBuiltinOrTempFunction(SessionCatalog.scala:2098)\n\t\tat org.apache.spark.sql.catalyst.analysis.FunctionResolution.resolveBuiltinOrTempFunction(FunctionResolution.scala:102)\n\t\tat org.apache.spark.sql.catalyst.analysis.FunctionResolution.$anonfun$resolveFunction$1(FunctionResolution.scala:51)\n\t\tat org.apache.spark.sql.catalyst.analysis.package$.withPosition(package.scala:104)\n\t\tat org.apache.spark.sql.catalyst.analysis.FunctionResolution.resolveFunction(FunctionResolution.scala:51)\n\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveFunctions$$anonfun$apply$19$$anonfun$applyOrElse$141.applyOrElse(Analyzer.scala:2256)\n\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveFunctions$$anonfun$apply$19$$anonfun$applyOrElse$141.applyOrElse(Analyzer.scala:2226)\n\t\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformUpWithPruning$2(TreeNode.scala:549)\n\t\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:107)\n\t\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformUpWithPruning(TreeNode.scala:549)\n\t\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformUpWithPruning$1(TreeNode.scala:546)\n\t\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1268)\n\t\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1267)\n\t\tat org.apache.spark.sql.catalyst.expressions.UnaryExpression.mapChildren(Expression.scala:587)\n\t\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformUpWithPruning(TreeNode.scala:546)\n\t\tat org.apache.spark.sql.catalyst.plans.QueryPlan.$anonfun$transformExpressionsUpWithPruning$1(QueryPlan.scala:245)\n\t\tat org.apache.spark.sql.catalyst.plans.QueryPlan.$anonfun$mapExpressions$1(QueryPlan.scala:257)\n\t\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:107)\n\t\tat org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpression$1(QueryPlan.scala:257)\n\t\tat org.apache.spark.sql.catalyst.plans.QueryPlan.recursiveTransform$1(QueryPlan.scala:269)\n\t\tat org.apache.spark.sql.catalyst.plans.QueryPlan.$anonfun$mapExpressions$4(QueryPlan.scala:275)\n\t\tat scala.collection.immutable.List.map(List.scala:236)\n\t\tat scala.collection.immutable.List.map(List.scala:79)\n\t\tat org.apache.spark.sql.catalyst.plans.QueryPlan.recursiveTransform$1(QueryPlan.scala:275)\n\t\tat org.apache.spark.sql.catalyst.plans.QueryPlan.$anonfun$mapExpressions$5(QueryPlan.scala:280)\n\t\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:337)\n\t\tat org.apache.spark.sql.catalyst.plans.QueryPlan.mapExpressions(QueryPlan.scala:280)\n\t\tat org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpressionsUpWithPruning(QueryPlan.scala:245)\n\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveFunctions$$anonfun$apply$19.applyOrElse(Analyzer.scala:2226)\n\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveFunctions$$anonfun$apply$19.applyOrElse(Analyzer.scala:2096)\n\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$3(AnalysisHelper.scala:139)\n\t\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:107)\n\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:139)\n\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:416)\n\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:135)\n\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:131)\n\t\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:37)\n\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveFunctions$.apply(Analyzer.scala:2096)\n\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveFunctions$.apply(Analyzer.scala:2093)\n\t\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:248)\n\t\tat scala.collection.LinearSeqOps.foldLeft(LinearSeq.scala:183)\n\t\tat scala.collection.LinearSeqOps.foldLeft$(LinearSeq.scala:179)\n\t\tat scala.collection.immutable.List.foldLeft(List.scala:79)\n\t\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:245)\n\t\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:237)\n\t\tat scala.collection.immutable.List.foreach(List.scala:323)\n\t\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:237)\n\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext(Analyzer.scala:344)\n\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:340)\n\t\tat org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:224)\n\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:340)\n\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:290)\n\t\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:207)\n\t\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)\n\t\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:207)\n\t\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.resolveInFixedPoint(HybridAnalyzer.scala:237)\n\t\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.$anonfun$apply$1(HybridAnalyzer.scala:92)\n\t\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.withTrackedAnalyzerBridgeState(HybridAnalyzer.scala:123)\n\t\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.apply(HybridAnalyzer.scala:85)\n\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:323)\n\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:423)\n\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:323)\n\t\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$2(QueryExecution.scala:139)\n\t\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:148)\n\t\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:325)\n\t\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:712)\n\t\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:325)\n\t\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:810)\n\t\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:324)\n\t\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$1(QueryExecution.scala:139)\n\t\tat scala.util.Try$.apply(Try.scala:217)\n\t\tat org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1392)\n\t\tat org.apache.spark.util.LazyTry.tryT$lzycompute(LazyTry.scala:46)\n\t\tat org.apache.spark.util.LazyTry.tryT(LazyTry.scala:46)\n\t\t... 21 more\n", "stacktrace": [{"class": null, "method": "deco", "file": "/home/gaogaotiantian/programs/spark/python/pyspark/errors/exceptions/captured.py", "line": "263"}, {"class": null, "method": "get_return_value", "file": "/home/gaogaotiantian/programs/spark/python/lib/py4j-0.10.9.9-src.zip/py4j/protocol.py", "line": "327"}]}}
{"ts": "2026-01-02 12:53:40.087", "level": "ERROR", "logger": "SQLQueryContextLogger", "msg": "[UNEXPECTED_POSITIONAL_ARGUMENT] Cannot invoke routine `test_udf` because it contains positional argument(s) following the named argument assigned to `a`; please rearrange them so the positional arguments come first and then retry the query again. SQLSTATE: 4274K", "context": {"errorClass": "UNEXPECTED_POSITIONAL_ARGUMENT"}, "exception": {"class": "Py4JJavaError", "msg": "An error occurred while calling o85.sql.\n: org.apache.spark.sql.AnalysisException: [UNEXPECTED_POSITIONAL_ARGUMENT] Cannot invoke routine `test_udf` because it contains positional argument(s) following the named argument assigned to `a`; please rearrange them so the positional arguments come first and then retry the query again. SQLSTATE: 4274K; line 1 pos 7\n\tat org.apache.spark.sql.errors.QueryCompilationErrors$.unexpectedPositionalArgument(QueryCompilationErrors.scala:128)\n\tat org.apache.spark.sql.catalyst.plans.logical.NamedParametersSupport$.$anonfun$splitAndCheckNamedArguments$2(FunctionBuilderBase.scala:114)\n\tat scala.collection.immutable.Vector1.map(Vector.scala:2155)\n\tat scala.collection.immutable.Vector1.map(Vector.scala:386)\n\tat org.apache.spark.sql.catalyst.plans.logical.NamedParametersSupport$.splitAndCheckNamedArguments(FunctionBuilderBase.scala:104)\n\tat org.apache.spark.sql.execution.python.UserDefinedPythonFunction.builder(UserDefinedPythonFunction.scala:62)\n\tat org.apache.spark.sql.classic.UDFRegistration.$anonfun$registerPython$1(UDFRegistration.scala:64)\n\tat org.apache.spark.sql.catalyst.analysis.SimpleFunctionRegistryBase.lookupFunction(FunctionRegistry.scala:246)\n\tat org.apache.spark.sql.catalyst.analysis.SimpleFunctionRegistryBase.lookupFunction$(FunctionRegistry.scala:240)\n\tat org.apache.spark.sql.catalyst.analysis.SimpleFunctionRegistry.lookupFunction(FunctionRegistry.scala:305)\n\tat org.apache.spark.sql.catalyst.catalog.SessionCatalog.$anonfun$resolveBuiltinOrTempFunctionInternal$1(SessionCatalog.scala:2121)\n\tat org.apache.spark.sql.catalyst.catalog.SessionCatalog.lookupTempFuncWithViewContext(SessionCatalog.scala:2143)\n\tat org.apache.spark.sql.catalyst.catalog.SessionCatalog.resolveBuiltinOrTempFunctionInternal(SessionCatalog.scala:2121)\n\tat org.apache.spark.sql.catalyst.catalog.SessionCatalog.resolveBuiltinOrTempFunction(SessionCatalog.scala:2098)\n\tat org.apache.spark.sql.catalyst.analysis.FunctionResolution.resolveBuiltinOrTempFunction(FunctionResolution.scala:102)\n\tat org.apache.spark.sql.catalyst.analysis.FunctionResolution.$anonfun$resolveFunction$1(FunctionResolution.scala:51)\n\tat org.apache.spark.sql.catalyst.analysis.package$.withPosition(package.scala:104)\n\tat org.apache.spark.sql.catalyst.analysis.FunctionResolution.resolveFunction(FunctionResolution.scala:51)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveFunctions$$anonfun$apply$19$$anonfun$applyOrElse$141.applyOrElse(Analyzer.scala:2256)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveFunctions$$anonfun$apply$19$$anonfun$applyOrElse$141.applyOrElse(Analyzer.scala:2226)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformUpWithPruning$2(TreeNode.scala:549)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:107)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformUpWithPruning(TreeNode.scala:549)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformUpWithPruning$1(TreeNode.scala:546)\n\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1268)\n\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1267)\n\tat org.apache.spark.sql.catalyst.expressions.UnaryExpression.mapChildren(Expression.scala:587)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformUpWithPruning(TreeNode.scala:546)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.$anonfun$transformExpressionsUpWithPruning$1(QueryPlan.scala:245)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.$anonfun$mapExpressions$1(QueryPlan.scala:257)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:107)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpression$1(QueryPlan.scala:257)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.recursiveTransform$1(QueryPlan.scala:269)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.$anonfun$mapExpressions$4(QueryPlan.scala:275)\n\tat scala.collection.immutable.List.map(List.scala:236)\n\tat scala.collection.immutable.List.map(List.scala:79)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.recursiveTransform$1(QueryPlan.scala:275)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.$anonfun$mapExpressions$5(QueryPlan.scala:280)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:337)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.mapExpressions(QueryPlan.scala:280)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpressionsUpWithPruning(QueryPlan.scala:245)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveFunctions$$anonfun$apply$19.applyOrElse(Analyzer.scala:2226)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveFunctions$$anonfun$apply$19.applyOrElse(Analyzer.scala:2096)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$3(AnalysisHelper.scala:139)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:107)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:139)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:416)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:135)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:131)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:37)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveFunctions$.apply(Analyzer.scala:2096)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveFunctions$.apply(Analyzer.scala:2093)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:248)\n\tat scala.collection.LinearSeqOps.foldLeft(LinearSeq.scala:183)\n\tat scala.collection.LinearSeqOps.foldLeft$(LinearSeq.scala:179)\n\tat scala.collection.immutable.List.foldLeft(List.scala:79)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:245)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:237)\n\tat scala.collection.immutable.List.foreach(List.scala:323)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:237)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext(Analyzer.scala:344)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:340)\n\tat org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:224)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:340)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:290)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:207)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:207)\n\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.resolveInFixedPoint(HybridAnalyzer.scala:237)\n\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.$anonfun$apply$1(HybridAnalyzer.scala:92)\n\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.withTrackedAnalyzerBridgeState(HybridAnalyzer.scala:123)\n\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.apply(HybridAnalyzer.scala:85)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:323)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:423)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:323)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$2(QueryExecution.scala:139)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:148)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:325)\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:712)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:325)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:810)\n\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:324)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$1(QueryExecution.scala:139)\n\tat scala.util.Try$.apply(Try.scala:217)\n\tat org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1392)\n\tat org.apache.spark.util.Utils$.getTryWithCallerStacktrace(Utils.scala:1453)\n\tat org.apache.spark.util.LazyTry.get(LazyTry.scala:58)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:150)\n\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:90)\n\tat org.apache.spark.sql.classic.Dataset$.$anonfun$ofRows$5(Dataset.scala:138)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:810)\n\tat org.apache.spark.sql.classic.Dataset$.ofRows(Dataset.scala:135)\n\tat org.apache.spark.sql.classic.SparkSession.$anonfun$sql$1(SparkSession.scala:534)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:810)\n\tat org.apache.spark.sql.classic.SparkSession.sql(SparkSession.scala:504)\n\tat org.apache.spark.sql.classic.SparkSession.sql(SparkSession.scala:539)\n\tat jdk.internal.reflect.GeneratedMethodAccessor77.invoke(Unknown Source)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:52)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:580)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:184)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:108)\n\tat java.base/java.lang.Thread.run(Thread.java:1583)\n\tSuppressed: org.apache.spark.util.Utils$OriginalTryStackTraceException: Full stacktrace of original doTryWithCallerStacktrace caller\n\t\tat org.apache.spark.sql.errors.QueryCompilationErrors$.unexpectedPositionalArgument(QueryCompilationErrors.scala:128)\n\t\tat org.apache.spark.sql.catalyst.plans.logical.NamedParametersSupport$.$anonfun$splitAndCheckNamedArguments$2(FunctionBuilderBase.scala:114)\n\t\tat scala.collection.immutable.Vector1.map(Vector.scala:2155)\n\t\tat scala.collection.immutable.Vector1.map(Vector.scala:386)\n\t\tat org.apache.spark.sql.catalyst.plans.logical.NamedParametersSupport$.splitAndCheckNamedArguments(FunctionBuilderBase.scala:104)\n\t\tat org.apache.spark.sql.execution.python.UserDefinedPythonFunction.builder(UserDefinedPythonFunction.scala:62)\n\t\tat org.apache.spark.sql.classic.UDFRegistration.$anonfun$registerPython$1(UDFRegistration.scala:64)\n\t\tat org.apache.spark.sql.catalyst.analysis.SimpleFunctionRegistryBase.lookupFunction(FunctionRegistry.scala:246)\n\t\tat org.apache.spark.sql.catalyst.analysis.SimpleFunctionRegistryBase.lookupFunction$(FunctionRegistry.scala:240)\n\t\tat org.apache.spark.sql.catalyst.analysis.SimpleFunctionRegistry.lookupFunction(FunctionRegistry.scala:305)\n\t\tat org.apache.spark.sql.catalyst.catalog.SessionCatalog.$anonfun$resolveBuiltinOrTempFunctionInternal$1(SessionCatalog.scala:2121)\n\t\tat org.apache.spark.sql.catalyst.catalog.SessionCatalog.lookupTempFuncWithViewContext(SessionCatalog.scala:2143)\n\t\tat org.apache.spark.sql.catalyst.catalog.SessionCatalog.resolveBuiltinOrTempFunctionInternal(SessionCatalog.scala:2121)\n\t\tat org.apache.spark.sql.catalyst.catalog.SessionCatalog.resolveBuiltinOrTempFunction(SessionCatalog.scala:2098)\n\t\tat org.apache.spark.sql.catalyst.analysis.FunctionResolution.resolveBuiltinOrTempFunction(FunctionResolution.scala:102)\n\t\tat org.apache.spark.sql.catalyst.analysis.FunctionResolution.$anonfun$resolveFunction$1(FunctionResolution.scala:51)\n\t\tat org.apache.spark.sql.catalyst.analysis.package$.withPosition(package.scala:104)\n\t\tat org.apache.spark.sql.catalyst.analysis.FunctionResolution.resolveFunction(FunctionResolution.scala:51)\n\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveFunctions$$anonfun$apply$19$$anonfun$applyOrElse$141.applyOrElse(Analyzer.scala:2256)\n\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveFunctions$$anonfun$apply$19$$anonfun$applyOrElse$141.applyOrElse(Analyzer.scala:2226)\n\t\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformUpWithPruning$2(TreeNode.scala:549)\n\t\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:107)\n\t\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformUpWithPruning(TreeNode.scala:549)\n\t\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformUpWithPruning$1(TreeNode.scala:546)\n\t\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1268)\n\t\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1267)\n\t\tat org.apache.spark.sql.catalyst.expressions.UnaryExpression.mapChildren(Expression.scala:587)\n\t\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformUpWithPruning(TreeNode.scala:546)\n\t\tat org.apache.spark.sql.catalyst.plans.QueryPlan.$anonfun$transformExpressionsUpWithPruning$1(QueryPlan.scala:245)\n\t\tat org.apache.spark.sql.catalyst.plans.QueryPlan.$anonfun$mapExpressions$1(QueryPlan.scala:257)\n\t\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:107)\n\t\tat org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpression$1(QueryPlan.scala:257)\n\t\tat org.apache.spark.sql.catalyst.plans.QueryPlan.recursiveTransform$1(QueryPlan.scala:269)\n\t\tat org.apache.spark.sql.catalyst.plans.QueryPlan.$anonfun$mapExpressions$4(QueryPlan.scala:275)\n\t\tat scala.collection.immutable.List.map(List.scala:236)\n\t\tat scala.collection.immutable.List.map(List.scala:79)\n\t\tat org.apache.spark.sql.catalyst.plans.QueryPlan.recursiveTransform$1(QueryPlan.scala:275)\n\t\tat org.apache.spark.sql.catalyst.plans.QueryPlan.$anonfun$mapExpressions$5(QueryPlan.scala:280)\n\t\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:337)\n\t\tat org.apache.spark.sql.catalyst.plans.QueryPlan.mapExpressions(QueryPlan.scala:280)\n\t\tat org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpressionsUpWithPruning(QueryPlan.scala:245)\n\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveFunctions$$anonfun$apply$19.applyOrElse(Analyzer.scala:2226)\n\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveFunctions$$anonfun$apply$19.applyOrElse(Analyzer.scala:2096)\n\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$3(AnalysisHelper.scala:139)\n\t\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:107)\n\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:139)\n\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:416)\n\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:135)\n\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:131)\n\t\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:37)\n\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveFunctions$.apply(Analyzer.scala:2096)\n\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveFunctions$.apply(Analyzer.scala:2093)\n\t\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:248)\n\t\tat scala.collection.LinearSeqOps.foldLeft(LinearSeq.scala:183)\n\t\tat scala.collection.LinearSeqOps.foldLeft$(LinearSeq.scala:179)\n\t\tat scala.collection.immutable.List.foldLeft(List.scala:79)\n\t\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:245)\n\t\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:237)\n\t\tat scala.collection.immutable.List.foreach(List.scala:323)\n\t\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:237)\n\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext(Analyzer.scala:344)\n\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:340)\n\t\tat org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:224)\n\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:340)\n\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:290)\n\t\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:207)\n\t\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)\n\t\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:207)\n\t\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.resolveInFixedPoint(HybridAnalyzer.scala:237)\n\t\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.$anonfun$apply$1(HybridAnalyzer.scala:92)\n\t\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.withTrackedAnalyzerBridgeState(HybridAnalyzer.scala:123)\n\t\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.apply(HybridAnalyzer.scala:85)\n\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:323)\n\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:423)\n\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:323)\n\t\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$2(QueryExecution.scala:139)\n\t\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:148)\n\t\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:325)\n\t\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:712)\n\t\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:325)\n\t\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:810)\n\t\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:324)\n\t\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$1(QueryExecution.scala:139)\n\t\tat scala.util.Try$.apply(Try.scala:217)\n\t\tat org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1392)\n\t\tat org.apache.spark.util.LazyTry.tryT$lzycompute(LazyTry.scala:46)\n\t\tat org.apache.spark.util.LazyTry.tryT(LazyTry.scala:46)\n\t\t... 21 more\n", "stacktrace": [{"class": null, "method": "deco", "file": "/home/gaogaotiantian/programs/spark/python/pyspark/errors/exceptions/captured.py", "line": "263"}, {"class": null, "method": "get_return_value", "file": "/home/gaogaotiantian/programs/spark/python/lib/py4j-0.10.9.9-src.zip/py4j/protocol.py", "line": "327"}]}}
</system-err>
	</testcase>
	<testcase classname="pyspark.sql.tests.test_udf.UDFTests" name="test_nested_array" time="0.213" timestamp="2026-01-02T12:53:40" file="python/pyspark/sql/tests/test_udf.py" line="1053"/>
	<testcase classname="pyspark.sql.tests.test_udf.UDFTests" name="test_nested_map" time="0.195" timestamp="2026-01-02T12:53:40" file="python/pyspark/sql/tests/test_udf.py" line="1038"/>
	<testcase classname="pyspark.sql.tests.test_udf.UDFTests" name="test_nested_struct" time="0.209" timestamp="2026-01-02T12:53:41" file="python/pyspark/sql/tests/test_udf.py" line="1023"/>
	<testcase classname="pyspark.sql.tests.test_udf.UDFTests" name="test_non_existed_udaf" time="0.018" timestamp="2026-01-02T12:53:41" file="python/pyspark/sql/tests/test_udf.py" line="626"/>
	<testcase classname="pyspark.sql.tests.test_udf.UDFTests" name="test_non_existed_udf" time="0.006" timestamp="2026-01-02T12:53:41" file="python/pyspark/sql/tests/test_udf.py" line="607"/>
	<testcase classname="pyspark.sql.tests.test_udf.UDFTests" name="test_non_existed_udf_with_sql_context" time="0.007" timestamp="2026-01-02T12:53:41" file="python/pyspark/sql/tests/test_udf.py" line="615">
		<system-err>/home/gaogaotiantian/programs/spark/python/pyspark/sql/context.py:157: FutureWarning: Deprecated in 3.0.0. Use SparkSession.builder.getOrCreate() instead.
  warnings.warn(
/home/gaogaotiantian/programs/spark/python/pyspark/sql/context.py:308: FutureWarning: Deprecated in 2.3.0. Use spark.udf.registerJavaFunction instead.
  warnings.warn(
</system-err>
	</testcase>
	<testcase classname="pyspark.sql.tests.test_udf.UDFTests" name="test_nondeterministic_udf" time="0.183" timestamp="2026-01-02T12:53:41" file="python/pyspark/sql/tests/test_udf.py" line="146"/>
	<testcase classname="pyspark.sql.tests.test_udf.UDFTests" name="test_nondeterministic_udf2" time="-0.938" timestamp="2026-01-02T12:53:40" file="python/pyspark/sql/tests/test_udf.py" line="157"/>
	<testcase classname="pyspark.sql.tests.test_udf.UDFTests" name="test_nondeterministic_udf3" time="0.015" timestamp="2026-01-02T12:53:40" file="python/pyspark/sql/tests/test_udf.py" line="177"/>
	<testcase classname="pyspark.sql.tests.test_udf.UDFTests" name="test_nondeterministic_udf_in_aggregate" time="0.129" timestamp="2026-01-02T12:53:40" file="python/pyspark/sql/tests/test_udf.py" line="189"/>
	<testcase classname="pyspark.sql.tests.test_udf.UDFTests" name="test_nonparam_udf_with_aggregate" time="0.299" timestamp="2026-01-02T12:53:40" file="python/pyspark/sql/tests/test_udf.py" line="825"/>
	<testcase classname="pyspark.sql.tests.test_udf.UDFTests" name="test_num_arguments" time="0.279" timestamp="2026-01-02T12:53:41" file="python/pyspark/sql/tests/test_udf.py" line="1197"/>
	<testcase classname="pyspark.sql.tests.test_udf.UDFTests" name="test_python_udf_segfault" time="1.682" timestamp="2026-01-02T12:53:42" file="python/pyspark/sql/tests/test_udf.py" line="1232"/>
	<testcase classname="pyspark.sql.tests.test_udf.UDFTests" name="test_raise_stop_iteration" time="0.221" timestamp="2026-01-02T12:53:42" file="python/pyspark/sql/tests/test_udf.py" line="1217"/>
	<testcase classname="pyspark.sql.tests.test_udf.UDFTests" name="test_same_accumulator_in_udfs" time="0.114" timestamp="2026-01-02T12:53:43" file="python/pyspark/sql/tests/test_udf.py" line="899"/>
	<testcase classname="pyspark.sql.tests.test_udf.UDFTests" name="test_single_udf_with_repeated_argument" time="0.083" timestamp="2026-01-02T12:53:43" file="python/pyspark/sql/tests/test_udf.py" line="215"/>
	<testcase classname="pyspark.sql.tests.test_udf.UDFTests" name="test_timeout_util_with_udf" time="1.019" timestamp="2026-01-02T12:53:44" file="python/pyspark/sql/tests/test_udf.py" line="1293">
		<system-err>ERROR:root:Exception while sending command.
Traceback (most recent call last):
  File "/home/gaogaotiantian/programs/spark/python/lib/py4j-0.10.9.9-src.zip/py4j/clientserver.py", line 535, in send_command
    answer = smart_decode(self.stream.readline()[:-1])
                          ^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.12/socket.py", line 707, in readinto
    return self._sock.recv_into(b)
           ^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/gaogaotiantian/programs/spark/python/pyspark/testing/utils.py", line 166, in handler
    raise TimeoutError(f"Function {func.__name__} timed out after {timeout} seconds")
TimeoutError: Function timeout_func timed out after 1 seconds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/gaogaotiantian/programs/spark/python/lib/py4j-0.10.9.9-src.zip/py4j/java_gateway.py", line 1038, in send_command
    response = connection.send_command(command)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/gaogaotiantian/programs/spark/python/lib/py4j-0.10.9.9-src.zip/py4j/clientserver.py", line 566, in send_command
    raise Py4JNetworkError(
py4j.protocol.Py4JNetworkError: Error while sending or receiving
</system-err>
	</testcase>
	<testcase classname="pyspark.sql.tests.test_udf.UDFTests" name="test_udf" time="0.080" timestamp="2026-01-02T12:53:44" file="python/pyspark/sql/tests/test_udf.py" line="88"/>
	<testcase classname="pyspark.sql.tests.test_udf.UDFTests" name="test_udf2" time="0.199" timestamp="2026-01-02T12:53:44" file="python/pyspark/sql/tests/test_udf.py" line="104"/>
	<testcase classname="pyspark.sql.tests.test_udf.UDFTests" name="test_udf3" time="0.075" timestamp="2026-01-02T12:53:44" file="python/pyspark/sql/tests/test_udf.py" line="111"/>
	<testcase classname="pyspark.sql.tests.test_udf.UDFTests" name="test_udf_and_common_filter_in_join_condition" time="0.400" timestamp="2026-01-02T12:53:44" file="python/pyspark/sql/tests/test_udf.py" line="281"/>
	<testcase classname="pyspark.sql.tests.test_udf.UDFTests" name="test_udf_as_join_condition" time="0.274" timestamp="2026-01-02T12:53:45" file="python/pyspark/sql/tests/test_udf.py" line="311"/>
	<testcase classname="pyspark.sql.tests.test_udf.UDFTests" name="test_udf_binary_type" time="0.514" timestamp="2026-01-02T12:53:45" file="python/pyspark/sql/tests/test_udf.py" line="1479"/>
	<testcase classname="pyspark.sql.tests.test_udf.UDFTests" name="test_udf_binary_type_in_nested_structures" time="1.506" timestamp="2026-01-02T12:53:47" file="python/pyspark/sql/tests/test_udf.py" line="1496">
		<!--Test that binary type in arrays, maps, and structs respects binaryAsBytes config-->
	</testcase>
	<testcase classname="pyspark.sql.tests.test_udf.UDFTests" name="test_udf_cache" time="0.041" timestamp="2026-01-02T12:53:47" file="python/pyspark/sql/tests/test_udf.py" line="965"/>
	<testcase classname="pyspark.sql.tests.test_udf.UDFTests" name="test_udf_daytime_interval" time="0.326" timestamp="2026-01-02T12:53:47" file="python/pyspark/sql/tests/test_udf.py" line="808"/>
	<testcase classname="pyspark.sql.tests.test_udf.UDFTests" name="test_udf_defers_judf_initialization" time="0.007" timestamp="2026-01-02T12:53:47" file="python/pyspark/sql/tests/test_udf.py" line="666"/>
	<testcase classname="pyspark.sql.tests.test_udf.UDFTests" name="test_udf_empty_frame" time="0.135" timestamp="2026-01-02T12:53:47" file="python/pyspark/sql/tests/test_udf.py" line="1402"/>
	<testcase classname="pyspark.sql.tests.test_udf.UDFTests" name="test_udf_globals_not_overwritten" time="0.125" timestamp="2026-01-02T12:53:47" file="python/pyspark/sql/tests/test_udf.py" line="932"/>
	<testcase classname="pyspark.sql.tests.test_udf.UDFTests" name="test_udf_in_filter_on_top_of_join" time="0.683" timestamp="2026-01-02T12:53:48" file="python/pyspark/sql/tests/test_udf.py" line="245"/>
	<testcase classname="pyspark.sql.tests.test_udf.UDFTests" name="test_udf_in_filter_on_top_of_outer_join" time="0.352" timestamp="2026-01-02T12:53:48" file="python/pyspark/sql/tests/test_udf.py" line="238"/>
	<testcase classname="pyspark.sql.tests.test_udf.UDFTests" name="test_udf_in_generate" time="0.482" timestamp="2026-01-02T12:53:49" file="python/pyspark/sql/tests/test_udf.py" line="498"/>
	<testcase classname="pyspark.sql.tests.test_udf.UDFTests" name="test_udf_in_join_condition" time="0.660" timestamp="2026-01-02T12:53:50" file="python/pyspark/sql/tests/test_udf.py" line="253"/>
	<testcase classname="pyspark.sql.tests.test_udf.UDFTests" name="test_udf_in_left_outer_join_condition" time="0.253" timestamp="2026-01-02T12:53:50" file="python/pyspark/sql/tests/test_udf.py" line="268"/>
	<testcase classname="pyspark.sql.tests.test_udf.UDFTests" name="test_udf_in_subquery" time="0.209" timestamp="2026-01-02T12:53:50" file="python/pyspark/sql/tests/test_udf.py" line="923"/>
	<testcase classname="pyspark.sql.tests.test_udf.UDFTests" name="test_udf_input_serialization_valuecompare_disabled" time="0.153" timestamp="2026-01-02T12:53:50" file="python/pyspark/sql/tests/test_udf.py" line="984"/>
	<testcase classname="pyspark.sql.tests.test_udf.UDFTests" name="test_udf_kill_on_timeout" time="1.216" timestamp="2026-01-02T12:53:51" file="python/pyspark/sql/tests/test_udf.py" line="1242"/>
	<testcase classname="pyspark.sql.tests.test_udf.UDFTests" name="test_udf_not_supported_in_join_condition" time="0.329" timestamp="2026-01-02T12:53:52" file="python/pyspark/sql/tests/test_udf.py" line="291"/>
	<testcase classname="pyspark.sql.tests.test_udf.UDFTests" name="test_udf_on_sql_context" time="0.102" timestamp="2026-01-02T12:53:52" file="python/pyspark/sql/tests/test_udf.py" line="94">
		<system-err>/home/gaogaotiantian/programs/spark/python/pyspark/sql/context.py:294: FutureWarning: Deprecated in 2.3.0. Use spark.udf.register instead.
  warnings.warn("Deprecated in 2.3.0. Use spark.udf.register instead.", FutureWarning)
</system-err>
	</testcase>
	<testcase classname="pyspark.sql.tests.test_udf.UDFTests" name="test_udf_registration_return_type_none" time="0.110" timestamp="2026-01-02T12:53:52" file="python/pyspark/sql/tests/test_udf.py" line="120"/>
	<testcase classname="pyspark.sql.tests.test_udf.UDFTests" name="test_udf_registration_return_type_not_none" time="0.006" timestamp="2026-01-02T12:53:52" file="python/pyspark/sql/tests/test_udf.py" line="129"/>
	<testcase classname="pyspark.sql.tests.test_udf.UDFTests" name="test_udf_registration_returns_udf" time="0.526" timestamp="2026-01-02T12:53:52" file="python/pyspark/sql/tests/test_udf.py" line="532"/>
	<testcase classname="pyspark.sql.tests.test_udf.UDFTests" name="test_udf_registration_returns_udf_on_sql_context" time="0.235" timestamp="2026-01-02T12:53:53" file="python/pyspark/sql/tests/test_udf.py" line="549"/>
	<testcase classname="pyspark.sql.tests.test_udf.UDFTests" name="test_udf_should_not_accept_noncallable_object" time="0.001" timestamp="2026-01-02T12:53:53" file="python/pyspark/sql/tests/test_udf.py" line="699"/>
	<testcase classname="pyspark.sql.tests.test_udf.UDFTests" name="test_udf_timestamp_ntz" time="0.290" timestamp="2026-01-02T12:53:53" file="python/pyspark/sql/tests/test_udf.py" line="792"/>
	<testcase classname="pyspark.sql.tests.test_udf.UDFTests" name="test_udf_with_256_args" time="0.323" timestamp="2026-01-02T12:53:53" file="python/pyspark/sql/tests/test_udf.py" line="952"/>
	<testcase classname="pyspark.sql.tests.test_udf.UDFTests" name="test_udf_with_aggregate_function" time="0.517" timestamp="2026-01-02T12:53:54" file="python/pyspark/sql/tests/test_udf.py" line="480"/>
	<testcase classname="pyspark.sql.tests.test_udf.UDFTests" name="test_udf_with_array_type" time="0.130" timestamp="2026-01-02T12:53:54" file="python/pyspark/sql/tests/test_udf.py" line="325"/>
	<testcase classname="pyspark.sql.tests.test_udf.UDFTests" name="test_udf_with_callable" time="0.136" timestamp="2026-01-02T12:53:54" file="python/pyspark/sql/tests/test_udf.py" line="63"/>
	<testcase classname="pyspark.sql.tests.test_udf.UDFTests" name="test_udf_with_char_varchar_return_type" time="0.139" timestamp="2026-01-02T12:53:54" file="python/pyspark/sql/tests/test_udf.py" line="1435"/>
	<testcase classname="pyspark.sql.tests.test_udf.UDFTests" name="test_udf_with_collated_string_types" time="0.428" timestamp="2026-01-02T12:53:55" file="python/pyspark/sql/tests/test_udf.py" line="1412"/>
	<testcase classname="pyspark.sql.tests.test_udf.UDFTests" name="test_udf_with_column_vector" time="0.415" timestamp="2026-01-02T12:53:55" file="python/pyspark/sql/tests/test_udf.py" line="997"/>
	<testcase classname="pyspark.sql.tests.test_udf.UDFTests" name="test_udf_with_complex_variant_input" time="0.264" timestamp="2026-01-02T12:53:55" file="python/pyspark/sql/tests/test_udf.py" line="365"/>
	<testcase classname="pyspark.sql.tests.test_udf.UDFTests" name="test_udf_with_complex_variant_output" time="0.272" timestamp="2026-01-02T12:53:56" file="python/pyspark/sql/tests/test_udf.py" line="400"/>
	<testcase classname="pyspark.sql.tests.test_udf.UDFTests" name="test_udf_with_decorator" time="0.190" timestamp="2026-01-02T12:53:56" file="python/pyspark/sql/tests/test_udf.py" line="703"/>
	<testcase classname="pyspark.sql.tests.test_udf.UDFTests" name="test_udf_with_filter_function" time="0.104" timestamp="2026-01-02T12:53:56" file="python/pyspark/sql/tests/test_udf.py" line="351"/>
	<testcase classname="pyspark.sql.tests.test_udf.UDFTests" name="test_udf_with_input_file_name" time="0.169" timestamp="2026-01-02T12:53:56" file="python/pyspark/sql/tests/test_udf.py" line="634"/>
	<testcase classname="pyspark.sql.tests.test_udf.UDFTests" name="test_udf_with_input_file_name_for_hadooprdd" time="0.425" timestamp="2026-01-02T12:53:57" file="python/pyspark/sql/tests/test_udf.py" line="642"/>
	<testcase classname="pyspark.sql.tests.test_udf.UDFTests" name="test_udf_with_logging" time="0.431" timestamp="2026-01-02T12:53:57" file="python/pyspark/sql/tests/test_udf.py" line="1562"/>
	<testcase classname="pyspark.sql.tests.test_udf.UDFTests" name="test_udf_with_order_by_and_limit" time="0.056" timestamp="2026-01-02T12:53:57" file="python/pyspark/sql/tests/test_udf.py" line="526"/>
	<testcase classname="pyspark.sql.tests.test_udf.UDFTests" name="test_udf_with_partial_function" time="0.134" timestamp="2026-01-02T12:53:57" file="python/pyspark/sql/tests/test_udf.py" line="76"/>
	<testcase classname="pyspark.sql.tests.test_udf.UDFTests" name="test_udf_with_pyspark_logger" time="0.110" timestamp="2026-01-02T12:53:57" file="python/pyspark/sql/tests/test_udf.py" line="1677"/>
	<testcase classname="pyspark.sql.tests.test_udf.UDFTests" name="test_udf_with_rand" time="0.090" timestamp="2026-01-02T12:53:57" file="python/pyspark/sql/tests/test_udf.py" line="1017"/>
	<testcase classname="pyspark.sql.tests.test_udf.UDFTests" name="test_udf_with_string_return_type" time="0.100" timestamp="2026-01-02T12:53:57" file="python/pyspark/sql/tests/test_udf.py" line="682"/>
	<testcase classname="pyspark.sql.tests.test_udf.UDFTests" name="test_udf_with_udt" time="1.386" timestamp="2026-01-02T12:53:59" file="python/pyspark/sql/tests/test_udf.py" line="1308"/>
	<testcase classname="pyspark.sql.tests.test_udf.UDFTests" name="test_udf_with_variant_input" time="0.097" timestamp="2026-01-02T12:53:59" file="python/pyspark/sql/tests/test_udf.py" line="358"/>
	<testcase classname="pyspark.sql.tests.test_udf.UDFTests" name="test_udf_with_variant_output" time="0.090" timestamp="2026-01-02T12:53:59" file="python/pyspark/sql/tests/test_udf.py" line="389"/>
	<testcase classname="pyspark.sql.tests.test_udf.UDFTests" name="test_udf_with_various_batch_size" time="3.080" timestamp="2026-01-02T12:54:02" file="python/pyspark/sql/tests/test_udf.py" line="1716"/>
	<testcase classname="pyspark.sql.tests.test_udf.UDFTests" name="test_udf_with_various_buffer_size" time="2.364" timestamp="2026-01-02T12:54:04" file="python/pyspark/sql/tests/test_udf.py" line="1734"/>
	<testcase classname="pyspark.sql.tests.test_udf.UDFTests" name="test_udf_without_arguments" time="0.077" timestamp="2026-01-02T12:54:05" file="python/pyspark/sql/tests/test_udf.py" line="319"/>
	<testcase classname="pyspark.sql.tests.test_udf.UDFTests" name="test_udf_wrapper" time="0.002" timestamp="2026-01-02T12:54:05" file="python/pyspark/sql/tests/test_udf.py" line="758"/>
	<testcase classname="pyspark.sql.tests.test_udf.UDFTests" name="test_worker_original_stdin_closed" time="0.080" timestamp="2026-01-02T12:54:05" file="python/pyspark/sql/tests/test_udf.py" line="939"/>
	<testcase classname="pyspark.sql.tests.test_udf.UDFTests" name="test_datasource_with_udf" time="0.000" timestamp="2026-01-02T12:53:35" file="python/pyspark/sql/tests/test_udf.py" line="834">
		<skipped type="skip" message="[TEST_CLASS_NOT_COMPILED] /home/gaogaotiantian/programs/spark/sql/core/target/*/test-classes doesn't exist. Spark sql test classes are not compiled."/>
	</testcase>
	<testcase classname="pyspark.sql.tests.test_udf.UDFTests" name="test_register_java_function" time="0.001" timestamp="2026-01-02T12:53:42" file="python/pyspark/sql/tests/test_udf.py" line="564">
		<skipped type="skip" message="[TEST_CLASS_NOT_COMPILED] /home/gaogaotiantian/programs/spark/sql/core/target/*/test-classes doesn't exist. Spark sql test classes are not compiled."/>
	</testcase>
	<testcase classname="pyspark.sql.tests.test_udf.UDFTests" name="test_register_java_udaf" time="0.000" timestamp="2026-01-02T12:53:42" file="python/pyspark/sql/tests/test_udf.py" line="585">
		<skipped type="skip" message="[TEST_CLASS_NOT_COMPILED] /home/gaogaotiantian/programs/spark/sql/core/target/*/test-classes doesn't exist. Spark sql test classes are not compiled."/>
	</testcase>
</testsuite><testsuite name="pyspark.testing.sqlutils.ReusedSQLTestCase-20260102111617" tests="1" file="pyspark/testing/sqlutils.py" time="0.013" timestamp="2026-01-02T11:16:21" failures="0" errors="0" skipped="0">
	<testcase classname="pyspark.testing.sqlutils.ReusedSQLTestCase" name="test_assert_classic_mode" time="0.013" timestamp="2026-01-02T11:16:21" file="python/pyspark/testing/sqlutils.py" line="324"/>
</testsuite><testsuite name="pyspark.testing.sqlutils.ReusedSQLTestCase-20260102112122" tests="1" file="pyspark/testing/sqlutils.py" time="0.009" timestamp="2026-01-02T11:21:25" failures="0" errors="0" skipped="0">
	<testcase classname="pyspark.testing.sqlutils.ReusedSQLTestCase" name="test_assert_classic_mode" time="0.009" timestamp="2026-01-02T11:21:25" file="python/pyspark/testing/sqlutils.py" line="324"/>
</testsuite><testsuite name="pyspark.testing.sqlutils.ReusedSQLTestCase-20260102125329" tests="1" file="pyspark/testing/sqlutils.py" time="0.008" timestamp="2026-01-02T12:53:32" failures="0" errors="0" skipped="0">
	<testcase classname="pyspark.testing.sqlutils.ReusedSQLTestCase" name="test_assert_classic_mode" time="0.008" timestamp="2026-01-02T12:53:32" file="python/pyspark/testing/sqlutils.py" line="324"/>
</testsuite></testsuites>